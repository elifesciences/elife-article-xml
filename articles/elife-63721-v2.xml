<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63721</article-id><article-id pub-id-type="doi">10.7554/eLife.63721</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Multiple decisions about one object involve parallel sensory acquisition but time-multiplexed evidence incorporation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-211001"><name><surname>Kang</surname><given-names>Yul HR</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8846-5296</contrib-id><email>yul.hr.kang@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">§</xref></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-211002"><name><surname>Löffler</surname><given-names>Anne</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9086-1290</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-108880"><name><surname>Jeurissen</surname><given-names>Danique</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3835-5977</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-172513"><name><surname>Zylberberg</surname><given-names>Ariel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2572-4748</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-137212"><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2011-2790</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-10455"><name><surname>Shadlen</surname><given-names>Michael N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2002-2210</contrib-id><email>shadlen@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Zuckerman Mind Brain Behavior Institute, Department of Neuroscience, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Engineering, University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Kavli Institute for Brain Science, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Howard Hughes Medical Institute, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>Department of Brain and Cognitive Sciences, University of Rochester</institution><addr-line><named-content content-type="city">Rochester</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>§</label><p>Department of Engineering, University of Cambridge, Cambridge, United Kingdom</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>10</day><month>03</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e63721</elocation-id><history><date date-type="received" iso-8601-date="2020-10-04"><day>04</day><month>10</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-03-06"><day>06</day><month>03</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Kang et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Kang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63721-v2.pdf"/><abstract><p>The brain is capable of processing several streams of information that bear on different aspects of the same problem. Here, we address the problem of making two decisions about one object, by studying difficult perceptual decisions about the color and motion of a dynamic random dot display. We find that the accuracy of one decision is unaffected by the difficulty of the other decision. However, the response times reveal that the two decisions do not form simultaneously. We show that both stimulus dimensions are acquired in parallel for the initial ∼0.1 s but are then incorporated serially in time-multiplexed bouts. Thus, there is a bottleneck that precludes updating more than one decision at a time, and a buffer that stores samples of evidence while access to the decision is blocked. We suggest that this bottleneck is responsible for the long timescales of many cognitive operations framed as decisions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>decision making</kwd><kwd>motion perception</kwd><kwd>reaction time</kwd><kwd>psychophysics</kwd><kwd>visual attention</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>T32EY01393</award-id><principal-award-recipient><name><surname>Kang</surname><given-names>Yul HR</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>414196</award-id><principal-award-recipient><name><surname>Jeurissen</surname><given-names>Danique</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000874</institution-id><institution>Brain and Behavior Research Foundation</institution></institution-wrap></funding-source><award-id>28476</award-id><principal-award-recipient><name><surname>Jeurissen</surname><given-names>Danique</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Shadlen</surname><given-names>Michael N</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01EY11378</award-id><principal-award-recipient><name><surname>Shadlen</surname><given-names>Michael N</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01NS113113</award-id><principal-award-recipient><name><surname>Shadlen</surname><given-names>Michael N</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>When making two decisions about one object, two streams of information can be acquired in parallel but must be incorporated into the two decisions serially, consistent with a central bottleneck.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Decisions are often informed by several aspects of a problem, each guided by different sources of information. In many instances, these aspects are combined to support a single judgment. For example, an observer might judge the distance of an animal by combining perspective cues, binocular disparity and motion parallax. In other instances, the aspects are distinct dimensions of the same object. For example, the animal’s distance and its identity as potential predator or prey. The former problem of cue combination (<xref ref-type="bibr" rid="bib40">Jacobs, 1999</xref>; <xref ref-type="bibr" rid="bib27">Ernst and Banks, 2002</xref>) is a topic of study in what has been termed the Bayesian Brain (<xref ref-type="bibr" rid="bib57">Knill and Pouget, 2004</xref>). The latter is the subject of this paper. It arises in a wide variety of problems whose solutions depend on identifying a set of conjunctions such as the ingredients of a favorite dish, or when one must make multiple judgments, or decisions, about the same stimulus.</p><p>The neuroscience of decision-making has focused largely on perceptual decisions, contrived to promote the integration of noisy evidence over time toward a categorical choice about one stimulus dimension. A well-studied example is a decision about the net direction of motion of randomly moving dots. In such binary decisions (e.g. left or right), behavioral and neural studies have shown that humans and monkeys accumulate noisy samples of evidence and commit to a choice when the accumulated evidence reaches a threshold (<xref ref-type="bibr" rid="bib87">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>; <xref ref-type="bibr" rid="bib34">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib97">Stine et al., 2020</xref>). The framework has been extended to more than two categories (e.g. <xref ref-type="bibr" rid="bib18">Churchland et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Bogacz et al., 2007</xref>; <xref ref-type="bibr" rid="bib24">Ditterich, 2010</xref>) but it remains focused on a common stream of evidence bearing on a single stimulus feature. Less is known about how multiple streams of evidence are accumulated for a multidimensional decision (<xref ref-type="bibr" rid="bib65">Lorteije et al., 2015</xref>). Given the parallel organization of the sensory systems, one might expect all available evidence to be integrated simultaneously. However, there are also reasons to suspect that two decisions cannot be made in parallel. This is based on a variety of experiments that expose a ‘psychological refractory period’ (PRP; <xref ref-type="bibr" rid="bib105">Welford, 1952</xref>). When participants are asked to make two decisions in a rapid succession, it appears that the second decision is delayed until the first decision is complete (<xref ref-type="bibr" rid="bib81">Pashler, 1994</xref>). Based on such observations, it has been argued that there is a structural bottleneck in the response selection step, such that only one response can be selected at a time (<xref ref-type="bibr" rid="bib94">Sigman and Dehaene, 2005</xref>).</p><p>Here, we develop a task in which the participant views one visual stimulus and makes two decisions about the same object. The stimulus comprises elements that give rise to two streams of evidence bearing on their motion and color, and the participant must decide on both aspects and report the combined category. The task was designed to allow participants to integrate both streams of evidence simultaneously from the same location in the visual field and to indicate both choices with just one response. We show that, even in this situation, the two streams of evidence are accumulated one at a time, and moreover, this seriality arises despite the parallel access of the visual system to both streams. We suggest that seriality is explained by a bottleneck between the parallel acquisition of evidence and its incorporation into separate decision processes. We elaborate a model of bounded evidence accumulation, used previously to explain both the speed and accuracy of motion (<xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>) and color decisions (<xref ref-type="bibr" rid="bib4">Bakkour et al., 2019</xref>), and show that these accumulations must occur in series. The results have implications for a variety of psychological observations concerning sequential vs. parallel operations, and they address the fundamental question of why mental processes take the time they do.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We studied variants of a perceptual task that required binary decisions about two properties of a dynamic random dot display. Human participants decided the dominant color and direction of motion in a small patch of dynamic random dots (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The stimulus is similar to one introduced by <xref ref-type="bibr" rid="bib68">Mante et al., 2013</xref>, who studied the problem of gating when making a decision about only a single dimension, either color or motion. On each video frame, each dot has a probability of being colored blue or yellow and it has another probability of being plotted either at a displacement <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> relative to a dot shown 40 ms earlier or, alternatively, at a random location in the display. We refer to the probability of a displacement as the coherence or strength and use its sign to designate the direction. We use an analogous signed probability for the color coherence or strength (see Materials and methods). In the main tasks, participants reported their answer by making an eye or hand movement to select one of four choice targets. We refer to this as a double-decision and refer to the two aspects as stimulus dimensions. We employed several variants of this basic task in our study.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Double decision task.</title><p>(<bold>A</bold>) Timeline of the behavioral task. Participants fixated a gray dot at the center of the screen. A dynamic random dot stimulus was displayed and the participant was asked to judge the overall motion direction and the dominant color (the arrow is for visualization purposes only and was not presented to the subject). They reported this double-decision by selecting one of four targets to indicate motion direction (left and right target for leftward and rightward motion, respectively) and color (top yellow vs. bottom blue targets). The response was deemed correct when both motion and color judgments were correct. Participants received auditory feedback as to whether they were correct and the correct target was also indicated by a white ring. Across the experiments the targets could be selected with an eye movement or a hand movement, either when the participant was ready to report (reaction time) or when the dot display was extinguished (experimenter-controlled duration). (<bold>B</bold>) Motion and color strengths were varied independently across trials, represented by a matrix of combinations of difficulty levels (here shown for the eye reaction-time experiment with 81 combinations; see Materials and methods for Exp. 1-eye). Insets illustrate typical motion and color for three of the conditions. For feedback only, choices on the weakest motion strength (0% coherence) were deemed correct randomly; same for the weakest color strength. For the combinations shown in purple, at least one stimulus dimension was at its strongest value (easiest). For some analyses, the data from these combinations are used to fit a model, which is evaluated by predicting the data from the remaining combinations (amber).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig1-v2.tif"/></fig><sec id="s2-1"><title>Roadmap of the experimental results</title><p>We first present the main finding using a free response paradigm, what we term <italic>double-decision reaction time</italic> (Experiment 1). It demonstrates no interference in choice accuracy—that is, the difficulty of the color decision does not affect the accuracy of motion decisions, and vice versa—but critically, the double-decision (2D) time is the sum of the two single-decision (1D) times. The analysis suggests that the motion and color decisions are not formed at the same time. This establishes the prediction that with brief stimulus presentations, successful color decisions ought to be attained at the expense of motion, and vice versa—that is, choice interference. We then test this prediction (Experiment 2) and fail to confirm it. We show that color and motion can be acquired in parallel but are unable to update the decision simultaneously. This confirms the response selection bottleneck predicted by Pashler (<xref ref-type="bibr" rid="bib29">Fagot and Pashler, 1992</xref>) and it implies the existence of buffers (<xref ref-type="bibr" rid="bib96">Sperling, 1960</xref>; <xref ref-type="bibr" rid="bib43">Kamienkowski and Sigman, 2008</xref>), where sensory information can be held before it updates a decision variable—the accumulated evidence for color or motion.</p><p>The combination of a buffer and serial updating leads to a revised prediction that interference in accuracy should occur over a narrow range of stimulus viewing duration, controlled by the experimenter. We confirm this prediction (Experiment 3), showing that there is no interference at short viewing times, but that there is a narrow regime of the stimulus duration in which accuracy on one dimension suffers because a limited amount of deliberation time needs to be shared with the other dimension, which reconciles conflicting observations of parallel and serial patterns of decision-making in the literature (e.g. <xref ref-type="bibr" rid="bib91">Schumacher et al., 2001</xref>; <xref ref-type="bibr" rid="bib99">Tombu and Jolicoeur, 2004</xref>). We then introduce a bimanual version of the task (Experiment 4) which affords direct reports of both the color and motion termination times. It confirms the assumption that the double-decision time is the sum of two sequential sampling processes, each with its own stopping time, and it shows that the color and motion decisions compete before the first decision terminates. This implies some form of time-multiplexed alternation. In the last experiment, we ask participants to judge whether the motion in a pair of patches is the same or different (Experiment 5) and find that this binary decision, based only on motion processing, also exhibits additive decision times. Finally, we introduce a conceptual model of the double-decision process that serves as a platform to connect the computational elements with known and unknown neural mechanisms.</p></sec><sec id="s2-2"><title>Experiment 1. Double-decision reaction time (eye and unimanual)</title><p>Participants were asked to judge both the net direction (left or right) and dominant color (yellow or blue) of a patch of dynamic random dots and to indicate both decisions with a single movement to one of four choice targets (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Different groups of participants performed the task by indicating their choices with an eye movement (Exp. 1-eye, N = 3) or a reach (Exp. 1-unimanual, N = 8; see Figure 5A). On each trial the strength and direction of motion as well as the strength and sign of color dominance were chosen independently, leading to 81 (9 × 9 in Exp. 1-eye) or 121 (11 × 11 in Exp. 1-unimanual) combinations. The single movement furnished two decisions and one response time (RT; n.b., We use the terms, response time and reaction time, interchangeably to respect usage in psychology and neurophysiology literatures). Participants were given feedback that the decision was correct if the motion and color were both correct (see Materials and methods). After initial training, each participant in Exp. 1-eye performed 4,624–10,969 trials over 11–17 sessions; each participant in Exp. 1-unimanual performed 2304 trials over two sessions.</p><p><xref ref-type="fig" rid="fig2">Figure 2A,B</xref> shows choices and mean RT as a function of stimulus strength for the eye and unimanual tasks, respectively. The graphs in the left column of each panel show the data plotted as a function of motion strength and direction. Each color on this graph corresponds to a different difficulty of the other dimension (i.e. the color decision). Similarly, the graphs in the right columns show the data plotted as a function of color strength and dominance; the strength of the uninformative dimension, motion, is shown by the purple/red shading. Unsurprisingly, the proportion of rightward choices increased as a function of the sign and strength of the motion coherence, and the proportion of blue choices increased as a function of the sign and strength of color coherence. The striking feature of these graphs is that sensitivity to variation in the stimulus along each dimension is unaffected by the difficulty along the uninformative dimension. This is evident from the superposition of the colored data points. It is also supported by a logistic regression analysis, which favored a choice model in which the sensitivity along one dimension is not influenced by the stimulus strength along the other dimension (ΔBIC = 23 and 22 for motion and color in the eye task, respectively; ΔBIC = 34 and 52 for the unimanual task; positive values are support for the regression model of <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> without the <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> term). It implies that the two stimulus dimensions do not interfere with each other. This is consistent with the well-established idea that color and motion are processed by parallel, independent channels (<xref ref-type="bibr" rid="bib64">Livingstone and Hubel, 1988</xref>; <xref ref-type="bibr" rid="bib86">Ramachandran and Gregory, 1978</xref>; <xref ref-type="bibr" rid="bib13">Carney et al., 1987</xref>; <xref ref-type="bibr" rid="bib14">Cavanagh et al., 1984</xref>). However, another possibility is that the two dimensions do not interfere because they are not processed simultaneously but serially.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Double-decisions exhibit additive response times but no interference in accuracy (Experiment 1).</title><p>Participants judged the dominant color and direction of dynamic random dots and indicated the double-decision by an eye movement (<bold>A</bold>; Exp. 1-eye) or reach (<bold>B</bold>; Exp. 1-unimanual) to one of four choice-targets. All graphs show the behavioral measure (proportion of choices, top row; mean RT, rows 2 and 3) as a function of either signed motion or color strength. Positive and negative color strength indicate blue- or yellow-dominance, respectively. Positive and negative motion strength indicate rightward or leftward, respectively. Colors of symbols and traces indicate the difficulty (unsigned coherence) of the other stimulus dimension (e.g., color, for the graphs with abscissae labeled 'Motion strength'). Symbols are combined data from three participants (Exp. 1-eye) and eight participants (Exp. 1-unimanual). Open symbols identify the conditions used to fit the serial (middle row) and parallel (bottom row) models. These are the conditions in which at least one of the two stimulus strengths was at its maximum (purple shading, <xref ref-type="fig" rid="fig1">Figure 1B</xref>). In the top row, fits of the serial and parallel models are shown by solid and dashed lines, respectively. The models comprise two bounded drift-diffusion processes, which explain the choices and decision times as a function of either color or motion. They differ only in the way they combine the decision times to explain the double-decision RT. For the serial model, the double-decision time is the sum of the color and motion decision times. For the parallel model, the double-decision time is the longer of the color and motion decisions (see Materials and methods). Smooth curves are the predictions based on the fits to the open symbols. Both models predict no interaction on choice (top row). The predictions of RT are superior for the serial model (middle row) compared to the parallel model (bottom row). Data are the same in the lower two rows. Stimulus strengths in A were not identical for the three participants and were normalized to a common ±1 scale before averaging, so the psychometric curves for eye and hand cannot be compared visually (see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> for comparison of parameters from the fits). For simplicity, only correct (and all 0% coherence trials) are shown in the RT graphs (see Materials and methods).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Statistical comparison of the drift diffusion model under serial vs. parallel rules (Experiments 1 and 4).</title><p>The analysis focuses on the data and predictions represented by the solid symbols and lines in <xref ref-type="fig" rid="fig2">Figure 2</xref>. <italic>Left</italic>, Difference in log likelihood of the predictions under parallel and serial rules for each participant and condition. Vertical gray shading centered at 0 encompasses ±2, corresponding to ‘decisive’ evidence (Bayes factor ≥ 100) in favor of serial (left extending bars) or parallel (right extending bars). <italic>Middle and right</italic>, Validation of the method. After fitting, these parameters were used to generate simulated data under the serial (left) and parallel (right) rules. Each dataset was then fit using both the serial and parallel rule. The validation shows that all simulated datasets were correctly categorized. Average <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo>±</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are −66 ± 26, –79 ± 14, and 42 ± 7 for the data, simulated serial, and simulated parallel, respectively. The magnitude of a wavy bars (e.g. eye, S2) exceeds the axis limit.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Comparison of parallel and serial rules applied to RT distributions (Experiments 1 and 4).</title><p>Instead of fits to drift-diffusion models, we employ an empirical approach to explain the observed distributions of double-decision RT by either additive or choose-max operations on unobserved distributions of color and motion decision times. The graphs show averages of the fitted cumulative distributions (thick colored traces) across participants. (<bold>A</bold>) Three participants who responded with an eye movement to one of four targets. (<bold>B</bold>) Eight participants who responded with a hand movement to one of four targets. (<bold>C</bold>) The same eight participants who responded with two hands (the RT is the time of the last movement). The averages are taken at each time bin across participants for each condition, weighted by the number of trials. Only the conditions with the weakest and strongest stimulus strengths are shown. The comparison provides decisive support for the serial combination rule (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Statistical comparison of parallel and serial rules applied to reaction time distributions (Experiments 1, 4, and 5).</title><p>The empirical analysis focuses on the full set of RT distributions, exemplified in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>. The results are presented in the same format as <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. Average <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo>±</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are −17 ± 3,–21 ± 2, 7 ± 2 for the data, simulated serial, and simulated parallel, respectively. For the binary-choice task (Experiment 5; pink), the simplified version of the RT model was used and 20 simulations were performed for each participant under the serial and parallel rule, respectively (bars represent the mean across the 20 simulations). For the remaining data, the full RT model was used and only a single serial/parallel simulation was performed for each participant. The magnitude of a wavy bars (e.g. bimanual, S10) exceeds the axis limit.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Mean reaction time for parallel and serial rules applied to the empirical analysis of reaction time distributions exemplified in Figure 2—figure supplement 2 (Experiments 1 and 4).</title><p>The graphs display the mean RTs and fits in the same format as <xref ref-type="fig" rid="fig2">Figure 2</xref>, with responses reported by eye (<bold>A</bold>), unimanually (<bold>B</bold>), or bimanually (<bold>C</bold>). Mean RTs are computed from the average RT distribution computed as in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, and plotted with the same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Note that these averages across time bins and across participants are used for visualization only; fits were performed for individual participants using the full RT distribution. Here, the fits are derived from the best fitting gamma distributions, described in association with <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>. Open symbols are the data; the traces are line segments connecting the fitted means. In each panel of four graphs, the upper and lower pair of graphs show fits to the serial and parallel models, respectively. Panels display data from the double-decision RT tasks using the three response modalities as indicated.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Application of the fit-prediction strategy in <xref ref-type="fig" rid="fig2">Figure 2</xref> using only reaction time distributions.</title><p>The analysis is similar to the model comparison in <xref ref-type="fig" rid="fig2">Figure 2</xref>, where fits to the conditions in which at least one dimension was at its greatest strength (open symbols) are used to predict the mean double-decision RT on the remaining conditions (filled symbols), under serial and parallel rules. Instead of using a drift diffusion model to obtain the fitted 1D distributions, here we use the empirical method exemplified in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>. Otherwise, same conventions as <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Sensitivity of color and motion choices on single-decision and double-decision tasks (Exp. 1-eye).</title><p>The proportion of rightward choices (<italic>top row</italic>) and blue choices (<italic>bottom row</italic>) are plotted as a function of stimulus strength. The data in green were obtained from blocks of trials in which the participant was instructed to answer only about one of the dimensions (motion or color) and to ignore the other dimension (color or motion). The displays and ranges of difficulty are the same as the ones used in the double-decision task. Data in orange were obtained from the double-decision task. The data are for eye-only as we did not include 1D blocks in the unimanual task. Sensitivity is the slope of a logistic fit to the data (see <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>). The comparison is unprincipled because one does not expect the sensitivities to be the same for 1D decisions and the corresponding element of a 2D decision. The difference in task demands differ (e.g. error rates approach <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> for difficult 1D and 2D conditions), which could motivate different speed–accuracy settings. Nonetheless, they are comparable here.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig2-figsupp6-v2.tif"/></fig></fig-group><p>The RTs support this serial hypothesis. The RTs, plotted as a function of either motion or color, are bell-shaped curves, such that longer RTs are associated with the most difficult stimulus strength and the fastest with the easiest. In contrast to the choice functions, the uninformative dimension—that is, with respect to the dimension of the abscissa—affects the scale of these RTs, giving rise to a stacked family of bell-shaped curves. The more difficult the other dimension, the longer the RT.</p><p>We attempted to explain the choice-RT data in <xref ref-type="fig" rid="fig2">Figure 2</xref> with models of bounded evidence integration (e.g. drift-diffusion; <xref ref-type="bibr" rid="bib87">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>). Such models provide excellent accounts of choice and RT on the motion-only and color-only versions of these tasks (<xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>; <xref ref-type="bibr" rid="bib4">Bakkour et al., 2019</xref>). To explain the double-decision data set, we pursued two variants of these models under the assumption that motion and color are processed in parallel or in series. The curves in <xref ref-type="fig" rid="fig2">Figure 2</xref> are a mixture of fits and predictions. To fit the data, we used all trials in which at least one of the dimensions was at its strongest level (open symbols <xref ref-type="fig" rid="fig2">Figure 2</xref>; 32 purple conditions in <xref ref-type="fig" rid="fig1">Figure 1B</xref> for the eye task and 40 conditions for the unimanual task). We used these fits to predict the data from the remaining conditions (filled symbols; 49 amber conditions in <xref ref-type="fig" rid="fig1">Figure 1B</xref> for the eye task; 81 for the unimanual task).</p><p>Both models are consistent with no interference in the choice functions. The models can be distinguished on the basis of the RT data. For an experiment with only a single dimension (e.g. motion), the RT is the sum of the amount of time that evidence is integrated to reach a terminating bound (the decision time, <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>, for motion and color choice, respectively) plus time delays that are not affected by task difficulty, such as sensory and motor delays, termed the non-decision time (<inline-formula><mml:math id="inf9"><mml:msub><mml:mi>T</mml:mi><mml:mi>nd</mml:mi></mml:msub></mml:math></inline-formula>). If the color and motion decisions are made in parallel, then the total decision time should be determined by the slower process (<inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>), whereas if the decisions are made serially, the total decision time would be determined by the sum of the two decision times (<inline-formula><mml:math id="inf11"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). In both cases, we expect both motion and color strengths to affect the RT. In the serial case, an increase in the difficulty of color, say, should augment the total RT by the same amount for all motion strengths, giving rise to stacked functions of the same shapes (solid curves, middle row, <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>). In the parallel case, an increase in the difficulty of color should augment the total RT by an amount that depends on the difficulty of motion (solid curves, bottom row <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>). The color dimension is likely to determine the total RT when motion is strong, but it has less control when the motion is weak. The logic should produce stacked bell-shaped functions that pinch together in the middle of the graph. The data are better explained by the serial predictions (e.g. large mismatches by the parallel model when both dimensions are weak). Formal model comparison provides strong support for the serial models overall (geometric mean of Bayes factor across participant and task combinations: <inline-formula><mml:math id="inf12"><mml:mrow><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mi>BF</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>66</mml:mn></mml:mrow></mml:math></inline-formula>) and for 9 of 11 participants individually (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>The systematic underestimate of RT by both models on the doubly difficult stimulus strengths arises for two reasons. First, the drift-diffusion model implements a restrictive assumption that the variance of the noisy momentary evidence is the same for all stimulus strengths. The variance of the momentary evidence is likely to be smaller near 0% (see Materials and methods). The overestimate of the variance in the model would lead to an underestimate of the RT in the middle of the graphs of RT vs. motion strength, and along the top of the graphs of RT vs. color strength. Second, the inclusion of all trials at 0% coherence tends to inflate the mean RT because just under half of the trials resemble errors in the sense that the choice is opposite the sign of the component of the drift rate that instantiates a direction or color bias (e.g. <inline-formula><mml:math id="inf13"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Importantly, we pursued a second approach to compare serial and parallel models which shows that the superiority of the serial model does not rest on the systematic underestimates of RT in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><p>In the second approach, we focus specifically on the decision times. It considers only the distribution of RTs and attempts to account for them under serial and parallel logic. Instead of fitting diffusion models, this <italic>empirical</italic> approach explains the observed double-decision RT distributions as either the serial or parallel combination of latent (i.e. unobservable) distributions of color and motion decision times, as well as the four <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>T</mml:mi><mml:mi>nd</mml:mi></mml:msub></mml:math></inline-formula> distributions (one for each choice). We estimate these latent distributions with gamma distributions. For the serial case, the predicted double-decision RT distributions are established by convolution of the latent single-dimension distributions and the distribution of <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>T</mml:mi><mml:mi>nd</mml:mi></mml:msub></mml:math></inline-formula>. For the parallel case, the latent distributions are combined using the max logic, and the result is convolved with the appropriate distribution of <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>T</mml:mi><mml:mi>nd</mml:mi></mml:msub></mml:math></inline-formula> (see Materials and methods). <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> shows fits to the double-decision RT distributions for the more informative conditions for the serial and parallel models. The model comparisons, based on all the data, yield ‘decisive’ support (<xref ref-type="bibr" rid="bib48">Kass and Raftery, 1995</xref>) for the serial processing of motion and color (geometric mean of Bayes factor for participant and task combinations <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mi>BF</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>17</mml:mn></mml:mrow></mml:math></inline-formula> with all but one out of 11 participants individually supporting the serial rule; <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). We also display the mean RTs derived from the fits in the same format as <xref ref-type="fig" rid="fig2">Figure 2</xref> (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> and <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>). Both approaches support the conclusion that the color-motion double-decisions are formed serially from two independent decision processes, each with its own termination rule. However, neither analysis discerns the nature of the serial processing (e.g. whether they alternate or one is prioritized). We will consider this issue later.</p></sec><sec id="s2-3"><title>Experiment 2. Brief stimulus presentation (eye)</title><p>The results from the double-decision RT experiment support sequential updating of two decision variables, which represent accumulated evidence for the motion and color choices. If this is true, it leads to a straightforward prediction. If the stimulus duration is not controlled by the decision maker but by the experimenter, and if it is brief, then the two stimulus dimensions would compete for the limited processing time, and we ought to observe choice-interference. We therefore conducted a second experiment with the participants from Exp. 1-eye (N = 3). In this experiment, we presented the same motion/color coherence combinations, but limited the duration of the stimulus viewing time to just 120 ms. We know from previous experiments with 1D tasks that performance continues to increase with stimulus duration up to at least one half second (<xref ref-type="bibr" rid="bib51">Kiani et al., 2008</xref>; <xref ref-type="bibr" rid="bib104">Waskom and Kiani, 2018</xref>). Thus, it is reasonable to assume that performance accuracy would suffer if it is not possible to make use of the full 120 ms of evidence for both motion and color. We predicted that sensitivity to both color and motion should be worse on the double-decision task than on color-only and motion-only versions of the identical task. Each participant performed a total of 7305–7741 trials (4052–4275 1D trials and 3240–3466 2D trials) over 12–19 days.</p><p>To our surprise, double-decisions were just as accurate as their 1D controls (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We also observed no change in the sensitivity to color across the range of motion difficulties, and vice versa (ΔBIC = 7.2 and 9.7 for motion and color choices, respectively, in support of no interaction; <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). This suggests that evidence for color and motion was acquired simultaneously, in parallel, and without interference. Further support for this conclusion is adduced from an analysis of the stimulus information used to make the decisions—what is known as psychophysical reverse correlation (<xref ref-type="bibr" rid="bib6">Beard and Ahumada, 1998</xref>; <xref ref-type="bibr" rid="bib79">Okazawa et al., 2018</xref>). <xref ref-type="fig" rid="fig3">Figure 3B</xref> displays the degree to which trial-by-trial variation in the noisy displays influences the choice (see Materials and methods). It shows that these stimulus fluctuations influenced choices almost identically in the double-decision task and 1D controls.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Parallel acquisition and serial incorporation of a brief color-motion pulse (Experiment 2).</title><p>Participants completed a short-duration variant of the double-decision task in which the stimulus was presented for only 120 ms. They also performed blocks in which they were asked to report only the color or only the motion direction (single decision in which they could ignore the irrelevant dimension). Data from double- and single-decision blocks are indicated by color. (<bold>A</bold>) Choices and RTs for single and double-decision blocks. <italic>Top-left</italic>, proportion of rightward choices as a function of motion strength. <italic>Top-right</italic>, proportion of blue choices as a function of color strength. The solid lines are logistic fits. They are nearly identical for single- and double-decisions. <italic>Bottom row</italic>, RT for the single- and double-decisions plotted as a function of motion strength (<italic>left</italic>) and color strength (<italic>right</italic>). For double-decisions, these are the same data plotted as a function of either the motion or color dimension. Data points show the average RT as a function of motion or color coherence, after grouping trials across participants and all strengths of the ‘other’ dimension (i.e. color, <italic>left</italic>; motion, <italic>right</italic>). Error bars indicate s.e.m. across trials. Although the stimulus was presented for only 120 ms, RTs were modulated by decision difficulty. Importantly, RTs were longer in the double-decision task than in the single-decision task. (<bold>B</bold>) Psychophysical reverse correlation analysis. <italic>Top</italic>, Time course of the motion information favoring rightward, extracted from the random-dot display on each trial, that gave rise to a left or right choice. Shading indicates s.e.m. <italic>Middle</italic>, Time course of the color information favoring blue, extracted from the random-dot display on each trial, that gave rise to a blue or yellow choice. Shading indicates s.e.m. The similarity of the green and orange curves indicates that participants were able to extract the same amount of information from the stimulus when making single- and double-decisions. <italic>Bottom</italic>, Impulse response of the filters used to extract the motion and color signals (see Materials and methods). They explain the long time course of the traces for the 120 ms duration pulse.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig3-v2.tif"/></fig><p>At first glance, the observation seems to be at odds with our interpretation of the double-decision RT experiment, which provided strong support for serial processing, primarily in the pattern of RTs. In Experiment 2, the entire stimulus stream lasts only 120 ms, which is less than a typical saccadic latency to a bright spot. Nevertheless, participants exhibited variation in the time of their responses as a function of stimulus strength (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom panels) and these RTs were surprisingly long. The fastest were ∼300 ms longer than the stimulus (RT&gt;400 ms). Importantly, they are approximately 100–200 ms longer in the double-decisions than in single decisions. It is difficult to make too much of this observation, because the participants might have procrastinated for reasons unrelated to the dynamics of the decision process. However, procrastination would not explain the difference between the two conditions. As parallel acquisition of the 120 ms color and motion take the same amount of time as acquisition of either of the streams alone (by definition), the extra time in the double-decision is probably explained by serial incorporation of evidence into the two decisions. This observation also implies the existence of buffers that store the information from one stream as it awaits incorporation into the decision.</p><p>Our results so far suggest that color and motion information are acquired in parallel but are incorporated into the decision in series. We therefore wondered if the same schema might apply to the double-decision RT task. For this to hold, some kind of alternation must occur such that segments of one or the other stimulus stream is not incorporated into its decision. Suppose, for example, that at <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula> ms, motion information had been incorporated into decision variable, <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>, and color information had been stored in a buffer. Suppose further that motion continues to update the decision variable, <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>, until it reaches a termination bound at <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and only then can the buffered color information be incorporated into decision variable, <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>. From then on color information could update <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> until this decision terminates. In this imagined scenario, the color information between 0.12 s and <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> is not incorporated in the decision.</p><p>One might also imagine two alternatives to the latter part of this scenario. In both, the information from color continues to update the buffer (but not <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>) throughout the motion decision without loss. Then at <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> either (i) all the information about color is incorporated immediately into <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> or (ii) the buffered information is incorporated in <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> over time (e.g. as if the recorded color information is played back). The first alternative is equivalent to the parallel model that is inconsistent with the data. The second alternative, implausible as it may seem, implies the color decision is blind to the color information in the display during the playback of the recorded color information. These alternatives are not intended as serious models but to convey two general intuitions. First, if there is a buffer at play in the double-decision RT task then it must take time for the buffered information to be incorporated, or the RTs would have conformed to the parallel logic. Second, if the duration of the buffer is finite, when both 1D processes require more processing time than the duration of the buffer, there will be portions of the color and/or motion stimulus that do not affect the decision.</p><p>One might therefore ask why the second point does not lead to a reduction in sensitivity (or accuracy) in color, say, when motion is weak and competes with color for processing time. The answer is that when the decision maker controls the termination of the decision, they can compensate for the missing information by collecting more, until the level reaches the same terminating bound. This leads to a straightforward prediction. If the experimenter controls the termination of the evidence stream, then missing portions of the color and/or motion stimulus might impair performance, especially when the other stimulus dimension is weak.</p></sec><sec id="s2-4"><title>Experiment 3. Variable-duration stimulus presentation (eye)</title><p>We therefore predicted that under conditions in which the experimenter controls the viewing duration, there is an intermediate range of viewing durations, greater than 120 ms and less than the average RT of difficult double-decisions, where we might observe interference in sensitivity. To appreciate this prediction, it is essential to recognize that when the experimenter controls viewing duration of a random dot display, the decision maker applies a termination criterion, as they do in choice-RT experiments (<xref ref-type="bibr" rid="bib51">Kiani et al., 2008</xref>). There is no overt manifestation of this termination, although it can be identified by introducing perturbations to the stimulus (see also <xref ref-type="bibr" rid="bib44">Kang et al., 2017</xref>). Before such termination, sensitivity improves by the square root of the stimulus viewing duration (<inline-formula><mml:math id="inf30"><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:math></inline-formula>) as expected for perfect integration of signal-plus-noise. In a double-decision, when the two decision processes are splitting the time equally, the sensitivity of each should only improve by <inline-formula><mml:math id="inf31"><mml:msqrt><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:math></inline-formula>. However, when one process terminates, the rate of improvement of the other process should recover, until that process reaches its terminating bound. The model predicts a range of stimulus strengths and viewing durations in which interference in accuracy ought to be evident. It also predicts that the range and degree of interference might depend on which stimulus dimension the participant prioritizes. Here, we set out to test this prediction.</p><p>Two participants each performed ∼11,800 trials over 12–16 sessions. The task was identical in structure to the brief-duration experiment. However, stimuli were presented at fixed durations ranging from 120 to 1200 ms (in steps of 120 ms). Only three levels of difficulty were used for each dimension: one easy and two difficult coherence levels. The two difficult coherence levels were adjusted individually to yield 80% and 65% accuracy, respectively, ensuring above-chance performance despite the high difficulty level. The easy coherence level was fixed at the highest motion/color coherence from Experiment 1-unimanual, as this coherence typically supports perfect accuracy. The number of coherence levels was reduced compared to Experiments 1 and 2 in light of the large number of conditions (6 signed motion coherences × 6 signed color coherences × 10 stimulus durations). The key comparison here is sensitivity to difficult color, say, when (i) motion is difficult and therefore likely to compete with the color for decision time vs. (ii) motion is easy and less likely to wrest time away from color. This comparison within the double-decision task is more appropriate than a comparison between the double and single decision tasks as these tasks are likely to elicit different termination bounds, as they have different error rates—0.75 and 0.5—on difficult trials (see Materials and methods).</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> shows the sensitivity to motion and color as a function of stimulus duration, when the other stimulus dimension was easy or difficult. The sensitivity is the slope of a logistic fit of the motion (or color) choices to the three levels of difficulty. Notice that for both participants there is no difference in sensitivity at the shortest stimulus duration (120 ms), consistent with the findings above. However both participants exhibited lower sensitivity at intermediate durations when color choices were coupled with difficult motion. This difference implies an interference. It is less compelling, if present at all, when motion choices are coupled with difficult color. This pattern, in which motion difficulty affects color sensitivity but not vice-versa, is consistent with participants prioritizing one decision over the other. This would arise if participants consistently monitored the motion stream first and turned to color after the motion decision terminated. In this case, the difficulty of the color would not affect the decisions for motion, but harder motion would take longer to terminate, thereby leaving less time for color processing. We therefore fit a model in which one decision was prioritized over another by including a parameter that determined the probability that motion would be processed first. We also included a parameter that controls the duration of the stimulus streams that can be held in the buffer. This is, effectively, the amount of stimulus information that can be acquired in parallel. The best fits of the model, shown by the smooth curves (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), suggest a buffer capacity of approximately 80 ms worth of stimulus information (<xref ref-type="fig" rid="fig4">Figure 4B</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) and prioritization of motion on approximately 80–96% of trials. The serial and parallel models are special cases of this model in which the buffer durations are very short or long, respectively. Both such buffer capacities provide very poor fits to the data (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Note that we continue to refer to the model as serial because even in the parallel phase, the decision is updated serially.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Interference in choice accuracy can be elicited at intermediate viewing durations (Experiment 3).</title><p>Two participants (columns) performed the color-motion double-decision task with a random dot display presented for 120–1200 ms. (<bold>A</bold>) <italic>Top</italic>, Motion sensitivity as a function of stimulus duration and color strength. Symbols are the slope of a logistic fit of the proportion of rightward choices as a function of signed motion strength, for each stimulus duration. Data are split by whether the color strength was strong (blue) or weak (green). Error bars are s.e. <italic>Bottom</italic>, Analogous color-sensitivity split by whether the motion strength was strong (purple) or weak (red). Curves are fits to the data from each participant using two bounded drift diffusion models that operate serially after an initial stage of parallel acquisition, here termed the buffer capacity. During the serial phase, one of the dimensions is prioritized until it terminates. The prioritization favored motion for both participants (<inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.80</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and 0.96, for participants S4 and S5, respectively). (<bold>B</bold>) Negative log likelihood of the model fits as a function of the buffer capacity, relative to the model fit at 80 ms capacity. The model is equivalent to a purely serial model, when the buffer capacity is zero, and to a purely parallel model when the buffer capacity exceeds the maximum stimulus duration. Negative log likelihoods were computed for a discrete set of buffer capacities (black points). Horizontal lines at <inline-formula><mml:math id="inf33"><mml:mrow><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mi>BF</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> indicate Bayes factor = 1. Dashed lines show where the Bayes factor = ± 100 (‘decisive’ evidence; <xref ref-type="bibr" rid="bib48">Kass and Raftery, 1995</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Parameter recovery analysis (Experiment 3).</title><p>The graphs evaluate the sensitivity and specificity of the estimates of buffer capacity (<inline-formula><mml:math id="inf34"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula>) shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Same plotting conventions as <xref ref-type="fig" rid="fig4">Figure 4B</xref>. Columns are the two participants. We used the parameters of the best fitting diffusion models to the data in <xref ref-type="fig" rid="fig4">Figure 4</xref> (solid curves; see <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>). The analysis in the top row addresses specificity. The simulations use 80 ms, but the model fits used <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula> fixed to each of the durations shown on the abscissa, computed for a discrete set of buffer capacities (black points). The ordinate shows the difference of each model's negative log likelihood from that of the 80 ms buffer model (smaller is better). Error bars are standard deviations across 12 simulations. The analysis suggests fiducial confidence limits of roughly 80–200 ms. The analysis in the bottom row addresses identifiability. The simulations use <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula> shown on the abscissa. We then compare two fits, using <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula> ms or the simulated value. Misidentification is limited to a narrow range similar to the fiducial confidence interval.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Fits to the choice data in Experiment 3 with strictly serial and parallel models.</title><p>The best fitting model to the choice data in the variable duration task implicates a finite buffer, allowing motion or color information to be held for a period before updating the decision. If <inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> or 0, the model is purely parallel or purely serial. The graphs show the best fits of these models for two subjects. The format of the graphs is identical to <xref ref-type="fig" rid="fig4">Figure 4</xref>. <italic>Left column</italic>, reproduction of the fits in <xref ref-type="fig" rid="fig4">Figure 4</xref>. <italic>Middle column</italic>, best fitting parallel model. <italic>Right column</italic>, best fitting serial model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig4-figsupp2-v2.tif"/></fig></fig-group><p>The findings therefore support our prediction, and in doing so, they support the hypothesis that a common principle underlies double-decisions ranging from a tenth to at least two seconds, independent of whether this duration is controlled by the experimenter or by the decision maker. Namely, there is parallel acquisition but serial incorporation of color and motion into the double-decision process. The interference in choice accuracy demonstrated in this experiment is the only example of choice interference in our study. It is remarkably elusive, because it can be observed only for stimulus durations for which three conditions are satisfied: (i) the duration of the stimulus is long enough that parallel acquisition is no longer possible; (ii) the duration of the stimulus is short enough that accuracy on one dimension would benefit from additional sensory evidence; (iii) the duration should support termination of the other dimension for strong but not weak stimuli. The interference is also deceptive. It is explained by a competition for processing time, not by an interaction affecting the fidelity of the sensory streams themselves. It is an example of resource sharing (<xref ref-type="bibr" rid="bib98">Tombu and Jolicoeur, 2002</xref>; <xref ref-type="bibr" rid="bib100">Tombu and Jolicoeur, 2005</xref>; <xref ref-type="bibr" rid="bib42">Kahneman, 1973</xref>), but the resource is time, specifically.</p></sec><sec id="s2-5"><title>Experiment 4. Two-effector double-decision reaction time (bimanual)</title><p>There are two important features of the serial model: the existence of two decision variables that are terminated independently, and that these accumulations are not updated at the same time but in series. A limitation in the experiments so far is that we had access to the completion of the double-decision but not to the completion of each component. Therefore, we could only speculate about which decision completed first, and when. Without knowledge of the first decision time, we cannot tell how often a participant switched between updating the motion and color decision variables. For example, the prioritization considered in the previous section could arise by completing one decision before deliberating on the second or by alternating back and forth on a schedule that allocates more time to motion. We therefore conducted an experiment in which participants indicated their choice and RT for each stimulus dimension using separate effectors.</p><p>The eight participants who performed the unimanual version of the double-decision RT task (Exp. 1-unimanual) also performed a bimanual version of the same task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). In the unimanual version, participants used a handle to move a cursor to one of four targets that simultaneously communicated color and motion decisions. In the bimanual version, participants indicated their motion decision by moving one of the handles in a left/right direction and indicated their color decision with a forward/backward movement of the other handle. Participants were instructed to report each 1D decision as soon as it was made. To facilitate this, they received extensive training, consisting of blocks in which one of the stimulus dimensions was set at its easiest level. Both the order of the tasks (unimanual and bimanual) and the hand assignments (left/right × color/motion) were balanced between the participants (see Materials and methods). Trial numbers (2304) and motion-color coherence levels (11 × 11 combinations of signed coherence levels) were identical for the uni- and bimanual version of the task.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Replication of double-decision choice-reaction time when the decisions are reported with two effectors (Experiment 4).</title><p>(<bold>A</bold>) Participants performed the color-motion double-decision choice-reaction task, but indicated the double-decision with either a unimanual movement to one of four choice-targets or a bimanual movement in which each hand reports one of the stimulus dimensions (N = 8 participants performed both tasks in a counterbalanced order). In both conditions, the hand or hands were constrained by a robotic interface to move only in directions relevant for choice (rectangular channels). The display was the same in the unimanual and bimanual tasks, with up-down movement reflecting color choice and left-right movement reflecting motion choice. A scrolling display of proportion correct was used to encourage accuracy. In the unimanual trials both choices were indicated simultaneously. However, in the bimanual trials each choice could be indicated separately and the dot display disappeared only when the second hand left the home position. (<bold>B</bold>) Choice proportions and double-decision mean RT on the bimanual task. The double-decision RT on the bimanual task is the latter of the two hand movements. The data are plotted as a function of either signed motion or color strength (abscissae), with the other dimension shown by color (same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>). Solid traces are identical to the ones shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref> for the unimanual task, generated by the method of fitting the conditions containing at least one stimulus condition at its maximum strength and predicting the rest of the data. They establish predictions for the bimanual data from the same participants. The agreement supports the conclusion that the participants used the same strategy to solve the bimanual and unimanual versions of the task. Note that a few symbols are occluded by others.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Choice and double-decision RT for the bimanual responses (Experiment 4) in the same format as <xref ref-type="fig" rid="fig2">Figure 2B</xref>.</title><p>These are the same data shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> but replacing the predictions from the unimanual fits with the fits to the data from the bimanual task. We use the same fit/prediction strategy as in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. The model comparison is summarized in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Model-free comparison of performance in the unimanual (blue) vs. bimanual (red) task (Experiment 4).</title><p>(<bold>A</bold>) <italic>Top:</italic> Sensitivity of color choices as a function of motion strength (unsigned coherence). Sensitivity is the slope of a logistic regression of color choice as a function of signed color coherence, obtained separately for each level of motion strength. <italic>Bottom:</italic> RTs in the uni- vs. bimanual task as a function of motion strength when color was weak (three lowest strengths; light shading) vs. strong (three highest strengths; dark shading). For the bimanual task, RTs correspond to the final response of a given trial. (<bold>B</bold>) Similar to A, but with the roles of color and motion swapped in the analysis. <italic>Top:</italic> motion sensitivity. <italic>Bottom:</italic> RTs as a function of color strength when motion was either weak (light shading) or strong (dark shading). No differences in overall choice sensitivity were found between the uni- and bimanual task (repeated-measures ANOVA, motion sensitivity: F<sub>1,7</sub> = 0.21, p=0.664; color sensitivity: F<sub>1,7</sub> = 0.70, p=0.431). Similarly, overall RTs were similar in the uni- and bimanual task (motion: F<sub>1,7</sub> = 0.56, p=0.477; color: F<sub>1,7</sub> = 0.57, p=0.476). Furthermore, the modulation of RTs by the informative and uninformative dimensions, respectively, was not affected by task (uni-/bimanual; all interactions p &gt; 0.05). This suggests that overall performance, and modulation of RTs by each decision dimension, were similar in the uni- and bimanual tasks. Data points represent mean ± s.e.m. (N = 8).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig5-figsupp2-v2.tif"/></fig></fig-group><p>Before tackling the questions that motivate the bimanual experiment, we first ascertained whether participants used the same strategy to make bimanual double-decisions as they did on the unimanual version. It seemed conceivable that by using separate hands to indicate the motion and color decisions, participants could achieve parallel decision formation, for example, as a pianist reads the treble and bass staves with the right and left hands, typically. We therefore conducted a model comparison similar to that of <xref ref-type="fig" rid="fig2">Figure 2</xref>. To fit the models, we used the color and motion choice on each trial along with the second response time (RT<sup>2nd</sup>) regardless of whether it was to indicate direction or color. This allows us to fit models that are identical to those used in the unimanual task (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In the bimanual task, the final RTs (RT<sup>2nd</sup>) are well described by the fits to the unimanual double-decision RTs (<xref ref-type="fig" rid="fig5">Figure 5</xref>). We illustrate this in two ways. In the figure, the solid traces are not fits to the bimanual data; they are fits to the unimanual data shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. Clearly the choice probabilities and RTs in the bimanual task are well captured by the model fit to the unimanual data. The fits to the bimanual data are shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, and model comparison favors the serial over the parallel model for seven of the eight participants (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Importantly, the participants’ behavior was strikingly similar in the unimanual and bimanual versions of the task. The similarity between the two versions of the task is also supported with a model-free analysis. In <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, we superimpose the accuracy and the RTs for the unimanual and bimanual tasks. There is an almost perfect overlap between these two aspects of choice behavior, providing further support for a common set of processes operating in both versions of the task.</p><p>The bimanual task allows us to distinguish between two variants of the serial model that were not distinguishable in the unimanual task. In the first variant, the <italic>single-switch</italic> model, the decision maker only switches from one decision to the next when the first decision is completed. Thus, the decision that terminates first (D<sup>1st</sup>) is the one that is evaluated first, and only then the other decision is evaluated. In the second variant, the <italic>multi-switch</italic> model, the decision maker can alternate between decisions even before finalizing one of them. If little time is wasted when switching, these two models make similar predictions for the RT in the unimanual task: the RT will be the sum of the two decision times plus the non-decision latencies. However, the models make qualitatively different predictions for how the response time for D<sup>1st</sup> depends on the difficulty of the other decision.</p><p>The single-switch model predicts that the RT for D<sup>1st</sup> is independent of the difficulty of the decision reported second (D<sup>2nd</sup>). This is because D<sup>2nd</sup> is not evaluated until the first decision is completed. The prediction of the multi-switch model is less straightforward. Suppose that in a given trial the motion decision is easy and the color decision is difficult. If the color was reported first, the motion was probably not evaluated at all before committing to D<sup>1st</sup>, since if it had been evaluated it would most likely have ended before the color decision. In contrast, if both dimensions were difficult, which decision was reported first is largely uninformative about the number of alternations between color and motion that occurred before committing to the first decision; since both decisions take longer to complete, it is possible that both have been evaluated before one of them terminated. Therefore, the multi-switch model predicts that the first decision takes longer the more difficult the other decision is: when D<sup>2nd</sup> is easy, it is more likely that it was not considered before committing to the D<sup>1st</sup> decision and thus the average RT<sup>1st</sup> is shorter.</p><p>To disambiguate between the single-switch and multi-switch models, we fit both models to the data from the bimanual task. First, we fit a serial model identical to that of <xref ref-type="fig" rid="fig2">Figure 2</xref> to the data from the bimanual task. We used the same procedure as in <xref ref-type="fig" rid="fig2">Figure 2</xref>; that is, we ignore RT<sup>1st</sup> and fit RT<sup>2nd</sup> and the choices given to the two decisions. Then, we used three additional parameters to attempt to explain RT<sup>1st</sup>. These parameters are the average time between switches (<inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">Δ</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), the probability of starting the trial evaluating the motion decision (<inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the non-decision time for the first decision (<inline-formula><mml:math id="inf41"><mml:msubsup><mml:mi>T</mml:mi><mml:mi>nd</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, see Materials and methods). These parameters only affect RT<sup>1st</sup>; they do not influence RT<sup>2nd</sup> or the choice probabilities. The three parameters were fit to minimize the mean-squared error between the models' predictions and the data points (<xref ref-type="fig" rid="fig6">Figure 6</xref>; <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>). The single switch model is a special case of the multi-switch model where <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">Δ</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is very large (i.e. longer than the slowest first decision time).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>First response times in the bimanual task suggest multiple switches in decision updating (Experiment 4).</title><p>For bimanual double-decisions, participants indicate two RTs per trial. Whereas up to now we have only considered the RT corresponding to completion of both color and motion decisions, the analyses in this figure concern the RT of the first of the two. Symbols are means ± s.e. (N = 8 participants). Curves are fits to single- and multi-switch model (orange and blue, respectively). (<bold>A</bold>) RT as a function of motion strength when motion was reported first. (<bold>B</bold>) RT as a function of color strength when motion was reported first. (<bold>C</bold>) RT as a function of motion strength when color was reported first. (<bold>D</bold>) RT as a function of color strength when color was reported first. In panels A and D, the first response corresponds to the stimulus dimension represented on the abscissa. The data exhibit the expected pattern of fast RT when the stimulus is strong and slow RT when the stimulus is weak (i.e. near 0). This would occur if the serial processing of motion and color ensued one after the other (single-switch) or with more than one alternation (multi-switch), although the latter provides a better account of the data. In panels B and C, the first response corresponds to the stimulus dimension that is not represented on the abscissa. Here the single-switch model fails to account for the data. If there were only one switch and color terminates first, then the strength of motion is irrelevant, because all processing time was devoted to color. Similarly, if there were only one switch and motion terminates first, then the strength of color is irrelevant, because all processing time was devoted to motion.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig6-v2.tif"/></fig><p>The model comparison provides clear support for multiple switches. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows the average RT for the decision reported first (RT<sup>1st</sup>), split by whether the first decision was color or motion, and grouped by either color or motion strength. Both the single- and multi-switch models provide a good explanation of the RT<sup>1st</sup> when grouped as a function of the coherence of the decision that was reported first (<xref ref-type="fig" rid="fig6">Figure 6</xref>, panels A and D). However, only the multi-switch model could explain the interaction between RT<sup>1st</sup> and the coherence of D<sup>2nd</sup> (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>). The graphs show that RT<sup>1st</sup> is longer when D<sup>2nd</sup> is more difficult, and this effect was well explained by the multi-switch model. Unlike what is seen in the data, the single-switch model predicts that RT<sup>1st</sup> should not vary with the coherence of D<sup>2nd</sup> (flat orange lines in panels B and D). Because we fit the models for each participant individually, we can analyze the frequency of alterations predicted by the model with multiple switches. For one of the participants, the best-fitting inter-switch interval was greater than the slowest decision time, and thus the model was no different from the single-switch model. For the other seven participants, alternations were sparse: the average inter-switch interval was 704 ± 205 ms (mean ± s.e.m. across participants).</p><p>To summarize, the bimanual version of the double-decision task allowed us to infer not only that the two dimensions were addressed serially, but that people may alternate between both attributes of the stimulus in a time-multiplexed manner. The model suggests that alternations were sparse, as if the participants considered one decision for several hundred milliseconds, and switched temporarily to the other decision if they found no conclusive evidence about the first. Moreover, it provides direct evidence for two termination events, as assumed in our model fits. This rules out a class of models of the double-decision as a race among four accumulations for each of the color-motion combinations, what we term target-wise integration, as these models preclude completion of one decision before the other.</p></sec><sec id="s2-6"><title>Experiment 5. Binary-response double-decision reaction time</title><p>Up to now, we have observed serial decision making when participants had to provide two answers—that is, four possible responses. A possible concern is that the reason we observed the serial pattern of double-decisions was that it required a quaternary response. We therefore designed a task that involves a double-decision but only a binary choice. Two participants, who had participated in Experiment 4, were asked to report whether the net direction in two patches of random dots were the same or different by pressing one of two response keys with the index fingers of each hand; (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). The two motion stimuli were presented to the left and right of a central fixation cross <xref ref-type="fig" rid="fig7">Figure 7A</xref>. The direction (up or down) and strength of motion (three coherence levels) were controlled independently in the two stimuli. Each participant completed four sessions (3072 trials) of this task and additionally completed a single session (768 trials) of a 1D task in which they judged the up/down direction of a single stimulus presented on the left or right side of the screen.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Serial decision making in a Same vs. Different task (Experiment 5).</title><p>(<bold>A</bold>) Task. Two dynamic random dot motion displays were presented in rectangular patches to the left and right of a central fixation cross. The direction and motion strength were randomized from trial to trial and between the patches (up or down × three motion strengths). Participants judged whether the dominant direction of the left and right patches is the same or different and indicated the decision when ready by pressing a response key with their left or right index finger. At the end of each trial, participants received feedback. In a separate block, participants also performed a 1D direction discrimination task in which only one patch of random dots was displayed. (<bold>B</bold>) Results and fits for two participants (columns). <italic>Top</italic>, Proportion of correct choices as a function of the level of motion strength (i.e. unsigned coherence; L = low; M = medium; H = High). <italic>Bottom</italic>, Response times for each level of motion strength. The first three bars represent the direction task where only a single motion stimulus was presented. The six bars on the right of each plot represent the same-different task. Horizontal red and blue lines are fits of serial and parallel drift-diffusion models to the means. Only correct trials were included for RT analyses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Comparison of parallel and serial rules applied to reaction time distributions in the Same vs Different task (Experiment 5).</title><p>Comparison of parallel and serial rules applied to reaction time distributions in the Same vs. Different task (Experiment 5). The analysis is a variant of the empirical approach introduced in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, applied to RT distributions associated with the six unique combinations of motion strength (correct choices only). The analysis optimizes the parameters of gamma distributions representing three 1D decision times, corresponding to the three unique motion strengths, and one non-decision time to best explain the six observed distributions of RTs. (<bold>A</bold>) Best fitting RT distributions for each participant, shown as cumulative probability distributions. Dashed black curves are data. Solid curves are best fitting distributions under serial (red) and parallel (blue) combination rules. (<bold>B</bold>) Superposition of the expectations obtained from the fitted distributions (panel A) on the mean RT.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig7-figsupp1-v2.tif"/></fig></fig-group><p>Both participants exhibited accuracy-RT functions that depended on the difficulty of both motion stimuli. <xref ref-type="fig" rid="fig7">Figure 7B</xref> shows the proportion of correct choices plotted as a function of the coherences for both the 1D (up-down) and 2D (same-difference) trials. The RTs associated with same-different judgment were almost twice as long as the RTs from a 1D direction judgment. Part of this difference might be attributed to the conversion from two direction judgments to the same-different response, but that should not depend on difficulty and it is hard to reconcile this with the magnitude of the difference. Instead they suggest additive decision times. The horizontal red and blue lines in <xref ref-type="fig" rid="fig7">Figure 7B</xref> are fits to a drift diffusion model that assume the 2D same/different decision is formed from two 1D direction decisions under serial and parallel models, respectively. We constrained the fits in the 1D and 2D tasks to share the same sensitivity to motion strength and the same up-down bias (see Materials and methods, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>). Bayes factor for both participants favored the serial model (<inline-formula><mml:math id="inf43"><mml:mrow><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mi>BF</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>49.6</mml:mn></mml:mrow></mml:math></inline-formula> and 31.6 for S7 and S12, respectively).</p><p>The comparison of serial vs. parallel rests on an understanding of the way distributions of the two up/down decision times are combined to generate the same/different decision times. A possible concern is that the parsimonious drift diffusion models used to estimate these latent distributions is wanting. For example, they assume stationary bounds, which distorts the shapes of the distributions (<xref ref-type="bibr" rid="bib26">Drugowitsch and Moreno-Bote, 2014</xref>). We therefore conducted the model comparison using an empirical method that uses only the observed same/different RTs and tries to account for them solely through combination of latent distributions of up/down decision times (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). This analysis also provides strong support for the serial account (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>; <inline-formula><mml:math id="inf44"><mml:mrow><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mi>BF</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> for both participants). Like the color-motion task, there is every reason to assume that the acquisition of evidence from the two patches of random dots occurs in parallel. Yet once again, the pattern of RTs supports serial incorporation into the double decision. The use of a binary response in the same-different task rules out the possibility that the long decision times in our 2D experiments are explained by the doubling of alternatives (Hick’s law; <xref ref-type="bibr" rid="bib36">Hick, 1952</xref>; <xref ref-type="bibr" rid="bib66">Luce, 1986</xref>; <xref ref-type="bibr" rid="bib102">Usher et al., 2002</xref>). Moreover, the findings demonstrate that the serial incorporation of evidence into a double-decision is not restricted to different perceptual modalities, such as color and motion.</p></sec><sec id="s2-7"><title>Parallel acquisition with serial incorporation model</title><p>Taken together, the results from our five experiments suggest that the prolongation of RTs in double-decisions is the result of serial integration of evidence during the decision-making process, independent of the modality of choice implementation and number of response options. Parallel acquisition of the two sensory streams followed by serial incorporation into decision variables reconciles the findings of the short duration experiment (Experiment 2) with those of the double-decision RT experiment (Experiment 1). The variable duration (Experiment 3) and bimanual (Experiment 4) experiments suggest that (i) parallel acquisition and serial incorporation is not limited to the short duration experiment and (ii) serial alternation of color and motion can occur before one process terminates. Here, we develop a conceptual framework that accommodates the findings from all five experiments with what is known about the neurobiology of similar 1D perceptual decisions. We will proceed by illustrating the steps that underlie the acquisition of evidence samples, their temporary storage in buffers, and their incorporation into the decision variables that govern choice and the two decision times. We first make the case for the buffer using a simulated trial from the short duration experiment. We then elaborate the diagram to account for the serial pattern of decision times when the stimulus duration is longer.</p><p>Consider the example in <xref ref-type="fig" rid="fig8">Figure 8A</xref> of a process leading to a decision in the short duration task. Suppose that visual processing of the 120 ms motion stream gives rise to a single sample of evidence that captures the information from the brief pulse, and the same is true for the color stream. These samples of evidence are acquired in parallel and placed in buffers, where they can be stored temporarily. The values in these buffers may be thought of as latent instructions to a cortical circuit to update a decision variable (<inline-formula><mml:math id="inf45"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>) by some amount (<inline-formula><mml:math id="inf47"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). While the samples can be acquired simultaneously, only one sample can update the corresponding decision variable at a time. This is the bottleneck that imposes serial multiplexing in the 2D tasks. One of the samples must be held (buffered) until the other update operation has cleared. If motion is the first to be updated, then <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> cannot be updated until the circuit receiving the motion-update instruction has received it (black arrow). This takes some amount of time, <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula> (for instruct). The update instruction is realized by an integrator with a time constant (<inline-formula><mml:math id="inf51"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> ms) leading to slow cortical dynamics (maroon and blue traces).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Parallel acquisition of evidence and serial updating of two decision variables.</title><p>An elaborated drift diffusion model permits reconciliation of the serial processing implied by the double-decision choice-RT experiment and the failure to observe interference in choice accuracy when the color-motion stimulus is restricted to a brief pulse. The main components of the model are introduced in panel A and elaborated in panels B and C. In all panels, maroon and blue indicate motion and color processes, respectively. (<bold>A</bold>) Simulated trial from the short duration experiment (Experiment 2). Information flows from top to middle graphs for motion; and from bottom to middle graphs for color. Time is left to right. The evidence from both color and motion is extracted from the 120 ms random dot stimulus in parallel. Both can be stored temporarily in separate buffers (filled rectangles), which send an instruction to the circuits representing the respective decision variables in their persistent firing rates. The instruction is to change the firing rate by an amount (<inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). This latency from clearance of the sample from the buffer to receipt of the <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> instruction takes time (<inline-formula><mml:math id="inf55"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula>, diagonal arrows), and this is followed by the realization of the instruction in the evolving firing rates of cortical neurons (smooth colored curves). In the example, the <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the first to update. A central bottleneck precludes updating <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>. The bottleneck is unblocked when the <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> instruction is received by the circuit that represents the motion decision variable (black arrow). This allows the buffered evidence for color to update <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>. Open rectangle represents clearance of the buffer content, which occurs immediately for motion and after a delay for color in this example. Dashed lines associated with the decision stage show the instructed change in the decision variable (<inline-formula><mml:math id="inf60"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). Smooth colored curves show the evolution of the decision variables. (<bold>B</bold>) Elaboration of the example in panel-A. The boxes representing the 120 ms stimulus are replaced by the two outer rows: (i) raw luminance and color data stream, <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, represented as biased Wiener processes (duration 120 ms); (ii) filtered evidence streams containing the relevant motion (right minus left) and color (blue minus yellow) signals. The filters introduce a delay and smoothing. The filtered signals can be sampled by the buffer every <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula> ms, so long as the buffer is available (i.e. empty). The bottleneck shows the process that is accessing the update channel. Other than the first sample, the prioritization is equal and alternating. Only one process can update at a time. Circled numbers identify the key events described in Results. Events sharing the same number are approximately coincidental. (<bold>C</bold>) Example of a double-decision in the choice-RT task. The first eight steps parallel the logic of the process shown in panel B. The decision variables then continue to update serially, in alternation, until <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> reaches a terminating bound <named-content content-type="author-callout-style-a1">⑨</named-content>. The decisions then continues as a single-dimension motion process until <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> reaches a terminating bound (<named-content content-type="author-callout-style-a3">⑩</named-content>). Note that the sampling rate is the same as it was in the parallel phase, whereas during alternation it was half this rate for each dimension. Bound height is indicated by <named-content content-type="author-callout-style-a1">⑨</named-content> and <named-content content-type="author-callout-style-a3">⑩</named-content>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Example of a 1D choice-reaction time trial.</title><p>The model in <xref ref-type="fig" rid="fig8">Figure 8</xref> retains compatibility with 1D decisions. The example uses identical settings to those in <xref ref-type="fig" rid="fig8">Figure 8</xref>, but there is only a color decision. Samples are acquired every <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula> ms, and the buffer is cleared immediately. Notice that the decision variable, <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>, does not reflect evidence until <inline-formula><mml:math id="inf69"><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>180</mml:mn></mml:mrow></mml:math></inline-formula> ms, which is similar to the latency observed in LIP and prefrontal cortex (<xref ref-type="bibr" rid="bib39">Huk and Shadlen, 2005</xref>; <xref ref-type="bibr" rid="bib54">Kim and Shadlen, 1999</xref>). The bottleneck is blocked while the content of the buffer is transmitted to <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> (i.e. for <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula>; angled arrows from buffer to <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>), and this implies that for all samples after the first, <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula> should equal <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula>, else some evidence could be missed. For example, suppose that a second sample were obtained at <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.12</mml:mn></mml:mrow></mml:math></inline-formula> s. It would be held until the bottleneck was unblocked, at <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.18</mml:mn></mml:mrow></mml:math></inline-formula> s, and this would miss 60 ms of evidence. From here on, the buffer can only be filled once it is cleared and this imposes the equality of <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula>, as in the late phase of motion processing in <xref ref-type="fig" rid="fig8">Figure 8C</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Example of a 2D decision with one switch after parallel acquisition.</title><p>After the parallel acquisition phase, motion is prioritized until termination at <named-content content-type="author-callout-style-a3">⑧</named-content>, which removes the bottleneck, allowing for the the next update of <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>, sampled at <named-content content-type="author-callout-style-a1">③</named-content> (<inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.18</mml:mn></mml:mrow></mml:math></inline-formula> s) and buffered until <named-content content-type="author-callout-style-a1">⑧</named-content>. The 1D color decision-process continues until termination at <named-content content-type="author-callout-style-a1">⑩</named-content>. The example approximates the drift-diffusion model used to fit data from Experiment 3. (Same conventions as in <xref ref-type="fig" rid="fig8">Figure 8C</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig8-figsupp2-v2.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 3.</label><caption><title>Example of a 2D decision with stochastic switching.</title><p>After the parallel acquisition phase (as in <xref ref-type="fig" rid="fig8">Figure 8B,C</xref>) there are three color samples obtained in succession, followed by a alternation to motion then color then motion. While this last motion was transmitted to instruct <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the color decision terminated at <named-content content-type="author-callout-style-a1">⑨</named-content>, thereby allowing for the final phase of motion sampling with termination at <named-content content-type="author-callout-style-a3">⑩</named-content>. Note that the final buffer clearance would only instruct <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> after decision termination. It could affect post-decision processes, such as change of mind (<xref ref-type="bibr" rid="bib90">Resulaj et al., 2009</xref>) or confidence. (Same conventions as in <xref ref-type="fig" rid="fig8">Figure 8C</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63721-fig8-figsupp3-v2.tif"/></fig></fig-group><p>In this example, each buffer receives all the information available in the stimulus. Were there additional samples in the stimulus, the motion buffer would be ready to receive another sample when it sends its content, whereas the color buffer cannot be updated until it is cleared, <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula> later. The bottleneck is between the buffer and the update of the decision variable, more specifically, the initiation of the dynamic process that implements this update in a cortical circuit. In this case, there is no consequence beyond a delay, because there is no more evidence from the stimulus after 120 ms.</p><p><xref ref-type="fig" rid="fig8">Figure 8B</xref> elaborates the diagram in panel A using another trial from the short duration experiment. We now represent the transformation of sensory data to evidentiary samples by applying a stage of signal processing to the raw luminance and color data, <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These functions are just shorthand for the noisy spatiotemporal displays. The motion filter is meant to capture the impulse response of direction selective simple and complex cells in the visual cortex (<xref ref-type="bibr" rid="bib75">Movshon et al., 1978a</xref>; <xref ref-type="bibr" rid="bib76">Movshon et al., 1978b</xref>; <xref ref-type="bibr" rid="bib2">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib11">Britten et al., 1993</xref>; <xref ref-type="bibr" rid="bib21">DeAngelis et al., 1993</xref>), and we assume a similar operation on the stimulus color stream. They are also shorthand for a difference signal, such as right minus left and blue minus yellow. The filtering introduces a delay and a smearing of these streams. While the motion filters must sample the <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at rates sufficient to support the extraction of fast fluctuations and fine spatial displacement, the neurons ultimately pool these signals nonlinearly over space and time (<xref ref-type="bibr" rid="bib11">Britten et al., 1993</xref>; <xref ref-type="bibr" rid="bib110">Zylberberg et al., 2016</xref>). These are the signals represented by the maroon filter traces in <xref ref-type="fig" rid="fig8">Figure 8B</xref>. This is the convolution of <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the function in <xref ref-type="fig" rid="fig3">Figure 3B</xref> (bottom). The same filter is applied to <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to make the filtered color traces (blue). Importantly, for purposes of integrating the information in the color-motion random dot displays, 11 Hz sampling (<inline-formula><mml:math id="inf89"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula> ms) is sufficient. Notice that the filtered representation lasts longer than the stimulus. Therefore, in this case, the decision is based on at least two samples of evidence per sensory stream.</p><p>The buffers acquire their first samples at <inline-formula><mml:math id="inf90"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula> ms (<xref ref-type="fig" rid="fig8">Figure 8B</xref>, arrows <named-content content-type="author-callout-style-a1">①</named-content> and <named-content content-type="author-callout-style-a3">①</named-content>). The motion buffer is cleared as soon as it is acquired (open maroon rectangle) to instruct a change in <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> (arrow <named-content content-type="author-callout-style-a3">②</named-content>). The instruction is received <inline-formula><mml:math id="inf92"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula> ms later. Thus, it is 180 ms after stimulus onset that the neurons representing <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> begin to reflect the motion evidence. We set <inline-formula><mml:math id="inf94"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula> ms mainly to simplify the figure (but see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). This unblocks the bottleneck (③), thereby allowing the first color sample to be cleared from its buffer (open blue rectangle) and replaced by a second color sample (filled blue rectangle, <named-content content-type="author-callout-style-a1">③</named-content>). Notice that the second motion sample is also acquired at <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>180</mml:mn></mml:mrow></mml:math></inline-formula> ms, that is, <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula> after the first acquisition (and its immediate clearance). The first color sample instructs <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>, thus blocking other updates for <inline-formula><mml:math id="inf98"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula> (<named-content content-type="author-callout-style-a1">④</named-content>) and is first registered by <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> at <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>270</mml:mn></mml:mrow></mml:math></inline-formula> ms, which unblocks the bottleneck (⑤). Because we are assuming alternation in this example, this leads to the second update of <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> (<named-content content-type="author-callout-style-a3">⑥</named-content>). With the motion buffer available, it would be possible to obtain a third sample from the motion stream at <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>270</mml:mn></mml:mrow></mml:math></inline-formula> ms, but the filtered signal has decayed to nearly zero, and we assume extinction of the stimulus is registered by the brain in time to terminate sampling. Upon receipt of <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the bottleneck is unblocked (<inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>360</mml:mn></mml:mrow></mml:math></inline-formula> ms; ⑦) and the second color sample is cleared from its buffer (<inline-formula><mml:math id="inf105"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>360</mml:mn></mml:mrow></mml:math></inline-formula> ms) to instruct <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> (<named-content content-type="author-callout-style-a1">⑧</named-content>). There is no signal left to integrate, and the decision is made based on the signs of <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>. Thus, the decision is based on simultaneous (parallel) acquisition of two samples of evidence, which are incorporated serially into their respective decision variables.</p><p>The exercise helps us appreciate how a stream of evidence lasting only 120 ms could lead to a double-decision 400–600 ms later (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). It also illustrates the compatibility of parallel acquisition and serial incorporation into the decisions, and it suggests that serial processing is imposed at the step between buffered samples and incorporation into the decision variables. This is the ‘response selection’ bottleneck hypothesized by <xref ref-type="bibr" rid="bib81">Pashler, 1994</xref> and others (e.g. <xref ref-type="bibr" rid="bib69">Marti et al., 2012</xref>; see Discussion).</p><p>The idea extends naturally to double-decisions that are extended in time. <xref ref-type="fig" rid="fig8">Figure 8C</xref> illustrates a simulated double-decision in a free response task. The double-decision is made once both decision variables reach their terminating bounds. The example follows the same initial steps as the short duration experiment, except that when the second motion and color samples are cleared from their respective buffers, they are replaced with a third sample. Notice that beginning with the third motion sample, the interval to the next sample has doubled (180 ms), because the example posits regular alternation (for purposes of illustration only; see <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref> and <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>). This longer interval begins with the second sample. From that point forward, until the color decision terminates, the streams are effectively undersampled. Decision processes ignore approximately half of the evidence supplied by the stimulus. This is because both streams supply independent samples of evidence at a rate greater than 5.5 Hz (i.e. an interval of 180 ms).</p><p>In the example, it is <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> that reaches the bound first (<inline-formula><mml:math id="inf110"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>1.4</mml:mn></mml:mrow></mml:math></inline-formula> s; <named-content content-type="author-callout-style-a1">⑨</named-content>). There may be no overt behavior associated with this terminating event, as in the eye and unimanual reaching tasks, but direct evidence for this termination is adduced from the bimanual reaching task. From this point forward, the processing is devoted solely to motion until it terminates at a negative value of <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> (<named-content content-type="author-callout-style-a3">⑩</named-content>). Notice that when the bottleneck is unblocked, there is always a buffered sample ready to be cleared, and this occurs at intervals of <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula> ms. The process is now as efficient as a single decision process. Indeed, a simple 1D decision about motion (or color) is likely to involve the same instruction delays and bottleneck (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). If <inline-formula><mml:math id="inf113"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then like the first sample of motion, all subsequent samples of motion could pass immediately from the buffer to update <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> without loss of information. The model is thus a variant of standard symmetrically bounded random walk or drift-diffusion (<xref ref-type="bibr" rid="bib60">Laming, 1968</xref>; <xref ref-type="bibr" rid="bib63">Link, 1975</xref>; <xref ref-type="bibr" rid="bib87">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib92">Shadlen et al., 2006</xref>; <xref ref-type="bibr" rid="bib88">Ratcliff and Rouder, 1998</xref>; <xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>). It is compatible with the long time it takes for visual evidence to impact the representation of the decision variable in cortical areas like the FEF and LIP (e.g., ∼180 ms).</p><p>The diagrams in <xref ref-type="fig" rid="fig8">Figure 8</xref> are intended for didactic purposes, to lay out the need for a buffer and the seriality imposed by a bottleneck between the buffer and the update of the DV in circuits associated with working memory. The values for the delays and time constants, <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:msub></mml:math></inline-formula>, were chosen mainly to simplify an already complex diagram, and the same holds for the assumption of strict alternation. The logic does not change if the serial processing were to involve many updates of color or motion before switching to the other dimension (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref> and <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>). The important assumption is that it takes time to update a decision variable, and during this update there is a bottleneck that precludes another update. Importantly, whether alternating, as in <xref ref-type="fig" rid="fig8">Figure 8C</xref>, or starting one process after completing the other, as in <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>, there is a period of time in which information in the sensory stream is not affecting one of the decisions. This loss is apparent in the additivity of decision times, but it leads to no interference in accuracy in the RT task, because the termination criterion has not changed, and this (and the stimulus strength) determines accuracy. This is the insight that led to the prediction that under certain conditions in which the experimenter controls the duration of the color-motion display, there ought to be interference between color and motion sensitivity (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In one sense, the present study extends the framework of bounded evidence accumulation to more complex decisions composed of the conjunction of two decisions about two distinct features. In another more important sense, the findings highlight a bottleneck in information processing that touches on the very speed of thought. The experimental findings demonstrate that a double-decision about the dominant color and direction of motion of a patch of random dots is formed serially. This is surprising, because color and motion are canonical examples of parallel visual pathways from the retina through the visual and extrastriate visual association cortex, and there are compelling demonstrations of this parallel processing on conscious perception (<xref ref-type="bibr" rid="bib14">Cavanagh et al., 1984</xref>; <xref ref-type="bibr" rid="bib15">Cavanagh et al., 1985</xref>; <xref ref-type="bibr" rid="bib13">Carney et al., 1987</xref>). Moreover, the stimulus was designed to minimize interference or competition for spatial attention. It was restricted to a small aperture in the center of the visual field, and the same individual dots supply the motion and color information. It seems fair to say that the deck was stacked in favor of parallel processing. Indeed we confirmed that the color and motion information in the random dot stimulus used here was acquired in parallel.</p><p>With one notable exception, there was not a hint of an interaction between color or motion on choice performance in our experiments. That is, changing the difficulty of one dimension, say color, did not affect the perceptual accuracy—or more precisely, sensitivity—to the other dimension, say motion. This held over a wide range of difficulties spanning chance to perfect performance. The one exception was when we controlled viewing duration (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and this turns out to be explained by a competition of the two streams for processing time, not by an interaction affecting the fidelity of the sensory streams themselves. Had we attended solely to the choice data, we would have likely concluded that the motion and color decisions were formed in parallel, consistent with 40 years of vision science (<xref ref-type="bibr" rid="bib64">Livingstone and Hubel, 1988</xref>; <xref ref-type="bibr" rid="bib86">Ramachandran and Gregory, 1978</xref>).</p><p>Evidence for seriality of the decision process is adduced mainly from the pattern of double-decision RTs. The RT is the time from the onset of the color-motion stimulus to the initiation of the movement used to indicate the decision: the sum of the time it takes to complete the double decision, plus time delays that are not affected by task difficulty, termed the non-decision time (<inline-formula><mml:math id="inf117"><mml:msub><mml:mi>T</mml:mi><mml:mi>nd</mml:mi></mml:msub></mml:math></inline-formula>). If the color and motion decisions are made in parallel, then the double-decision time is the larger of the two decision times, <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If the decisions are made serially, the double-decision time is the sum, <inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We focused on the <italic>max</italic> vs. <italic>sum</italic> distinction using a combination of fitting and prediction. The simplest approach relies only on empirical fits of the double-decision RT distributions (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) derived from a smaller set of latent distributions of one-dimensional color and motion decision-times, under the appropriate operations for parallel and serial combination (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref> and convolution, respectively). The approach focuses solely on the RTs and was therefore essential for Experiment 5, where we had no access to the direction choices in the two patches of random dots. It reveals ‘decisive’ support (<xref ref-type="bibr" rid="bib48">Kass and Raftery, 1995</xref>) for seriality in all but one of the 11 participants (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). A drawback of the approach is that it does not constrain the relationship between choice accuracy and decision time. For this, we used a variety of bounded drift-diffusion models. These are the fits shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Here too, we attempted to contrast the <italic>max</italic> and <italic>sum</italic> logic by predicting the RT distribution for the majority of conditions. We fit the choice-RT data from the subset of conditions in which at least one of the stimulus dimensions was at its strongest level. The fits, under the <italic>max</italic> or <italic>sum</italic> rule, supply the marginal distributions of color and motion decision times to predict the RT of the remaining conditions, through application of the same rule. This approach also provides decisive support for the serial model (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><sec id="s3-1"><title>The case for buffers and bottlenecks</title><p>The strong support for serial processing does not specify where in the processing chain the seriality arises. The answer to this question resolves the apparent contradiction with vision science, and highlights a connection with a body of literature from psychology that addresses the topic of dual task interference, more specifically the psychological refractory period. The key is the short and variable duration experiments (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). If seriality were imposed at the level of sensory acquisition then when both color and motion are difficult, accuracy on one dimension should come at the expense of accuracy on the other, on average. We did not observe this at short durations, and not for lack of power, as made clear by the interference that was detected at intermediate durations. Nor did we observe any reduction in accuracy compared to single decisions, and there was no difference in the magnitude and time course over which momentary fluctuations of color and motion predicted the individual choices on single- and double-decisions (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). These observation also rule out the possibility that there was interference but it was balanced across trials—that is, a mixture of trials in which successful motion processing impaired color processing on half the trials and successful color processing impaired motion processing on the other half.</p><p>Thus, the short duration experiment demonstrates parallel processing and the necessity of at least one buffer. The results in the variable duration experiment might lead us to entertain the possibility that only color is buffered, because motion was prioritized. However, the bimanual task demonstrates that motion is not always processed first, and both color and motion are processed before the first process terminates. We therefore conclude that there are two buffers which are capable of holding a sample of evidence about color or motion, respectively, while the other dimension is incorporated into the decision. This places the bottleneck between the buffered evidence and the representation of the decision variable. We believe the bottleneck arises because of an anatomical constraint. It is simply impossible to connect in parallel every possible source of evidence with the neural circuits responsible for representing a proposition or plan. As <xref ref-type="bibr" rid="bib108">Zylberberg et al., 2010</xref> theorized, the brain's routing problem holds the key to why many mental operations operate serially. We will return to this idea after interpreting our results in the context of the neurobiology of decision making. We do this by pursuing the neural correlates of a computational model that supports parallel acquisition of sensory evidence and its serial incorporation into two decisions.</p></sec><sec id="s3-2"><title>Connecting computational models to neurobiology</title><p>The operations depicted in <xref ref-type="fig" rid="fig8">Figure 8</xref> are intended to reconcile what is known about the neurobiology of simple 1D decisions with the constraints introduced by the double-decision task. The mathematical instantiation of the model requires only minor modifications of two bounded drift-diffusion processes with temporal multiplexing (see Materials and methods). However, the architecture implied by <xref ref-type="fig" rid="fig8">Figure 8B,C</xref> facilitates interpretation of the experimental findings in relation to neural processing. In the mathematical depiction of drift-diffusion, the momentary evidence is a biased Wiener process. However, in reality the stimulus is not a Wiener process, nor is the representation of momentary evidence by neurons (<xref ref-type="bibr" rid="bib110">Zylberberg et al., 2016</xref>), which arise through application of a transfer function that effectively spreads the impact of a pair of displaced dots over 100–150 ms (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, bottom; <xref ref-type="bibr" rid="bib2">Adelson and Bergen, 1985</xref>). Thus, the neural representation of the motion can be approximated by the leaky integral of a biased Wiener process (<xref ref-type="bibr" rid="bib12">Cain et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Barlow and Tripathy, 1997</xref>). Such smoothing would not be warranted for the detection of fast changes, but it is adequate for a signal that is to be integrated over time. We know less about the filtration of a color difference, but the same logic applies.</p><p>The conceptual transition from Wiener processes to discrete samples allows us to appreciate the similarity between the accumulation of evidence from movie-like stimuli and the broader class of decisions based on discrete samples of evidence from the environment and memory. This informs hypotheses about the neurobiology, because the sample of evidence ultimately bears on a decision in units of belief or relative value. That is obvious when considering a choice between items on a menu, but it has been camouflaged to some extent in the perceptual decision-making literature. This is in part because the time-integral of a difference in firing rates from right- and left-preferring neurons is the number of excess spikes for right, which is itself proportional to the accumulated log-likelihood ratio that this excess was observed because motion was in fact rightward (<xref ref-type="bibr" rid="bib33">Gold and Shadlen, 2001</xref>; <xref ref-type="bibr" rid="bib92">Shadlen et al., 2006</xref>; <xref ref-type="bibr" rid="bib7">Beck et al., 2008</xref>). For the wider class of decisions, such difference variables are elusive, whereas the possibility of associating a sample with log-likelihood is a natural dividend of learning and memory (<xref ref-type="bibr" rid="bib107">Yang and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib56">Kira et al., 2015</xref>; <xref ref-type="bibr" rid="bib93">Shadlen and Shohamy, 2016</xref>).</p><p>The results imply the maintenance of separate decision variables each capable of reconciling decision and choice for the one stimulus dimension. There must be separate control of termination and negligible cross talk. Specifically, the state of the accumulated evidence bearing on the direction of motion does not affect the amount of accumulated evidence required to reach a decision about color dominance, and the same can be said about the state of the accumulated evidence about color on the decision about direction of motion. In the model the decision variables, <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/></mml:mrow></mml:mstyle></mml:math></inline-formula>, represent the integrated evidence for right (and against left) and for blue (and against yellow). Neural correlates of these 1D processes are known, mainly in the parietal and prefrontal cortex (<xref ref-type="bibr" rid="bib34">Gold and Shadlen, 2007</xref>), although they are organized in pairs: <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (and presumably <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Each of the four processes is the accumulation of positive and negative increments, and each is terminated by an upper bound. Because evidence for <inline-formula><mml:math id="inf126"><mml:mi>R</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mi>L</mml:mi></mml:math></inline-formula> are anticorrelated (likewise for <inline-formula><mml:math id="inf128"><mml:mi>B</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf129"><mml:mi>Y</mml:mi></mml:math></inline-formula>), the pair of opposing processes is approximated by one-dimensional drift-diffusion to symmetric upper and lower terminating bounds. All model-fits adopt this approximation.</p><p>An alternative formulation, which we term target-wise integration, would accumulate evidence for the pair of features associated with each choice target (e.g. <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>). If such mechanism were to terminate when the total accumulation reaches a threshold, it would predict a type of choice-interference such that sensitivity to motion, say, would be impaired when the color strength was high, because the decision time is shortened by the stronger stimulus. We have not pursued all variants of target-wise integration, but critically, the bimanual experiment demonstrates that the double-decision comprises two terminating events.</p></sec><sec id="s3-3"><title>Integration as instruction</title><p>We find it useful to characterize integration as the implementation of a sequence of instructions to increment and decrement persistent activity in cortical areas that represent the decision variables. In <xref ref-type="fig" rid="fig8">Figure 8</xref>, the instructed change is realized by simple first order dynamics chosen to approximate neural responses from area LIP. The implementation is merely phenomenological, but it jibes with emerging ideas in theoretical neuroscience that characterize computation as a change in circuit configuration to establish stable states and dynamics (<xref ref-type="bibr" rid="bib89">Remington et al., 2018</xref>). For decision making, it replaces the requirement for continuous integration, with the realization of instructions as if drawn from a memory stage. This characterization also extends to the buffer.</p><p>Recall, the buffer was introduced to explain the observation that a brief pulse of color-motion, acquired in parallel, appears to be incorporated into the decision serially. We characterized the length of the buffer—its storage capacity—using the data from the variable duration experiment (<xref ref-type="fig" rid="fig4">Figure 4</xref>), where we equate it with the duration of parallel acquisition. This is reasonable because thereafter, the process is serial. However, this depiction appears to limit the role of the buffer to the beginning of the decision, and it fails to specify how long the information can be held. If there are alternations between color and motion processing before the first process terminates, as shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>, then information might be buffered beyond the initial parallel phase. As shown in <xref ref-type="fig" rid="fig8">Figure 8C</xref>, during alternation a sample might be held for at least <inline-formula><mml:math id="inf134"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>—that is, the time it takes the cleared sample to instruct the appropriate decision process and the time the bottleneck is in play while the other dimension performs its update. If the alternations are less frequent, the buffer might need to hold information longer, and if there is only one transition, then the buffer might be expected to hold a sample of information for the duration of the entire first decision (e.g. <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). There is presumably a limit on how long a sample can be stored, but studies of visual iconic memory suggest that a sample of evidence might be buffered for ∼500 ms (<xref ref-type="bibr" rid="bib96">Sperling, 1960</xref>; <xref ref-type="bibr" rid="bib32">Gegenfurtner and Kiper, 1992</xref>).</p><p>We conceive of the buffer residing between the cortical areas that represent the filtered evidence and other cortical circuits that represent the decision variables. Notice that the operations depicted in <xref ref-type="fig" rid="fig8">Figure 8</xref> assign two duties to the buffer: (i) storage of a sample of evidence while the bottleneck precludes updating the associated decision variable and (ii) conversion of the sample into an instruction to update a decision variable by <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula>. These duties could be carried out by different circuits. An appealing candidate for both operations is the striatum. The striatum receives input from the extrastriate visual cortex (<xref ref-type="bibr" rid="bib23">Ding and Gold, 2012b</xref>), and it is known to play a role in connecting value to action selection (<xref ref-type="bibr" rid="bib37">Hikosaka et al., 2014</xref>) as well as working memory (<xref ref-type="bibr" rid="bib3">Akhlaghpour et al., 2016</xref>). In the context of our results, we would characterize the operation as follows. A sample of filtered evidence, represented by the firing rates of neurons in extrastriate cortex (e.g. areas MT/MST) leads to a change in the state of a striatal circuit, such that its reactivation transmits the <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> instruction to the cortical areas that represent the decision variables, and this takes time (<inline-formula><mml:math id="inf137"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula>). On this view, the bottleneck is the striato-thalamo-cortical pathway. There has been an observation of the bottleneck in a split-brain patient, supporting such a subcortical bottleneck at least in certain instances (<xref ref-type="bibr" rid="bib82">Pashler et al., 1994</xref>).</p><p>A second possibility is that the buffered evidence is stored in visual cortical association areas, especially areas with persistent representations. For example, it has been suggested that short-term visual iconic memory is supported by the slowly decaying spike rates of neurons in area V2 (<xref ref-type="bibr" rid="bib78">O'Herron and von der Heydt, 2009</xref>) and the anterior superior temporal sulcus (STSa) (<xref ref-type="bibr" rid="bib49">Keysers et al., 2005</xref>). This would place the bottleneck between extrastriate cortex and the parietal and prefrontal areas that represent the decision variable (see also <xref ref-type="bibr" rid="bib69">Marti et al., 2012</xref>). This possibility does not provide an explanation for why communication between these areas would impose a substantial delay (e.g. <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula>).</p><p>A third possibility would identify the buffer with control circuitry within the very cortical areas that represent the decision variables. This might seem far-fetched but there is evidence for such an operation in the premotor cortex of mice, where it underlies the implementation of the logical ‘exclusive or’ (XOR) operation (<xref ref-type="bibr" rid="bib106">Wu et al., 2020</xref>). In that case the bottleneck would be intracortical. It would correspond to the implementation of a circuit state from its ‘silent’ representation—that is, in cellular and subcellular (e.g. synaptic) states rather than persistent spike activity (<xref ref-type="bibr" rid="bib73">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib67">Lundqvist et al., 2018</xref>). The bottleneck is the conversion from this state to the establishment of the spiking dynamics that instantiate the <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> instruction. This might resemble the recall of an associative memory, which must facilitate the establishment of cortical persistent activity in a state suitable for computation, be it for further updating or comparison to a criterion. The three possibilities are not mutually exclusive; nor are they exhaustive. In any case, the instigating event is the unblocking of the bottleneck, signaled by the circuit that receives the <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> instruction.</p></sec><sec id="s3-4"><title>The bottleneck</title><p>Up to now, we have alluded to the bottleneck as a temporary obstruction to color or motion processing, but the bottleneck itself does not add time. It is the instructive step that takes time (<inline-formula><mml:math id="inf141"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:math></inline-formula>). This step comprises the conversion of a sample of evidence to a <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> instruction and its transmission to a cortical circuit. Indeed, the same delay is encountered in simpler decisions. For example, in the 1D random dot motion task, the incorporation of evidence into the neural representation of the decision variable is first evident 80–100 ms after neurons in area MT exhibit direction selective responses (<xref ref-type="bibr" rid="bib19">de Lafuente et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Ding and Gold, 2012a</xref>; <xref ref-type="bibr" rid="bib54">Kim and Shadlen, 1999</xref>) and this delay holds for perturbations of the stimulus throughout decision formation (<xref ref-type="bibr" rid="bib39">Huk and Shadlen, 2005</xref>). This is too long to be explained by synaptic latencies. It implies either a complex routing through intermediate structures or more sophisticated processing that serves to facilitate the linkage and/or the conversion of the sample to an instruction suitable for establishing the cortical dynamics that ultimately realize the <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> instruction. The delay corresponds to the sum, <inline-formula><mml:math id="inf144"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>ins</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (circles 1 and 2 in <xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><p>Decision variables are represented in the persistent activity of neurons in the parietal and prefrontal cortex of primates. Such persistent activity is associated with working memory, attention and planning. This functional localization conforms to the notion of a ‘response selection’ bottleneck hypothesized by Harold Pashler to explain dual task interference (<xref ref-type="bibr" rid="bib81">Pashler, 1994</xref>), in particular a phenomenon known as the psychological refractory period (PRP): the prolonged latency of the second of two adjacent decisions without an effect on accuracy. In his and our formulation, it reflects a limitation that restricts the flow of information to affect higher processes such as decision-making and short-term working memory. On initial consideration, there is no obvious reason why the formation of working memory should necessitate a bottleneck. If acquisition can be parallel, why not working memory or the formation of a provisional plan or intention?</p><p>Framed in the language of decision-making, seriality arises as a consequence of limited connectivity between the brain's evidence acquisition systems—sensory, memory, and emotion—and the systems that represent information in an intentional frame of reference, that is, as provisional affordances. Any possible intention might be informed by a variety of sources of evidence, which may be acquired in parallel but from different locations in the brain. The brain lacks the anatomy to support independent connections from all sources of evidence to all possible intentions—that is, the circuits that represent them. Instead the communication must share connections, and this invites some form of time-slice multiplexing. It is not possible for every source of evidence to communicate with the circuits that form decisions at the same time. For some dedicated operations, it is likely that many sources of ‘evidence’ do converge on the same intentional circuitry (e.g. escape response; <xref ref-type="bibr" rid="bib28">Evans et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Lee et al., 2020</xref>), and the tracts can be established through development. But, for flexible cognitive systems that learn and solve problems, the connections between evidence and intention must be multipotent and malleable, since connecting <inline-formula><mml:math id="inf145"><mml:mi>N</mml:mi></mml:math></inline-formula> sources of evidence and <inline-formula><mml:math id="inf146"><mml:mi>M</mml:mi></mml:math></inline-formula> intentions will need at least <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> wires if they are connected exhaustively, whereas if they are routed centrally, it will only need <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>. This solution necessitates some type of multiplexing (<xref ref-type="bibr" rid="bib108">Zylberberg et al., 2010</xref>; <xref ref-type="bibr" rid="bib30">Feng et al., 2014</xref>; <xref ref-type="bibr" rid="bib77">Musslick et al., 2017</xref>).</p></sec><sec id="s3-5"><title>Relation to cognitive tasks</title><p>We suspect that the constraints leading to serial processing in the color-motion task also apply to other decisions and cognitive functions. For example, deciding between two familiar food items can take a surprisingly long time when those items are valued similarly. This holds when the items are both highly valued or both undesired or both of moderate value. Like decisions about the direction of random dot motion, there is a lawful relationship between the RT to choose an item and the likelihood that the preference is consistent with one's previously stated value (<xref ref-type="bibr" rid="bib58">Krajbich et al., 2010</xref>; <xref ref-type="bibr" rid="bib59">Krajbich and Rangel, 2011</xref>). Like the choice-RT accompanying 1D motion (or color) decisions, the relationship suggests that some type of process like noisy evidence accumulation—or more generally, sequential sampling with optional stopping—reconciles choice and decision time. However, such expressions of preference differ from perceptual decisions in two important ways. First, there is no objectively correct response, only consistency with the sign of the inequality in the decision-maker’s valuations of the individual items, which are ascertained before the experiment. Second, the food items are not shown as a movie and there is no uncertainty about their identity. Therefore, it is not clear what gives rise to independent samples of evidence. <xref ref-type="bibr" rid="bib4">Bakkour et al., 2019</xref> showed that the samples are likely to arise through constructive processes using hippocampal memory systems. This begs the question why this process would unfold in time like a movie of random dots. An attractive idea is that the use of memory guided valuation—in particular the step to enable it to affect a decision variable—encounters a bottleneck. Even if memories could be retrieved in parallel, they would require buffering and serial updates of the decision variable (<xref ref-type="bibr" rid="bib93">Shadlen and Shohamy, 2016</xref>).</p><p>While it is unsurprising that a movie of random dots supplies evidence to be incorporated serially toward a decision, it is shocking that two samples of evidence, supplied simultaneously by the same dots and acquired through parallel sensory channels, do not support simultaneous decisions. In the experiments that require prolonged viewing, non-simultaneity manifests in serial time-multiplexed alternation of the decision processes and the failure to incorporate all information in the stimulus stream into one or both decisions. In a free response design, the decision maker compensates by acquiring more evidence, so the interference is not apparent in the accuracy of the perceptual choice. However, if such compensation is precluded by the experimenter (variable duration experiment), the failure to incorporate information can affect accuracy too. That this bottleneck arises despite parallel acquisition of color and motion (or motion from two locations), whether we use one or two effectors to express the decision, and whether we decide between 2 × 2 conjunctions or two categories (same/different) suggests that the bottleneck is pervasive. In addition to the PRP, we suspect that it plays a role in other psychological phenomena, such as post-stimulus masking, the attentional blink (e.g. as shown in the rapid serial visual presentation tasks), and conjunction search (<xref ref-type="bibr" rid="bib85">Potter, 1976</xref>; <xref ref-type="bibr" rid="bib101">Treisman and Gelade, 1980</xref>; <xref ref-type="bibr" rid="bib50">Keysers and Perrett, 2002</xref>; <xref ref-type="bibr" rid="bib70">Marti et al., 2015</xref>; <xref ref-type="bibr" rid="bib71">Marti and Dehaene, 2017</xref>). These phenomena represent forms of sequential interference and all can be stated as challenges to the brain’s routing system (<xref ref-type="bibr" rid="bib108">Zylberberg et al., 2010</xref>).</p><p>On the other hand, one must wonder if the brain can ever take advantage of parallel acquisition to perform cognitive functions in parallel. It certainly seems so to a musician using their feet and hands to convey time and sonority on a piano or polyrhythm on a drum kit. Yet the time scales of alternation discussed in this paper are on the order of 10 Hz. It seems possible that we achieve parallel processing despite the bottleneck by enhancing signal-processing at the filter stage before the bottleneck and by grouping (or chunking) processes after the bottleneck in higher order controllers of movement and strategy. For example, face selective neurons compute conjunctions of features in less than 100 ms (<xref ref-type="bibr" rid="bib31">Freiwald and Tsao, 2010</xref>). This is just one example of the sophisticated properties of association sensory neurons in the extrastriate visual cortex, and analogous operations are presumed to occur in secondary somatosensory cortex and belt regions of the auditory cortex (e.g. <xref ref-type="bibr" rid="bib8">Bizley and Cohen, 2013</xref>; <xref ref-type="bibr" rid="bib74">Moses et al., 2016</xref>; <xref ref-type="bibr" rid="bib41">Jiang et al., 1997</xref>). Similarly, complex movement sequences and the rules to coordinate them may be specified in premotor cortex or at the level of the controller. If so, then the only way to overcome the bottleneck is to develop the expertise of the reader or the musician/athlete, leaving most of flexible cognition to negotiate the bottleneck between the acquisition of information and its incorporation into representations that support states of knowledge: decisions, working memory, plans of action. It is the price the brain pays to use its senses (and memory) to bear on a plethora of possible intentions, despite its limited connectivity. The payment is in time, but in another sense, it is time well spent, for without seriality of thought there is no contour to our experiences, no appreciation of cause and consequence, no meaning or narrative.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Thirteen participants (five male and eight female, age 23–40, median = 26, IQR = 25–32) provided written informed consent and took part in the study. All participants had normal or corrected-to-normal vision and were naïve about the hypotheses of the experiment. The study was approved by the local ethics committee (Institutional Review Board of Columbia University Medical Center).</p></sec><sec id="s4-2"><title>Apparatus</title><p>Visual stimuli were displayed on high resolution CRT monitors. The experiments were conducted in two labs. <xref ref-type="table" rid="table1">Table 1</xref> lists the display parameters used in all experiments. In the eye-tracking experiments, a head- and chin-rest was used, and eye position was monitored at 1 kHz using an Eyelink 1000 device (SR Research Ltd., Mississauga, Ontario, Canada). In the reaching task participants used robotic handles (vBots, <xref ref-type="bibr" rid="bib38">Howard et al., 2009</xref>) to indicate their choices, and movement trajectories were recorded at 1 kHz. The experiments were run using Matlab and Psychtoolbox (<xref ref-type="bibr" rid="bib10">Brainard, 1997</xref>) and for the online experiments jsPsych (<xref ref-type="bibr" rid="bib20">de Leeuw, 2015</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Experimental parameters.</title><p>Experiment 1. Double-decision reaction time (eye and unimanual), Experiment 2. Brief stimulus presentation (eye), Experiment 3. Variable-duration stimulus presentation (eye), Experiment 4. Two-effector double-decision reaction time (bimanual) and Experiment 5. Binary-response double-decision reaction time.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Exp 1.-eye</th><th>Exp 2.</th><th>Exp 3.</th><th>Exp 1.-uni. &amp; Exp 4.</th><th>Exp 5.</th></tr></thead><tbody><tr><td>Dot density (dots deg<sup>-2</sup> s<sup>-1</sup>)</td><td>15.3</td><td>15.3</td><td>16</td><td>16</td><td>16</td></tr><tr><td>Dot speed (deg/s)</td><td>1.67</td><td>1.67</td><td>5</td><td>5</td><td>5</td></tr><tr><td>Dot diameter (deg)</td><td>0.075</td><td>0.075</td><td>0.061</td><td>0.082</td><td>S7: 0.098; S12: 0.119</td></tr><tr><td>Fixation marker diameter (deg)</td><td>0.4 gray circle</td><td>0.4 gray circle</td><td>0.6 red cross and bullseye</td><td>0.6 red cross and bullseye</td><td>0.6 red cross and bullseye</td></tr><tr><td>Random delay (s)</td><td>0.1–0.5</td><td>0.1–0.5</td><td>0.5–0.8</td><td>0.5–0.8</td><td>0.4–0.8</td></tr><tr><td>Visual target diameter (deg)</td><td>0.4</td><td>0.4</td><td>1.2</td><td>N/A</td><td>N/A</td></tr><tr><td>Target eccentricity (deg)</td><td>6</td><td>6</td><td>15</td><td>N/A</td><td>N/A</td></tr><tr><td>Movement initiation</td><td>gaze &gt; 2.5°</td><td>gaze &gt; 2.5°</td><td>gaze &gt; 3°</td><td>hand &gt; 1 cm</td><td>key press</td></tr><tr><td>Target detection window (radius)</td><td>3°</td><td>3°</td><td>2.4°</td><td>0.75 cm</td><td>N/A</td></tr><tr><td>CRT</td><td>Vision Master 1451</td><td>Vision Master 1451</td><td>Sony CRT CPD-G420S</td><td>Dell CRT P1110</td><td>N/A</td></tr><tr><td>Refresh rate (Hz)</td><td>75</td><td>75</td><td>75</td><td>75</td><td>60</td></tr><tr><td>Resolution (pixels)</td><td>1400 × 1050</td><td>1400 × 1050</td><td>1280 × 1024</td><td>1280 × 1024</td><td>S7: 1280 × 720; S12: 1440 × 900</td></tr><tr><td>Pixels per degree</td><td>39.6</td><td>39.6</td><td>32.7</td><td>24.3</td><td>S7: 40.94; S12: 33.45</td></tr><tr><td>Viewing distance (cm)</td><td>55</td><td>55</td><td>50</td><td>38</td><td>S7: 54; S12: 38</td></tr><tr><td>Blue <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/></mml:mrow></mml:mstyle></mml:math></inline-formula> [M(SD)]</td><td>N/A</td><td>N/A</td><td>25.20 (0.81)</td><td>12.16 (1.93)</td><td>N/A</td></tr><tr><td>Blue CIE x/y [M]</td><td>N/A</td><td>N/A</td><td>x = 0.26, y = 0.24</td><td>x = 0.27, y = 0.24</td><td>N/A</td></tr><tr><td>Yellow <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/></mml:mrow></mml:mstyle></mml:math></inline-formula> [M(SD)]</td><td>N/A</td><td>N/A</td><td>22.98 (0.05)</td><td>12.68 (1.80)</td><td>N/A</td></tr><tr><td>Yellow CIE x/y [M]</td><td>N/A</td><td>N/A</td><td>x = 0.54, y = 0.38</td><td>x = 0.54, y = 0.38</td><td>N/A</td></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>Overview of experimental tasks</title><p>Participants sat in a semi-dark booth in front of a CRT monitor. They were required to decide the net direction and the dominant color in a patch of dynamic random dots. Individual dots were displayed for a single video frame (1/75 s). Task difficulty for motion was conferred by the probability that in frame <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> (i.e. <inline-formula><mml:math id="inf152"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> ms), it would be displaced in apparent motion vs. randomly replaced in the aperture. We prepend the probability by plus or minus to indicate the direction, and refer to this signed quantity in units of coherence (coh). For color, task difficulty was conferred by the probability that a dot would be colored blue or yellow on each frame. We refer to the signed quantity, <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, as the color coherence. Both coherences share the range <inline-formula><mml:math id="inf154"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. Throughout, we use positive coherence for rightward and blue dominant stimuli. The coherences were stationary during a trial but randomized independently across trials. A calibration procedure was used to match the luminance of the blue and yellow for each participant (see below). For the first experiment (choice-RT, participants S1–﻿3), the color of the dots in the first three frames of a trial was balanced to give no net color information. The procedure was intended to match the state of the motion stimulus which is effectively zero-coherence until the fourth video frame. Subsequent experience demonstrated that this procedure was unnecessary, and we discontinued this practice for the other experiments.</p><p>We conducted three types of tasks: <inline-formula><mml:math id="inf155"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> double-decision (2D), in which both the dominant color and motion direction were reported on each trial (color-motion task, Experiments 1–4); <inline-formula><mml:math id="inf156"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> single-decision (1D), in which only the dominant color or net motion direction were reported, as in <xref ref-type="bibr" rid="bib68">Mante et al., 2013</xref> (motion-only and color-only tasks, Experiments 1–3; motion-only, Experiment 5); and <inline-formula><mml:math id="inf157"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> same vs. different, in which the directions of motion in two patches of random dots are compared (Experiment 5). For the color-motion task, four choice targets appeared at four corners of the display, evenly spaced from each other and the same distance from the fixation spot. The top two targets were colored yellow and the bottom two blue, consistent with the color choices they indicate. For example, to report rightward motion and yellow color, the participant would saccade (Experiment 1 [eye], 2. and 3) or reach (Experiments 1 [hand] and 4) to the top right target, which was yellow. For the motion-only task, two white targets were shown to the left and right of the stimulus. For the color-only task, one blue and one yellow target were presented above and below the center of the screen. For the 1D task, the signed coherence of the ‘irrelevant’ dimension was varied from trial to trial just as in the 2D task. For the same vs. different task, participants reported by pressing one of two keys.</p><p>Note that in general, the 1D experiments do not provide a principled comparison to the 2D decision tasks (<xref ref-type="bibr" rid="bib83">Pashler, 1999</xref>; <xref ref-type="bibr" rid="bib72">McLeod, 1977</xref>). The error rate on difficult conditions are different (0.5 and 0.75 for 1D and 2D, respectively), and therefore there is no reason to expect a decision maker to implement the same speed-accuracy settings in the two contexts (<xref ref-type="bibr" rid="bib18">Churchland et al., 2008</xref>; <xref ref-type="bibr" rid="bib102">Usher et al., 2002</xref>; <xref ref-type="bibr" rid="bib24">Ditterich, 2010</xref>). This consideration applies to the RT experiments, naturally, but also to experiments in which the duration is controlled experimentally (e.g. Experiment 3), where it is known that decision makers also control termination criteria (<xref ref-type="bibr" rid="bib51">Kiani et al., 2008</xref>; <xref ref-type="bibr" rid="bib44">Kang et al., 2017</xref>). This consideration does not apply to the experiment using very brief stimuli (Experiment 2) because differences in termination criteria would not be expected to play a role; all difficult trials would benefit from more information. Therefore, except for Experiment 2, we do not dwell on the results of the 1D experiments in this paper (but see <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref> and <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>). The data are also included in the online materials.</p><p>For each experiment, the sample size was determined based on prior psychophysics studies with within-subject designs (<xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>; <xref ref-type="bibr" rid="bib90">Resulaj et al., 2009</xref>; <xref ref-type="bibr" rid="bib109">Zylberberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib52">Kiani et al., 2014</xref>). We recruited three participants for the first and second experiment (Choice-reaction time task and short duration, eye). For the remaining experiments we recruited 2–8 participants. A larger number was necessary for the arm experiments because fewer trials per hour are acquired and the effort is greater. Unless otherwise stated, participants were randomly allocated to experiments.</p></sec><sec id="s4-4"><title>Experiment 1. Double-decision reaction time (eye and unimanual)</title><p>Two versions of this experiment were conducted, one where participants indicated their decision with an eye movement (Exp. 1-eye) and one where they indicated their decision with a unimanual reach (Exp. 1-unimanual). Here, we describe the Methods for the eye movement task. The methods for the unimanual task are described with Experiment 4 - Two-effector double-decision reaction time (bimanual).</p><p>Three participants (1 male and 2 female, aged 25–40) performed the task in which they could view the random dots until ready with a response (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Participants were required to fixate a central spot for 0.5 s to initiate a trial. After a random delay, a patch of dynamic random dots appeared which were restricted to an invisible circular aperture (5° diameter), centered on the fixation spot. The random dots were extinguished when the participant initiated the choice response. Participants were required to respond within 5 s of the stimulus onset. Trials in which no response was initiated and those aborted by breaking fixation were repeated at a later time in the experiment. When the participant indicated the decision, a gray circle was drawn around the correct choice. The participant earned one point if both decisions were correct; and otherwise they lost one point. The two outcomes were signaled by different sounds. Participants were instructed to maximize points earned per time by responding as fast and accurately as possible. To encourage this, the points accumulated in the current block, as well as a graph of their scores in each 1 min period in the block, were displayed as feedback.</p><p>Participants performed three trial types—1D color-only, 1D motion-only and 2D color-motion decisions—in 24–49 interleaved blocks (∼13 min, 200 trials) over 11–17 days. Five levels of difficulty (i.e. nine signed coherences including 0) for color and motion were employed for all trial types, although for the 1D task, two of the strengths were not used for the irrelevant dimension. The set of non-zero motion strengths was doubled for one participant (S1) because they failed to achieve &gt;90% correct at coherence = 0.256 during training. Likewise, the range of color strengths was doubled for two participants (S1 and S3). We therefore use a normalized scale for plotting the data in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. The actual coherence values (<xref ref-type="table" rid="table2">Table 2</xref>) are used in all fits.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Motion and color strength parameters.</title><p>For 2D trials all combinations of motion and color strengths were used. For 1D trials all strengths were used for the dimension that informed the decision but some strengths (<inline-formula><mml:math id="inf158"><mml:msup><mml:mi/><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>) were omitted for the other dimension.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Experiment</th><th>Participant</th><th>Motion strengths</th><th>Color strengths</th></tr></thead><tbody><tr><td rowspan="3">1. Double-decision RT (eye)</td><td>S1</td><td>0, 0.064*, 0.128, 0.256*, 0.512</td><td>0, 0.062*, 0.124, 0.245*, 0.462</td></tr><tr><td>S2</td><td>0, 0.032*, 0.064, 0.128*, 0.256</td><td>0, 0.031*, 0.062, 0.124*, 0.245</td></tr><tr><td>S3</td><td>0, 0.032*, 0.064, 0.128*, 0.256</td><td>0, 0.062*, 0.124, 0.245*, 0.462</td></tr><tr><td>2. Brief stimulus presentation (eye)</td><td>S1–3</td><td>0, 0.064*, 0.128, 0.256*, 0.512</td><td>0, 0.124*, 0.245, 0.462*, 0.762</td></tr><tr><td rowspan="2">3. Variable-duration stimulus presentation (eye)</td><td>S4</td><td>0.03, 0.063, 0.512</td><td>0.052, 0.104, 0.758</td></tr><tr><td>S5</td><td>0.044, 0.084, 0.512</td><td>0.046, 0.104, 0.758</td></tr><tr><td>4. Two-effector double-decision reaction time (bimanual, same strengths for unimanual)</td><td>S6–13</td><td>0, 0.032, 0.064, 0.128, 0.256, 0.512</td><td>0, 0.064, 0.128, 0.250, 0.472, 0.758</td></tr><tr><td>5. Binary-response double-decision RT</td><td>S7 and S12</td><td>0.128, 0.256, 0.512</td><td>N/A</td></tr></tbody></table></table-wrap><p>The instructions for the 2D task encouraged participants to make a combined judgment, rather than judging color and motion serially (verbal instruction provided by experimenter: “Your task is to answer based on both motion and color. When the motion is right and color is yellow, the answer is top right, and when the motion is right and color is blue, the answer is bottom right.”). Participant S1’s performance on the motion was at chance level in the last three blocks (one motion-only and two color-motion decision blocks; accuracy = 38–50% within each of the three blocks, containing 287 out of 4911 trials performed by S1). Those blocks were excluded from all analyses (minimum accuracy [motion or color] of all other blocks of S1 and every block of other participants: 68%), leaving 4624–10,969 trials across the participants (2491–5968 1D trials and 2133–5001 2D trials).</p><sec id="s4-4-1"><title>Training sessions</title><p>Participants completed 11–13 training blocks (13 min) over 4–7 days, beginning with either an easy motion or color 1D task. S1 and S2 began training with the 1D color-only task; S3 began training with 1D motion-only. All were then trained on the other 1D task, before they were trained on the 2D task. For initial training, viewing duration was controlled by the experimenter. The incorporation of weaker stimulus strengths and the range of stimulus durations were adjusted progressively. Transitions to the next level were made if the participant met fixation requirements and achieved &gt;90% accuracy on the strongest coherence. The aim was to identify four levels of motion strength <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi/><mml:mo>≥</mml:mo><mml:mn>0.032</mml:mn></mml:mrow></mml:math></inline-formula> and four levels of color strength <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi/><mml:mo>≥</mml:mo><mml:mn>0.031</mml:mn></mml:mrow></mml:math></inline-formula> in octave steps such that the strongest level (eight times the lowest logit) supported &gt;90% accuracy. We then changed from variable duration to the RT version of the 1D task, again ensuring that the range of difficulties led to at least 90% accuracy for the easiest condition. We then repeated these steps for the other stimulus dimension before introducing the 2D choice-RT task. They received a session of practice to gain familiarity with the 4-choice design. For participants S1 and S3, we made a final adjustment of the difficulty levels. The stimulus strengths were then fixed for all test sessions (<xref ref-type="table" rid="table2">Table 2</xref>).</p></sec><sec id="s4-4-2"><title>Minimum-motion procedure</title><p>Prior to the experiment, we calibrated the two colors (yellow and blue) to be equiluminant using the minimum-motion procedure (<xref ref-type="bibr" rid="bib16">Cavanagh et al., 1987</xref>). A flickering vertical sinusoidal grating (spatial frequency 1.25 cyc/deg; temporal frequency 6.25 Hz) is composed as the sum of a blue/yellow counterphase chromatic grating and a light-green/dark-green counterphase luminance grating. The spatial and temporal phases of the gratings differ by <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> radians, like the functions <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. If yellow is more luminant than blue, the grating appears to move in one direction (e.g. left), and vice versa. Participants adjusted the luminance of yellow until they did not see motion. Each participants repeated this procedure 24 times, starting from a random luminance value. The mean luminance of yellow was then used throughout the experiment.</p></sec></sec><sec id="s4-5"><title>Experiment 2. Brief stimulus presentation (eye)</title><p>The same participants from Exp. 1-eye then performed a task that was identical except that the dynamic random dots turned off after 120 ms from the onset. Participants were free to respond after the offset of the dynamic random dots. The RT in this task was measured as the time between the onset of the stimulus and the response (the time the gaze left the center of the screen). Participants completed 35–43 test blocks (∼13 min) over 12–19 days, comprising 7309–7745 trials (4056–4192 1D trials and 3240–3466 2D trials). The stimulus strengths are in <xref ref-type="table" rid="table2">Table 2</xref>.</p></sec><sec id="s4-6"><title>Experiment 3. Variable-duration stimulus presentation (eye)</title><p>Two participants (female, aged 26 and 32; both right-handed) completed 12–26 sessions (after the training sessions), each requiring 1–2 hr. The task alternated between blocks of 72–144 trials where participants either performed the 2D variable duration task, a 1D variable duration task or a 2D choice-RT time task. The majority of blocks were 2D variable duration (∼11,800 trials per participant). Ten stimulus durations, ranging from 120 to 1200 ms (in steps of 120 ms), were presented in pseudo-random order. Warning messages were displayed if participants initiated an eye movement before the end of the stimulus (‘too early!') or if a movement was not initiated within five seconds of stimulus offset (‘too slow!'). In both cases, the trial was aborted and repeated at a later, randomly determined, point within the same block. Participants received auditory and visual feedback for correct and error trials, as in Experiment 1. They were instructed to be as accurate as possible and received feedback about the percentage of correct choices for each decision dimension at the end of each block.</p><p>Only three levels of difficulty were used for each dimension: one easy and two difficult coherence levels. The easy coherence level was 0.512 for motion and 0.758 for color. The two difficult coherence levels were adjusted individually in order to match color and motion performance. Specifically, low coherences (hard) for each dimension were chosen to yield 65% and 80% accuracy on each dimension, respectively, based on participants' performance in the final two training sessions (2D choice-RT). All 6 × 6 combinations of signed motion × signed color were presented (see <xref ref-type="table" rid="table2">Table 2</xref>). Since the main model predictions are based on a comparison of trials with hard-hard vs. hard-easy combinations, easy-easy combinations were only presented in ∼2.4% of trials. All other coherence combinations were presented with equal frequency and counter-balanced within each stimulus duration. Participants also completed 2160 trials each of motion-only and color-only trials and 1296 trials of the 2D choice-RT task which were included to ensure that they maintained appropriate speed-accuracy trade-offs throughout the experiment.</p><sec id="s4-6-1"><title>Training sessions</title><p>Participants first completed 6–9 training sessions. In the first two sessions, they were trained on a variable duration task. Stimulus durations were drawn randomly from a truncated exponential distribution ranging between 500 and 2000 ms (session 1) or 100 and 1600 ms (session 2). Participants first completed 1D-motion and 1D-color tasks in separate blocks (order counterbalanced across participants), followed by the 2D task. The following instructions for the 2D task were presented on the screen: “Your task is to judge both the direction and color of the dots by moving your eyes to the corresponding left/right, yellow/blue target'. In the remaining training sessions, participants mainly performed a 2D RT task until they reached stable performance (at least 60% accuracy on the second coherence level for both decision dimensions, with little change in choice performance or RTs over blocks). Occasionally, additional 1D blocks were introduced to attain similar performance levels for motion and color judgments. Throughout training, all six coherence levels for motion <inline-formula><mml:math id="inf164"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.032</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.064</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.128</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.256</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.512</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and color <inline-formula><mml:math id="inf165"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.064</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.128</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.250</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.472</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.758</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, and all their possible pairwise combinations, were presented. For double decisions, this makes 121 unique signed-coherence combinations.</p></sec><sec id="s4-6-2"><title>Isoluminance calibration</title><p>At the start of the experiment, participants completed a flicker fusion procedure to match luminance of yellow and blue. A square (<inline-formula><mml:math id="inf166"><mml:mrow><mml:msup><mml:mn>4.9</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mn>4.9</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) was presented in the center of the screen. The color of the square flickered at 37.5 Hz between blue and yellow. For efficiency, we only explored values <inline-formula><mml:math id="inf167"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mi>R</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:mi>G</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:mi>x</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf168"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mi>R</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:mi>G</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mi>y</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mn> 0</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for blue and yellow, respectively, where <inline-formula><mml:math id="inf169"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>ℕ</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>,</mml:mo><mml:mn>255</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Participants pressed the left or right arrow key to minimize the perceived flicker. One key changed <inline-formula><mml:math id="inf170"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf171"><mml:mi>y</mml:mi></mml:math></inline-formula> by +1 and −1, respectively, and the other key had the opposite effect. Participants pressed the space bar to signal the subjective point of minimal flicker. This procedure was repeated 10 times, each time starting with new initial values <inline-formula><mml:math id="inf172"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mi>x</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, chosen pseudo-randomly, such that either yellow or blue began darker (counter-balanced across trials). The precise initial values were equidistant from 225: between 195 and 200 for the darker color and 250 and 255 for the brighter color (e.g. blue: [0 197 197]; yellow: [253 253 0]). This ensured sufficient contrast to induce the perception of flicker at the start of each trial. The averages across the 10 trials were adopted as the isoluminant setting for the participant. After the procedure, participants were <bold>pr</bold>esented with a single trial with the obtained color values and were asked to report if they perceived flicker. If they did, the procedure would be repeated; but this never occured. The same calibration procedure was also used for Experiment 4.</p></sec></sec><sec id="s4-7"><title>Experiment 4. Two-effector double-decision reaction time (bimanual)</title><p>This experiment examined a double-decision task in which the report was made with a single effector (unimanual) or with two effectors (bimanual). We present the unimanual results with Experiment 1, although we describe the methods here.</p><p>Twelve right-handed participants were initially recruited for the experiment. After training, eight participants were selected for the actual experimental sessions based on their overall performance. Participants completed two test sessions with a unimanual version of the task and two test sessions with a bimanual version (order counterbalanced across participants). In each experimental session, all 121 color-motion combinations (see <xref ref-type="table" rid="table2">Table 2</xref>) were presented pseudo-randomly in 12 blocks of 96 trials each (total of 1152 trials per participant).</p><p>Unlike the eye experiments, no choice targets were present on the screen. Instead, there were arrow icons that indicated the mapping of color and motion to forward/backward (appropriately colored) and left/right directions of the hand (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The mapping of blue/yellow to bottom/top target locations was counterbalanced across participants. The movements themselves were restricted to virtual channels in the plane. In the unimanual task, participants moved a single robotic handle with either their left or right hand (counterbalanced across each half of a session) in one of the four diagonal target directions (2 color × 2 motion; as in the other experiments). In the bimanual task, participants used two separate robotic handles to move their left and right hands in a left/right (motion judgments) and forward/backward (color judgments) direction, respectively (hand assignments counterbalanced across participants). Feedback about the hand position(s) was provided by two black bars on top of the arrow icons (for clarity shown as gray in <xref ref-type="fig" rid="fig5">Figure 5A</xref>). Participants were instructed to move each bar in the chosen direction until their hand(s) reached a virtual ‘wall’ at the end of the channel, at which point their decisions were registered. Movement distances between starting positions and target locations were identical in the uni- and bimanual task (5 cm). On 2D trials, the random dots were extinguished when both decisions were indicated, that is when the hand left the home position in the unimanual task and when both hands had left the home position in the bimanual task. Warning messages were presented if participants initiated a response before stimulus onset or within 200 ms of stimulus onset (‘too early’) or when RTs exceeded 5 s (‘too slow!'). In both cases, the trial was aborted and was repeated at a later, randomly determined, trial within the same block.</p><p>Once participants indicated their decision, a green or red frame appeared around each response arrow to indicate whether the choice on the corresponding dimension was correct. If both decisions were correct, additional auditory feedback was provided (700 Hz tone) and the participant earned one point. Participants were instructed to maximize points by responding as fast and accurately as possible. At the end of each trial, they received feedback regarding their current rate of rewards (points/min) as well as a graph of their scores in each 2 min period over the last 10 min. To further motivate participants to adopt appropriate speed-accuracy trade-offs, the feedback duration was longer for errors than correct responses, thus delaying the onset of the next trial (correct: 1.25 s; error on one dimension: 2 s; error on both dimensions: 3 s). At the end of the trial the robotic interface actively moved the hand(s) back to the home position(s).</p><sec id="s4-7-1"><title>Training</title><p>All participants completed three to four initial training sessions, using the version of the task that they were assigned to first (uni- or bimanual, counterbalanced). In the first two training sessions, participants performed a variable duration task with stimulus durations varying between 500 and 2000 ms. The third training session introduced the choice-RT design. To train participants to maximally separate their two hands in the bimanual version, the RT training task alternated between easy motion and easy color blocks. Participants were encouraged to respond as quickly as possible to the easy dimension while taking more time to make a correct choice on the harder dimension. For participants who were first trained on the unimanual version, stimulus coherences were also presented in blocks of easy motion and easy color to ensure consistency in training across all participants. Participants were invited for the experimental sessions only if their overall rate of warning messages was less than 5% and if their average accuracy was at least 95% on the easy dimension and at least 65% on the 3rd highest coherence level of the harder dimension (e.g. motion: 0.064; color: 0.128).</p><p>All participants were immediately trained on the 2D task (without prior 1D training) in order to familiarize them with the motor response required to indicate their double decisions. The following instructions were presented on the screen: 'You will see some dots 'swirling in the wind'. Your task is to judge whether the wind is blowing the dots more strongly to the left or right AND whether the majority of dots is blue or yellow'.</p><p>After initial training, participants completed two experimental sessions of the task they had been trained on (either uni- or bimanual RT task). They then completed another practice session, in which they were trained on the other version of the task (either bi- or unimanual RT task), before completing two final experimental sessions with this version of the task. Experimental sessions only differed in motor implementation of decisions (uni- vs. bimanual), but were otherwise identical, and S-R mappings were kept consistent. Participants typically mastered the movements required for both the uni- and bimanual task within the first few blocks of training. All participants included in the final sample were able to report decisions for the high-coherence stimulus component in both the uni- and bimanual task with accuracy &gt;95%.</p><p>Note that the data from the unimanual task are presented in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, (Exp. 1-unimanual). The fits in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, but not the data, are also displayed in <xref ref-type="fig" rid="fig5">Figure 5B</xref>.</p></sec></sec><sec id="s4-8"><title>Experiment 5. Binary-response double-decision reaction time</title><p>The experiment was conducted remotely during the SARS-CoV-2 pandemic (summer 2020). Two participants who had also completed the uni- and bimanual tasks were recruited for this experiment. Participants completed the task online using a Google Chrome browser on Windows 10 and macOS Catalina (version 10.15.4), respectively. Both participants completed eight separate 1 hr sessions within a 2-week time period. The task was programmed in JavaScript and jsPsych (<xref ref-type="bibr" rid="bib20">de Leeuw, 2015</xref>).</p><p>During the task, two random dot motion patches with rectangular apertures (<inline-formula><mml:math id="inf173"><mml:mrow><mml:msup><mml:mn>3</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mn>5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) were presented to the left and right of a red fixation cross and separated by a central gray bar (<inline-formula><mml:math id="inf174"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mn>5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Motion direction (up/down) and coherence (<inline-formula><mml:math id="inf175"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.128</mml:mn><mml:mo>,</mml:mo><mml:mn>0.256</mml:mn><mml:mo>,</mml:mo><mml:mn>0.512</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, referred to as low, medium, and high) of the two stimuli were independent of each other. The six unique signed coherence combinations were presented with equal frequency and in randomized order. The stimuli directions and allocation to the left vs. right side of the screen were counterbalanced. Participants had to judge whether the dominant motion directions of the two stimuli were the same or different and indicate their choice by pressing the F or J key with their left or right index finger, respectively, when ready. The response mapping was counterbalanced across the two participants and was shown at the bottom of the screen throughout the task. Visual feedback was provided at the end of each trial. For correct responses, participants won one point. After errors and miss trials (too early/late), participants lost one point. Miss trials were repeated later during the same block. Participants were instructed to try and win as many points as possible and they received an extra bonus of one cent for every point they won. Their point score was shown in the corner of the screen throughout the task and additional feedback about percent accuracy was provided at the end of every block.</p><p>Participants first completed three training sessions in which they started on the 2D task without prior 1D training. Participants received the following instructions (shown on screen): 'Your task is to judge whether the dominant motion direction of the dots on the left side is the SAME or DIFFERENT than the dominant motion direction of the dots on the right'. After training, they completed four test sessions of the same-different task (3072 trials). Finally, participants completed a single session (768 trials) of a 1D task in which the random dot motion was restricted to the left or right patch (counterbalanced across trials) and participants had to judge the motion direction (up vs. down) by pressing the M or K key using their right index and middle finger.</p><p>At the end of each session, participants completed a separate block of 32 trials with 100% coherence stimuli only (sessions 1–7: same-different task; sessions 8: 1D task). Participants were instructed that decisions in this block would be very easy and that they should respond as fast as they could while still being accurate. The RTs obtained from these blocks (not shown) serve as a check on our estimate of the non-decision time (<xref ref-type="bibr" rid="bib97">Stine et al., 2020</xref>), but they were not used in the analyses. Participants were instructed to maintain fixation throughout the task. At the end of each session, they provided self-report judgments indicating to what extent they kept fixation during the task on a scale from 1 (‘not at all’) to 4 (‘always’). The mean and interquartile range of the reports were 3.75 and 3.5–4 (combined for the two participants). Prior to the experiment, participants completed a virtual chin-rest procedure in order to estimate viewing distance and calibrate the screen pixels per degree (<xref ref-type="bibr" rid="bib62">Li et al., 2020</xref>). This involves first adjusting objects of known size displayed on the screen to match their physical size and then measuring the horizontal distance from fixation to the blind spot on the screen (<inline-formula><mml:math id="inf176"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msup><mml:mn>13.5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-9"><title>Serial and parallel drift diffusion models</title><p>Both the serial and parallel models assume that decisions are based on the accumulation of evidence over time. The decision processes for color and motion are described by two independent Wiener processes with drift. The decision variable for one of the dimensions (here motion), evolves according to the sum of a deterministic and a stochastic component:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msqrt><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The deterministic term depends on the drift <inline-formula><mml:math id="inf177"><mml:msub><mml:mi>μ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> is the stimulus motion strength (signed coherence). By convention, <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> is positive (negative) when the motion is to the right (left). <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a parameter that converts coherence to a signal-to-noise ratio, which we fit to the data. <inline-formula><mml:math id="inf181"><mml:msubsup><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:math></inline-formula> is a bias that allows us to explain, for example, why left and right responses may not be equiprobable even when there is no net motion in either direction. We model the bias term as an offset in the coherence rather than the starting point of the accumulation. This approximates the optimal way of incorporating a bias in drift-diffusion models when there is uncertainty about the reliability of evidence (e.g. the coherence levels vary across trials) (<xref ref-type="bibr" rid="bib35">Hanks et al., 2011</xref>; <xref ref-type="bibr" rid="bib111">Zylberberg et al., 2018</xref>).</p><p>The second term of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> describes the stochasticity that affects the evolution of the decision variable. It captures the variability introduced by the stimulus and the brain. This variability is modeled as samples from a normal distribution with zero mean. By convention, the standard deviation is <inline-formula><mml:math id="inf182"><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:math></inline-formula>, which results in the variance of the decision variable equal to 1 after accumulating evidence for 1 s. This choice does not lead to any loss of generality since for any other value it would be possible to define a new model that has the same behavior in which the variance is 1 and the other parameters are a scaled version of the original ones (<xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>). The assumption is restrictive, however, in its requirement that the variance is the same for all motion and color strengths. Based on recordings from direction selective neurons in macaque extrastriate cortex, it seems likely that the variance of the momentary evidence increases with stronger motion, owing to the imbalance between the response of neurons to motion in their preferred vs. antipreferred direction (<xref ref-type="bibr" rid="bib11">Britten et al., 1993</xref>; <xref ref-type="bibr" rid="bib92">Shadlen et al., 2006</xref>). We suspect this leads to an overestimate of the variance at low motion strengths, consistent with underestimation of RT in these conditions (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>The accumulation process stops and a decision is made when the accumulated evidence reaches one of two bounds. The choice is ‘rightward' if the decision terminates at the upper bound, and ‘leftward' if it terminates at the lower bound. The decision time is the time <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> that it takes the decision variable to cross the bound. The upper and lower bounds are assumed symmetric with respect to zero. To explain why errors are (often) slower than correct responses, the bounds are allowed to collapse over time. We parameterize the bound as a logistic function with slope <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The bound reaches a value of <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and approaches 0 as <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>t</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>with lower bound simply <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The collapsing bounds explain slower errors because the smaller <inline-formula><mml:math id="inf189"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> at later time induces an increased probability that the <inline-formula><mml:math id="inf190"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> will terminate in the bound opposite the sign of drift. Although we regard decisions about 0% coherence as neither correct nor incorrect, any bias implies a drift rate <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). This leads to a small discrepancy between the model fits to the mean RT on these trials (<xref ref-type="fig" rid="fig2">Figure 2</xref>), because the curves are predictions of the mean double-decision RT for correct choices, but the data plotted at 0% coherence include all trials, just under half of which are effectively slow errors (i.e. choices opposite the bias). This issue affects only the plots of mean RT. All statistical analyses use estimates of the joint probabilities of decision time and choice for each trial.</p><p>The same equations describe the decision process for color. We use subscript <inline-formula><mml:math id="inf192"><mml:mi>c</mml:mi></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf193"><mml:mi>m</mml:mi></mml:math></inline-formula> to refer to the color decision, and adopt the convention that positive (negative) evidence supports the blue (yellow) choice. Given a set of parameters (<inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>), where <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can estimate the probability density function for the decision times <inline-formula><mml:math id="inf196"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:msub></mml:math></inline-formula>, and the two possible choices <inline-formula><mml:math id="inf197"><mml:mi>R</mml:mi></mml:math></inline-formula> (right/left for motion and blue/yellow for color). This density function, denoted <inline-formula><mml:math id="inf198"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>x</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, depends on the signed stimulus coherence, <inline-formula><mml:math id="inf199"><mml:mi>s</mml:mi></mml:math></inline-formula>. We obtain it by numerically solving the Fokker-Planck equation associated with the Wiener process with drift (<xref ref-type="bibr" rid="bib53">Kiani and Shadlen, 2009</xref>), using the numerical method of <xref ref-type="bibr" rid="bib17">Chang and Cooper, 1970</xref>.</p><p>So far, the model description applies to single decisions about motion. The serial and parallel models explain how these components are combined to form color-motion double-decisions. In the serial model the accumulation of evidence at any time can only be for color or motion. Therefore, the total decision time <inline-formula><mml:math id="inf200"><mml:mi>T</mml:mi></mml:math></inline-formula> is the sum of the decision times for motion (<inline-formula><mml:math id="inf201"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>) and color (<inline-formula><mml:math id="inf202"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>), and the distribution of decision time is given by:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>serial</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf203"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula> and, <inline-formula><mml:math id="inf204"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> are the responses (i.e., choices) for color and motion, respectively, and ∗ denotes convolution.</p><p>In the parallel model, both the motion and color are processed simultaneously. Therefore, the decision time is the maximum of either decision time: <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We can numerically derive the distribution of decision times from the single-modality distributions by noting that the decision time is equal to <inline-formula><mml:math id="inf206"><mml:mi>t</mml:mi></mml:math></inline-formula> if <inline-formula><mml:math id="inf207"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> motion ended at time <inline-formula><mml:math id="inf208"><mml:mi>t</mml:mi></mml:math></inline-formula> and color ended before time t, <inline-formula><mml:math id="inf209"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> color ended at time <inline-formula><mml:math id="inf210"><mml:mi>t</mml:mi></mml:math></inline-formula> and motion ended before time <inline-formula><mml:math id="inf211"><mml:mi>t</mml:mi></mml:math></inline-formula>. Thus,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>parallel</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>τ</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thickmathspace"/><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This is equivalent to the derivative of the product of the color and motion cumulative distributions:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>parallel</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where uppercase <inline-formula><mml:math id="inf212"><mml:mi>P</mml:mi></mml:math></inline-formula> indicates cumulative probability.</p><p>Besides the decision-time, there are sensory, motor and processing delays that contribute to the total RT. We assume that the combined non-decision latencies, <inline-formula><mml:math id="inf213"><mml:msub><mml:mi>T</mml:mi><mml:mi>nd</mml:mi></mml:msub></mml:math></inline-formula> are normally distributed with a mean of <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and a standard deviation of <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The observed RT distribution for each stimulus condition and choice is obtained by convolving the distributions of the double-decision times <inline-formula><mml:math id="inf216"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> and the distribution of non-decision times, which follows from the assumption that decision and non-decision times are additive and independent. This holds for both serial and parallel models.</p><p>To avoid over-fitting, our strategy for comparing the serial and parallel models was to fit all parameters using the subset of trials in which one of the two stimulus dimensions had maximum strength (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We used the Bayesian Adaptive Direct Search method (<xref ref-type="bibr" rid="bib1">Acerbi and Ma, 2017</xref>) to search over the space of parameters. The best-fitting parameters are shown in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, for each participant and model type. From the marginal distributions, we predicted the double-decision choice probabilities and RTs for all combinations of motion and color coherence. To compare serial and parallel models, we used the model likelihoods of observing the choice-RT data that was not used for fitting. Because the two models have the same number of parameters (12, comprising 5 for <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, 5 for <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and 2 for <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> &amp; <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), we directly compare the raw likelihoods (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>We conducted a model recovery exercise to verify that our fitting procedure would recover the correct model if the data were generated by either the serial or the parallel model. For each participant and model type (serial/parallel), we generated a synthetic data set with the same number of trials per condition (combination of color and motion coherence) as completed by the participant. The parameters used to generate the synthetic data set were those that best fit the participants’ data (that is, those shown in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). We conducted the model comparison, just as we did for the participants’ data, and assessed whether it favored the model that was used to generate the simulated data. <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> shows that our model comparison procedure can reliably identify the correct model for all 38 comparisons.</p><p>Except for Experiment 5, model fits were to all trials—correct, error and 0% coherence. For simplicity, plots of mean RT show only non-error trials (i.e. correct on both stimulus dimensions, treating either choice as correct for 0% coherence). The associated curve fits for serial and parallel models are derived from the drift-diffusion models using the trials in which the choice matches the sign of the coherence. Error trials are typically longer, as explained by the symmetrically collapsing bounds. We do not plot the error trials simply to reduce the complexity of the graphs, which would require approximately twice the number of symbols and curves. The fits and model comparisons are based on all trials. In theory, for 0% coherence both choices should have identical distributions, because the process is pure diffusion to symmetric bounds. In practice, participants typically exhibit a bias for one direction and color, and these are best approximated by a change in drift rate (e.g., <inline-formula><mml:math id="inf221"><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). This can produce a systematic underestimate of the mean RT when one of the two stimulus strengths is 0% coherence, owing to the inclusion of what are effectively slow errors in the data. The degree of the underestimate depends on the shape of the collapsing bounds and degree of bias. Two exceptions to this plotting convention are noted. In <xref ref-type="fig" rid="fig3">Figure 3A</xref> (Experiment 2), all trials contribute to the mean RTs; these data are not fit by a drift-diffusion model. In <xref ref-type="fig" rid="fig6">Figure 6</xref> (Experiment 4), all trials contribute to the mean RT, and the fits are generated from simulations that include all trials, including errors. We reach identical conclusions if we restrict the analysis to correct choices.</p><p>For the binary-response (<italic>same-different</italic>) choice-RT task (Experiment 5), we fit both serial and parallel models using a simple proportional-rate, drift-diffusion model (e.g., <xref ref-type="bibr" rid="bib80">Palmer et al., 2005</xref>). For each subject, we fit the choice-RT data from both the 1D and 2D tasks simultaneously. For the 1D task, this is a parsimonious model that uses only four degrees of freedom, <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the first two terms are as defined above, <inline-formula><mml:math id="inf223"><mml:msubsup><mml:mi>T</mml:mi><mml:mi>nd</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the mean non-decision time, and <inline-formula><mml:math id="inf224"><mml:mrow><mml:mo>±</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the height of the upper and lower terminating bounds. The model explains the mean RT of correct up and down choices in the 1D task. For the 2D trials, we assumed that participants applied the same decision process to each stimulus to determine an up-down choice and that the same-difference response was made by comparing the two decisions. We assumed that sensitivity (<inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and any up-down bias (<inline-formula><mml:math id="inf226"><mml:msubsup><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:math></inline-formula>) was the same for the 1D and 2D choices but allowed a different bound (<inline-formula><mml:math id="inf227"><mml:msubsup><mml:mi>B</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) and non-decision time (<inline-formula><mml:math id="inf228"><mml:msubsup><mml:mi>T</mml:mi><mml:mi>nd</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>).</p><p>The application of stationary (i.e. non-collapsing) bounds fails to account for the distribution of RTs and it underestimates the mean RT on errors (<xref ref-type="bibr" rid="bib88">Ratcliff and Rouder, 1998</xref>; <xref ref-type="bibr" rid="bib25">Drugowitsch et al., 2012</xref>). The fits to the RT data from the 1D blocks (up vs. down) included only correct trials. For the same-different decisions, the up-down decisions on the two patches are not indicated. We therefore fit the mean RTs for the correct same-different choices, assuming negligible contribution of double errors (i.e. incorrect direction decisions for both the left and right patch) to the mean RT. The fit maximized the likelihood of the choice assuming binomial error (from the model) and Gaussian error for the mean RTs (from the data).</p></sec><sec id="s4-10"><title>Empirical analysis of double-decision reaction times under serial and parallel rules</title><p>We pursued a second approach to compare serial and parallel integration strategies, focusing specifically on the decision times. Unlike the fits to choice-RT, this method uses each participant’s choices as ground truth. It considers only the distribution of double-decision RTs and attempts to account for them under serial and parallel logic. Instead of diffusion models, we attempt to explain the empirical distribution of double-decision RTs from unobserved probability density functions associated with the component color and motion decision times. Specifically, for each motion strength and choice (<inline-formula><mml:math id="inf229"><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf230"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>) and each color strength and choice (<inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf232"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>) we modeled the 1D decision time distributions as a gamma distribution (two parameters governing mean and standard deviation). These 1D distributions allowed us to predict the decision time on 2D trials under a serial (additive) and parallel (max) rule. The non-decision times were also modeled as four gamma distributions, one for each combination of the four choices (<inline-formula><mml:math id="inf233"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf234"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>). The RT distribution was obtained by convolution of the double-decision time and non-decision time distribution. Each participant’s data was fit under the serial and parallel model by maximum likelihood (using Matlab fmincon). For robustness, only combinations of strengths and choices with more than 10 trials were included in the fit. The analysis is therefore heavily weighted toward correct trials. Comparison of models was based on log likelihoods of the data given the fitted parameters for each participant. We validated this method on synthetic data from a parallel and serial simulation and showed that model recovery was accurate (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p><p>We also deployed the fit-predict strategy used in <xref ref-type="fig" rid="fig2">Figure 2</xref>, where we estimated the gamma distributions for the 1D decision times and using only the conditions in which one or the other stimulus dimension was at its maximum strength (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>).</p><p>For the binary response task (Experiment 5), a simplified version of this model was used (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Only unsigned coherence levels of each motion stimulus were considered to fit the marginal gamma distributions (i.e. combined across direction). Additionally, only RTs from correct trials were included in this model. Finally, in order to estimate the distribution of <inline-formula><mml:math id="inf235"><mml:msub><mml:mi>T</mml:mi><mml:mi>nd</mml:mi></mml:msub></mml:math></inline-formula>, only a single gamma distribution was fitted.</p></sec><sec id="s4-11"><title>Variable duration model (Experiment 3)</title><p>We assume that when the duration of the color-motion stimulus is controlled by the experimenter, the choices are still governed by bounded integration. Thus, decisions can terminate (e.g. at time <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> for motion) before the stimulus duration, <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>T</mml:mi><mml:mi>dur</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib51">Kiani et al., 2008</xref>). For example, in a 1D decision about motion stimulus with strength <inline-formula><mml:math id="inf238"><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>, the choice is determined by (1) the distribution of termination times, <inline-formula><mml:math id="inf239"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf240"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, at the positive and negative bounds, respectively, up to <inline-formula><mml:math id="inf241"><mml:msub><mml:mi>T</mml:mi><mml:mi>dur</mml:mi></mml:msub></mml:math></inline-formula> and (2) the probability that the sign of the unabsorbed <inline-formula><mml:math id="inf242"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>dur</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is of the corresponding sign. For example, the probability of rightward decision for a stimulus duration <inline-formula><mml:math id="inf243"><mml:msub><mml:mi>T</mml:mi><mml:mi>dur</mml:mi></mml:msub></mml:math></inline-formula> is<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that <inline-formula><mml:math id="inf244"><mml:msubsup><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:msubsup></mml:math></inline-formula> is not a proper density; the total probability at <inline-formula><mml:math id="inf245"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>dur</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> comprises absorption times at both bounds and the probability of unterminated <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>To fit the data in <xref ref-type="fig" rid="fig4">Figure 4</xref> (Experiment 3) we employ two drift diffusion models, for color and motion, which only interact in the way they access the stream of sensory evidence. This interaction is governed by two parameters, one that determines the amount of time (<inline-formula><mml:math id="inf247"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula>) for which processing occurs in parallel before proceeding to a serial processing stage, and a second (<inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) that determines the probability that motion is prioritized over color during the serial stage. If motion is prioritized on a particular trial, for example, the motion process accumulates evidence in the serial phase until a decision bound is crossed, at which point color evidence continues to accumulate. Therefore, if <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> does not reach a decision bound before the sensory stream terminates, no further color evidence is accumulated after the parallel phase.</p><p>To model the double-decisions, we used the two 1D processes to specify the duration of the stimulus that was used for motion processing, <inline-formula><mml:math id="inf250"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>, and color processing, <inline-formula><mml:math id="inf251"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msub></mml:math></inline-formula>. On a trial in which motion is prioritized, the time component that contributed to the motion accumulation (<inline-formula><mml:math id="inf252"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>) is either the time, <inline-formula><mml:math id="inf253"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula>, that <inline-formula><mml:math id="inf254"><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> reaches a termination bound or <inline-formula><mml:math id="inf255"><mml:msub><mml:mi>T</mml:mi><mml:mi>dur</mml:mi></mml:msub></mml:math></inline-formula> if it does not reach a bound (in which case there is no <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). These two possibilities bear on the maximum time available for color processing (<inline-formula><mml:math id="inf257"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>max</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:msubsup></mml:math></inline-formula>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> and </mml:mtext></mml:mstyle><mml:mi>∄</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The three conditions in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> can be understood intuitively. (1) If the stimulus is shorter than the parallel phase or if motion has terminated in this phase, then the maximum time available for color processing is the full duration of the stimulus. (2) If motion terminates in the serial phase then the maximum time available for color is the duration of the parallel phase and what time remains of the serial phase after motion has terminated. (3) If motion does not terminate, then color is only processed during the parallel phase. With probability <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, color is prioritized, and the complementary logic holds.</p><p>Note that if <inline-formula><mml:math id="inf259"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the model is purely serial with one change from motion to color with probability <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> or from color to motion with probability <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Although realized as a single switch, the model is qualitatively indistinguishable from other alternation schedules that preserve the same competition for processing time. For <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>dur</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the model is effectively parallel. We fit a parallel model to the data (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>) by fixing <inline-formula><mml:math id="inf263"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula> to the longest duration tested (1.2 s).</p><p>Each of the 1D diffusions were modeled similar to those used for the RT task, except for the following minor modifications. (1) We did not include a parameter for non-decision times, because we only modeled choices. (2) We parameterized the bound as an exponential function that is clipped to have a maximum at <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and to start decreasing from <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with a half-life of <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with lower bound simply <inline-formula><mml:math id="inf267"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The same parameterization applies to the color bound (terms with subscript <inline-formula><mml:math id="inf268"><mml:mi mathvariant="normal">c</mml:mi></mml:math></inline-formula> in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>).</p><p>The model was implemented in PyTorch (<xref ref-type="bibr" rid="bib84">Paszke et al., 2019</xref>) with an Adam optimizer (<xref ref-type="bibr" rid="bib55">Kingma and Ba, 2014</xref>) and a modified version of the cyclical learning rate schedule that simply switched back and forth between 0.05 and 0.025 every 25 epochs (<xref ref-type="bibr" rid="bib95">Smith, 2015</xref>). We verified that this procedure reliably recovers the <inline-formula><mml:math id="inf269"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Briefly, in Adam, the learning rate gives an approximate upper bound to the change each parameter takes per epoch, and the step size is also adapted for individual parameters based on the running estimates of the first and second moments of the gradient. That is, a high learning rate updates parameters fast and a low learning rate allows better convergence at the expense of speed. We fit the model separately for each <inline-formula><mml:math id="inf270"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula> in steps of 40 ms from 0 to 240 ms and then in steps of 120 ms up to 1200 ms, the longest duration of the stimulus we used. The coarser sampling is justified because the model predicts decision terminations with decreasing density at longer durations, and hence the log likelihood of the choices depends less on the buffer duration when it is long. The reported estimate of <inline-formula><mml:math id="inf271"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula> is the sample value with maximum log likelihood.</p><p>To evaluate the validity of the estimates of buffer capacity (<inline-formula><mml:math id="inf272"><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub></mml:math></inline-formula>) shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we performed two types of analyses for each participant (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The first approximates the specificity, the second the sensitivity of the estimates. (1) We used the parameters of the best fitting diffusion models to the data in <xref ref-type="fig" rid="fig4">Figure 4</xref> (solid curves; see <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>) to simulate synthetic data using a buffer duration of <inline-formula><mml:math id="inf273"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula> ms. We fit the synthetic data with models with the buffer capacity fixed to other values (from 0 to 240 ms in steps of 40 ms, and from 240 to 1200 ms in steps of 120 ms). We then compared log likelihood of those fits with that of the 80 ms buffer model, and repeated the simulation 12 times. (2) We used the parameters of the best fitting diffusion models to the data in <xref ref-type="fig" rid="fig4">Figure 4</xref> to simulate synthetic data using the buffer durations, <inline-formula><mml:math id="inf274"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula> ms, and compared two fits: with <inline-formula><mml:math id="inf275"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>buf</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula> ms or the simulated value.</p></sec><sec id="s4-12"><title>Multi-switch model (Experiment 4)</title><p>In the serial phase of the 2D task, the motion and color processes alternate. Experiments that provide only one RT to report both decisions allow us to estimate the overall prioritization of one stream over the other but not the frequency of alternation. In contrast, the bimanual task provides two RTs on each trial. This allows us to estimate the frequency of alternation between stimulus dimensions by fitting a model with multiple switches to the response times of the first decision in the bimanual task (Experiment 4).</p><p>The fitting was carried out in two steps. First, we fit the serial model described in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> to the second response in the bimanual task. The parameters that best fit the data are shown in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>. Second, with the serial model parameters fixed, we used three additional parameters to account for the RTs to the decision that was reported first. The three parameters are: <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">Δ</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, controlling the average time between alternations of color and motion; <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the probability of starting with motion; and <inline-formula><mml:math id="inf278"><mml:msubsup><mml:mi>T</mml:mi><mml:mi>nd</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, the expectation of the non-decision time for the first response.</p><p>The alternations are modeled as a renewal. The intervals are independent and identically distributed (<italic>iid</italic>) as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo rspace="4.2pt" stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf279"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are draws from an exponential distribution with mean <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">Δ</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The expectation of the interval is<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">Δ</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We chose this parameterization so that the distribution of inter-switch intervals has a single peak and the max operation reduced the probability of very short intervals.</p><p>Because there is no closed-form solution to the multi-switch model, we used simulations to fit the model parameters to each participants' data. For fitting, we simulate the model 1000 times for each unique combination of color and motion strengths. From the simulations, we average the RTs for the first decisions split by whether motion or color was reported first, and binned them by both motion strength and color strength. This gives the four groupings in <xref ref-type="fig" rid="fig6">Figure 6</xref>. The parameters were fit to minimize the sum of squared-errors summed over these four groups; in other words, we minimize the sum of the squared errors for the data points shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. We used this approach rather than maximum likelihood because of the difficulties of reliably estimating the likelihood of the parameters from model simulations for continuous quantities (here, RT; <xref ref-type="bibr" rid="bib103">van Opheusden et al., 2020</xref>).</p></sec><sec id="s4-13"><title>Data analysis</title><p>We used logistic regression to evaluate the influence of task type (single,double) on performance in the short-stimulus duration task (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Separate regression models were fit for the color and motion decisions. The logistic regression model is:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>double</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>double</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>subj</mml:mtext><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>subj</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf282"><mml:msub><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> is the probability of a positive choice (‘rightward' for the motion task, ‘blue' for the color task) , <inline-formula><mml:math id="inf283"><mml:mi>s</mml:mi></mml:math></inline-formula> is (signed) stimulus strength, <inline-formula><mml:math id="inf284"><mml:msub><mml:mi>I</mml:mi><mml:mtext>double</mml:mtext></mml:msub></mml:math></inline-formula> is an indicator variable for task type (single or double), <inline-formula><mml:math id="inf285"><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> is an interaction term which indicates how the influence of strength on choice changes in the double task relative to the single task, and <inline-formula><mml:math id="inf286"><mml:msub><mml:mi>I</mml:mi><mml:mtext>subj</mml:mtext></mml:msub></mml:math></inline-formula> is an indicator variable that takes a value of 1 if the trial was completed by subject subj and 0 otherwise. The final term with the summation allows for the possibility that different participants had different overall choice biases.</p><p>We also used logistic regression to assess whether the strength of one stimulus dimension affected the accuracy of the other decision. Separate regression models were fit for the color and motion decisions. The logistic regression model to assess whether color strength affects motion choice is<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where the <inline-formula><mml:math id="inf287"><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> term accommodates the possibility that the color coherence could affect the slope of the logistic function of motion coherence. We used an analogous equation to ask whether motion strength affected color sensitivity. For both logistic regression models (<xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>), to test whether the interaction (<inline-formula><mml:math id="inf288"><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>) has explanatory power in the model we compared the Bayesian Information Criterion (BIC) for nested regression models with and without the <inline-formula><mml:math id="inf289"><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> term. For <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> data were fit for each participant and the BICs were summed.</p><p>For the model-free analysis of the time course of the influence of motion and color information on choice <xref ref-type="fig" rid="fig3">Figure 3</xref>, we obtained choice-conditioned averages of the color and motion energies extracted from the random-dot stimuli. Because the stimulus is stochastic, the motion and color energies vary from one trial to another and also within a trial. We quantify the motion fluctuations by convolving the sequence of random dots presented in each trial <inline-formula><mml:math id="inf290"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf291"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with spatiotemporal nonlinear filters, <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, selective to rightward and leftward motion, respectively. These filters are specified in other publications (e.g. <xref ref-type="bibr" rid="bib2">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib51">Kiani et al., 2008</xref>). The results of the convolutions, <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are combined over space to obtain the motion energy for each direction, as a function of time <inline-formula><mml:math id="inf296"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf297"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The net motion energy is the difference of the right and left signals, detrended by the average over all trials with the same motion strength and direction (i.e., the signed coherence, <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>):<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mo>⋯</mml:mo><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> connotes the mean over all trials sharing the same signed coherence. <inline-formula><mml:math id="inf300"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the residual fluctuation of motion energy caused by the stochastic nature of the stimulus on that trial. The four traces in <xref ref-type="fig" rid="fig3">Figure 3B</xref> (<italic>top</italic>) are averages of the residuals from all motion strengths and directions, grouped by whether the choice was rightward or leftward in the 1D and 2D tasks.</p><p>We performed a similar analysis to extract the color energy from the stimulus. We calculated the difference between the number of blue and yellow dots shown on each video frame. We filtered this discrete representation by convolution with the impulse response function shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref> (<italic>bottom</italic>). We constructed residual <italic>excess color for blue</italic>, <inline-formula><mml:math id="inf301"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> analogous to <inline-formula><mml:math id="inf302"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>. The four traces in <xref ref-type="fig" rid="fig3">Figure 3B</xref> (<italic>middle</italic>) are averages of the residuals from all trials, grouped by whether the choice was blue or yellow in the 1D and 2D tasks. Note that the blurring step does not affect the conclusions drawn from this analysis. The unfiltered color residuals also support the conclusion that the weighting of evidence samples used to form color decisions (in Experiment 2) was the same for 1D and 2D trials.</p></sec><sec id="s4-14"><title>Data and code availability</title><p>Data are available on the figshare repository (<xref ref-type="bibr" rid="bib45">Kang et al., 2021a</xref>). Code is available on the GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/yulkang/2D_Decision">https://github.com/yulkang/2D_Decision</ext-link>; (<xref ref-type="bibr" rid="bib46">Kang et al., 2021b</xref>; <xref ref-type="bibr" rid="bib47">Kang et al., 2021c</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c755f421633127bcf6ebe675c2db0f84a94e1b65;origin=https://github.com/yulkang/2D_Decision;visit=swh:1:snp:d617e980589f182494bb0d4da3c8f0526ba84680;anchor=swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac/">swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Daphna Shohamy and Mariano Sigman for contributions to the theoretical underpinnings of our study, and we thank Stanislas Dehaene, Gabriel Stine, Naomi Odean, S Shushruth, and Aniruddha Das for comments on an earlier draft of the manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Methodology, Project administration, Conceptualization, Investigation, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Software, Supervision, Validation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: The study was approved by the local ethics committee (Institutional Review Board of Columbia University Medical Center IRB-AAAL0658 &amp; IRB-AAAR9148). Thirteen participants (5 male and 8 female, age 23-40, median = 26, IQR = 25-32, mean = 28.3, SD = 5.74) provided written informed consent and took part in the study.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-63721-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data is on figshare at: <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.6084/m9.figshare.13607255">https://dx.doi.org/10.6084/m9.figshare.13607255</ext-link>. The code is available at the following repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/yulkang/2D_Decision">https://github.com/yulkang/2D_Decision</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac/">https://archive.softwareheritage.org/swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac/</ext-link>). The figshare (allows deposition of big data) and github (suitable for maintenance of code) repositories refer to each other.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>YHR</given-names></name><name><surname>Löffler</surname><given-names>A</given-names></name><name><surname>Jeurissen</surname><given-names>D</given-names></name><name><surname>Zylberberg</surname><given-names>A</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data for &quot;Multiple decisions about one object involve parallel sensory acquisition but time-multiplexed evidence incorporation&quot;</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.13607255</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname> <given-names>L</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical bayesian optimization for model fitting with bayesian adaptive direct search</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1836</fpage><lpage>1846</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname> <given-names>EH</given-names></name><name><surname>Bergen</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id><pub-id pub-id-type="pmid">3973762</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akhlaghpour</surname> <given-names>H</given-names></name><name><surname>Wiskerke</surname> <given-names>J</given-names></name><name><surname>Choi</surname> <given-names>JY</given-names></name><name><surname>Taliaferro</surname> <given-names>JP</given-names></name><name><surname>Au</surname> <given-names>J</given-names></name><name><surname>Witten</surname> <given-names>IB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociated sequential activity and stimulus encoding in the dorsomedial striatum during spatial working memory</article-title><source>eLife</source><volume>5</volume><elocation-id>e19507</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19507</pub-id><pub-id pub-id-type="pmid">27636864</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakkour</surname> <given-names>A</given-names></name><name><surname>Palombo</surname> <given-names>DJ</given-names></name><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Kang</surname> <given-names>YHR</given-names></name><name><surname>Reid</surname> <given-names>A</given-names></name><name><surname>Verfaellie</surname> <given-names>M</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The hippocampus supports deliberation during value-based decisions</article-title><source>eLife</source><volume>8</volume><elocation-id>e46080</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46080</pub-id><pub-id pub-id-type="pmid">31268419</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname> <given-names>H</given-names></name><name><surname>Tripathy</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Correspondence noise and signal pooling in the detection of coherent visual motion</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>7954</fpage><lpage>7966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-20-07954.1997</pub-id><pub-id pub-id-type="pmid">9315913</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Beard</surname> <given-names>BL</given-names></name><name><surname>Ahumada</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Technique to extract relevant image features for visual tasks</article-title><conf-name>Human Vision and Electronic Imaging III, International Society for Optics and Photonics</conf-name><fpage>79</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1117/12.320099</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Hanks</surname> <given-names>T</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Roitman</surname> <given-names>J</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Probabilistic population codes for bayesian decision making</article-title><source>Neuron</source><volume>60</volume><fpage>1142</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.021</pub-id><pub-id pub-id-type="pmid">19109917</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname> <given-names>JK</given-names></name><name><surname>Cohen</surname> <given-names>YE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The what, where and how of auditory-object perception</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>693</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1038/nrn3565</pub-id><pub-id pub-id-type="pmid">24052177</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Usher</surname> <given-names>M</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Extending a biologically inspired model of choice: multi-alternatives, nonlinearity and value-based multidimensional choice</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>362</volume><fpage>1655</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2059</pub-id><pub-id pub-id-type="pmid">17428774</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname> <given-names>KH</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Responses of neurons in macaque MT to stochastic motion signals</article-title><source>Visual Neuroscience</source><volume>10</volume><fpage>1157</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1017/S0952523800010269</pub-id><pub-id pub-id-type="pmid">8257671</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cain</surname> <given-names>N</given-names></name><name><surname>Barreiro</surname> <given-names>AK</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Shea-Brown</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural integrators for decision making: a favorable tradeoff between robustness and sensitivity</article-title><source>Journal of Neurophysiology</source><volume>109</volume><fpage>2542</fpage><lpage>2559</lpage><pub-id pub-id-type="doi">10.1152/jn.00976.2012</pub-id><pub-id pub-id-type="pmid">23446688</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carney</surname> <given-names>T</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Switkes</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Parallel processing of motion and colour information</article-title><source>Nature</source><volume>328</volume><fpage>647</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1038/328647a0</pub-id><pub-id pub-id-type="pmid">3614368</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>P</given-names></name><name><surname>Tyler</surname> <given-names>CW</given-names></name><name><surname>Favreau</surname> <given-names>OE</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Perceived velocity of moving chromatic gratings</article-title><source>Journal of the Optical Society of America A</source><volume>1</volume><fpage>893</fpage><lpage>899</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.1.000893</pub-id><pub-id pub-id-type="pmid">6470841</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>P</given-names></name><name><surname>Boeglin</surname> <given-names>J</given-names></name><name><surname>Favreau</surname> <given-names>OE</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Perception of motion in equiluminous kinematograms</article-title><source>Perception</source><volume>14</volume><fpage>151</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1068/p140151</pub-id><pub-id pub-id-type="pmid">4069945</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>P</given-names></name><name><surname>MacLeod</surname> <given-names>DIA</given-names></name><name><surname>Anstis</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Equiluminance: spatial and temporal factors and the contribution of blue-sensitive cones</article-title><source>Journal of the Optical Society of America A</source><volume>4</volume><fpage>1428</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.4.001428</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>JS</given-names></name><name><surname>Cooper</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>A practical difference scheme for Fokker-Planck equations</article-title><source>Journal of Computational Physics</source><volume>6</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/0021-9991(70)90001-X</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision-making with multiple alternatives</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>693</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nn.2123</pub-id><pub-id pub-id-type="pmid">18488024</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lafuente</surname> <given-names>V</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Representation of accumulating evidence for a decision in two parietal areas</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>4306</fpage><lpage>4318</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2451-14.2015</pub-id><pub-id pub-id-type="pmid">25762677</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>jsPsych: a JavaScript library for creating behavioral experiments in a web browser</article-title><source>Behavior Research Methods</source><volume>47</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3758/s13428-014-0458-y</pub-id><pub-id pub-id-type="pmid">24683129</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Ohzawa</surname> <given-names>I</given-names></name><name><surname>Freeman</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Spatiotemporal organization of simple-cell receptive fields in the cat's striate cortex. I. General characteristics and postnatal development</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>1091</fpage><lpage>1117</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.4.1091</pub-id><pub-id pub-id-type="pmid">8492151</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>L</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Neural correlates of perceptual decision making before, during, and after decision commitment in monkey frontal eye field</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>1052</fpage><lpage>1067</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr178</pub-id><pub-id pub-id-type="pmid">21765183</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>L</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Separate, causal roles of the caudate in Saccadic choice and execution in a perceptual decision task</article-title><source>Neuron</source><volume>75</volume><fpage>865</fpage><lpage>874</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.07.021</pub-id><pub-id pub-id-type="pmid">22958826</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ditterich</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A comparison between mechanisms of multi-alternative perceptual decision making: ability to explain human behavior, predictions for neurophysiology, and relationship with decision theory</article-title><source>Frontiers in Neuroscience</source><volume>4</volume><elocation-id>184</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2010.00184</pub-id><pub-id pub-id-type="pmid">21152262</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The cost of accumulating evidence in perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>32(1)</volume><fpage>3612</fpage><lpage>3628</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4010-11.2012</pub-id><pub-id pub-id-type="pmid">22423085</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal decision-making with time-varying evidence reliability</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>748</fpage><lpage>756</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname> <given-names>MO</given-names></name><name><surname>Banks</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>DA</given-names></name><name><surname>Stempel</surname> <given-names>AV</given-names></name><name><surname>Vale</surname> <given-names>R</given-names></name><name><surname>Ruehle</surname> <given-names>S</given-names></name><name><surname>Lefler</surname> <given-names>Y</given-names></name><name><surname>Branco</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A synaptic threshold mechanism for computing escape decisions</article-title><source>Nature</source><volume>558</volume><fpage>590</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0244-6</pub-id><pub-id pub-id-type="pmid">29925954</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fagot</surname> <given-names>C</given-names></name><name><surname>Pashler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Making two responses to a single object: implications for the central attentional bottleneck</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>18</volume><fpage>1058</fpage><lpage>1079</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.18.4.1058</pub-id><pub-id pub-id-type="pmid">1431744</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname> <given-names>SF</given-names></name><name><surname>Schwemmer</surname> <given-names>M</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multitasking versus multiplexing: toward a normative account of limitations in the simultaneous execution of control-demanding behaviors</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>14</volume><fpage>129</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.3758/s13415-013-0236-9</pub-id><pub-id pub-id-type="pmid">24481850</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gegenfurtner</surname> <given-names>KR</given-names></name><name><surname>Kiper</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Contrast detection in luminance and chromatic noise</article-title><source>Journal of the Optical Society of America A</source><volume>9</volume><fpage>1880</fpage><lpage>1888</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.9.001880</pub-id><pub-id pub-id-type="pmid">1432339</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural computations that underlie decisions about sensory stimuli</article-title><source>Trends in Cognitive Sciences</source><volume>5</volume><fpage>10</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01567-9</pub-id><pub-id pub-id-type="pmid">11164731</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname> <given-names>TD</given-names></name><name><surname>Mazurek</surname> <given-names>ME</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Hopp</surname> <given-names>E</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Elapsed decision time affects the weighting of prior probability in a perceptual decision task</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>6339</fpage><lpage>6352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5613-10.2011</pub-id><pub-id pub-id-type="pmid">21525274</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hick</surname> <given-names>WE</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>On the rate of gain of information</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>4</volume><fpage>11</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1080/17470215208416600</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hikosaka</surname> <given-names>O</given-names></name><name><surname>Kim</surname> <given-names>HF</given-names></name><name><surname>Yasuda</surname> <given-names>M</given-names></name><name><surname>Yamamoto</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Basal ganglia circuits for reward value-guided behavior</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>289</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-013924</pub-id><pub-id pub-id-type="pmid">25032497</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>IS</given-names></name><name><surname>Ingram</surname> <given-names>JN</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A modular planar robotic manipulandum with end-point torque control</article-title><source>Journal of Neuroscience Methods</source><volume>181</volume><fpage>199</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2009.05.005</pub-id><pub-id pub-id-type="pmid">19450621</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huk</surname> <given-names>AC</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>10420</fpage><lpage>10436</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4684-04.2005</pub-id><pub-id pub-id-type="pmid">16280581</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Optimal integration of texture and motion cues to depth</article-title><source>Vision Research</source><volume>39</volume><fpage>3621</fpage><lpage>3629</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(99)00088-7</pub-id><pub-id pub-id-type="pmid">10746132</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>W</given-names></name><name><surname>Tremblay</surname> <given-names>F</given-names></name><name><surname>Chapman</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Neuronal encoding of texture changes in the primary and the secondary somatosensory cortical areas of monkeys during passive texture discrimination</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>1656</fpage><lpage>1662</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.3.1656</pub-id><pub-id pub-id-type="pmid">9084631</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kahneman</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1973">1973</year><source>Attention and Effort</source><publisher-loc>Englewood Cliffs, New Jersey</publisher-loc><publisher-name>Prentice Hall, INC</publisher-name></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamienkowski</surname> <given-names>JE</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Delays without mistakes: response time and error distributions in dual-task</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e3196</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0003196</pub-id><pub-id pub-id-type="pmid">18787706</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname> <given-names>YHR</given-names></name><name><surname>Petzschner</surname> <given-names>FH</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Piercing of consciousness as a Threshold-Crossing operation</article-title><source>Current Biology</source><volume>27</volume><fpage>2285</fpage><lpage>2295</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.047</pub-id><pub-id pub-id-type="pmid">28756951</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Kang</surname> <given-names>YHR</given-names></name><name><surname>Löffler</surname> <given-names>A</given-names></name><name><surname>Jeurissen</surname> <given-names>D</given-names></name><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2021">2021a</year><data-title>Data for &quot;Multiple decisions about one object involve parallel sensory acquisition but time-multiplexed evidence incorporation&quot;</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.13607255.v1</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kang</surname> <given-names>YHR</given-names></name><name><surname>Löffler</surname> <given-names>A</given-names></name><name><surname>Jeurissen</surname> <given-names>D</given-names></name><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2021">2021b</year><data-title>2D_Decision</data-title><source>Software Heritage</source><version designator="swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac">swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c755f421633127bcf6ebe675c2db0f84a94e1b65;origin=https://github.com/yulkang/2D_Decision;visit=swh:1:snp:d617e980589f182494bb0d4da3c8f0526ba84680;anchor=swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac/">https://archive.softwareheritage.org/swh:1:dir:c755f421633127bcf6ebe675c2db0f84a94e1b65;origin=https://github.com/yulkang/2D_Decision;visit=swh:1:snp:d617e980589f182494bb0d4da3c8f0526ba84680;anchor=swh:1:rev:91922907c5ecaa832bdc6ee6cb285095905f4cac/</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kang</surname> <given-names>YHR</given-names></name><name><surname>Löffler</surname> <given-names>A</given-names></name><name><surname>Jeurissen</surname> <given-names>D</given-names></name><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2021">2021c</year><data-title>Code for &quot;Multiple Decisions About One Object Involve Parallel Sensory Acquisition but Time-Multiplexed Evidence Incorporation&quot;</data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/yulkang/2D_Decision">https://github.com/yulkang/2D_Decision</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname> <given-names>RE</given-names></name><name><surname>Raftery</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayes factors</article-title><source>Journal of the American Statistical Association</source><volume>90</volume><fpage>773</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keysers</surname> <given-names>C</given-names></name><name><surname>Xiao</surname> <given-names>DK</given-names></name><name><surname>Foldiak</surname> <given-names>P</given-names></name><name><surname>Perrett</surname> <given-names>DI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Out of sight but not out of mind: the neurophysiology of iconic memory in the superior temporal sulcus</article-title><source>Cognitive Neuropsychology</source><volume>22</volume><fpage>316</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1080/02643290442000103</pub-id><pub-id pub-id-type="pmid">21038253</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keysers</surname> <given-names>C</given-names></name><name><surname>Perrett</surname> <given-names>DI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual masking and RSVP reveal neural competition</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>120</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01852-0</pub-id><pub-id pub-id-type="pmid">11861189</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Hanks</surname> <given-names>TD</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Bounded integration in parietal cortex underlies decisions even when viewing duration is dictated by the environment</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>3017</fpage><lpage>3029</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4761-07.2008</pub-id><pub-id pub-id-type="pmid">18354005</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Corthell</surname> <given-names>L</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Choice certainty is informed by both evidence and decision time</article-title><source>Neuron</source><volume>84</volume><fpage>1329</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.015</pub-id><pub-id pub-id-type="pmid">25521381</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of confidence associated with a decision by neurons in the parietal cortex</article-title><source>Science</source><volume>324</volume><fpage>759</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1126/science.1169405</pub-id><pub-id pub-id-type="pmid">19423820</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>JN</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neural correlates of a decision in the dorsolateral prefrontal cortex of the macaque</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>176</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/5739</pub-id><pub-id pub-id-type="pmid">10195203</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kira</surname> <given-names>S</given-names></name><name><surname>Yang</surname> <given-names>T</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural implementation of wald's sequential probability ratio test</article-title><source>Neuron</source><volume>85</volume><fpage>861</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.01.007</pub-id><pub-id pub-id-type="pmid">25661183</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname> <given-names>DC</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id><pub-id pub-id-type="pmid">15541511</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname> <given-names>I</given-names></name><name><surname>Armel</surname> <given-names>C</given-names></name><name><surname>Rangel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual fixations and the computation and comparison of value in simple choice</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1292</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1038/nn.2635</pub-id><pub-id pub-id-type="pmid">20835253</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname> <given-names>I</given-names></name><name><surname>Rangel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</article-title><source>PNAS</source><volume>108</volume><fpage>13852</fpage><lpage>13857</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101328108</pub-id><pub-id pub-id-type="pmid">21808009</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Laming</surname> <given-names>DRJ</given-names></name></person-group><year iso-8601-date="1968">1968</year><source>Information Theory of Choice-Reaction Times</source><publisher-name>Academic Press</publisher-name><pub-id pub-id-type="doi">10.1111/j.2044-8317.1969.tb00423.x</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>KH</given-names></name><name><surname>Tran</surname> <given-names>A</given-names></name><name><surname>Turan</surname> <given-names>Z</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The sifting of visual information in the superior colliculus</article-title><source>eLife</source><volume>9</volume><elocation-id>e50678</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50678</pub-id><pub-id pub-id-type="pmid">32286224</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Joo</surname> <given-names>SJ</given-names></name><name><surname>Yeatman</surname> <given-names>JD</given-names></name><name><surname>Reinecke</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Controlling for participants' viewing distance in large-scale, psychophysical online experiments using a virtual chinrest</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>904</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-57204-1</pub-id><pub-id pub-id-type="pmid">31969579</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Link</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>The relative judgment theory of two choice response time</article-title><source>Journal of Mathematical Psychology</source><volume>12</volume><fpage>114</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/0022-2496(75)90053-X</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname> <given-names>M</given-names></name><name><surname>Hubel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Segregation of form, color, movement, and depth: anatomy, physiology, and perception</article-title><source>Science</source><volume>240</volume><fpage>740</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1126/science.3283936</pub-id><pub-id pub-id-type="pmid">3283936</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorteije</surname> <given-names>JAM</given-names></name><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Ouellette</surname> <given-names>BG</given-names></name><name><surname>De Zeeuw</surname> <given-names>CI</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The formation of hierarchical decisions in the visual cortex</article-title><source>Neuron</source><volume>87</volume><fpage>1344</fpage><lpage>1356</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.08.015</pub-id><pub-id pub-id-type="pmid">26365766</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luce</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>Response Times: Their Role in Inferring Elementary Mental Organization</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195070019.001.0001</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundqvist</surname> <given-names>M</given-names></name><name><surname>Herman</surname> <given-names>P</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Working memory: delay activity, yes! persistent activity? maybe not</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7013</fpage><lpage>7019</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2485-17.2018</pub-id><pub-id pub-id-type="pmid">30089640</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname> <given-names>V</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marti</surname> <given-names>S</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A shared cortical bottleneck underlying attentional blink and psychological refractory period</article-title><source>NeuroImage</source><volume>59</volume><fpage>2883</fpage><lpage>2898</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.063</pub-id><pub-id pub-id-type="pmid">21988891</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marti</surname> <given-names>S</given-names></name><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Time-resolved decoding of two processing chains during Dual-Task interference</article-title><source>Neuron</source><volume>88</volume><fpage>1297</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.040</pub-id><pub-id pub-id-type="pmid">26627309</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marti</surname> <given-names>S</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Discrete and continuous mechanisms of temporal selection in rapid visual streams</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1955</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02079-x</pub-id><pub-id pub-id-type="pmid">29208892</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLeod</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Parallel processing and the psychological refractory period</article-title><source>Acta Psychologica</source><volume>41</volume><fpage>381</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(77)90016-6</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname> <given-names>G</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic theory of working memory</article-title><source>Science</source><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><pub-id pub-id-type="pmid">18339943</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moses</surname> <given-names>DA</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural speech recognition: continuous phoneme decoding using spatiotemporal representations of human cortical activity</article-title><source>Journal of Neural Engineering</source><volume>13</volume><elocation-id>056004</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/13/5/056004</pub-id><pub-id pub-id-type="pmid">27484713</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Thompson</surname> <given-names>ID</given-names></name><name><surname>Tolhurst</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1978">1978a</year><article-title>Receptive field organization of complex cells in the cat's striate cortex</article-title><source>The Journal of Physiology</source><volume>283</volume><fpage>79</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1978.sp012489</pub-id><pub-id pub-id-type="pmid">722592</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Thompson</surname> <given-names>ID</given-names></name><name><surname>Tolhurst</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1978">1978b</year><article-title>Spatial summation in the receptive fields of simple cells in the cat's striate cortex</article-title><source>The Journal of Physiology</source><volume>283</volume><fpage>53</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1978.sp012488</pub-id><pub-id pub-id-type="pmid">722589</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Musslick</surname> <given-names>S</given-names></name><name><surname>Saxe</surname> <given-names>A</given-names></name><name><surname>Özcimder</surname> <given-names>K</given-names></name><name><surname>Dey</surname> <given-names>B</given-names></name><name><surname>Henselman</surname> <given-names>G</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multitasking capability versus learning efficiency in neural network architectures</article-title><conf-name>Proceedings of the 39th Annual Meeting Cognitive Science Society</conf-name></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Herron</surname> <given-names>P</given-names></name><name><surname>von der Heydt</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Short-term memory for figure-ground organization in the visual cortex</article-title><source>Neuron</source><volume>61</volume><fpage>801</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.014</pub-id><pub-id pub-id-type="pmid">19285475</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname> <given-names>G</given-names></name><name><surname>Sha</surname> <given-names>L</given-names></name><name><surname>Purcell</surname> <given-names>BA</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Psychophysical reverse correlation reflects both sensory and decision-making processes</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3479</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05797-y</pub-id><pub-id pub-id-type="pmid">30154467</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname> <given-names>J</given-names></name><name><surname>Huk</surname> <given-names>AC</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The effect of stimulus strength on the speed and accuracy of a perceptual decision</article-title><source>Journal of Vision</source><volume>5</volume><fpage>1</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1167/5.5.1</pub-id><pub-id pub-id-type="pmid">16097871</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Dual-task interference in simple tasks: data and theory</article-title><source>Psychological Bulletin</source><volume>116</volume><fpage>220</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.116.2.220</pub-id><pub-id pub-id-type="pmid">7972591</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashler</surname> <given-names>H</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name><name><surname>O'Brien</surname> <given-names>S</given-names></name><name><surname>Gazzaniga</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Sequential operation of disconnected cerebral hemispheres in split-brain patients</article-title><source>NeuroReport</source><volume>5</volume><fpage>2381</fpage><lpage>2384</lpage><pub-id pub-id-type="doi">10.1097/00001756-199411000-00042</pub-id><pub-id pub-id-type="pmid">7881063</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pashler</surname> <given-names>HE</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>The Psychology of Attention</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib84"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname> <given-names>A</given-names></name><name><surname>Gross</surname> <given-names>S</given-names></name><name><surname>Massa</surname> <given-names>F</given-names></name><name><surname>Lerer</surname> <given-names>A</given-names></name><name><surname>Bradbury</surname> <given-names>J</given-names></name><name><surname>Chanan</surname> <given-names>G</given-names></name><name><surname>Killeen</surname> <given-names>T</given-names></name><name><surname>Lin</surname> <given-names>Z</given-names></name><name><surname>Gimelshein</surname> <given-names>N</given-names></name><name><surname>Antiga</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pytorch: an imperative style, high-performance deep learning library</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>8026</fpage><lpage>8037</lpage></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Short-term conceptual memory for pictures</article-title><source>Journal of Experimental Psychology: Human Learning and Memory</source><volume>2</volume><fpage>509</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.2.5.509</pub-id><pub-id pub-id-type="pmid">1003124</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramachandran</surname> <given-names>VS</given-names></name><name><surname>Gregory</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Does colour provide an input to human motion perception?</article-title><source>Nature</source><volume>275</volume><fpage>55</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/275055a0</pub-id><pub-id pub-id-type="pmid">683341</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Rouder</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modeling response times for Two-Choice decisions</article-title><source>Psychological Science</source><volume>9</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00067</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remington</surname> <given-names>ED</given-names></name><name><surname>Egger</surname> <given-names>SW</given-names></name><name><surname>Narain</surname> <given-names>D</given-names></name><name><surname>Wang</surname> <given-names>J</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A dynamical systems perspective on flexible motor timing</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>938</fpage><lpage>952</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.07.010</pub-id><pub-id pub-id-type="pmid">30266152</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resulaj</surname> <given-names>A</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Changes of mind in decision-making</article-title><source>Nature</source><volume>461</volume><fpage>263</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nature08275</pub-id><pub-id pub-id-type="pmid">19693010</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schumacher</surname> <given-names>EH</given-names></name><name><surname>Seymour</surname> <given-names>TL</given-names></name><name><surname>Glass</surname> <given-names>JM</given-names></name><name><surname>Fencsik</surname> <given-names>DE</given-names></name><name><surname>Lauber</surname> <given-names>EJ</given-names></name><name><surname>Kieras</surname> <given-names>DE</given-names></name><name><surname>Meyer</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Virtually perfect time sharing in dual-task performance: uncorking the central cognitive bottleneck</article-title><source>Psychological Science</source><volume>12</volume><fpage>101</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00318</pub-id><pub-id pub-id-type="pmid">11340917</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Hanks</surname> <given-names>TD</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Yang</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>The speed and accuracy of a simple perceptual decision: A mathematical primer</chapter-title><person-group person-group-type="editor"><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><source>Bayesian Brain: Probabilistic Approaches to Neural Coding</source><publisher-name>MIT Press</publisher-name><fpage>209</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262042383.003.0010</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decision making and sequential sampling from memory</article-title><source>Neuron</source><volume>90</volume><fpage>927</fpage><lpage>939</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.036</pub-id><pub-id pub-id-type="pmid">27253447</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Parsing a cognitive task: a characterization of the mind's bottleneck</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e37</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030037</pub-id><pub-id pub-id-type="pmid">15719056</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>LN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cyclical learning rates for training neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.01186">https://arxiv.org/abs/1506.01186</ext-link></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sperling</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>The information available in brief visual presentations</article-title><source>Psychological Monographs: General and Applied</source><volume>74</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1037/h0093759</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stine</surname> <given-names>GM</given-names></name><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Ditterich</surname> <given-names>J</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Differentiating between integration and non-integration strategies in perceptual decision making</article-title><source>eLife</source><volume>9</volume><elocation-id>e55365</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.55365</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tombu</surname> <given-names>M</given-names></name><name><surname>Jolicoeur</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>All-or-none bottleneck versus capacity sharing accounts of the psychological refractory period phenomenon</article-title><source>Psychological Research</source><volume>66</volume><fpage>274</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1007/s00426-002-0101-x</pub-id><pub-id pub-id-type="pmid">12466925</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tombu</surname> <given-names>M</given-names></name><name><surname>Jolicoeur</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Virtually no evidence for virtually perfect time-sharing</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>30</volume><fpage>795</fpage><lpage>810</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.30.5.795</pub-id><pub-id pub-id-type="pmid">15462621</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tombu</surname> <given-names>M</given-names></name><name><surname>Jolicoeur</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Testing the predictions of the central capacity sharing model</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>31</volume><fpage>790</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.31.4.790</pub-id><pub-id pub-id-type="pmid">16131250</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treisman</surname> <given-names>AM</given-names></name><name><surname>Gelade</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>A feature-integration theory of attention</article-title><source>Cognitive Psychology</source><volume>12</volume><fpage>97</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(80)90005-5</pub-id><pub-id pub-id-type="pmid">7351125</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usher</surname> <given-names>M</given-names></name><name><surname>Olami</surname> <given-names>Z</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Hick's Law in a Stochastic Race Model with Speed–Accuracy Tradeoff</article-title><source>Journal of Mathematical Psychology</source><volume>46</volume><fpage>704</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1006/jmps.2002.1420</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van Opheusden</surname> <given-names>B</given-names></name><name><surname>Acerbi</surname> <given-names>L</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unbiased and efficient log-likelihood estimation with inverse binomial sampling</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008483</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname> <given-names>ML</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decision making through integration of sensory evidence at prolonged timescales</article-title><source>Current Biology</source><volume>28</volume><fpage>3850</fpage><lpage>3856</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.021</pub-id><pub-id pub-id-type="pmid">30471996</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welford</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>The “psychological refractory period” and the timing of high speed performance: A review and a theory</article-title><source>British Journal of Psychology. General Section</source><volume>43</volume><fpage>2</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8295.1952.tb00322.x</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>Z</given-names></name><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Shamash</surname> <given-names>P</given-names></name><name><surname>Taylor</surname> <given-names>A</given-names></name><name><surname>Axel</surname> <given-names>R</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Context-dependent decision making in a premotor circuit</article-title><source>Neuron</source><volume>106</volume><fpage>316</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.034</pub-id><pub-id pub-id-type="pmid">32105611</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>T</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Probabilistic reasoning by neurons</article-title><source>Nature</source><volume>447</volume><fpage>1075</fpage><lpage>1080</lpage><pub-id pub-id-type="doi">10.1038/nature05852</pub-id><pub-id pub-id-type="pmid">17546027</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Fernández Slezak</surname> <given-names>D</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The brain's router: a cortical network model of serial processing in the primate brain</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000765</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000765</pub-id><pub-id pub-id-type="pmid">20442869</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Ouellette</surname> <given-names>B</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decision making during the psychological refractory period</article-title><source>Current Biology</source><volume>22</volume><fpage>1795</fpage><lpage>1799</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.07.043</pub-id><pub-id pub-id-type="pmid">22921368</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Fetsch</surname> <given-names>CR</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The influence of evidence volatility on choice, reaction time and confidence in a perceptual decision</article-title><source>eLife</source><volume>5</volume><elocation-id>e17688</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.17688</pub-id><pub-id pub-id-type="pmid">27787198</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Counterfactual reasoning underlies the learning of priors in decision making</article-title><source>Neuron</source><volume>99</volume><fpage>1083</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.035</pub-id><pub-id pub-id-type="pmid">30122376</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Parameter values for the best-fitting serial model (Experiments 1 and 4).</title><p>Note that the rate of collapse parameters (<inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are limited to a maximum of 10 (an almost instantaneous bound collapse) and the time of the start of the collapse (<inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are limited to 4 s.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Task</th><th>Subj.</th><th><inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td rowspan="3">Eye RT</td><td>1</td><td>9.97</td><td>0.98</td><td>6.45</td><td>3.47</td><td>−0.02</td><td>5.77</td><td>0.83</td><td>10</td><td>4</td><td>−0.01</td><td>0.3</td><td>0.001</td></tr><tr><td>2</td><td>21.99</td><td>0.99</td><td>3.65</td><td>2.52</td><td>0.01</td><td>11.39</td><td>0.68</td><td>10</td><td>2.61</td><td>0.02</td><td>0.35</td><td>0.002</td></tr><tr><td>3</td><td>39.25</td><td>0.83</td><td>9.91</td><td>3.26</td><td>−0.01</td><td>6.29</td><td>3.62</td><td>1.23</td><td>−0.19</td><td>0</td><td>0.31</td><td>0.004</td></tr><tr><td rowspan="8">Unimanual</td><td>6</td><td>14.02</td><td>1.3</td><td>1.97</td><td>3.96</td><td>0.01</td><td>7.24</td><td>0.91</td><td>3.09</td><td>2.18</td><td>0.04</td><td>0.34</td><td>0.001</td></tr><tr><td>7</td><td>13.94</td><td>1.42</td><td>2.01</td><td>3.6</td><td>−0.01</td><td>4.97</td><td>1.33</td><td>1.21</td><td>2.79</td><td>0.06</td><td>0.41</td><td>0.001</td></tr><tr><td>8</td><td>9.98</td><td>1.11</td><td>4.07</td><td>4</td><td>0</td><td>7.16</td><td>1.03</td><td>2.15</td><td>2.93</td><td>0.02</td><td>0.31</td><td>0.001</td></tr><tr><td>9</td><td>12.84</td><td>0.75</td><td>10</td><td>2.64</td><td>0</td><td>7.18</td><td>0.91</td><td>10</td><td>3.28</td><td>0.02</td><td>0.69</td><td>0.08</td></tr><tr><td>10</td><td>20.81</td><td>0.88</td><td>10</td><td>4</td><td>−0.01</td><td>7.46</td><td>0.91</td><td>3.72</td><td>2.14</td><td>−0.06</td><td>0.37</td><td>0.001</td></tr><tr><td>11</td><td>19.56</td><td>0.84</td><td>4.81</td><td>2.72</td><td>0</td><td>7.67</td><td>1.73</td><td>0.32</td><td>0.51</td><td>−0.03</td><td>0.42</td><td>0.004</td></tr><tr><td>12</td><td>12.05</td><td>0.96</td><td>2.51</td><td>3.98</td><td>0</td><td>5.28</td><td>0.96</td><td>10</td><td>3.35</td><td>0.03</td><td>0.44</td><td>0.001</td></tr><tr><td>13</td><td>13.13</td><td>1</td><td>2.67</td><td>4</td><td>−0.02</td><td>5.87</td><td>1.06</td><td>0.98</td><td>3.06</td><td>0.02</td><td>0.41</td><td>0.001</td></tr><tr><td rowspan="8">Bimanual</td><td>6</td><td>8.39</td><td>0.74</td><td>10</td><td>3.44</td><td>0</td><td>4.36</td><td>0.98</td><td>10</td><td>4</td><td>0.04</td><td>0.33</td><td>0.001</td></tr><tr><td>7</td><td>9.45</td><td>0.8</td><td>10</td><td>4</td><td>−0.02</td><td>4.56</td><td>0.97</td><td>7.67</td><td>3.92</td><td>0.05</td><td>0.3</td><td>0.001</td></tr><tr><td>8</td><td>11.89</td><td>1.44</td><td>0.65</td><td>1.64</td><td>−0.04</td><td>6.9</td><td>1.03</td><td>1.78</td><td>1.85</td><td>−0.02</td><td>0.37</td><td>0.007</td></tr><tr><td>9</td><td>13.57</td><td>0.87</td><td>10</td><td>4</td><td>0</td><td>7.05</td><td>0.86</td><td>4.57</td><td>2.49</td><td>−0.1</td><td>0.44</td><td>0.002</td></tr><tr><td>10</td><td>13.03</td><td>1.34</td><td>1.51</td><td>3.84</td><td>0</td><td>6.9</td><td>1.07</td><td>2.1</td><td>2.81</td><td>0.07</td><td>0.38</td><td>0.022</td></tr><tr><td>11</td><td>12.56</td><td>1.68</td><td>0.77</td><td>2.35</td><td>0</td><td>6.78</td><td>0.95</td><td>2.32</td><td>3.05</td><td>0.05</td><td>0.46</td><td>0.001</td></tr><tr><td>12</td><td>12.65</td><td>1.03</td><td>6.16</td><td>3.33</td><td>0.01</td><td>6.01</td><td>1.08</td><td>10</td><td>3.96</td><td>−0.03</td><td>0.31</td><td>0.001</td></tr><tr><td>13</td><td>8.91</td><td>1.16</td><td>5.14</td><td>3.91</td><td>0.01</td><td>4.25</td><td>1.05</td><td>1.42</td><td>3.87</td><td>−0.09</td><td>0.3</td><td>0.001</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Parameter values for the best-fitting switching model (Experiment 4–bimanual).</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Subj.</th><th><inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">Δ</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td>6</td><td>1.48</td><td>0.5</td><td>0.42</td></tr><tr><td>7</td><td>0.88</td><td>0.33</td><td>0.59</td></tr><tr><td>8</td><td>8.73</td><td>0.86</td><td>0.38</td></tr><tr><td>9</td><td>1.27</td><td>0.88</td><td>0.43</td></tr><tr><td>10</td><td>0.16</td><td>0.05</td><td>0.38</td></tr><tr><td>11</td><td>0.18</td><td>0.86</td><td>0.71</td></tr><tr><td>12</td><td>0.22</td><td>0.6</td><td>0.5</td></tr><tr><td>13</td><td>0.74</td><td>0.92</td><td>0.64</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Parameter values for the best-fitting drift-diffusion model (Experiment 5–binary-choice).</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Task</th><th>Subj.</th><th><inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf323"><mml:msubsup><mml:mi>B</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf324"><mml:msubsup><mml:mi>B</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf325"><mml:msubsup><mml:mi>s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td rowspan="2">Binary choice</td><td>7</td><td>8.96</td><td>0.85</td><td>1.09</td><td>0.03</td><td>0.35</td><td>0.24</td></tr><tr><td>12</td><td>6.83</td><td>1.17</td><td>1.03</td><td>−0.04</td><td>0.43</td><td>0.35</td></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Sensitivity of color and motion choices obtained on single decision and double-decision tasks.</title><p><italic>Eye-RT</italic>. Sensitivity is the slope of a logistic fit to the proportion of rightward (blue) choices as a function of motion (color) strength. Values in the 1D columns are obtained from different blocks in which participants were instructed to answer only the motion direction or color dominance. Values in the 2D columns are obtained from the double-decisions, using either the motion or color choice. <italic>Binary choice</italic>. Here, the 2D task refers to the same-different task. Sensitivity for the 1D task is the slope of a logistic fit to direction choices for one patch (ignoring the other). For the same-different task, the direction choices on individual patches is not reported. The 1D sensitivity is estimated by fitting the proportion of same and different choices assuming the same sensitivity to motion direction for each patch. Parentheses show s.e.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th colspan="2">Motion sensitivity</th><th colspan="2">Color sensitivity</th></tr><tr><th>Task</th><th>Subj.</th><th>1D task</th><th>2D task</th><th>1D task</th><th>2D task</th></tr></thead><tbody><tr><td rowspan="3">Eye-RT</td><td>S1</td><td>17.5 (3.0)</td><td>18.7 (0.8)</td><td>10.6 (0.7)</td><td>9.5 (0.5)</td></tr><tr><td>S2</td><td>43.4 (10.9)</td><td>48.1 (2.1)</td><td>15.8 (0.7)</td><td>16 (0.5)</td></tr><tr><td>S3</td><td>59 (2.0)</td><td>68.2 (1.8)</td><td>10.5 (0.4)</td><td>11.5 (0.3)</td></tr><tr><td rowspan="2">Binary choice</td><td>S7</td><td>13.3 (1.0)</td><td>13.9 (1.0)</td><td/><td/></tr><tr><td>S12</td><td>13.2 (0.9)</td><td>19.3 (1.6)</td><td/><td/></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Parameter values for the best-fitting buffer + serial model (Experiment 3–Variable-duration stimulus presentation).</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Task</th><th>Subj.</th><th><inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td rowspan="2">VD</td><td>4</td><td>22.29</td><td>0.86</td><td>0.47</td><td>1.19</td><td>0.01</td><td>9.18</td><td>1.39</td><td>0.18</td><td>0.27</td><td>−0.01</td><td>0.80</td><td align="left">0.08</td></tr><tr><td>5</td><td>9.60</td><td>0.99</td><td>0.68</td><td>0.40</td><td>0.01</td><td>12.11</td><td>0.74</td><td>0.11</td><td>0.11</td><td>0.04</td><td>0.96</td><td align="left">0.08</td></tr></tbody></table></table-wrap></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63721.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Mamassian</surname><given-names>Pascal</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib><contrib contrib-type="reviewer"><name><surname>Verghese</surname><given-names>Preeti</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Kang, Shadlen and colleagues investigate the processes of evidence accumulation and evaluation when more than one decision about an object is required in order to arrive at a correct response. The two perceptual decisions are about the direction of motion (left/right) and the dominant color of the dots (blue/yellow) of a random-dot kinematogram. This systematic psychophysical study is underpinned by a full drift diffusion model that suggests that while the two aspects of the stimulus are acquired in parallel, the evidence is integrated serially into the decision process. The authors makes a persuasive case that when a decision has to be made about two feature dimensions of a stimulus, processing is subject to a bottleneck that allows only a limited amount of information from one dimension to be passed on to a decision stage. The methods are detailed and the experiments are rigorous and convincing. The complex set of experimental results is impressively fitted with a single model. This is a careful study of general significance for our understanding of the timescales of decision processes involved in cognition.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Multiple decisions about one object involve parallel sensory acquisition but time-multiplexed evidence incorporation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Pascal Mamassian (Reviewer #2); Preeti Verghese (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>The manuscript &quot;Multiple decisions about one object involve parallel sensory acquisition but time-multiplexed evidence incorporation&quot; by Shadlen and colleagues investigates the processes by which evidence is accumulated and evaluated when more than one decision about an object has to be made in order to make a correct response. The visual stimulus is a random-dot kinematogram and the two decisions are about the direction of motion (left/right) and the dominant color of the dots (blue/yellow). This systematic psychophysical study is underpinned by a full drift diffusion model that suggests that while the two aspects of the stimulus are acquired in parallel, the evidence is integrated serially into the decision process. The manuscript makes a persuasive case that when a decision has to be made about two feature dimensions of a stimulus, processing is subject to a bottleneck that allows only a limited amount of information from one dimension to be passed on to a decision stage. The methods are detailed and the experiments are rigorous and convincing. The complex set of experimental results is impressively fitted with a single model.</p><p>This is a careful study of general significance for our understanding of the timescales of decision processes involved in cognition.</p><p>The manuscript is quite long and would benefit from a more stringent organisation, some additional information about task and set-up, as well as more consideration of the processes downstream from the computation of the decision variable.</p><p>Essential revisions:</p><p>1) All three referees found the manuscript lengthy, particularly in results and discussion, and therefore difficult to read. It would be very helpful to streamline the different result sections for each individual experiment to allow easier cross-reference and for both results and discussion to focus on the key findings. As a consequence of the complex text structure and meandering explanations, we found it at times difficult to piece all the relevant task and analysis information together.</p><p>Here are a few key suggestion for re-organization and shortening:</p><p>a) There are a lot of different experiments described in the text for which it is difficult to find the related methods in the Materials and methods section (e.g. which experiment in the main text corresponds to the &quot;Choice-reaction time task (eye)&quot;?).</p><p>Maybe as well as a consistent name for each experiment in Results, Figures, Tables and Materials and methods, the different experiments could also be numbered in their treatment in the paper – the names are not necessarily intuitive for the reader who needs to cross reference to the Materials and methods.</p><p>b) For the Results in particular, it would facilitate reading if there was a concise section near the beginning of each psychophysical experiment that stated briefly number of participants, number of trials (and sessions) per subject, task, visual stimulus parameters, response effector, what randomisation and controls. For some of the first experiments described, one has to go through results, figure legends and methods to piece all this together.</p><p>c) It is important to systematically compare the results from experiments where observers performed the double task (what the authors call &quot;2D&quot;) to experiments with a single task (&quot;1D&quot;). This &quot;1D&quot; condition gives the baseline for both sensitivity and reaction times measures. It seems that the authors have run the &quot;1D&quot; condition sometimes (see e.g. Figure 3), but we were not convinced that they have run it in all the experiments. When available, we think the results from this &quot;1D&quot; condition should be systematically presented (e.g. in dashed lines in Figure 2). Ultimately, is it possible to test their model by only fitting the &quot;1D&quot; data and predict the &quot;2D&quot; results?</p><p>d) The choice and response time experiments for eyes and hands, the brief duration stimulus, the variable duration stimulus, and the bimanual task systematically build up the case for a parallel information acquisition stage and a serial decision stage. However, we think that the binary response with the double-decision does not add so much to the evidence. This task may have an additional stage of comparing the two motion stimuli to decide whether they are the same or different, which might introduce additional elements not present in the independent decisions associated with judging the motion and color aspects of a single stimulus.</p><p>Maybe this or some other experiment can be moved to Supplementary Materials so that the main text focuses on the essential aspects of this nice work.</p><p>e) The Discussion is quite complex without a clear organisation. Is it really necessary to have a 6-page discussion? Could the key points be made more concisely and a clearer organisation, for instance with a small number of subheadings or clear sign posts be imposed?</p><p>2) It is not clear to us why the response time data are fit with γ distributions, when the drift diffusion model can handle both choice and response time. Removing the additional and unnecessary γ distribution fits will help focus the paper.</p><p>3) One piece of information that is important for this type of psychophysical experiment are the actual instructions that subjects received before/when they were doing each of the experiments. I assume this happened with a training task, on the computer screen or through an information sheet to ensure that participants carried out the same cognitive task? We are sorry if we missed this.</p><p>The specific concern is to what extent participants were guided to make a combined or serial judgements. For instance, were the participants instructed to &quot;respond upper right for yellow dots moving right&quot; OR &quot;move your eyes to one of the right targets for rightwards moving dots, choose the upper target for yellow&quot;.</p><p>Could the authors include the precise wording for each experiment in the Materials and methods, please? If the instruction was of a serial nature for colour and motion, this issue needs to be discussed in more depth.</p><p>4) We were concerned whether motor preparation and issues for the motor response were ignored by the authors.</p><p>a) The authors argue that there is an initial 0.1s when both stimulus dimensions are acquired in parallel. I did not find a motor execution part in the non-decision time in the drift-diffusion model of the authors. What is the evidence that the 0.1s is purely sensory and not partly motor?</p><p>b) Areas in parietal and prefrontal cortex that have been proposed to compute/represent the decision variable show persistent activity, necessary for temporarily buffering information – rather than most visual cortical areas, in particular with regards to visual motion areas. How do the authors exclude that the bottleneck is not downstream from the computation of the decision variable(s), for instance by the process that integrates two decision variables in one motor response?</p><p>More generally how do the authors account for motor preparation time in their model?</p><p>5) Serial vs Parallel model: The serial model does a much better job than the parallel model of describing the response time data in Figure 1 Figure 2, Figure 3, Figure 4 and Figure 5. However, there is no discussion of the systematic underestimation of response times at very low values of motion and color coherence.</p><p>6) Eye vs. Hand: Why are participants less sensitive to color coherence in the eye condition? Does the longer manual response time mask these differences? In addition, the discrepancies of the serial model at low coherence values are greater for the eye data (see point 3 above). How does this relate to the shorter latency of eye movements, relative to manual responses?</p><p>7) The motion tasks is a temporal task requiring the integration of information over time while the colour task is fundamentally a spatial task that could be solved using a single frame. To what extent can this explain the results the authors have obtained on differential cross-interference for the variable stimulus duration?</p><p>8) Given these are human subjects, could the serial nature of the decision process have to do with the subjects internally verbalising the individual decisions before combining? We noted in this context that even for eye movement responses reaction times are quite long, much longer than a monkey would take for similar tasks. How do the authors exclude the possibility of verbalisation playing a role in the time course?</p><p>9) In the experiment with variable stimulus duration, it seems that the different durations are interleaved in a block of trials. If so, this creates additional uncertainty for the observer. Would the authors predict the same results if only one such duration was presented in a block?</p><p>10) In the using a double decision with binary response:</p><p>a) How can we be assured that participants can execute two independent movements at roughly the same time? We thought that bimanual coordination tasks were quite difficult to master (e.g. Mechsner et al., 2001). Could the authors comment on the amount of training required to perform their task at a reasonable motor performance level?</p><p>b) We discovered that only correct trials were analysed in the reaction time data (last line of caption of Figure 7). Why is this an appropriate choice? Was this also the case for the other experiments?</p><p>c) We were wondering why the authors did not use a conjunction judgment on their original stimulus, something like &quot;respond UP&quot; when the stimulus is either &quot;left-blue&quot; or &quot;right-yellow&quot;, and &quot;respond DOWN&quot; when the stimulus is either &quot;left-yellow&quot; or &quot;right-blue&quot; to &quot;down response&quot;. Presenting two motion fields in the two hemifields seems quite different from all the other experiments.</p><p>11) Is it really reasonable to assume that evidence is retained in the buffer without any loss? If there was a loss, this would be seen in the accuracy data, so I suspect this is the reason the authors did not consider this possibility. But what is the evidence for retaining information in working memory without any loss?</p><p>12) Figure 8 is one possible instantiation of a serial bottleneck at the decision stage. But it is not clear why the transfer from the buffer has such a large delay (180ms). What is the argument against a shorter transfer time, but a longer refractory period before the decision stage can accept new information? Since this is the speculative part of the paper, perhaps it does not have to be so detailed.</p><p>13) In addition to the preference to prioritize motion, was there a tendency to prioritize the easier task, or was the motion task prioritized because observers were more sensitive to motion (at least for the eye condition)?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63721.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) All three referees found the manuscript lengthy, particularly in results and discussion, and therefore difficult to read. It would be very helpful to streamline the different result sections for each individual experiment to allow easier cross-reference and for both results and discussion to focus on the key findings. As a consequence of the complex text structure and meandering explanations, we found it at times difficult to piece all the relevant task and analysis information together.</p><p>Here are a few key suggestion for re-organization and shortening:</p><p>a) There are a lot of different experiments described in the text for which it is difficult to find the related methods in the Materials and methods section (e.g. which experiment in the main text corresponds to the &quot;Choice-reaction time task (eye)&quot;?).</p><p>Maybe as well as a consistent name for each experiment in Results, Figures, Tables and Materials and methods, the different experiments could also be numbered in their treatment in the paper – the names are not necessarily intuitive for the reader who needs to cross reference to the Materials and methods.</p></disp-quote><p>We have now numbered the experiments in the revised paper in the following way:</p><p>Experiment 1. Double-decision reaction time (eye and unimanual)</p><p>Experiment 2. Brief stimulus presentation (eye)</p><p>Experiment 3. Variable-duration stimulus presentation (eye)</p><p>Experiment 4. Two-effector double-decision reaction time (bimanual)</p><p>Experiment 5. Binary-response double-decision reaction time</p><p>We have revised the methods and Results sections, as well as all figures, accordingly, to facilitate cross referencing.</p><disp-quote content-type="editor-comment"><p>b) For the Results in particular, it would facilitate reading if there was a concise section near the beginning of each psychophysical experiment that stated briefly number of participants, number of trials (and sessions) per subject, task, visual stimulus parameters, response effector, what randomisation and controls. For some of the first experiments described, one has to go through results, figure legends and methods to piece all this together.</p></disp-quote><p>We have now added a brief description at the beginning of each experiment in the Results section, including the number of participants/sessions/trials, coherence levels and their randomization, response effectors, and controls.</p><disp-quote content-type="editor-comment"><p>c) It is important to systematically compare the results from experiments where observers performed the double task (what the authors call &quot;2D&quot;) to experiments with a single task (&quot;1D&quot;). This &quot;1D&quot; condition gives the baseline for both sensitivity and reaction times measures. It seems that the authors have run the &quot;1D&quot; condition sometimes (see e.g. Figure 3), but we were not convinced that they have run it in all the experiments. When available, we think the results from this &quot;1D&quot; condition should be systematically presented (e.g. in dashed lines in Figure 2). Ultimately, is it possible to test their model by only fitting the &quot;1D&quot; data and predict the &quot;2D&quot; results?</p></disp-quote><p>The answer to the final question is, no, it is not possible to test the model this way. The intuition for why is that the decision maker controls speed-accuracy tradeoff, and there is no reason to presume that the same setting would be applied in 1D and 2D decisions. Consider that the error rate for difficult 1D and 2D decisions is 0.5 and 0.75, respectively. As we and others have shown, reaction times are longer on 4- vs. 2-choice tasks (Churchland, Kiani and Shadlen, 2008; Usher, Olami and McClelland, 2002; Ditterich, 2010). This is why we chose to focus on the more informative comparisons: (i) difficult motion paired with easy vs. difficult color, and (ii) difficult color paired with easy vs. difficult motion. These comparisons get to the heart of the question of parallel vs. serial. Consideration of speed vs. accuracy also apply to the variable duration task (Experiment 3) because decision makers still terminate their decisions with what they deem as sufficient evidence even in this setting (Kiani, Hanks and Shadlen, 2008). However, the consideration does not apply to the experiment using very short durations (Experiment 2) because differences in termination criteria would not be expected to play a role. All difficult trials would benefit from more information in this task, and the comparisons of 1D and 2D versions of this task are informative (Figure 3).</p><disp-quote content-type="editor-comment"><p>(d) The choice and response time experiments for eyes and hands, the brief duration stimulus, the variable duration stimulus, and the bimanual task systematically build up the case for a parallel information acquisition stage and a serial decision stage. However, we think that the binary response with the double decision does not add so much to the evidence. This task may have an additional stage of comparing the two motion stimuli to decide whether they are the same or different, which might introduce additional elements not present in the independent decisions associated with judging the motion and color aspects of a single stimulus.</p><p>Maybe this or some other experiment can be moved to Supplementary Materials so that the main text focuses on the essential aspects of this nice work.</p></disp-quote><p>We agree that the same-different task (Experiment 5) involves a comparison of the two direction decisions, but this should add a fixed amount of time because it should not depend on the difficulty of the direction decisions for each patch. Such a coherence-independent stage would be captured by the non-decision time. Like color and motion, the assessment of direction in two parts of the visual field would be expected to occur in parallel, based on what is known from neurophysiology and visual psychophysics (e.g., masking and aftereffects), but that is not what we find. Our experience presenting this material is that people find the additive decision times surprising. More importantly, this experiment addresses a potential confound by showing that it is the double decision that adds time, not the doubling of the number of possible responses. Moreover, the findings demonstrate that the serial incorporation of evidence into a double decision is not restricted to different perceptual modalities, such as color and motion. We therefore favor keeping this experiment in the main text.</p><disp-quote content-type="editor-comment"><p>(e) The Discussion is quite complex without a clear organisation. Is it really necessary to have a 6-page discussion? Could the key points be made more concisely and a clearer organisation, for instance with a small number of subheadings or clear sign posts be imposed?</p></disp-quote><p>We are aware of this and share the reviewers’ concern. We removed material that could be deemed tangential or non-essential or redundant, but this amounted to under one page. We added subheadings and revised the text to render the structure of the argument clearer.</p><disp-quote content-type="editor-comment"><p>(2) It is not clear to us why the response time data are fit with γ distributions, when the drift diffusion model can handle both choice and response time. Removing the additional and unnecessary γ distribution fits will help focus the paper.</p></disp-quote><p>The approach using γ distributions provides an empirical comparison of summation vs. max logic. It is unconstrained by the choice behavior and does not depend on the assumptions inherent in drift-diffusion models. It explains the observed double-decision RT distributions as either the serial or parallel combination of latent (i.e., unobservable) distributions of color and motion decision times. It escapes the systematic discrepancies noted in Figure 2, because its only penalty is this very discrepancy between observed and predicted RT. Yet it still favors the serial combination rule. It is also important for the comparison of parallel and serial models in Experiment 5 (binary same-different) because the parsimonious drift-diffusion model (with flat bounds) does not depict realistic distributions of decision times, and this could penalize the max and sum operations differently. Of course, the limitation to the approach is that it does not constrain the relationship between choice accuracy and decision time, so it cannot replace drift diffusion models. We have revised the manuscript in several places to make these points clearer to the reader.</p><disp-quote content-type="editor-comment"><p>(3) One piece of information that is important for this type of psychophysical experiment are the actual instructions that subjects received before/when they were doing each of the experiments. I assume this happened with a training task, on the computer screen or through an information sheet to ensure that participants carried out the same cognitive task? We are sorry if we missed this.</p><p>The specific concern is to what extent participants were guided to make a combined or serial judgements. For instance, were the participants instructed to &quot;respond upper right for yellow dots moving right&quot; OR &quot;move your eyes to one of the right targets for rightwards moving dots, choose the upper target for yellow&quot;.</p><p>Could the authors include the precise wording for each experiment in the Materials and methods, please? If the instruction was of a serial nature for colour and motion, this issue needs to be discussed in more depth.</p></disp-quote><p>We have added the instructions for each experiment to the Materials and methods section. Importantly, participants were guided to make combined, rather than serial, judgments in the 2D task. For example, the instructions for “Experiment 1. Double-decision reaction time (eye)” were as follows: “Your task is to answer based on both motion and color. When the motion is right and color is yellow, the answer is top right, and when the motion is right and color is blue, the answer is bottom right.”</p><disp-quote content-type="editor-comment"><p>(4) We were concerned whether motor preparation and issues for the motor response were ignored by the authors.</p><p>(a) The authors argue that there is an initial 0.1s when both stimulus dimensions are acquired in parallel. I did not find a motor execution part in the non-decision time in the drift-diffusion model of the authors. What is the evidence that the 0.1s is purely sensory and not partly motor?</p></disp-quote><p>The reviewer could be referring to two issues. The 0.12 s is the duration of the brief stimulus in Experiment 2 that had to be acquired in parallel. This experiment is not the usual choice-RT design because the stimulus is restricted in time. We do not apply a drift-diffusion model to these data in part because it would be unclear how to characterize the non-decision time. However, the evidence that the 0.12 s worth of sensory information has been acquired in parallel is that it is used to make the double decision. It has nothing to do with motor preparation or non-decision time, which typically combines both sensory processing delays and motor preparation, along with any other delay that does not depend on trial type (e.g., difficulty).</p><p>Alternatively, the reviewer could be referring to the estimate of the buffer in Figure 4 (Experiment 3, variable duration), which we refer to as the initial parallel phase. Perhaps the reviewer is wondering why we assume the parallel acquisition occurs at the beginning and not the end of the decision? That is supported by Experiment 2 and the psychophysical reverse correlation analyses.</p><p>We are unsure what it means to regard acquisition as “partly motor”. We are simply saying that information from both dimensions affect the decision. For this to be true, the information had to be acquired in parallel.</p><p>The revised manuscript contains a clearer explanation of the non-decision time as well as the reasoning about the parallel acquisition and buffer.</p><disp-quote content-type="editor-comment"><p>(b) Areas in parietal and prefrontal cortex that have been proposed to compute/represent the decision variable show persistent activity, necessary for temporarily buffering information – rather than most visual cortical areas, in particular with regards to visual motion areas. How do the authors exclude that the bottleneck is not downstream from the computation of the decision variable(s), for instance by the process that integrates two decision variables in one motor response?</p></disp-quote><p>If the bottleneck were downstream from the computation of the DV, the RTs would not be additive. The process would be coherence-independent and therefore absorbed into non-decision time. There would also be no interference effects in Experiment 3 (variable duration). The bimanual task is also relevant. We see the same seriality even when the participant does not integrate the two decisions into one motor response. From this and other concerns (e.g., #8, below), we sense that we failed to get across the point that all delays that do not vary as a function of stimulus strength are absorbed in the non-decision time. We have revised the manuscript to reinforce this point.</p><disp-quote content-type="editor-comment"><p>More generally how do the authors account for motor preparation time in their model?</p></disp-quote><p>It’s a component of the non-decision time (<italic>T</italic><sub>nd</sub>) along with other coherence-independent contributions. This is now better explained in the revised manuscript. See also response to point 4b.</p><disp-quote content-type="editor-comment"><p>(5) Serial vs Parallel model: The serial model does a much better job than the parallel model of describing the response time data in Figure 1, Figure 2, Figure 3, Figure 4 and Figure 5. However, there is no discussion of the systematic underestimation of response times at very low values of motion and color coherence.</p></disp-quote><p>This is an important point. One possibility we considered is that switching between stimulus dimensions incurs a time penalty. We have not convinced ourselves that this is correct, and suspect that it is at best partially true. We are more confident about two other factors: (i) a constant variance assumption in the DDM fits that leads to an underestimate of RT at low motion coherences, and (ii) inclusion of all trials at 0% coherence (color and motion) that inflates the mean RTs. Regarding the first, our diffusion model is under-parameterized. In the standard DDM championed by Ratcliff, Wagenmakers and others, each stimulus strength would be assigned its own drift rate, whereas we impose a proportionality constraint. This assumes a linear relationship between the ratio of signal and variance, what amounts to an assumption of constant variance. In reality the variance at low coherences is likely to be smaller than what the model asserts (Shadlen et al., 2006; Britten et al., 1993). By freeing up this constraint, we can fit the data nicely as we have shown in other publications (Zylberberg, Fetsch and Shadlen, 2016). We elected not to do this here because (don’t laugh) we were trying to keep matters simple. Incorporating the extra degree of freedom in the parameterization of the DDM would remedy the issue, but we do not feel it is worth the added complexity since the model comparison is already decisive and it is confirmed by a an even more compelling approach (the empirical approach using γ distributions), which completely resolves the systematic error (see Figure 2—figure supplement 5). Regarding the second factor, all participants have a small bias in favor of a color and a direction. This means that just under half of the 0% decisions are effectively errors (i.e., incongruent with the drift rate). They inflate the observed mean and are not reflected in the curves. They are accounted for in the actual fits. We now discuss the systematic discrepancy in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>(6) Eye vs. Hand: Why are participants less sensitive to color coherence in the eye condition? Does the longer manual response time mask these differences? In addition, the discrepancies of the serial model at low coherence values are greater for the eye data (see point 3 above). How does this relate to the shorter latency of eye movements, relative to manual responses?</p></disp-quote><p>We assume the reviewer is referring to the apparently shallower choice function in Figure 2A (right, top) and the corresponding function in panel B. This is explained by the different scales. The abscissae in panel-A used a scale normalized to ±1 for each participant before averaging because the three participants did not view identical motion strengths. In panel B all participants viewed the same strengths, so we plot by the true motion and color strengths. Therefore, the sensitivity parameter (k<sub>c</sub>) offers a more informative comparison. It is a conversion of units of color coherence into units of signal-to-noise, and it reveals little difference between 2 of the 3 “eye” participants and the 8 “unimanual” participants. We have further clarified the normalization in the figure legend and methods. (We assume by point 3 the reviewer means point 5.)</p><disp-quote content-type="editor-comment"><p>(7) The motion tasks is a temporal task requiring the integration of information over time while the colour task is fundamentally a spatial task that could be solved using a single frame. To what extent can this explain the results the authors have obtained on differential cross-interference for the variable stimulus duration?</p></disp-quote><p>The reviewer’s assertion is partially correct. Information about color dominance is present in all video frames, whereas the relevant information about motion direction is first presented in the fourth frame (40 ms after the first). From then on, all motion frames contribute informatively to direction. However, both tasks benefit from “integration of information over time”, as is clear from the variable duration task (Experiment 3), improving accuracy over several hundred ms. Therefore, we think the “differential cross-interference” is not explained by low level differences in color vs. motion processing but by prioritization, which is only partially addressed in our study (e.g., Experiment 4). Please see our response to point 13, below, on this matter. The new examples of alternation in Figure 8—figure supplement 2 and 3 illuminate the potential complexity of the prioritization mechanism. This is a topic we are currently pursuing.</p><disp-quote content-type="editor-comment"><p>(8) Given these are human subjects, could the serial nature of the decision process have to do with the subjects internally verbalising the individual decisions before combining? We noted in this context that even for eye movement responses reaction times are quite long, much longer than a monkey would take for similar tasks. How do the authors exclude the possibility of verbalisation playing a role in the time course?</p></disp-quote><p>The 2D RTs are long, as the reviewer says, but that’s because they are the sum of two 1D decision times (plus <italic>T</italic><sub>nd</sub>). The 1D decision times (inferred from the fits) are normal for these types of displays, and they are similar to 1D decisions in humans on similar tasks (e.g. Bakkour et al., 2019). Of course, something like verbalization—that is, the ideation that precedes the expression of a decision (or thought)—might occur in 1D decisions too. However, there is no reason to suppose that this step would be coherence dependent, so it would be absorbed in non-decision time (see reply to 4b). It would not explain the additivity of the coherence-dependent decision times. It might help the reviewer to know that we are in the process of collecting data from monkeys. Their 1D decision times are a little faster, as is typical, but their 2D decision times also appear to be explained by a serial model. In the revised manuscript, we have modified the exposition of the non-decision time to emphasize that <italic>T</italic><sub>nd</sub> absorbs all coherence-independent contributions to the reaction times.</p><disp-quote content-type="editor-comment"><p>9) In the experiment with variable stimulus duration, it seems that the different durations are interleaved in a block of trials. If so, this creates additional uncertainty for the observer. Would the authors predict the same results if only one such duration was presented in a block?</p></disp-quote><p>Yes, we would expect the same qualitative result, but the participant might not apply the same termination criteria. The challenge, however, was to identify the one duration to use. We chose the interleaved design because it promotes the use of the same time-dependent criterion for all coherences and we wanted to compare effects on the choice functions across durations. Our approach avoids potential strategic changes of decision thresholds that could occur in a blocked design.</p><disp-quote content-type="editor-comment"><p>10) In the using a double decision with binary response:</p><p>a) How can we be assured that participants can execute two independent movements at roughly the same time? We thought that bimanual coordination tasks were quite difficult to master (e.g. Mechsner et al., 2001). Could the authors comment on the amount of training required to perform their task at a reasonable motor performance level?</p></disp-quote><p>This comment seems to pertain to the bimanual task (not the “double decision with binary response”). The bimanual movement was facilitated by virtual channels guiding the robotic handle along each movement dimension (see Materials and methods). In fact, making the bimanual movements at roughly the same time is very easy and participants mastered the movements required for both the uni- and bimanual task within the first few blocks of training. We have added this to the Materials and methods.</p><disp-quote content-type="editor-comment"><p>b) We discovered that only correct trials were analysed in the reaction time data (last line of caption of Figure 7). Why is this an appropriate choice? Was this also the case for the other experiments?</p></disp-quote><p>This and the next comment pertain to the “double decision with binary response” (Experiment 5)</p><p>We thank the reviewer for pointing this out. All analyses of RT are performed on all trials except for experiment 5. In experiment 5 (same-different task) we only analyze correct trials as there is an additional challenge: We do not know which patch of dots caused the error on an error trial. That is why we resort to the <italic>empirical</italic> approach using γ distributions to characterize 1D decision times that would combine—by serial or parallel logic—to explain the 2D decision time, to which the <italic>T</italic><sub>nd</sub> is added.</p><p>In RT tasks like ours, error trials typically have longer RT than correct trials (at the same motion coherence). This is explained by a time-dependent collapsing bound, which is the normative extension of the optimal model for binary decisions (i.e., Wald’s sequential probability ratio test) when there is uncertainty about difficulty (Drugowitsch et al., 2012). Thus, we used a model with collapsing bounds in order to fit the RT on correct as well as error trials. However, for simplicity we only plot the data (and fits) for non-error trials. This is simply to reduce the number of points and fits on the graphs. By non-error, we mean the correct choices at non-zero coherence and all trials at 0% coherence. However, in Experiment 2 with very short duration stimuli we do not expect terminating bounds to play a role, so plot all trials. We also show all data in Figure 6 for reasons explained in the manuscript (see Materials and methods. Serial and parallel drift diffusion models).</p><disp-quote content-type="editor-comment"><p>c) We were wondering why the authors did not use a conjunction judgment on their original stimulus, something like &quot;respond UP&quot; when the stimulus is either &quot;left-blue&quot; or &quot;right-yellow&quot;, and &quot;respond DOWN&quot; when the stimulus is either &quot;left-yellow&quot; or &quot;right-blue&quot; to &quot;down response&quot;. Presenting two motion fields in the two hemifields seems quite different from all the other experiments.</p></disp-quote><p>We considered this alternative design for Experiment 5, but favored the same-different version of the binary-choice task. First, we believe that the response mapping for same vs. different is more natural and easier to learn, and it requires fewer additional computations than mapping two different color-motion conjunctions onto two response keys. This is certainly true for the subjects in Experiment 5, who had previously completed the uni- and bimanual version of the motion-color task. The alternative conjunction proposed by the Reviewers would require learning a completely new S-R mapping. Second, we believe that replicating our main findings from Experiment’s 1-4 with a different stimulus display in Experiment 5 is a strength of this experiment which we have now pointed out in the results. Also see our reply to comment 1d above.</p><disp-quote content-type="editor-comment"><p>11) Is it really reasonable to assume that evidence is retained in the buffer without any loss? If there was a loss, this would be seen in the accuracy data, so I suspect this is the reason the authors did not consider this possibility. But what is the evidence for retaining information in working memory without any loss?</p></disp-quote><p>The reviewer seems to have answered their own question. We reason there is a buffer and the loss of information is minimal. The only loss of information we observe is explained by ignoring information that could not be obtained because the buffer is full and its content had not been transferred yet, owing to the bottleneck. We agree that conceptualizing a buffer as a persistent representation invites the possibility of degradation, be it through decay or corruption. This is one of the motivations for the scheme proposed in Figure 8, where a sample is held in a form that is an instruction to update the decision variable, and by how much. As the reviewer notes, the lack of interference in the choice function, combined with the bookkeeping on decision time—implies there is no substantial loss. Also, we should have seen much more interference in Experiment 3—variable stimulus duration. In the conceptual model (Figure 8), the buffer can be viewed as holding an instruction; in that sense it might be compared to the time scale of holding a plan, as in an instructed memory-delay. The new Figure 8—figure supplement 2 allows for the possibility that the buffer could hold information substantially longer than 180 ms. We do not know if this is correct, but it is compatible with the class of models that ought to be considered as we forge a connection between perceptual decision-making, iconic short-term memory and dual task interference (e.g., the psychological refractory period).</p><disp-quote content-type="editor-comment"><p>12) Figure 8 is one possible instantiation of a serial bottleneck at the decision stage. But it is not clear why the transfer from the buffer has such a large delay (180ms). What is the argument against a shorter transfer time, but a longer refractory period before the decision stage can accept new information? Since this is the speculative part of the paper, perhaps it does not have to be so detailed.</p></disp-quote><p>We thank the reviewers for raising this concern. It demonstrates that despite our best efforts we failed to get across the rationale for the choices we made in the conceptual model (Figure 8). The following two paragraphs answer the questions. They also speak to the reason that, if anything, more detail would be desirable. In the third paragraph we describe changes to the manuscript which are intended to improve understanding of the model without exacerbating the logorrhea.</p><disp-quote content-type="editor-comment"><p>“…why the transfer from the buffer has such a large delay (180ms)” The buffer does not have a set delay. It simply cannot be emptied until the bottleneck clears, and the buffer can’t be filled until it is cleared. The limiting factor is <bold>t</bold><sub>ins</sub>. Notice that in the 1D phase (after color has terminated in the example) the buffer is held <bold>t</bold><sub>ins</sub>=90 ms, because after <bold>t</bold><sub>ins</sub> the cortex has received the ∆V instruction and clears the bottleneck to solicit another sample. This is a seamless process, which satisfies one of the desiderata guiding the exercise, namely, to maintain consistency with what is known about 1D choice-RT decisions. The 180 ms delay from buffer acquisition to release arises in the alternation phase because of the added <bold>t</bold><sub>ins</sub> of the color update. The motion sample is acquired when the preceding motion sample is cleared. After <bold>t</bold><sub>ins</sub>, the cortical area that represents V<sub>m</sub> clears the bottleneck, as in the 1D case, but now the color buffer releases its content, and it is another <bold>t</bold><sub>ins</sub> before the motion buffer can be cleared. We recognize that this is potentially confusing, and we have revised the manuscript to better communicate the ideas. See below.</p><p>“…argument against a shorter transfer time, but a longer refractory period before the decision stage can accept new information” We are not sure what the reviewer means by transfer time and refractory period. Perhaps the explanation above addresses this. We are not wed to <bold>t</bold><sub>ins</sub>=90 ms. Given latencies from display to area MT, any value greater than 60 ms would be reasonable, although the motion stimulus has another 40 ms before there is any signal. There’s also the motion filter operation which is causal and therefore adds to the delay. For the process to be non-lossy, the buffered sample must represent all of the information in the display. If the samples were acquired at shorter intervals than <bold>t</bold><sub>ins</sub>, the buffer would not contain all the information in the stimulus. If the filters were leakier (faster time constant) the sampling at <bold>t</bold><sub>s</sub>=<bold>t</bold><sub>ins</sub> would be insufficient. While it is not the main point of the current paper, this is an important insight about the temporal blurring of information in extrastriate visual cortex. We refer to Cain et al., (2013) who exploit this insight in the context of robust integration. During alternation, the sampling time is 2<bold>t</bold><sub>ins</sub>, which leads to undersampling, as shown by the interference in Experiment 3 (variable duration) and which is compensated by obtaining more samples in the RT design.</p></disp-quote><p>The revised manuscript provides several brief clarifications that we hope will pre-empt the points of confusion that the reviewers experienced. We have also added three supplementary figures (to Figure 8) that demonstrate the variety of phenotypes that this scheme can exhibit. The examples provide intuition about the connection to 1D decisions, a simpler model with only one switch, like the drift diffusion model used to fit the variable duration data (Experiment 3), and probabilistic switching. In addition to furthering pedagogical goals and demystification, we think it will help readers understand why the bottleneck to perceptual decisions had escaped detection until now. If the autocorrelation times of the momentary evidence are long, temporal multiplexing would not lead to any degradation in signal to noise. The supplementary captions also serve to flesh out our choice of parameters so that an interested reader can appreciate which values were chosen in a principled manner.</p><disp-quote content-type="editor-comment"><p>13) In addition to the preference to prioritize motion, was there a tendency to prioritize the easier task, or was the motion task prioritized because observers were more sensitive to motion (at least for the eye condition)?</p></disp-quote><p>They were not more sensitive to motion (see our response to point 6). In the variable-duration task, where we present evidence for prioritization of motion, motion and color difficulty were adjusted individually in order to match the overall difficulty of the two dimensions. Thus, subjects were not more sensitive to motion than color in that task. Furthermore, we do not have a trial-by-trial measure of prioritization in this task that would allow us to investigate whether the easier dimension was prioritized on a given trial. Instead, prioritization in our model reflected a general tendency to integrate motion before color, regardless of task difficulty.</p><p>While we were unable to measure prioritization on a trial-by-trial basis in the variable-duration task, the bimanual task provided some interesting insights (results not shown in manuscript). First, in the bimanual task participants typically responded to the easier dimension first. Note that this does not necessarily imply prioritization, but rather, it reflects the fact that the easier dimension reached the decision bound earlier than the harder dimension. We additionally found that the coherence level on previous trials predicted which dimension participants responded to first. Specifically, participants were more likely to respond to motion first on a given trial when motion coherence was high on the previous 1-3 trials (same for color). Thus, prioritization may be streaky, suggesting that participants may apply top-down cognitive control that guides prioritization across trials, rather than prioritization being a mere bottom-up process that operates within trials. We plan to follow this up with further studies so do not focus on this observation in the current paper.</p></body></sub-article></article>