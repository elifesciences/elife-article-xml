<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">84602</article-id><article-id pub-id-type="doi">10.7554/eLife.84602</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Bayesian analysis of phase data in EEG and MEG</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-297510"><name><surname>Dimmock</surname><given-names>Sydney</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0163-2048</contrib-id><email>sd14814@bristol.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-263657"><name><surname>O'Donnell</surname><given-names>Cian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2031-9177</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-33850"><name><surname>Houghton</surname><given-names>Conor</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5017-9473</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>Faculty of Engineering, University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yp9g959</institution-id><institution>School of Computing, Engineering &amp; Intelligent Systems, Ulster University</institution></institution-wrap><addr-line><named-content content-type="city">Derry/Londonderry</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>09</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e84602</elocation-id><history><date date-type="received" iso-8601-date="2022-10-31"><day>31</day><month>10</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-09-11"><day>11</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-10-17"><day>17</day><month>10</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/2vcsy"/></event></pub-history><permissions><copyright-statement>© 2023, Dimmock et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Dimmock et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-84602-v3.pdf"/><abstract><p>Electroencephalography and magnetoencephalography recordings are non-invasive and temporally precise, making them invaluable tools in the investigation of neural responses in humans. However, these recordings are noisy, both because the neuronal electrodynamics involved produces a muffled signal and because the neuronal processes of interest compete with numerous other processes, from blinking to day-dreaming. One fruitful response to this noisiness has been to use stimuli with a specific frequency and to look for the signal of interest in the response at that frequency. Typically this signal involves measuring the coherence of response phase: here, a Bayesian approach to measuring phase coherence is described. This Bayesian approach is illustrated using two examples from neurolinguistics and its properties are explored using simulated data. We suggest that the Bayesian approach is more descriptive than traditional statistical approaches because it provides an explicit, interpretable generative model of how the data arises. It is also more data-efficient: it detects stimulus-related differences for smaller participant numbers than the standard approach.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Phase coherence is a measurement of waves, for example, brain waves, which quantifies the similarity of their oscillatory behaviour at a fixed frequency. That is, while the waves may vibrate the same number of times per minute, the relative timing of the waves with respect to each other may be different (incoherent) or similar (coherent).</p><p>In neuroscience, scientists study phase coherence in brain waves to understand how the brain responds to external stimuli, for example if they occur at a fixed frequency during an experiment. To do this, phase coherence is usually quantified with a statistic known as ‘inter-trial phase coherence’ (ITPC). When ITPC equals one, the waves are perfectly coherent, that is, there is no shift between the two waves and the peaks and troughs occur at exactly the same time. When ITPC equals zero, the waves are shifted from each other in an entirely random way.</p><p>Phase coherence can also be modelled on phase angles – which describe the shift in each wave relative to a reference angle of zero – and wrapped distributions. Wrapped distributions are probability distributions over phase angles that express their relative likelihood. Wrapped distributions have statistics, including a mean and a variance. The variance of a wrapped distribution can be used to model phase coherence because it explicitly represents the similarity of phase angles relative to the mean: larger variance means less coherence.</p><p>While the ITPC is a popular method for analysing phase coherence, it is a so-called ‘summary statistic’. Analyses using the ITPC discard useful information in the trial-to-trial-level data, which might not be lost using phase angles.</p><p>Thus, Dimmock, O’Donnell and Houghton set out to determine whether they could create a model of phase coherence that works directly on phase angles (rather than on the ITPC) and yields better results than existing methods.</p><p>Dimmock, O’Donnell and Houghton compare their model to the ITPC using both experimental and simulated data. The comparison demonstrates that their model can detect entrainment of the brain to grammatical phrases compared to ungrammatical ones at smaller sample sizes than ITPC, and with fewer false positives. Traditional tools for studying how the brain processes language often yield a lot of noise in the data, which makes it difficult to analyse measurements. Dimmock, O’Donnell and Houghton demonstrates that the brain is not simply responding to the ‘surprise factor’ of words in a phrase, as some have suggested, but also to their grammatical category.</p><p>These results of this study will benefit scientists who analyse phase coherence. By using the model in addition to other approaches to study phase coherence, researchers can provide a different perspective on their results and potentially identify new features in their data. This will be particularly powerful in studies with small sample sizes, such as pilot studies where maximising the use of data is important.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>EEG</kwd><kwd>MEG</kwd><kwd>Bayesian</kwd><kwd>circular statistics</kwd><kwd>neurolinguistics</kwd><kwd>frequency-tagging</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>RF-2021-533</award-id><principal-award-recipient><name><surname>Houghton</surname><given-names>Conor</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/S026630/1</award-id><principal-award-recipient><name><surname>O'Donnell</surname><given-names>Cian</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/R513179/1</award-id><principal-award-recipient><name><surname>Dimmock</surname><given-names>Sydney</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A Bayesian model of phase angles illustrates a novel approach to the analysis of phase coherence in frequency-tagged experiments.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In an electroencephalography (EEG) or magnetoencephalography (MEG) <italic>frequency-tagged</italic> experiment, the stimuli are presented at a specific frequency and the neural response is quantified at that frequency. This provides a more robust response than the typical event-related potential (ERP) paradigm because the response the brain makes to the stimuli occurs at the predefined stimulus frequency while noise from other frequencies, which will correspond to other cognitive and neurological processes, does not contaminate the response of interest. This quantification is often approached by calculating the inter-trial phase coherence (ITPC). Indeed, estimating coherence is an important methodological tool in EEG and MEG research and is used to answer a wide variety of scientific questions. There is, however, scope for improving how the phase coherence is measured by building a Bayesian approach to estimation. This is a per-item analysis and gives a better description of uncertainty. In contrast, the ITPC discards information by averaging across trials. As a demonstration, both approaches are compared by applying them to two different frequency-tagged experimental datasets and through the use of simulated data.</p><p>Frequency tagging is a well-established tool in the study of vision, where it is often referred to as the steady-state visual-evoked potential (<xref ref-type="bibr" rid="bib53">Regan, 1966</xref>). At first, it was predominately used to study low-level processing and attention (see <xref ref-type="bibr" rid="bib41">Norcia et al., 2015</xref> for a review). Latterly, though, it been used for more complex cognitive tasks, such as face recognition and discrimination (<xref ref-type="bibr" rid="bib2">Alonso-Prieto et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Farzin et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Liu-Shuang et al., 2014</xref>), perception of number (<xref ref-type="bibr" rid="bib25">Guillaume et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Van Rinsveld et al., 2020</xref>), and the ‘human quality’ of dance movements (<xref ref-type="bibr" rid="bib3">Alp et al., 2017</xref>). It has been applied to other modalities: audition (<xref ref-type="bibr" rid="bib21">Galambos et al., 1981</xref>; <xref ref-type="bibr" rid="bib49">Picton et al., 2003</xref>; <xref ref-type="bibr" rid="bib7">Bharadwaj et al., 2014</xref>), somatosensation (<xref ref-type="bibr" rid="bib58">Tobimatsu et al., 1999</xref>; <xref ref-type="bibr" rid="bib22">Galloway, 1990</xref>), and nociception (<xref ref-type="bibr" rid="bib13">Colon et al., 2012</xref>; <xref ref-type="bibr" rid="bib14">Colon et al., 2014</xref>). It has been used to study broad phenomenon like memory (<xref ref-type="bibr" rid="bib34">Lewis et al., 2018</xref>) and lateralisation (<xref ref-type="bibr" rid="bib36">Lochy et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Lochy et al., 2016</xref>) along with more specific types of neurocognitive response, such as visual acuity (<xref ref-type="bibr" rid="bib4">Barzegaran and Norcia, 2020</xref>) and the perception of music (<xref ref-type="bibr" rid="bib42">Nozaradan, 2014</xref>). Furthermore, frequency tagging can be used in the assessment of disorders such as autism (<xref ref-type="bibr" rid="bib62">Vettori et al., 2020a</xref>; <xref ref-type="bibr" rid="bib63">Vettori et al., 2020b</xref>) and schizophrenia (<xref ref-type="bibr" rid="bib12">Clementz et al., 2008</xref>). It has even been used to study neural responses to social interaction (<xref ref-type="bibr" rid="bib44">Oomen et al., 2022</xref>). Beyond EEG and MEG, phase coherence has been proposed as a mechanism for signal routing (<xref ref-type="bibr" rid="bib1">Abeles, 1982</xref>; <xref ref-type="bibr" rid="bib54">Salinas and Sejnowski, 2001</xref>; <xref ref-type="bibr" rid="bib8">Börgers and Kopell, 2008</xref>), assembly formation (<xref ref-type="bibr" rid="bib56">Singer, 1999</xref>; <xref ref-type="bibr" rid="bib10">Buzsáki, 2010</xref>), and coding (<xref ref-type="bibr" rid="bib43">O’Keefe and Recce, 1993</xref>; <xref ref-type="bibr" rid="bib45">Panzeri et al., 2010</xref>), so the measurement of phase coherence for electrocorticography, local field potentials, and neuronal spiking is important for the neuroscience of neuronal systems.</p><p>One striking application of frequency tagging is in neurolinguistics (<xref ref-type="bibr" rid="bib15">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib16">Ding et al., 2017</xref>). Neurolinguistic experiments are difficult; since language experiments inevitably involve humans and target a phenomenon whose temporal grain is often too fine for magnetic resonance imaging, the principal neural imaging techniques are EEG and MEG. However, the complexity of the neural processing of language makes the signals recorded in these experiments particularly noisy, difficult to analyse, and difficult to disentangle from other cognitive processes. A further difficulty in neurolinguistics is that an ERP is often difficult to obtain because the tens or hundreds of repetitions required would render the stimulus meaningless to the participant, a phenomenon known as the semantic satiation (<xref ref-type="bibr" rid="bib31">Jakobovits, 1962</xref>).</p><p>Consider, as an example, the frequency-tagged experiment described in <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref> and following the paradigm shown in <xref ref-type="bibr" rid="bib16">Ding et al., 2017</xref>. This is used here as a paradigmatical example of a frequency-tagged experiment in neurolinguistics and, although the description here is specific to this example, much of the methodology is typical. In <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>, the neural response to phrases was investigated by comparing the response to grammatical adjective–noun (AN) phrases<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mtext> </mml:mtext><mml:munder><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mtext> </mml:mtext><mml:munder><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>to ungrammatical adjective–verb (AV) pairs<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mtext> </mml:mtext><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where care had been taken to have a similar 2 g frequency for adjacent word pairs in each condition. The words are all presented at 3.125 Hz; however, the frequency of interest is the <italic>phrase rate</italic>, 1.5625 Hz, corresponding to the phrase structure of the AN stimuli (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). In <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>, it is suggested that the strength of the response to AN stimuli relative to AV stimuli at this frequency measures a neural response to the grammatical structure, rather than to lexical category of the words.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The syntactic target for the experiment.</title><p>In the adjective–noun (AN) stimulus, there are noun phrases at the phrase rate, 1.5625 Hz; this structure is absent in the adjective–verb (AV) stimulus because AV pairs do not form a phrase. 3.125 Hz corresponds to the syllable rate in this experiment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig1-v3.tif"/></fig><p>This investigation required a quantitative measurement of the strength of the response. The obvious choice: the induced power at 1.5625 Hz does not work, empirically this proves too noisy a quantity for the stimulus-dependent signal to be easily detected and, indeed, although the frequency tag produces a more robust signal than an ERP, for more high-level or cognitive tasks, particularly neurolinguistic tasks, where frequency-tagging is now proving valuable, the power is not a useful measure; more needs to be done to remove the noise. Typically this is done by assuming the response is phase-locked to the stimulus, and so for frequency-tagged data in cognitive tasks it is common to use the ITPC. The ITPC is defined using the mean phase angle:<disp-formula id="equ3"><label>(1)</label><mml:math id="m3"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>f</mml:mi><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:mi>f</mml:mi></mml:math></inline-formula> is the frequency, <inline-formula><mml:math id="inf2"><mml:mi>k</mml:mi></mml:math></inline-formula> is the trial index, <inline-formula><mml:math id="inf3"><mml:mi>K</mml:mi></mml:math></inline-formula> is the number of trials, <inline-formula><mml:math id="inf4"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> represents other indices such as electrode number or experimental condition, and <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the phase of the complex Fourier coefficient for the EEG or MEG trace <inline-formula><mml:math id="inf6"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Across different applications, the mean phase angle is often called the mean resultant, a term we will use here. The ITPC is the length of the mean resultant:<disp-formula id="equ4"><label>(2)</label><mml:math id="m4"><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The ITPC is chosen as a quantitative measure to extract the portion of the response at the frequency of interest that is phase-locked to the stimulus and therefore consistent in phase from trial to trial. This is another of the denoising strategies required by these noisy data.</p><p>In the case of the experiment, we are discussing here the hold-all index <inline-formula><mml:math id="inf7"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is made up of participant, condition, and electrode indices. To produce a result, the ITPC is averaged over the 32 electrodes used in the experiment to give <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf9"><mml:mi>p</mml:mi></mml:math></inline-formula> labels participants, <inline-formula><mml:math id="inf10"><mml:mi>c</mml:mi></mml:math></inline-formula> labels conditions, and <inline-formula><mml:math id="inf11"><mml:mi>f</mml:mi></mml:math></inline-formula> labels frequency. The principal result of <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref> is that, in the language of frequentist statistics, <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1.5626</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> Hz</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>AN</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is significantly larger than <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1.5626</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> Hz</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>AV</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The result of these experiments analysed using ITPC are summarised in <xref ref-type="fig" rid="fig2">Figure 2</xref>. This plots the ITPC measure for all six experimental conditions, the two, AN and AV, that have already been described and four others; a table of the experimental conditions is provided in Appendix 3. In <xref ref-type="fig" rid="fig2">Figure 2A</xref>, it is seen that there is a strong peak in ITPC at the syllable rate, 3.125 Hz, and, in the case of AN, at the phrase rate 1.5625 Hz. The ITPC at the phase rate is graphed in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, where, again, it appears only AN has a response at the phrase rate.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Summarising the inter-trial phase coherence (ITPC) for different conditions.</title><p>(<bold>A</bold>) Coloured lines show the ITPC for each participant after averaging over electrodes and is traced across all frequencies. The mean trace, calculated by averaging over all participant traces, is overlaid in black. Vertical lines mark the <italic>sentence</italic>, <italic>phrase</italic>, and <italic>syllable</italic> frequencies as increasing frequencies, respectively. (<bold>B</bold>) Statistical significance was observed with an uncorrected paired two-sided Wilcoxon signed-rank test (∗0.05, ∗∗0.01). (<bold>C</bold>) ITPC differences for each condition pair calculated at the phrase frequency and interpolated across the skull. Filled circular points mark clusters of electrodes that were discovered to be significantly different (p&lt;0.05) by the cluster-based permutation test.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig2-v3.tif"/></fig><p>In <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>, all analyses are done for ITPC averaged across electrodes; nonetheless in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, we show the condition-to-condition difference in ITPC at each electrode, but averaged across participants:<disp-formula id="equ5"><label>(3)</label><mml:math id="m5"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>p</mml:mi></mml:msub></mml:math></disp-formula></p><p>where<disp-formula id="equ6"><label>(4)</label><mml:math id="m6"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5626</mml:mn><mml:mtext> Hz</mml:mtext><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>AN</mml:mtext></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5626</mml:mn><mml:mtext> Hz</mml:mtext><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>RR</mml:mtext></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Visually these data appear to show a left temporal and right parietal response. However, the data are noisy and deciding the significance of any comparison is complicated: a straightforward calculation of the p-value using an uncorrected paired two-sided Wilcoxon signed-rank test gives a value &lt;0.05 for 54 of the 480 possible comparisons. This includes some comparisons that fit well with the overall picture, for example, when comparing AN to AV the P4 electrode shows a difference with <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mn>0.002</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and the T7 electrode with <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mn>0.044</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. It also includes some more surprising results: for the comparison of RR to RV, the CP5 electrode has <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mn>0.0182</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and the FC1 electrode has <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mn>0.0213</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. If we interpret these as significant difference, the apparent difference between two conditions without an apparent phrase structure is odd and presumably misleading. However, a naïve Bonferroni correction would use a factor of <inline-formula><mml:math id="inf18"><mml:mrow><mml:mn>32</mml:mn><mml:mo>×</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>, and in a manner typical of this very conservative approach, it massively reduces the number of significantly different responses, to one in this case.</p><p>As part of our reanalysis of these data, we used cluster-based permutation testing (<xref ref-type="bibr" rid="bib38">Maris and Oostenveld, 2007</xref>) to identify significant clusters of electrodes for each ITPC condition difference, thereby providing a quantification of the observed structure in the headcaps. See Appendix 4 for an outline of the method. This statistical test is a widely adopted approach to significance testing EEG data because it does not fall prey to the high false-negative rate of a Bonferroni correction and takes advantage of spatiotemporal correlations in the data. However, this is different to testing individual electrode effects, and we must be careful to articulate this difference. With this method it is not possible to make inferential claims about the strength of the effect of any one electrode appearing in a significant cluster; electrodes appearing in significant clusters cannot be defined as significant as these comparisons are not controlled for under the null (<xref ref-type="bibr" rid="bib55">Sassenhagen and Draschkow, 2019</xref>). As will be discussed later, this is a weaker claim than that based of the Bayesian posterior that can quantify this effect through a probabilistic statement.</p><p>There are a number of disadvantages to the ITPC. The most obvious problem is that the item in the statistical analysis of ITPC is a participant, not a trial. In the results described in <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>, the statistical significance relied on a <inline-formula><mml:math id="inf19"><mml:mi>t</mml:mi></mml:math></inline-formula>-test between conditions with a pair of data points for each participant: there are actually 24 trials for each participant but these are used to calculate the ITPC values. Some of the analysis in <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref> is done using 20 participants, some using 16; sticking to the latter for simplicity, the hypothesis testing is performed using 16 pairs of values, rather than <inline-formula><mml:math id="inf20"><mml:mrow><mml:mrow><mml:mn>16</mml:mn><mml:mo>×</mml:mo><mml:mn>24</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mn>384</mml:mn></mml:mrow></mml:math></inline-formula> or even <inline-formula><mml:math id="inf21"><mml:mrow><mml:mrow><mml:mn>16</mml:mn><mml:mo>×</mml:mo><mml:mn>24</mml:mn><mml:mo>×</mml:mo><mml:mn>32</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mn>12288</mml:mn></mml:mrow></mml:math></inline-formula> items if the individual electrodes are included. In short, the ITPC is itself a summary statistic, a circular version of variance, and so it hides the individual items inside a two-stage analysis<disp-formula id="equ7"><label>(5)</label><mml:math id="m7"><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>items</mml:mtext></mml:mstyle><mml:mo stretchy="false">→</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>ITPC</mml:mtext></mml:mstyle><mml:mo stretchy="false">→</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>statistical analysis</mml:mtext></mml:mstyle></mml:math></disp-formula></p><p>However, this is hard to rectify: it is difficult to compare items across participants, or across electrodes, because the mean phase, <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, is very variable and not meaningful to the scientific questions of interest. This variability is graphed in Figure 4: this figure shows the value of<disp-formula id="equ8"><label>(6)</label><mml:math id="m8"><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5625</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>AN</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:math></disp-formula></p><p>the phase of the mean resultant for the AN condition. For illustrative purposes, three example electrodes are picked out and the distribution across participants is plotted. What is clear is how variable these phases are; this means that individual responses cannot be compared across participants and electrode since <inline-formula><mml:math id="inf23"><mml:mi>p</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mi>e</mml:mi></mml:math></inline-formula> have such a strong effect on their value.</p><p>There are other classical tests of coherence which use phase information. One example is the Rayleigh test (<xref ref-type="bibr" rid="bib51">Rayleigh, 1880</xref>; <xref ref-type="bibr" rid="bib52">Rayleigh, 1919</xref>); this test can be used to check for either significant departure from uniformity or from the ‘expected phase’, that is, a particular phase angle specified by the researcher based on some other insight into the behaviour. Other tests, such as Hotelling’s <inline-formula><mml:math id="inf25"><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, apply jointly to phase and amplitude (<xref ref-type="bibr" rid="bib28">Hotelling, 1931</xref>; <xref ref-type="bibr" rid="bib48">Picton et al., 2001</xref>; <xref ref-type="bibr" rid="bib47">Picton et al., 1987</xref>). These classical approaches are incompatible with the neurolinguistic study presented here. Firstly, it would be difficult to provide an expected phase; as demonstrated in Figure 4, the mean phase angle is highly variable across participants. There is also no substantive prior information available that could be used to supplement this value because language experiments vary from experiment to experiment. Secondly, because of the problem of semantic satiation the experiments we consider here are relatively short and lack the frequency resolution these classical approaches require.</p><p>Here we provide a Bayesian approach to phase data. We believe this has advantages when compared to the ITPC: it permits a per-item analysis and correspondingly a more statistically efficient and richer use of the data. Furthermore, as a Bayesian approach, it supports a better description of the data because it quantifies uncertainty and because it describes a putative abstraction of the stochastic process that may have generated the data while explicitly stating the underpinning assumptions. This replaces a hypothesis-testing and significance-based account with a narrative phrased in terms of models and their consequences, so, in place of an often contentious or Procrustean framework based on hypotheses, a Bayesian approach describes a putative model and quantifies the evidence for it.</p><p>A Bayesian account starts with a parameterised probabilistic model of the data. The model proposes a distribution for the data given a set of model parameters: this is the likelihood. In our case, the likelihood will be the probability distribution for the phases of the responses, given our model. Our interest is in how the variance of this distribution depends on the condition. In addition to the likelihood, the full Bayesian model also includes a prior distribution for parameters, for example, it includes priors for the parameters which determine the relationship between the condition and the distribution of phases. The goal is to calculate the <italic>posterior distribution</italic>, the probability distribution for the parameters given the experimental data; this follows from Bayes’ theorem:<disp-formula id="equ9"><label>(7)</label><mml:math id="m9"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf26"><mml:mi mathvariant="normal">Θ</mml:mi></mml:math></inline-formula> are the parameters and <inline-formula><mml:math id="inf27"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula> the data. Essentially, <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the likelihood, the distribution of the data given some parameters: the goal is to take the data and from them calculate the posterior distribution of the parameters: <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The denominator <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can usually be ignored because it is just a normalising constant that is independent of the parameter values and therefore does not change the shape of the posterior distribution. There are a number of excellent descriptions of the Bayesian approach to data, including the textbook (<xref ref-type="bibr" rid="bib24">Gelman et al., 1995</xref>) and a recent review (<xref ref-type="bibr" rid="bib60">van de Schoot et al., 2021</xref>); our terminology and notation will often follow conventions established by the textbook (<xref ref-type="bibr" rid="bib39">McElreath, 2018</xref>).</p><p>In many ways Bayesian descriptions are more intuitive and easier to follow than the frequentist approaches that have been favoured over the last century. The impediment to their use has been the difficulty of calculating the posterior distribution. These days, however, powerful computing resources and new insight into how to treat these models mean that there are a variety of approaches to estimating the posterior; one approach, the one used here, is to sample from the posterior without calculating it analytically using Markov chain Monte Carlo (MCMC) techniques. Probabilistic programming languages such as Stan (<xref ref-type="bibr" rid="bib11">Carpenter et al., 2017</xref>) and Turing (<xref ref-type="bibr" rid="bib23">Ge et al., 2018</xref>) make it easy to use advanced MCMC sampling methods such as Hamiltonian/Hybrid Monte Carlo (HMC) and the no U-turn sampler (NUTS) (<xref ref-type="bibr" rid="bib17">Duane et al., 1987</xref>; <xref ref-type="bibr" rid="bib40">Neal, 2011</xref>; <xref ref-type="bibr" rid="bib5">Betancourt, 2013</xref>), making the complexity of a frequentist analysis unnecessary. Here, we report results calculated using Stan though many of the computations were carried out in both Stan and Turing.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Data</title><p>In this article, we consider two experimental datasets and simulated data. The first experimental dataset is the phrase data described above; this can be considered the primary example, and these data are familiar to us and formed the target data while developing the approach. In this section, the methods are described with reference to these particular data; we believe using a particular example allows us to describe the method with greater clarity. However, to demonstrate the generality of the approach we also apply it to another dataset measuring statistical learning of an artificial language. These data are described briefly here. The experiments we performed with simulated data used data generated by the Bayesian model with different effect sizes; this is described in the ‘Results’ section.</p><p>The second experimental dataset is related to statistical learning of an artificial language. Statistical learning refers to the implicit capacity of the brain to extract the rules and relationships between different stimuli in the environment. We used our Bayesian model to analyse data from an interesting frequency-tagged experiment that investigated statistical learning in an artificial language task (<xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref>). In the experiment, 18 syllables are arranged into six three-syllable words and played in a stream so that the transition probabilities inside a pseudoword are 1 while the transition between the last syllable of a pseudoword and the first of another is 0.2. The goal is to assess the degree to which pseudowords are learned. A frequency-tagged paradigm was used. The syllables were presented at a constant rate <inline-formula><mml:math id="inf31"><mml:mi>f</mml:mi></mml:math></inline-formula>, such that three-syllable pseudowords had a frequency of <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>f</mml:mi><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>. Evidence of statistical learning can then be quantified using ITPC at this frequency and its harmonics.</p><p>In the experiment, syllables were presented at 4 Hz, resulting in a three-syllable pseudoword frequency of 1.33 Hz. Each participant was subject to two conditions, which we refer to as baseline (BL) and experimental (EXP) in line with <xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref>. For the EXP condition, there were six pseudowords with a between-word transitional probability of 0.2; a summary of these are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The BL condition contained the same 18 syllables as EXP, but adopted different transitional probability rules to remove regularities. In this study, recordings were taken from 40 adult participants (25 females, 35 right-handed, ages 20–38) using 64 electrodes sampled over three blocks of 44 trials; of these participants, 39 had complete EEG recordings to use in the analysis. In the ‘Results’ section, we recapitulate the original ITPC analysis of these data and consider a Bayesian model.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Pseudowords and position-controlled syllables.</title><p>All six pseudowords used in the experiment are numbered. Groups of position-controlled syllables have been coloured.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig3-v3.tif"/></fig></sec><sec id="s2-2"><title>Circular distributions</title><p>Here, the data are a set of phases and so the model for the data is a probability distribution on a circle. The motivation which informs the ITPC is that the phases to a greater or lesser extent have a unimodal distribution around the circle and so the model should be a unimodal distribution on the circle (see Figure 5). One common class of distributions on the circle is given by the wrapped distributions with probability density function<disp-formula id="equ10"><label>(8)</label><mml:math id="m10"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf33"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a probability density of a distribution on the real line and <inline-formula><mml:math id="inf34"><mml:mi>θ</mml:mi></mml:math></inline-formula> is the angle. It might seem that the obvious choice for <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would be the Gaussian distribution. In fact, the wrapped Gaussian distribution is not a very satisfactory example of a wrapped distribution because <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> cannot be calculated in closed form. A much better example is the Cauchy distribution<disp-formula id="equ11"> <label>(9)</label><mml:math id="m11"><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <italic>x</italic><sub>0</sub> is a parameter giving the median of the distribution and <inline-formula><mml:math id="inf37"><mml:mi>γ</mml:mi></mml:math></inline-formula> is a scale parameter; the corresponding wrapped distribution has the closed form:<disp-formula id="equ12"><label>(10)</label><mml:math id="m12"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>sinh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>γ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>cosh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>γ</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>and, in contrast to the Cauchy distribution on the real line, where the moments are not defined, the wrapped distribution has a well-defined and convenient value for the mean resultant:<disp-formula id="equ13"><label>(11)</label><mml:math id="m13"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula></p><p>The circular variance <inline-formula><mml:math id="inf38"><mml:mi>S</mml:mi></mml:math></inline-formula> of the wrapped Cauchy distribution can be derived from the length of this complex vector:<disp-formula id="equ14"><label>(12)</label><mml:math id="m14"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>S</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ15"><label>(13)</label><mml:math id="m15"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Thus, as illustrated in Figure 5A, a large value of <inline-formula><mml:math id="inf39"><mml:mi>γ</mml:mi></mml:math></inline-formula> corresponds to a highly dispersed distribution; a low value to a concentrated one. With this explicit relationship between parameter values and the mean resultant, the Cauchy distribution is a convenient choice for our model.</p></sec><sec id="s2-3"><title>Prior distributions</title><p>The next important element is the choice of priors both for the mean of the wrapped distribution, µ, and for <inline-formula><mml:math id="inf40"><mml:mi>γ</mml:mi></mml:math></inline-formula>, which determines how dispersed the distribution is. The prior for µ is the more straightforward: a different value of µ is required for each participant, condition, and electrode. This prior should be uniform over the unit circle (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Although there is likely to be correlations in µ values for the same electrode across participants and for the electrodes for a given participant, since the value of µ is not of interest, it is convenient to ignore this and pick an independent value <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each triplet of participant–condition–electrode values. Future studies that aim to extend this model could consider adding correlations to µ.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Mean phases are uniform across participants.</title><p>The left-hand panel shows the distribution of phases across electrodes for each participant: each column corresponds to one participant and each dot marks the mean phase µ for each of the 32 electrodes calculated at the phrase frequency for the adjective–noun (AN) grammatical condition. To show how a given electrode varies across participant, three example electrodes are marked, T7 in green, P4 in orange, and F8 in purple. The right-hand panel shows the distribution of mean phases, µ, across participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig4-v3.tif"/></fig><p>Since µ has a uniform prior over the unit circle, it would seem that the correct prior is<disp-formula id="equ16"><label>(14)</label><mml:math id="m16"><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>This is, however, wrong; the ideal distribution is a uniform distribution on the circle, not on the interval, and while the uniform distribution on an interval has the same numerical probability values, it has a different topology. This matters when sampling using an MCMC method. In MCMC, to create the list of samples, referred to as the chain, the sampler moves from sample to sample, exploring the parameter space. In this exploration for µ, if the posterior value is, for example, close to <inline-formula><mml:math id="inf42"><mml:mi>π</mml:mi></mml:math></inline-formula>, then the chain should explore the region near to <inline-formula><mml:math id="inf43"><mml:mi>π</mml:mi></mml:math></inline-formula>, which includes values near <inline-formula><mml:math id="inf44"><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="inf45"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. A small change should move the sampler from <inline-formula><mml:math id="inf46"><mml:mi>π</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf47"><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula>. However, dynamics on the interval <inline-formula><mml:math id="inf48"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can only get from one to the other by traversing the potentially low-likelihood interval in between. Nothing in the mathematical description of the common MCMC samplers, such as NUTS, prevents the prior from being defined on a circle or other compact region. However, there is a problem: the current high-quality implementations of these methods in do not allow priors over circles.</p></sec><sec id="s2-4"><title>Sampling from a circular prior distribution</title><p>As a practical approach to avoiding this difficulty, we introduce a two-dimensional distribution which, in polar coordinates, is uniform in the angle coordinate and in the radial part restricts sampling to a ring around the origin. Because its probability density function resembles the Bundt cake tin, used to make kugelhopf (<xref ref-type="bibr" rid="bib30">Hudgins, 2010</xref>, see <xref ref-type="fig" rid="fig5">Figure 5B</xref>), this will be referred to as a Bundt distribution. The choice of the radial profile of the Bundt distribution is not critical; its purpose is to restrict the samples to a ring: we sample points <inline-formula><mml:math id="inf49"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> on a plane so that their radius <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> is drawn from a gamma distribution<disp-formula id="equ17"><label>(15)</label><mml:math id="m17"><mml:mi>ρ</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model construction and geometry.</title><p>(<bold>A</bold>) Example wrapped Cauchy density functions for different values of the scale parameter. (<bold>B</bold>) The Bundt distribution has a shape reminiscent of a cake made in a Bundt tin. (<bold>C</bold>) The mean phase is sampled from an axially symmetric prior distribution with a soft constraint on the radius. Highlighted pairs <inline-formula><mml:math id="inf51"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> give the location of example points; these points correspond to the mean angle for a wrapped Cauchy distribution using <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>angle</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>D</bold>) An example distribution for <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> which determines the <inline-formula><mml:math id="inf54"><mml:mi>γ</mml:mi></mml:math></inline-formula> parameter in the wrapped Cauchy distribution. <inline-formula><mml:math id="inf55"><mml:mi>S</mml:mi></mml:math></inline-formula> is related to other parameters such a condition and participant number through a logistic regression, as in <xref ref-type="disp-formula" rid="equ21">Equation 19</xref>, the priors for the slopes in the regression are used to produce the distribution shown here; as before, two example points are chosen, each will correspond to a different value of <inline-formula><mml:math id="inf56"><mml:mi>γ</mml:mi></mml:math></inline-formula> in the corresponding wrapped Cauchy distribution. (<bold>E</bold>) Example wrapped Cauchy distributions are plotted in correspondence with the numbered prior proposals in (<bold>C</bold>) and (<bold>D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig5-v3.tif"/></fig><p>giving what we will call a Bundt-gamma distribution. This distribution has mean 1 and standard deviation 0.1, giving the golden ring of likely <inline-formula><mml:math id="inf57"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> values seen in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. In fact, the radial values are not used in the model; what is used is the angle:<disp-formula id="equ18"><label>(16)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s2-5"><title>A linear model for the scale of the wrapped Cauchy distribution</title><p>The final element of the model is the prior for <inline-formula><mml:math id="inf58"><mml:mi>γ</mml:mi></mml:math></inline-formula>; obviously the intention is to have this depend on the condition. To make our priors easier to interpret, it is convenient to use a link function, first converting from <inline-formula><mml:math id="inf59"><mml:mi>γ</mml:mi></mml:math></inline-formula> to the circular variance <inline-formula><mml:math id="inf60"><mml:mi>S</mml:mi></mml:math></inline-formula>:<disp-formula id="equ19"><label>(17)</label><mml:math id="m19"><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf61"><mml:mi>S</mml:mi></mml:math></inline-formula> is bound between 0 and 1, so a second link function is applied<disp-formula id="equ20"><label>(18)</label><mml:math id="m20"><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>υ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>υ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the logistic function. The quantity <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>υ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> quantifies the effect of participant, condition, and electrode on response. In this model it is linear<disp-formula id="equ21"><label>(19)</label><mml:math id="m21"><mml:msub><mml:mi>υ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>so <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is understood as quantifying the effect of condition, <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the effect of the participant, and <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the effect of electrode. In the language of regression, these are slopes. In the case of <inline-formula><mml:math id="inf67"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, experimenting with different models has demonstrated a better fit when these are interaction terms, allowing the effect of respectively participant and electrode to be condition dependent.</p><p>Thus, the main objects of interest are <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and our result is calculated by sampling the posterior distribution for these quantities. Of course, these quantities also require priors. The obvious place to start is the condition effects <italic>a</italic><sub><italic>c</italic></sub>; because effects are weak in these data, our prior belief is that for any condition the circular variance should be reasonably large, likely bigger than a half. Conversely, the parameters <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> correspond to deviations about the baseline level <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> which can be represented easily using unbounded symmetric distributions. The prior for the slopes <inline-formula><mml:math id="inf75"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> has a hierarchical structure, allowing correlations across conditions; <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> models the participant response: roughly speaking the idea that a participant who is not paying attention in one condition is likely to be inattentive for all of them. The participants slopes, <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, were assigned a multivariate <inline-formula><mml:math id="inf78"><mml:mi>t</mml:mi></mml:math></inline-formula>-distribution, chosen because its heavy tails give a more robust estimation in the presence of ‘unusual’ participants: exceptionally strong or exceptionally weak, probably due to lack of attention. This multivariate parameterisation allows for a simultaneous two-way regularisation process due to information sharing both within conditions and across conditions. The idea of self-regularising priors is common in hierarchical Bayesian models and is often referred to as partial pooling (see <xref ref-type="bibr" rid="bib24">Gelman et al., 1995</xref> for a review). A similar approach was adopted for the electrode slopes, but with partially pooling only within condition, and not across conditions: testing showed that this was not useful. These priors are described in further detail as part of a full description of the model in the supporting information (see Appendix 1).</p><p>At the moment one disadvantage of Bayesian analysis is that the process of selecting priors is unfamiliar and this might appear intimidating, particularly for experimental scientists hoping to benefit from the approach without being interested in the nitty-gritty of defining priors. Hopefully, as our understanding matures, this process will become both better established and better understood, with good default choices available as suggestions from analysis libraries.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>The posterior distribution was sampled using the NUTS algorithm implementation in Stan. Four chains were run for 4000 iterations, with half dedicated to a warm up or calibration epoch. Details of the software packages and libraries used can be found in Appendix 2.</p><sec id="s3-1"><title>Comparison with ITPC results</title><p>The posterior distributions are described in <xref ref-type="fig" rid="fig6">Figure 6</xref>. This figure reprises, using our analysis, the ITPC analysis exhibited in <xref ref-type="fig" rid="fig2">Figure 2</xref>. <xref ref-type="fig" rid="fig6">Figure 6A</xref> shows a point estimate of the mean resultant length across all frequencies estimated using the optimise function within RStan; as in the earlier figure (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), there is a phase peak visible at the phrase frequency 1.5626 Hz for AN, but not for the other conditions.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Posterior distributions.</title><p>(<bold>A</bold>) The traces show point estimates of the mean resultant length calculated across all 58 frequencies using the optimisation procedure. (<bold>B</bold>) The marginal posterior distributions for each transformed condition effect <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> are shown with a violin plot. Posteriors over condition differences are given directly above, the colour of which represents the condition against which the comparison is made. For example, the green interval above the adjective–verb (AV) condition describes the posterior difference <inline-formula><mml:math id="inf80"><mml:mrow><mml:mtext class="ltx_ulem_uline">AN</mml:mtext><mml:mo>-</mml:mo><mml:mtext class="ltx_ulem_uline">AV</mml:mtext></mml:mrow></mml:math></inline-formula>. Posterior differences and marginal intervals are all given as 90% highest density intervals (HDIs) marked with posterior medians. (<bold>C</bold>) Posterior medians are interpolated across the skull for all condition comparisons. Filled circle shows those electrodes where zero was not present in the 95% HDI for the marginal posteriors over the quantity in <xref ref-type="disp-formula" rid="equ24">Equation 22</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig6-v3.tif"/></fig><p><xref ref-type="fig" rid="fig6">Figure 6B</xref> represents our attempt to find a way to present the results of Bayesian analysis in a way which resembles as much as possible the ‘significance difference bracket’ often used in presenting experimental results. At the bottom of <xref ref-type="fig" rid="fig6">Figure 6B</xref>, we see the posterior distributions over the mean resultant length for each condition. These posteriors are obtained by transforming posterior samples of the parameter <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, which describes the effect of condition on the response within the regression to circular variance <inline-formula><mml:math id="inf82"><mml:msub><mml:mi>S</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, as described in <xref ref-type="disp-formula" rid="equ20">Equation 18</xref>, then subtracting from one to obtain the mean resultant length.<disp-formula id="equ22"><label>(20)</label><mml:math id="m22"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ23"><label>(21)</label><mml:math id="m23"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>It appears that the AN condition has a higher value of the mean resultant length than the other five conditions. To examine this further, the upper panel in <xref ref-type="fig" rid="fig6">Figure 6B</xref> also shows the 90% highest density intervals (HDIs) and posterior medians of the posterior distribution over the differences between the mean resultant length of all condition pairings. The HDI provides a summary of the full posterior distribution: it is the smallest width interval that contains a specified proportion of the total probability, and here, above the violin plot for each <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, we have plotted the HDI for that condition relative to the other four: this could be considered as a Bayesian equivalent to the confidence brackets common in frequentist plots like <xref ref-type="fig" rid="fig2">Figure 2</xref>. Here, only the HDIs that do not overlap zero are the ones corresponding to a difference between AN and another condition: this clearly shows that in our model there is a neural response at the phrase stimulus frequency for AN but not for the other conditions. It appears, for example, that although the MP condition consists of grammatical phrases, the fact that these phrases are of different types means that there does not appear to be a response. This suggests that the neuronal response observed for AN is a response to a specific type of phrase, not to any phrase.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6C</xref>, we see the electrode-by-electrode comparisons across conditions. These graphs show a clearer structure than the corresponding ITPC analysis in <xref ref-type="fig" rid="fig2">Figure 2C</xref>; there is a left temporal and right parietal response for AN and nothing else. In an attempt to draw a comparison with the headcaps in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, we have highlighted the electrodes whose marginal posteriors did not contain zero. This is not a one-to-one comparison: in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, no claims of significance can be made about any specific electrode, only the clusters of activity themselves are significant. Here we are presenting evidence given by the Bayesian model based on each marginal distribution and argue that false-positives arising from these 32 comparisons should be reduced by the multivariate machinery of the Bayesian model. In <xref ref-type="fig" rid="fig6">Figure 6C</xref>, only a summary of the posterior for each electrode-by-electrode comparison is shown. It is important to note that, in contrast with the ITPC analysis in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, the posterior is much more than a point estimate. In Appendix 9, an example of the plausible patterns of activity captured by the posterior distribution for the AN–AV comparison is provided.</p></sec><sec id="s3-2"><title>Participant effects</title><p>In <xref ref-type="fig" rid="fig7">Figure 7A</xref>, we plot the 90% HDIs for the participant slopes, <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, for <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> AN; more positive values of <inline-formula><mml:math id="inf86"><mml:mi>β</mml:mi></mml:math></inline-formula> correspond to less attentive participants, more negative values correspond to more attentive. These have been arranged in increasing order of <inline-formula><mml:math id="inf87"><mml:mi>β</mml:mi></mml:math></inline-formula> with the participant number <inline-formula><mml:math id="inf88"><mml:mi>p</mml:mi></mml:math></inline-formula> given on the x-axis. From an experimental point of view, this plot gives some reassurance that there is no systematic trend, with participation becoming better or worse as the experiment progressed through participants. Our model includes a condition-dependent standard deviation for the participant response (see Appendix 1); posterior distributions for these standard deviations are plotted in <xref ref-type="fig" rid="fig7">Figure 7B</xref>. This appears to indicate that there is more across-participant variation in responses to the MP and ML conditions, where there is a structure but a complicated or confusing one, than to either the highly structured and grammatical AN condition or the RV and RR conditions, with little or no structure at the phrase rate.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Participant attentiveness and localised electrode effects.</title><p>(<bold>A</bold>) The intervals show participant effects for the grammatical adjective–noun (AN) condition given as 50/90% highest density intervals (HDIs) and posterior medians. (<bold>B</bold>) The posterior distributions over the standard deviation of participant slopes for each condition. Outer vertical lines mark the 90% posterior HDIs, inner lines mark the posterior median. (<bold>C</bold>) The skull plot from <xref ref-type="fig" rid="fig6">Figure 6C</xref> for the AN–AV difference with electrode names marked. (<bold>D</bold>) Posterior distributions over electrode differences for those positions on the skull where the grammatical condition shows a higher coherence of phases at the average participant in (<bold>C</bold>). Intervals give 50/90% HDIs and the posterior medians. AV, adjective–verb.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig7-v3.tif"/></fig></sec><sec id="s3-3"><title>Electrode effects</title><p>To investigate the electrode-dependent response, <xref ref-type="fig" rid="fig7">Figure 7C</xref> is an enlarged version of the first headcap plot from <xref ref-type="fig" rid="fig6">Figure 6C</xref>: the difference in mean resultant between AN and AV. The heatmap colour scale is recalibrated since here it refers only to this one plot. The localisation of the response is seen very clearly. It is difficult to combine a headcap plot and information about the posterior distribution, so the HDI for<disp-formula id="equ24"><label>(22)</label><mml:math id="m24"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and<disp-formula id="equ25"><label>(23)</label><mml:math id="m25"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>is plotted for three example electrodes, one electrode from each of the two active areas and one from an area that shows little activity. The response for P4 and T7 is clearly different from zero, indicating that there is a stronger response to the AN condition than to the AV condition at these two electrode. The same HDI analysis for RR versus RN does not show any electrodes whose HDI does not overlap zero; the presumably misleading results for CP5 and FC1 noted in the discussion of ITPC results do not appear here.</p><p>In <xref ref-type="fig" rid="fig2">Figure 2C</xref>, we see that even for conditions, such as RR and RV, which contain no linguistic structure at the phase rate, there are patterns of electrode activity in the topographic headcaps. In contrast, the analogous Bayesian headcaps in <xref ref-type="fig" rid="fig6">Figure 6C</xref> did not show similar patterns. We used simulated data to investigate whether the Bayesian model is correctly demonstrating that there is no phrase-level response for these conditions, rather than the other possibility: that the beguiling patterns seen in the ITPC headcaps represent a real activity invisible to the Bayesian analysis. In fact, the evidence points to the first alternative; <xref ref-type="fig" rid="fig8">Figure 8</xref> presents evidence that the Bayesian model is more faithful to the data when there is no meaningful variation in electrode effects. <xref ref-type="fig" rid="fig8">Figure 8A</xref> shows the real data again; however, whereas previously the headcap was plotted for differences between conditions, here we fit directly to the RR condition. There is no effect visible for the Bayesian headcap, but for the ITPC headcap there are variations that may suggest localised activity, even though this condition does not have any structure at the phrase rate. In <xref ref-type="fig" rid="fig8">Figure 8B</xref>, four datasets were simulated from the generative Bayesian model with electrode effects set to zero; other parameters were centred on the posterior means of the ungrammatical AV condition. The four simulations are marked as <inline-formula><mml:math id="inf91"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> in the figure. For simplicity, there is only one condition, but in other respects the simulated data mimics the real data: it has 16 participants, 32 electrodes, and 24 trials. These simulations are intended to represent four different iterations of the same experiment; apart from differing in any random numbers, they are identical. The data resulting from these four simulations were fitted with both methods. Evidently, the Bayesian results are much closer to the ground truth. The ITPC results show variations that could easily be misinterpreted.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Comparison of electrode effects for no signal.</title><p>(<bold>A</bold>) Topographic headcaps for the phrase data using the random words condition (RR). When calculating phase coherence using the inter-trial phase coherence (ITPC) for this condition, there is an apparent high but misleading variation in electrodes across the skull. This does not manifest in the Bayesian result due to regularisation of the electrode effects. (<bold>B</bold>) Data was simulated from the generative Bayesian model four times with electrode effects set to zero to provide a known ground truth. Plots <inline-formula><mml:math id="inf92"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> can be thought of as results from four separate experiments. On this simulated data, the ITPC shows variation similar to (<bold>A</bold>); the Bayesian results are consistent with the ground truth. The ITPC has an upward bias, so in all figures the mean was subtracted for ease of comparison.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig8-v3.tif"/></fig></sec><sec id="s3-4"><title>Sampler diagnostics</title><p>When calculating posteriors using MCMC, it is necessary to check the success of sampling; sometimes it can become stuck in one part of the parameter space (<xref ref-type="bibr" rid="bib24">Gelman et al., 1995</xref>; <xref ref-type="bibr" rid="bib39">McElreath, 2018</xref>). <xref ref-type="fig" rid="fig9">Figure 9</xref> plots the standard MCMC diagnostic measures calculated from our posterior samples. There does not appear to have been any problems: the most commonly used measure of the success of sampling is <inline-formula><mml:math id="inf93"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, often referred to as R-hat. This is a measure of convergence that compares the means and variances of chains; ideally it would be 1.0, but typically a value of &lt;1.05 is considered acceptable and &lt;1.02 desirable. Here, all values of R-hat are &lt;1.006, indicating good mixing; values are plotted in <xref ref-type="fig" rid="fig9">Figure 9A</xref>; <xref ref-type="fig" rid="fig9">Figure 9C</xref> plots the chains for the parameter with the largest R-hat value for each parameter type; none of these plots appear to show the sort of pathological behaviour associated with poor sampling, and chains are both stationary and convergent. Another measure of sampling success, the comparison of marginal and transitional probabilities of the Hamiltonian, is exhibited in <xref ref-type="fig" rid="fig9">Figure 9B</xref>; this also indicates good sampling. See Appendix 2 for a note on tree depth warnings.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Sampler performance and diagnostics.</title><p>(<bold>A</bold>) The performance of the sampler is illustrated by plotting <inline-formula><mml:math id="inf94"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> (R-hat) against the ratio of the effective number of samples for each parameter in the model. Points represent individual model parameters grouped by colour with a different colour for each parameter type. For convenience, the dot sizes are scaled, so the more numerous parameters have smaller dots, the less numerous, fewer, so, for example, <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> with only six examples, is large. (<bold>B</bold>) A histogram comparing the marginal energy distribution <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>π</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>, and the transitional energy distribution <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the Hamiltonian. (<bold>C</bold>) Post-warmup trace plots. All four chains for the poorest performing parameter within each parameter group are overlaid. Corresponding points in (<bold>A</bold>) are marked with a black border and zero transparency.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig9-v3.tif"/></fig></sec><sec id="s3-5"><title>Case study: Statistical learning for an artificial language</title><p>As for the phrase data in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we perform a standard ITPC analysis at the group level for this dataset. In <xref ref-type="fig" rid="fig10">Figure 10A</xref>, we replicate the original statistical analysis in <xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref> with a one-tailed <italic>t</italic>-test for each frequency. Since ITPC is bounded by zero and one, it cannot be normally distributed, so we also present the results of a Wilcoxon signed-rank test. There is a strong response at the syllable frequency (4 Hz) for both the BL and EXP conditions; however, statistical tests give complicated results. A small increase in coherence can be observed at the pseudoword rate (1.33 Hz) and an even stronger one at the second harmonic (5.33 Hz). No significant difference was observed between BL and EXP at the first harmonic (2.66 Hz), although four participants showed a considerable increase in coherence at this frequency, exceeding values <inline-formula><mml:math id="inf98"><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>*</mml:mo><mml:mtext>IQR</mml:mtext></mml:mrow></mml:math></inline-formula> above the 75th percentile of the data.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Inter-trial phase coherence (ITPC) analysis.</title><p>(<bold>A</bold>) ITPC averages across all trials and electrodes for each of the 39 participants. We replicate the statistical procedure as stated in <xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref> of paired one-sided test of a greater ITPC mean of experimental condition (EXP) at the pseudoword rate (1.33 Hz) and its first and second harmonics (2.66 Hz, 5.33 Hz). A one-sided test for a larger ITPC value of the baseline condition (BL) at the syllable rate is also performed. Significance values on the specified test results of an uncorrected paired Wilcoxon signed-rank test (left) and an uncorrected paired <italic>t</italic>-test (right). (<bold>B</bold>) Statistically significant clusters of electrodes were found using a cluster-based permutation test between these two conditions at 4 Hz and 5.33 Hz.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig10-v3.tif"/></fig><p>The headcaps in <xref ref-type="fig" rid="fig10">Figure 10B</xref> present the condition-to-condition difference in ITPC at each electrode averaged across participants (<xref ref-type="disp-formula" rid="equ5">Equation 3</xref>) and interpolated across the skull. We used cluster-based permutation testing to identify significant clusters of activity that describe the differences in average electrode response between conditions. No significant clusters of activity were found at the pseudoword frequency or its first harmonic; however, a stronger mid-to-frontal cluster appears at the second harmonic, suggesting a larger ITPC of electrodes in EXP compared to BL. A significant cluster of activity also appears at the syllable rate and describes an opposite effect of condition: frontal electrodes have a larger ITPC for BL compared to EXP.</p><sec id="s3-5-1"><title>Bayesian analysis</title><p>The model was fit separately for each frequency using four chains sampling for 2000 iterations each, with half of these attributed to the warmup phase. No divergent transitions were detected during sampling, and convergence diagnostic <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1.03</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The posteriors over the difference in mean resultant length for each frequency are shown in <xref ref-type="fig" rid="fig11">Figure 11A</xref>. Despite a preference of the posterior at the pseudoword rate to prefer values greater than zero, there are some values in the left tail consistent with no difference. From the summary statistics of the posterior differences in <xref ref-type="table" rid="table1">Table 1</xref>, we can calculate that zero is approximately 1.6 standard deviations from the posterior mean.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Bayes analysis for the statistical learning data set.</title><p>(<bold>A</bold>) Posterior distributions over the condition difference <inline-formula><mml:math id="inf100"><mml:mrow><mml:mtext class="ltx_ulem_uline">EXP</mml:mtext><mml:mo>-</mml:mo><mml:mtext class="ltx_ulem_uline">BL</mml:mtext></mml:mrow></mml:math></inline-formula> are shown for all frequencies of interest. In each case, the full posterior is given by the histogram and is annotated by its 90% highest density interval (HDI) and posterior median. (<bold>B</bold>) Marginal posterior distributions over the mean resultant length for each condition and frequency of interest. (<bold>C</bold>) Posterior means for the difference <inline-formula><mml:math id="inf101"><mml:mrow><mml:mtext class="ltx_ulem_uline">EXP</mml:mtext><mml:mo>-</mml:mo><mml:mtext class="ltx_ulem_uline">BL</mml:mtext></mml:mrow></mml:math></inline-formula> at each of the 64 electrodes are interpolated across the skull. Filled circles label those electrodes where zero was not contained by the 95% HDI calculated from the quantity in <xref ref-type="disp-formula" rid="equ24">Equation 22</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig11-v3.tif"/></fig><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Summary of posterior values for the statistical learning dataset.</title><p>All values are rounded to three decimal places. The difference shown is EXP-BL. HDI: highest density interval.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Frequency (Hz)</th><th align="left" valign="bottom">Mean</th><th align="left" valign="bottom">Median</th><th align="left" valign="bottom">SD</th><th align="left" valign="bottom">90% HDI</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1.33</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">[0.001, 0.032]</td></tr><tr><td align="char" char="." valign="bottom">2.66</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">[–0.014, 0.015]</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">–0.025</td><td align="char" char="." valign="bottom">–0.025</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">[–0.052, 0.000]</td></tr><tr><td align="char" char="." valign="bottom">5.33</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">[0.016, 0.046]</td></tr></tbody></table></table-wrap><p>If we are interested in the full extent of the posterior distribution, it would be more appropriate to calculate the total probability associated with any value of the parameter that supports it. For example, we can calculate from posterior samples that <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mn>0.956</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. This analysis indicates that there is no strong evidence for a large difference in expectation between conditions at the pseudoword frequency, but it is plausible that a difference exists. The first harmonic of the pseudoword rate clearly demonstrates no difference between the experimental groups. The posterior is peaked symmetrically around a mean of zero and has low variance. The second harmonic shows the largest difference between the groups; zero is approximately 3.24 standard deviations away from the mean. Consistent with the ITPC analysis, the results at the syllable frequency also show BL as having a larger value than EXP. In this case, zero lies approximately 1.59 standard deviations from the mean.</p><p>Posterior means at each electrode for the same difference are shown in <xref ref-type="fig" rid="fig11">Figure 11B</xref>. As before, filled points are those electrodes where the 95% HDI of the marginal posterior over the condition difference does not contain zero. In line with the ITPC analysis for the pseudoword frequency, and its first harmonic (<xref ref-type="fig" rid="fig10">Figure 10B</xref>), there is no evidence to suggest any localised patterns of activity occurring in either condition by the Bayesian model. A strong result appears for the second harmonic; according to the Bayesian result, every electrode has a higher mean resultant length in EXP compared to BL. At the syllable rate, a frontal response is discovered; this also reprises the findings from the ITPC analysis.</p><p>The Bayesian results give evidence of statistical learning in the second harmonic of the pseudoword frequency; however, results for the pseudoword frequency are not so strong as to rule out no difference entirely. It appears that the strength of conclusions to be made is limited by some participants demonstrating an opposite effect. In Appendix 8, we plot the posterior distribution over the EXP-BL comparison at each frequency and for each participant. In the strongest result, 5.33 Hz, the majority of participants show an increased response in EXP. However in 2.33 Hz and 1.33 Hz, the number of participants that show an opposite effect of condition to those that do not is much more even. In <xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref>, it is suggested that evidence of SL is difficult to find in individual participants. The high variance of participant posteriors both within and across frequencies supports this conclusion.</p></sec></sec><sec id="s3-6"><title>Simulation study</title><p>In this article, we have tried to concentrate on real experimental data: real data often has characteristics and irregularities that are hard to reproduce using fictive data. In our approach here, we have been fortunate that there are datasets which have already been analysed using other, more traditional methods, allowing us to test our method in a situation with something akin to ground truth. Nonetheless, it is useful to also explore the characteristics of our method on fictive data; using fictive data allows us to manipulate effect sizes and the amount of data and contains a ground truth, albeit one located in the perhaps too regular a context provided by simulated data generation.</p><p>The Bayesian model reduces bias in the estimation of <inline-formula><mml:math id="inf103"><mml:mi>R</mml:mi></mml:math></inline-formula>. If <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the true value of the mean resultant length for a condition <inline-formula><mml:math id="inf105"><mml:mi>i</mml:mi></mml:math></inline-formula>, then <inline-formula><mml:math id="inf106"><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, as calculated by the formula for ITPC (<xref ref-type="disp-formula" rid="equ4">Equation 2</xref>), is a positively biased overestimate of this quantity (<xref ref-type="bibr" rid="bib32">Kutil, 2012</xref>). In a simulation study, we demonstrate that our Bayesian model reduces bias in the estimation of this quantity compared to a typical ITPC analysis. We sampled <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub> as ground truth mean resultant lengths for two fictive conditions uniformly over the interval <inline-formula><mml:math id="inf107"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> by replacing the Beta distribution, which described an explicit prior assumption, with a uniform distribution (see <xref ref-type="disp-formula" rid="equ37">Equation 35</xref>). We then use this modified model to generate fictive datasets with different numbers of participants and trials over the sampled ground truth values <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub>. The estimation bias in each dataset was then analysed with both the ITPC and Bayesian approaches.</p><p><xref ref-type="fig" rid="fig12">Figure 12A</xref> plots the result of this study for fictive datasets of 5 participants, 10 trials, and 8 electrodes. Each point on the graph is a summary of the performance of each method on its estimation of the true difference in mean resultant length. The <inline-formula><mml:math id="inf108"><mml:mi>x</mml:mi></mml:math></inline-formula> axis gives the absolute value of the true difference, and the <inline-formula><mml:math id="inf109"><mml:mi>y</mml:mi></mml:math></inline-formula> axis gives the ratio of the estimated difference and the true difference:<disp-formula id="equ26"><label>(24)</label><mml:math id="m26"><mml:mtext>ratio of difference</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Simulation study.</title><p>(<bold>A</bold>) The Bayesian model has a higher true detection rate for lower participant numbers. The bias of the estimate is also greatly reduced by the Bayesian model. As the real difference increases along the <italic>x</italic> axis, the variation in model estimates reduces in both methods; however, the distribution of these points around <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is much more symmetric for the Bayesian model; this result highlights its bias reduction. The <italic>y</italic> axis has been restricted to help highlight the behaviour of interest. (<bold>B</bold>) Simulation-based calibration for the same participant and trial numbers where the rank of <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> is analysed. There is no evidence to suggest a tendency of the Bayesian model to overestimate or underestimate the difference.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig12-v3.tif"/></fig><p>Any systematic deviation from the ideal value of 1 implies a bias in the estimation. Such a trend is present with the ITPC estimation but reduced in the Bayesian one.</p><p>Using the same simulated datasets, we looked at how well each method detects a real difference in the mean resultant length. In <xref ref-type="fig" rid="fig12">Figure 12A</xref>, points are coloured orange when a difference is correctly detected by the method (zero lies outside the 95% HDI for Bayes, p&lt;0.05 for the ITPC using a paired two-tailed Wilcoxon signed-rank test). The Bayesian model can detect a real difference in mean resultant length for smaller participant numbers. Interestingly, even after a doubling of the number of trials for the same participant number, a difference still cannot be detected by the statistical test: see Appendix 5 and Appendix 6 for the comparison of true-positive and false-positive rates on simulated data between both methods.</p><p>We used simulation-based calibration (SBC) (<xref ref-type="bibr" rid="bib57">Talts et al., 2018</xref>) to demonstrate the calibration of our model. SBC is primarily an algorithm for unit-testing a Bayesian model and is used to provide a visual proof that the model – and its geometry – are not so complex that the sampling algorithm, on average, cannot sample from it without providing a dishonest description of parameters in the posterior. We can use this method to show that our Bayesian model does not provide a biased estimation of <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> arising from systematic overestimates or underestimates of <inline-formula><mml:math id="inf113"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula>.</p><p>Checks for calibration using SBC require that the histogram of rank statistics produced by the algorithm is demonstrably uniform. <xref ref-type="fig" rid="fig12">Figure 12B</xref> gives the straightforward histogram of rank statistics and the quantiles containing 99% of the total variation we expect of a true uniform distribution estimated from a finite sample size (details are provided in Appendix 7). The histogram gives no indication of behaviour deviating from a uniform distribution in specific bins or by a more general trend such as a sloping histogram of ranks. Smaller deviations can be hard to detect using this approach, so a second, more sensitive approach is recommended (<xref ref-type="bibr" rid="bib57">Talts et al., 2018</xref>). The second plot shows the difference between the empirical cumulative distribution function (ECDF) for a uniform distribution and the ECDF estimated from the histogram of rank statistics. The area containing the variability expected of the difference between the uniform ECDF and uniform CDF is shaded. Even with this more sensitive approach no deviation from permissible variation is present. From this result, we can be confident that under its generating process the Bayesian model does not provide a biased estimate of <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>Here, we have presented a Bayesian description of phase data using examples from neurolinguistics. Our approach reprises the original conclusions of the statistical analysis of these data, but, we believe, does so in a more expressive and more natural way. Our account focuses on neurolinguistics, where frequency tagging is common, and we use specific neurolinguistic examples: an example with which we are familiar and for which data is openly available (<xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>), and an example from a recent study that investigated the presence of statistical learning for an artificial language (<xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref>). However, we believe that our approach has broad application across the multiple applications of frequency tagging. Bayesian analysis is, essentially, a more modest approach to data than the more commonly used frequentist analysis: where a frequentist approach seeks to establish with significant certainty whether a hypothesis is true or false, perhaps also using Bayes factors to quantify evidence for a particular hypothesis using a discrete set of models, in a Bayesian analysis we restrict ourselves to the more achievable goal of estimating the values of parameters in a model of the data and calculating our certainty or uncertainty in making those estimates.</p><sec id="s4-1"><title>Model extensions</title><p>The resemblance of a logistic regression for the scale of the wrapped Cauchy allows the model to be easily adjusted to include other terms typical of a linear model. For example, the statistical learning dataset (<xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref>) records from participants in blocks. One extension to the model would be to include an additional term to capture any block effects; alternatively it could also be implemented as an interaction between block and condition.</p><p>It is clear from the ITPC headcaps in both analyses that electrodes have spatial correlation. The datasets used in the analyses have been relatively large, both in terms of participants and trials; this has been an important factor in helping the Bayesian model, and the ITPC, resolve activity patterns across the skull. However, for smaller datasets this is not a guarantee. An extension to the model would incorporate prior knowledge about electrode locations, modifying the current independent approach to one that encodes a correlated response between neighbouring electrodes. An initial starting point would be a multivariate normal with a correlation matrix calculated using an appropriate kernel function, such as an exponentiated quadratic, applied to a 2/3D distance matrix of electrode locations.</p><p>The statistical learning dataset (<xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref>) differs from the phrase dataset (<xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>) as effects of condition also appear in the harmonics of the main frequency. In the Bayesian analysis we presented, each harmonic is fitted independently of each other. However, if a response is expected at a selection of harmonics of the baseline, then a model that jointly handles these frequencies would be potential avenue for improvement, especially for smaller datasets where information sharing between dependent parameters is a powerful tool for obtaining better estimates.</p></sec><sec id="s4-2"><title>Data efficiency</title><p>The Bayesian approach also appears to make more efficient use of the data. In order to investigate the data efficiency of the frequentist and Bayesian approaches, we used the phrase data (<xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>) and simulated the result we would have had if the experiment had been stopped early, with fewer participants. It can be misleading to directly compare frequentist and Bayesian results; the aims of the two approaches are different. Nonetheless, we have done just that. In <xref ref-type="fig" rid="fig13">Figure 13A</xref>, we plot the confidence intervals arising from the two-sided paired Wilcoxon signed-rank test alongside the HDIs from the posterior distribution for decreasing participants numbers. This is produced by removing participants from the data starting with the last recorded. It shows that the posterior still points to a real difference of condition in cases where the low participant number causes the frequentist confidence interval to overlap with zero and fail. The width of these intervals is derived from the critical values outlined below. In <xref ref-type="fig" rid="fig13">Figure 13B</xref>, we plot the p-value produced by the same test, and the corresponding probability, calculated from the posterior, of observing a difference less than zero. We also mark the lines for <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.003</mml:mn></mml:mrow></mml:math></inline-formula>; these correspond to the critical values in an uncorrected one-sided test, an uncorrected two-sided test, and a two-sided test in which a Bonferroni correction of <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is used to correct for multiple comparisons across the six phrase conditions. We are not advocating for any of these <inline-formula><mml:math id="inf119"><mml:mi>α</mml:mi></mml:math></inline-formula> values, and the uncertainty in deciding an appropriate value of <inline-formula><mml:math id="inf120"><mml:mi>α</mml:mi></mml:math></inline-formula> plagues frequentist approaches. Interestingly, when both participants 15 and 16 are removed from the data, leaving 14 participants, the p-value increases by a factor of approximately 2 (<inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>: 0.005, <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:math></inline-formula> : 0.011). <xref ref-type="fig" rid="fig7">Figure 7A</xref> can explain this result: the posteriors for <inline-formula><mml:math id="inf123"><mml:mi>β</mml:mi></mml:math></inline-formula> show that participant 15 performs better on the task than participant 16, so removing participant 15 from the analysis weakens the result more than removing participant 16.</p><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>Efficiency of the frequentist and Bayesian approaches for participant number.</title><p>(<bold>A</bold>) Confidence intervals arising from a two-sided paired Wilcoxon signed-rank (left), alongside Bayesian highest density intervals (right), calculated for the condition difference AN-RR in the phrase dataset. The intervals give widths 90/95/99.7% for each method respectively. (<bold>B</bold>) The p-value arising from the same significance test (left) compared with the probability of observing a value less than zero by the posterior distribution (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig13-v3.tif"/></fig><p><xref ref-type="fig" rid="fig14">Figure 14</xref> is similar to <xref ref-type="fig" rid="fig13">Figure 13</xref>; however, it uses the statistical learning dataset (<xref ref-type="bibr" rid="bib50">Pinto et al., 2022</xref>), comparing conditions BL and EXP at the frequency 5.33 Hz. In fact, for these data we saw little evidence that the Bayesian approach works better when the number of participants is reduced: we attribute this to the large number of trials; generally the extra efficiency of a Bayesian approach appears most apparent in low data regimes and the statistical learning dataset is admirably well sampled. For this reason, we used this dataset to investigate data efficiency when the number of trials is reduced for a fixed number of participants. In <xref ref-type="fig" rid="fig14">Figure 14</xref>, data from the first 20 participants are considered and the analysis is repeated with different numbers of trials, discarding trials from the end. It is clear from <xref ref-type="fig" rid="fig14">Figure 14A</xref> that the Bayesian model can reliably detect the signal in the data with at least half the number of trials that the frequentist approach requires; this is potentially useful especially because of the challenge semantic satiation poses to some neurolinguistic experiments. <xref ref-type="fig" rid="fig14">Figure 14B</xref> compares the p-values arising from the significance test with <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> calculated from the posterior and shows the fast convergence of the posterior to the signal; the p-value is much slower and also more variable across trials. For these analyses regarding data efficiency, the degrees of freedom parameter <inline-formula><mml:math id="inf125"><mml:mi>ν</mml:mi></mml:math></inline-formula> was fixed to 30 to address divergent transitions arising for small participant numbers. HDIs were calculated from 8000 posterior samples.</p><fig id="fig14" position="float"><label>Figure 14.</label><caption><title>Efficiency of the frequentist and Bayesian approaches for trial number.</title><p>(<bold>A</bold>) Confidence intervals arising from a two-sided paired Wilcoxon signed-rank (left), alongside Bayesian highest density intervals (HDIs) (right), calculated for the condition difference EXP-BL in the statistical learning dataset. The intervals are given for confidence levels of 90 and 95%. On the right are the HDIs for the same levels. (<bold>B</bold>) The p-value arising from the significance test (left) compared with the probability of observing a value less than zero by the posterior distribution (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-fig14-v3.tif"/></fig><p>Through simulation we have shown that for lower participant numbers there is evidence that the Bayesian model can detect a true difference more quickly. Similarly, if you have many participants but few trials the Bayesian model also provides a benefit. The probability of making a type 1 error also appeared markedly reduced when using the Bayesian approach for a range of data sizes. Together, these promote the adoption of the Bayesian approach to analysing phase data, especially in studies where data is limited, such as pilot studies, where findings influence the direction of subsequent research.</p><p>It may appear that our motivation is contradictory; we first explain that frequency-tagging produces robust encephalography results, but then explain that a new framework is required to analyse these results because they are often too noisy to study using a naïve power analysis. Of course, there is no contradiction; the encephalographic study of cognitive phenomena like language demands both a robust experimental paradigm and a cutting-edge analysis pipeline!</p></sec><sec id="s4-3"><title>EEG data can benefit from a Bayesian analysis</title><p>The Bayesian approach we have advanced in this article is undoubtedly much more computationally demanding than a frequentist approach; it also demands some thought and experiment in the formulation of the model and its priors. Frequency tagging is, in this regard, a particularly demanding application of the approach. However, we believe that the clarity of a Bayesian description and the complete way it presents the model and its evidence, along with the great data efficiency it provides, make it superior. Some of the complexity of our approach derives from the difficulty of sampling a circle, and we hope this example will be helpful in incorporating compact distributions into the standard probabilistic packages such as <monospace>Stan</monospace> and <monospace>Turing</monospace>.</p><p>In general, Bayesian models become worth the effort in scenarios with two properties: (1) where the data are limited and noisy, so statistical uncertainty is high and therefore worth representing explicitly; (2) where the dataset has a strong structure, which the Bayesian model can be designed to match and therefore share information across parameters. For these reasons, we also believe that similar Bayesian approaches will have broad application to EEG data. The nature of EEG data, its noisiness high-dimension, and the tendency to small participant numbers make it likely that Bayesian methods will be helpful. This certainly is evident in the preliminary work report in <xref ref-type="bibr" rid="bib59">Turco and Houghton, 2022</xref>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Funding acquisition, Methodology, Project administration, Supervision, Visualization</p></fn><fn fn-type="con" id="con2"><p>Writing – original draft, Writing – review and editing, Methodology, Project administration, Supervision, Visualization</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Funding acquisition, Writing – original draft, Writing – review and editing, Methodology, Project administration, Supervision, Software, Visualization</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-84602-mdarchecklist1-v3.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>This manuscript is a computational study, so no data have been generated. All modelling code for this study is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/conorhoughton/NeuralProcessingOfPhrases">GitHub</ext-link> (also provided in appendix 2). The statistical learning dataset used as a case study in this paper is available from <ext-link ext-link-type="uri" xlink:href="https://osf.io/syn3h/?view_only=af8fb548e79f4b6a8c36f9985f2cf247">OSF</ext-link> (contact author EZ Golumbic for data related correspondence).</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Burroughs</surname><given-names>A</given-names></name><name><surname>Kazanina</surname><given-names>N</given-names></name><name><surname>Houghton</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Grammatical category and the neural processing of phrases - EEG data</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.4385970</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>D</given-names></name><name><surname>Golumbic</surname><given-names>EZ</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Assessing the sensitivity of EEG-based frequency-tagging as a metric for statistical learning</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/syn3h/">syn3h</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>CH is a Leverhulme Research Fellow (RF-2021-533). We would also like to acknowledge funding from the MRC (MR/S026630/1 to COD) and an EPSRC Doctoral Training Partnership (EP/R513179/1) award to SD. We would like to recognise the discussion '<ext-link ext-link-type="uri" xlink:href="https://discourse.mc-stan.org/t/divergence-treedepth-issues-with-unit-vector/8059">divergence treedepth issues with unit vector</ext-link>' that highlighted some of the difficulties involved with sampling directional statistics and potential ways to ameliorate them in our model.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abeles</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Role of the cortical neuron: integrator or coincidence detector?</article-title><source>Israel Journal of Medical Sciences</source><volume>18</volume><fpage>83</fpage><lpage>92</lpage><pub-id pub-id-type="pmid">6279540</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso-Prieto</surname><given-names>E</given-names></name><name><surname>Belle</surname><given-names>GV</given-names></name><name><surname>Liu-Shuang</surname><given-names>J</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The 6 Hz fundamental stimulation frequency rate for individual face discrimination in the right occipito-temporal cortex</article-title><source>Neuropsychologia</source><volume>51</volume><fpage>2863</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.08.018</pub-id><pub-id pub-id-type="pmid">24007879</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alp</surname><given-names>N</given-names></name><name><surname>Nikolaev</surname><given-names>AR</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Kogo</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>EEG frequency tagging dissociates between neural processing of motion synchrony and human quality of multiple point-light dancers</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>44012</elocation-id><pub-id pub-id-type="doi">10.1038/srep44012</pub-id><pub-id pub-id-type="pmid">28272421</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barzegaran</surname><given-names>E</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural sources of letter and Vernier acuity</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>15449</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-72370-3</pub-id><pub-id pub-id-type="pmid">32963270</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Betancourt</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Generalizing the No-U-Turn Sampler to Riemannian Manifolds</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1304.1920">https://doi.org/10.48550/arXiv.1304.1920</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Girolami</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hamiltonian Monte Carlo for hierarchical models</article-title><source>Current Trends in Bayesian Methodology with Applications</source><volume>79</volume><fpage>2</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1201/b18502</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharadwaj</surname><given-names>HM</given-names></name><name><surname>Lee</surname><given-names>AKC</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Measuring auditory selective attention using frequency tagging</article-title><source>Frontiers in Integrative Neuroscience</source><volume>8</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2014.00006</pub-id><pub-id pub-id-type="pmid">24550794</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Börgers</surname><given-names>C</given-names></name><name><surname>Kopell</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Gamma oscillations and stimulus selection</article-title><source>Neural Computation</source><volume>20</volume><fpage>383</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.07-06-289</pub-id><pub-id pub-id-type="pmid">18047409</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burroughs</surname><given-names>A</given-names></name><name><surname>Kazanina</surname><given-names>N</given-names></name><name><surname>Houghton</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Grammatical category and the neural processing of phrases</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>2446</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-81901-5</pub-id><pub-id pub-id-type="pmid">33510230</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural syntax: cell assemblies, synapsembles, and readers</article-title><source>Neuron</source><volume>68</volume><fpage>362</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.023</pub-id><pub-id pub-id-type="pmid">21040841</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>B</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Goodrich</surname><given-names>B</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Brubaker</surname><given-names>MA</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Riddell</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stan: a probabilistic programming language</article-title><source>Journal of Statistical Software</source><volume>76</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id><pub-id pub-id-type="pmid">36568334</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clementz</surname><given-names>BA</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Keil</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Normal electrocortical facilitation but abnormal target identification during visual sustained attention in schizophrenia</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>13411</fpage><lpage>13418</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4095-08.2008</pub-id><pub-id pub-id-type="pmid">19074014</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colon</surname><given-names>E</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Legrain</surname><given-names>V</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Steady-state evoked potentials to tag specific components of nociceptive cortical processing</article-title><source>NeuroImage</source><volume>60</volume><fpage>571</fpage><lpage>581</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.12.015</pub-id><pub-id pub-id-type="pmid">22197788</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colon</surname><given-names>E</given-names></name><name><surname>Legrain</surname><given-names>V</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>EEG frequency tagging to dissociate the cortical responses to nociceptive and nonnociceptive stimuli</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>2262</fpage><lpage>2274</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00648</pub-id><pub-id pub-id-type="pmid">24738772</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id><pub-id pub-id-type="pmid">26642090</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Characterizing neural entrainment to hierarchical linguistic units using electroencephalography (EEG)</article-title><source>Frontiers in Human Neuroscience</source><volume>11</volume><elocation-id>481</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2017.00481</pub-id><pub-id pub-id-type="pmid">29033809</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duane</surname><given-names>S</given-names></name><name><surname>Kennedy</surname><given-names>AD</given-names></name><name><surname>Pendleton</surname><given-names>BJ</given-names></name><name><surname>Roweth</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hybrid monte carlo</article-title><source>Physics Letters B</source><volume>195</volume><fpage>216</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/0370-2693(87)91197-X</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farzin</surname><given-names>F</given-names></name><name><surname>Hou</surname><given-names>C</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Piecing it together: infants’ neural responses to face and object structure</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1167/12.13.6</pub-id><pub-id pub-id-type="pmid">23220577</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabry</surname><given-names>J</given-names></name><name><surname>Simpson</surname><given-names>D</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visualization in Bayesian Workflow</article-title><source>Journal of the Royal Statistical Society Series A</source><volume>182</volume><fpage>389</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1111/rssa.12378</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gabry</surname><given-names>J</given-names></name><name><surname>Mahr</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Bayesplot: plotting for Bayesian models</data-title><version designator="r package version 1.9.0">r package version 1.9.0</version><source>Bayesplot</source><ext-link ext-link-type="uri" xlink:href="https://mc-stan.org/bayesplot">https://mc-stan.org/bayesplot</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galambos</surname><given-names>R</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name><name><surname>Talmachoff</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A 40-Hz auditory potential recorded from the human scalp</article-title><source>PNAS</source><volume>78</volume><fpage>2643</fpage><lpage>2647</lpage><pub-id pub-id-type="doi">10.1073/pnas.78.4.2643</pub-id><pub-id pub-id-type="pmid">6941317</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galloway</surname><given-names>NR</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Human brain electrophysiology: evoked potentials and evoked magnetic fields in science and medicine</article-title><source>British Journal of Ophthalmology</source><volume>74</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1136/bjo.74.4.255-a</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ge</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>K</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Turing: A language for flexible probabilistic inference</article-title><conf-name>International conference on artificial intelligence and statistics PMLR</conf-name><fpage>1682</fpage><lpage>1690</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Carlin</surname><given-names>JB</given-names></name><name><surname>Stern</surname><given-names>HS</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Bayesian Data Analysis</source><publisher-name>Chapman and Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1201/9780429258411</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guillaume</surname><given-names>M</given-names></name><name><surname>Mejias</surname><given-names>S</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Dzhelyova</surname><given-names>M</given-names></name><name><surname>Schiltz</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A rapid, objective and implicit measure of visual quantity discrimination</article-title><source>Neuropsychologia</source><volume>111</volume><fpage>180</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.01.044</pub-id><pub-id pub-id-type="pmid">29408421</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo</article-title><source>Journal of Machine Learning Research: JMLR</source><volume>15</volume><fpage>1593</fpage><lpage>1623</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horner</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1946">1946</year><article-title>A problem on the summation of simple harmonic functions of the same, amplitude and frequency but of random phase</article-title><source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source><volume>37</volume><fpage>145</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1080/14786444608561070</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotelling</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1931">1931</year><article-title>The generalization of student’s ratio</article-title><source>The Annals of Mathematical Statistics</source><volume>2</volume><fpage>360</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177732979</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Houghton</surname><given-names>C</given-names></name><name><surname>Dimmock</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Neuralprocessingofphrases</data-title><version designator="swh:1:rev:cc063783cd6d974d65509d05311c999b728945cc">swh:1:rev:cc063783cd6d974d65509d05311c999b728945cc</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:ce91c6914c6b57f1e0b81a0b1a9c0db2bdeba61b;origin=https://github.com/conorhoughton/NeuralProcessingOfPhrases;visit=swh:1:snp:ea3c97826d783a7c159a8e7cdc4667138456a69c;anchor=swh:1:rev:cc063783cd6d974d65509d05311c999b728945cc">https://archive.softwareheritage.org/swh:1:dir:ce91c6914c6b57f1e0b81a0b1a9c0db2bdeba61b;origin=https://github.com/conorhoughton/NeuralProcessingOfPhrases;visit=swh:1:snp:ea3c97826d783a7c159a8e7cdc4667138456a69c;anchor=swh:1:rev:cc063783cd6d974d65509d05311c999b728945cc</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hudgins</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Alsatian kugelhopf: a cake for all seasons</article-title><source>Gastronomica</source><volume>10</volume><fpage>62</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1525/gfc.2010.10.4.62</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jakobovits</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1962">1962</year><data-title>Effects of repeated stimulation on cognitive aspects of behavior: some experiments on the phenomenon of semantic Satiation</data-title><source>PhD Thesis</source></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutil</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Biased and unbiased estimation of the circular mean resultant length and its variance</article-title><source>Statistics</source><volume>46</volume><fpage>549</fpage><lpage>561</lpage><pub-id pub-id-type="doi">10.1080/02331888.2010.543463</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowski</surname><given-names>D</given-names></name><name><surname>Kurowicka</surname><given-names>D</given-names></name><name><surname>Joe</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating random correlation matrices based on vines and extended onion method</article-title><source>Journal of Multivariate Analysis</source><volume>100</volume><fpage>1989</fpage><lpage>2001</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2009.04.008</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>AG</given-names></name><name><surname>Schriefers</surname><given-names>H</given-names></name><name><surname>Bastiaansen</surname><given-names>M</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Assessing the utility of frequency tagging for tracking memory-based reactivation of word representations</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>7897</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-26091-3</pub-id><pub-id pub-id-type="pmid">29785037</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu-Shuang</surname><given-names>J</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An objective index of individual face discrimination in the right occipito-temporal cortex by means of fast periodic oddball stimulation</article-title><source>Neuropsychologia</source><volume>52</volume><fpage>57</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.10.022</pub-id><pub-id pub-id-type="pmid">24200921</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lochy</surname><given-names>A</given-names></name><name><surname>Van Belle</surname><given-names>G</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A robust index of lexical representation in the left occipito-temporal cortex as evidenced by EEG responses to fast periodic visual stimulation</article-title><source>Neuropsychologia</source><volume>66</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.11.007</pub-id><pub-id pub-id-type="pmid">25448857</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lochy</surname><given-names>A</given-names></name><name><surname>Van Reybroeck</surname><given-names>M</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Left cortical specialization for visual letter strings predicts rudimentary knowledge of letter-sound association in preschoolers</article-title><source>PNAS</source><volume>113</volume><fpage>8544</fpage><lpage>8549</lpage><pub-id pub-id-type="doi">10.1073/pnas.1520366113</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McElreath</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</source><publisher-name>Chapman and Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1201/9781315372495</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>MCMC using Hamiltonian dynamics</article-title><source>Markov Chain Monte Carlo</source><volume>2</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1201/b10905</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Appelbaum</surname><given-names>LG</given-names></name><name><surname>Ales</surname><given-names>JM</given-names></name><name><surname>Cottereau</surname><given-names>BR</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The steady-state visual evoked potential in vision research: A review</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/15.6.4</pub-id><pub-id pub-id-type="pmid">26024451</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Exploring how musical rhythm entrains brain activity with electroencephalogram frequency-tagging</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>369</volume><elocation-id>20130393</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0393</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Recce</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Phase relationship between hippocampal place units and the EEG theta rhythm</article-title><source>Hippocampus</source><volume>3</volume><fpage>317</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1002/hipo.450030307</pub-id><pub-id pub-id-type="pmid">8353611</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oomen</surname><given-names>D</given-names></name><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name><name><surname>Wiersema</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>EEG frequency tagging evidence of social interaction recognition</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>17</volume><fpage>1044</fpage><lpage>1053</lpage><pub-id pub-id-type="doi">10.1093/scan/nsac032</pub-id><pub-id pub-id-type="pmid">35452523</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Sensory neural codes using multiplexed temporal scales</article-title><source>Trends in Neurosciences</source><volume>33</volume><fpage>111</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2009.12.001</pub-id><pub-id pub-id-type="pmid">20045201</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papaspiliopoulos</surname><given-names>O</given-names></name><name><surname>Roberts</surname><given-names>GO</given-names></name><name><surname>Sköld</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A General framework for the parametrization of hierarchical models</article-title><source>Statistical Science</source><volume>22</volume><fpage>59</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1214/088342307000000014</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picton</surname><given-names>TW</given-names></name><name><surname>Vajsar</surname><given-names>J</given-names></name><name><surname>Rodriguez</surname><given-names>R</given-names></name><name><surname>Campbell</surname><given-names>KB</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Reliability estimates for steady-state evoked potentials</article-title><source>Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section</source><volume>68</volume><fpage>119</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/0168-5597(87)90039-6</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picton</surname><given-names>TW</given-names></name><name><surname>Dimitrijevic</surname><given-names>A</given-names></name><name><surname>John</surname><given-names>MS</given-names></name><name><surname>Van Roon</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The use of phase in the detection of auditory steady-state responses</article-title><source>Clinical Neurophysiology</source><volume>112</volume><fpage>1698</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(01)00608-3</pub-id><pub-id pub-id-type="pmid">11514253</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picton</surname><given-names>TW</given-names></name><name><surname>John</surname><given-names>MS</given-names></name><name><surname>Dimitrijevic</surname><given-names>A</given-names></name><name><surname>Purcell</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Human auditory steady-state responses: Respuestas auditivas de estado estable en humanos</article-title><source>International Journal of Audiology</source><volume>42</volume><fpage>177</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.3109/14992020309101316</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>D</given-names></name><name><surname>Prior</surname><given-names>A</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Assessing the Sensitivity of EEG-Based Frequency-Tagging as a Metric for Statistical Learning</article-title><source>Neurobiology of Language</source><volume>3</volume><fpage>214</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1162/nol_a_00061</pub-id><pub-id pub-id-type="pmid">37215560</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayleigh</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1880">1880</year><article-title>On the resultant of a large number of vibrations of the same pitch and of arbitrary phase</article-title><source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source><volume>10</volume><fpage>73</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1080/14786448008626893</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayleigh</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1919">1919</year><article-title>On the problem of random vibrations, and of random flights in one, two, or three dimensions</article-title><source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source><volume>37</volume><fpage>321</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1080/14786440408635894</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Regan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Some characteristics of average steady-state and transient responses evoked by modulated light</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>20</volume><fpage>238</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(66)90088-5</pub-id><pub-id pub-id-type="pmid">4160391</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>E</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Correlated neuronal activity and the flow of neural information</article-title><source>Nature Reviews. Neuroscience</source><volume>2</volume><fpage>539</fpage><lpage>550</lpage><pub-id pub-id-type="doi">10.1038/35086012</pub-id><pub-id pub-id-type="pmid">11483997</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sassenhagen</surname><given-names>J</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cluster-based permutation tests of MEG/EEG data do not establish significance of effect latency or location</article-title><source>Psychophysiology</source><volume>56</volume><elocation-id>e13335</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13335</pub-id><pub-id pub-id-type="pmid">30657176</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuronal synchrony: a versatile code for the definition of relations?</article-title><source>Neuron</source><volume>24</volume><fpage>49</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80821-1</pub-id><pub-id pub-id-type="pmid">10677026</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Talts</surname><given-names>S</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Simpson</surname><given-names>D</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Validating bayesian inference algorithms with simulation-based calibration</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.06788">https://arxiv.org/abs/1804.06788</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobimatsu</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>YM</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Steady-state vibration somatosensory evoked potentials: physiological characteristics and tuning function</article-title><source>Clinical Neurophysiology</source><volume>110</volume><fpage>1953</fpage><lpage>1958</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(99)00146-7</pub-id><pub-id pub-id-type="pmid">10576493</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Turco</surname><given-names>D</given-names></name><name><surname>Houghton</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Bayesian Modeling of Language-Evoked Event-Related Potentials</article-title><conf-name>2022 Conference on Cognitive Computational Neuroscience</conf-name><pub-id pub-id-type="doi">10.32470/CCN.2022.1051-0</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Schoot</surname><given-names>R</given-names></name><name><surname>Depaoli</surname><given-names>S</given-names></name><name><surname>King</surname><given-names>R</given-names></name><name><surname>Kramer</surname><given-names>B</given-names></name><name><surname>Märtens</surname><given-names>K</given-names></name><name><surname>Tadesse</surname><given-names>MG</given-names></name><name><surname>Vannucci</surname><given-names>M</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Veen</surname><given-names>D</given-names></name><name><surname>Willemsen</surname><given-names>J</given-names></name><name><surname>Yau</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bayesian statistics and modelling</article-title><source>Nature Reviews Methods Primers</source><volume>1</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1038/s43586-020-00001-2</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Rinsveld</surname><given-names>A</given-names></name><name><surname>Guillaume</surname><given-names>M</given-names></name><name><surname>Kohler</surname><given-names>PJ</given-names></name><name><surname>Schiltz</surname><given-names>C</given-names></name><name><surname>Gevers</surname><given-names>W</given-names></name><name><surname>Content</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The neural signature of numerosity by separating numerical and continuous magnitude extraction in visual cortex with frequency-tagged EEG</article-title><source>PNAS</source><volume>117</volume><fpage>5726</fpage><lpage>5732</lpage><pub-id pub-id-type="doi">10.1073/pnas.1917849117</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vettori</surname><given-names>S</given-names></name><name><surname>Dzhelyova</surname><given-names>M</given-names></name><name><surname>Van der Donck</surname><given-names>S</given-names></name><name><surname>Jacques</surname><given-names>C</given-names></name><name><surname>Steyaert</surname><given-names>J</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Boets</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Frequency-Tagging electroencephalography of superimposed social and non-social visual stimulation streams reveals reduced saliency of faces in autism spectrum disorder</article-title><source>Frontiers in Psychiatry</source><volume>11</volume><elocation-id>332</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyt.2020.00332</pub-id><pub-id pub-id-type="pmid">32411029</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vettori</surname><given-names>S</given-names></name><name><surname>Dzhelyova</surname><given-names>M</given-names></name><name><surname>Van der Donck</surname><given-names>S</given-names></name><name><surname>Jacques</surname><given-names>C</given-names></name><name><surname>Van Wesemael</surname><given-names>T</given-names></name><name><surname>Steyaert</surname><given-names>J</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Boets</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Combined frequency-tagging EEG and eye tracking reveal reduced social bias in boys with autism spectrum disorder</article-title><source>Cortex</source><volume>125</volume><fpage>135</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2019.12.013</pub-id><pub-id pub-id-type="pmid">31982699</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Full model</title><p>In the Bayesian model, the individual phases are modelled as draws from a wrapped Cauchy distribution:<disp-formula id="equ27"><label>(25)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Wrapped-Cauchy</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where, as above, <inline-formula><mml:math id="inf126"><mml:mi>p</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf127"><mml:mi>c</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf128"><mml:mi>e</mml:mi></mml:math></inline-formula> are participant, condition, and electrode number, and <inline-formula><mml:math id="inf129"><mml:mi>k</mml:mi></mml:math></inline-formula> is the trial number. The mean phase is derived from the Bundt-gamma distribution:<disp-formula id="equ28"><label>(26)</label><mml:math id="m28"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>The probability density function for the Bundt-gamma distribution can be derived through a Jacobian adjustment from polar to Cartesian coordinates. Our assumptions in polar coordinates are a uniform angle, and a gamma distributed radius:<disp-formula id="equ29"><label>(27)</label><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><label>(28)</label><mml:math id="m30"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Unlike the choice of a uniform distribution for the mean, the choice for the distribution for the radius is somewhat arbitrary because it has no implication for quantities that we analyse. It is simply a mathematical tool that can promote more efficient sampling by soft-constraining the sampler in parameter space. To represent these assumptions in Cartesian coordinates, we multiply these independent assumptions by the Jacobian of the transformation <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ31"><label>(29)</label><mml:math id="m31"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ32"><label>(30)</label><mml:math id="m32"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msub><mml:mi>ρ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This gives an angle uniform on the circle, not on the interval:<disp-formula id="equ33"><label>(31)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>As described above, the model for <inline-formula><mml:math id="inf131"><mml:mi>γ</mml:mi></mml:math></inline-formula> uses a pair of link functions so<disp-formula id="equ34"><label>(32)</label><mml:math id="m34"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ35"><label>(33)</label><mml:math id="m35"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>υ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with a linear model for <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>υ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ36"><label>(34)</label><mml:math id="m36"><mml:msub><mml:mi>υ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>We have priors for each of <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, what in linear regression are referred to as slopes. The prior for <inline-formula><mml:math id="inf136"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is induced through placing a prior over <inline-formula><mml:math id="inf137"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which represents the baseline circular variance for each condition<disp-formula id="equ37"><label>(35)</label><mml:math id="m37"><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>By applying the change of variables formula, we can work out the pdf for the prior induced on <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ38"><label>(36)</label><mml:math id="m38"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow></mml:mstyle><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mstyle><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>As discussed above, for <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> we have a hierarchical structure modelling covariance of participant responses across conditions, thus:<disp-formula id="equ39"><label>(37)</label><mml:math id="m39"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ν</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">0</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector over the <inline-formula><mml:math id="inf141"><mml:mi>c</mml:mi></mml:math></inline-formula> index. With <italic>C</italic> conditions, the scale matrix <inline-formula><mml:math id="inf142"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> matrix. It is made up of a correlation matrix <inline-formula><mml:math id="inf144"><mml:mi mathvariant="normal">Ω</mml:mi></mml:math></inline-formula>, and a set of scales, <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf146"><mml:msub><mml:mi>σ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula>.<disp-formula id="equ40"><label>(38)</label><mml:math id="m40"><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>⋅</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>To facilitate the interpretation as a covariance matrix, this scale matrix needs to be multiplied by <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The correlation matrix has a Lewandowski–Kurowicka–Joe prior (<xref ref-type="bibr" rid="bib33">Lewandowski et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Gelman et al., 1995</xref>):<disp-formula id="equ41"><label>(39)</label><mml:math id="m41"><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">J</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2.0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>The prior for the degrees of freedom parameter <inline-formula><mml:math id="inf148"><mml:mi>ν</mml:mi></mml:math></inline-formula> is given a gamma prior:<disp-formula id="equ42"><label>(40)</label><mml:math id="m42"><mml:mi>ν</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and the scales have half-normal priors:<disp-formula id="equ43"><label>(41)</label><mml:math id="m43"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>Finally, for <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> we partially pool electrodes within condition only:<disp-formula id="equ44"><label>(42)</label><mml:math id="m44"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ45"><label>(43)</label><mml:math id="m45"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>To attempt a standard notation, we have followed the conventions set by the julia library Distribution.jl by writing the distributions as words and using the same arguments as are found there: in particular, the two parameters for the Gamma distribution correspond to shape and scale.</p><p>The prior distributions for <inline-formula><mml:math id="inf150"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:mi>δ</mml:mi></mml:math></inline-formula> were implemented using a reparameterisation known as <italic>non-centring</italic> (<xref ref-type="bibr" rid="bib46">Papaspiliopoulos et al., 2007</xref>). This is a commonly adopted technique in hierarchical Bayesian modelling to help alleviate funnels, a class of pathological feature in the target distribution that cause slow and biased sampling. This reparameterisaton does not change the mathematical model; its sole purpose is to help the numerical computation. See <xref ref-type="bibr" rid="bib39">McElreath, 2018</xref> and <xref ref-type="bibr" rid="bib6">Betancourt and Girolami, 2015</xref> for an introduction to this approach.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Software</title><p>Posteriors were sampled using rstan v2.21.5 and cmdstanr v0.5.2. Data and posteriors were analysed using R v4.2.1; tidyverse v1.3.1; reshape2 v1.4.4; and HDInterval v0.2.2. All graphs were plotted in ggplot2 v3.3.6. <xref ref-type="fig" rid="fig2">Figure 2B</xref> used ggsignif v0.6.3 for hypothesis testing and additional plotting functionality; <xref ref-type="fig" rid="fig5">Figure 5B</xref> used viridis v0.6.2 for heatmap colours; headcaps were interpolated using mgcv v1.8–40 for <xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="fig" rid="fig6">Figure 6C</xref>, and <xref ref-type="fig" rid="fig7">Figure 7C</xref>; ridgeplots were created for <xref ref-type="fig" rid="fig7">Figure 7B</xref> with ggridges v0.5.3; Hamiltonian energy distributions were plotted in <xref ref-type="fig" rid="fig9">Figure 9B</xref> using bayesplot v1.9.0 (<xref ref-type="bibr" rid="bib20">Gabry and Mahr, 2022</xref>; <xref ref-type="bibr" rid="bib19">Gabry et al., 2019</xref>). All panels were composed using inkscape v1.1.1.</p></sec><sec sec-type="appendix" id="s10"><title>Code and data</title><p>The data used here are from the open dataset (<xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>); all codes are available on <ext-link ext-link-type="uri" xlink:href="https://github.com/conorhoughton/NeuralProcessingOfPhrases">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib29">Houghton and Dimmock, 2023</xref>).</p></sec><sec sec-type="appendix" id="s11"><title>Tree depth warnings</title><p>The sampler has been observed to produce a low number (&lt;1%) of max_treedepth warnings. This does not imply biased computation like those arising from divergences, but it is a warning about efficiency. A higher tree depth comes at the cost of doubling the number of gradient evaluations required at the previous depth (<xref ref-type="bibr" rid="bib26">Hoffman and Gelman, 2014</xref>), adding a penalty to the run time.</p></sec><sec sec-type="appendix" id="s12"><title>Computing resources</title><p>Posteriors were sampled locally on a Dell XPS 13 7390 laptop (Intel i7-10510U @ 1.80 GHz, 16 GB of RAM) running under Ubuntu 20.04.4 LTS.</p></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s13"><title>Table of experimental conditions</title><p>The six experimental conditions are shown in <xref ref-type="table" rid="app3table1">Appendix 3—table 1</xref>.</p><table-wrap id="app3table1" position="float"><label>Appendix 3—table 1.</label><caption><title>Table of conditions for the phrase dataset.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Condition</th><th align="left" valign="bottom">Description</th><th align="left" valign="bottom">Example</th></tr></thead><tbody><tr><td align="left" valign="bottom">AN</td><td align="left" valign="bottom">Adjective–noun pairs</td><td align="left" valign="bottom">…<underline>old rat sad man</underline>…</td></tr><tr><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">Adjective–verb pairs</td><td align="left" valign="bottom">…rough give ill tell…</td></tr><tr><td align="left" valign="bottom">ML</td><td align="left" valign="bottom">Adjective–pronoun verb–preposition</td><td align="left" valign="bottom">…old this ask in…</td></tr><tr><td align="left" valign="bottom">MP</td><td align="left" valign="bottom">Mixed grammatical phrases</td><td align="left" valign="bottom">…<underline>not full more green</underline>…</td></tr><tr><td align="left" valign="bottom">RV</td><td align="left" valign="bottom">Random words with every fourth a verb</td><td align="left" valign="bottom">…his old from think…</td></tr><tr><td align="left" valign="bottom">RR</td><td align="left" valign="bottom">Random words</td><td align="left" valign="bottom">…large out fetch her…</td></tr></tbody></table></table-wrap><p>Of these, four are ‘phrase conditions,’ AN, AV, MP, and RR, and were analysed in <xref ref-type="bibr" rid="bib9">Burroughs et al., 2021</xref>; the other two, ML and RV, are ‘sentence conditions’ which formed part of the experiment and are used to investigate phenomena which proved to be absent. ML stands for ‘mixed lexical’ and provides a four-word analogue of the AV condition, repeating lexical category but avoiding grammatical structure. All stimuli are available; see the data and code availability list in Appendix 2.</p></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s14"><title>Cluster-based permutation testing</title><p>For the non-Bayesian results, significant clusters of electrodes were identified using cluster-based permutation testing on the ITPC differences. First the mean resultant length was calculated for each participant, condition, and electrode by averaging over trials at the frequency of interest <inline-formula><mml:math id="inf152"><mml:mi>f</mml:mi></mml:math></inline-formula>.<disp-formula id="equ46"><label>(44)</label><mml:math id="m46"><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="2.470em" minsize="2.470em">|</mml:mo></mml:mrow></mml:mstyle><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>f</mml:mi><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="2.470em" minsize="2.470em">|</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Cluster-based permutation testing requires a test statistic (separate of any significance testing of clusters) to threshold values, in this case electrodes, that contribute to cluster formulation. The test statistic we chose for deciding if an electrode should appear in a cluster is the mean of the difference in mean resultant length:<disp-formula id="equ47"><label>(45)</label><mml:math id="m47"><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>The threshold that signifies an electrode as being important enough to appear in a cluster is based on it being larger than some value that would be unlikely assuming no signal in the data. Specifically, assuming uniformity in the angle of an electrode across trials, and sufficiently large number of trials <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, the quantity <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be modelled using a Rayleigh distribution (<xref ref-type="bibr" rid="bib51">Rayleigh, 1880</xref>; <xref ref-type="bibr" rid="bib27">Horner, 1946</xref>).<disp-formula id="equ48"><label>(46)</label><mml:math id="m48"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:msqrt><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To determine a threshold for values of <italic>z</italic><sub><italic>e</italic></sub> that may be due to a real signal in the data, we bootstrap the distribution of <italic>z</italic><sub><italic>e</italic></sub> as the mean of the difference of two Rayleigh distributions. This is a distribution of test statistics arising from the assumption that observed phase angles are uniform across trials for each participant, electrode and condition. Values of the 2.5 and 97.5% quantiles of this distribution were used to threshold the test statistic; we estimated<disp-formula id="equ49"><label>(47)</label><mml:math id="m49"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mn>0.065</mml:mn><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mn>0.975</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ50"><label>(48)</label><mml:math id="m50"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>39</mml:mn><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>132</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mn>0.018</mml:mn><mml:mstyle scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>≈</mml:mo><mml:mn>0.975</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>With this setup defined we can proceed with the cluster-based permutation algorithm. For 5000 iterations, <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was calculated for each participant by randomly swapping condition labels. For this permuted dataset, the value of the test statistic is calculated and electrodes are thresholded. Clusters were formed based on the spatial proximity of thresholded electrodes in a normalised 2D map. The size of the largest cluster for each iteration (sum of the absolute value of test statistics <italic>z</italic><sub><italic>e</italic></sub> of all electrodes within the cluster) was appended to the null distribution. Finally, this procedure is replicated without permuting the data to calculate a value of the test statistic for the observed data. Clusters identified in the non-shuffled data that had a sum of test statistics greater than the 95th percentile of the approximated null distribution are marked as significant. These appear as filled points in the plots.</p></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s15"><title>True discovery rates</title><p>It is important to quantify how well the Bayesian model can correctly detect a true difference in mean resultant length. We simulated from the generative model 500 times for two conditions over a range of participant–trial pairs. From this simulation experiment, we conclude that for lower participant numbers the Bayesian model can detect a true difference much more consistently than the ITPC. The disparity between the two is completely reduced once enough data has been included. From the plot, we can see that the type 2 error is approximately 25% and appears to be slowly decreasing with data size. This is as expected as more data should increase the power of the ITPC, and through a similar reasoning, should also benefit the Bayesian analysis.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>True discovery rates.</title><p>A summary of more participant and trial numbers of the plots shown in <xref ref-type="fig" rid="fig12">Figure 12A</xref>. For five participants, a doubling of the number of trials still provides insufficient information for the inter-trial phase coherence (ITPC) and resulting significance test to conclude that a difference exists in any simulation. Bars show the 95% confidence intervals on this measure estimated through bootstrap.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-app5-fig1-v3.tif"/></fig></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s16"><title>False discovery rates</title><p>Alongside our comparison of the true discovery rate between the Bayesian model and ITPC, we also looked at the false discovery, or type 1 error rates. This required a slight change in the generative model for the data, namely, setting the true value of the <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> in both fictive condition groups equal. Our simulation showed that the false discovery rates between the models are considerably different and dependent on data size. It is immediately clear from the plot below that the Bayesian result outperforms the ITPC in almost every combination of participant and trial number.</p><p>To investigate what was driving this discrepancy between the two approaches, we compared the posterior distribution to a bootstrapped sampling distribution of the mean difference for simulated datasets where the Bayesian model gives a true-negative, but the ITPC a false-positive. This highlighted that the Bayesian estimate is a more conservative one, likely taking into account more sources of variation in the data to form its conclusion about the difference. The ITPC was overconfident in its estimates compared to the Bayesian counterpart; its paradoxical trend of increasing false-positive rates with increasing data size is due to making already overconfident conclusions worse. The statistical test only operates on a summary statistic of the data; it does not know about the number of trials, or even the number of electrodes, the Bayesian model does. Increasing participants for the same trial number, over increasing trials for the same, helps reduce the type 1 error rate of the ITPC because this is the only dimension that can effectively inform the test about variation in the population.</p><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>False discovery rates.</title><p>The frequentist approach to inter-trial phase coherence (ITPC) differences using a paired Wilcoxon signed-rank test has a type 1 error that increases with both participant and trial number. Bars show the 95% confidence intervals on this measure estimated through bootstrap.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-app6-fig1-v3.tif"/></fig></sec></app><app id="appendix-7"><title>Appendix 7</title><sec sec-type="appendix" id="s17"><title>Simulation-based calibration</title><p>To generate the set of rank statistics, we iterated <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:math></inline-formula> times taking <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1023</mml:mn></mml:mrow></mml:math></inline-formula> post warm up sampled at each iteration. This results in a distribution of 2000 rank statistics: in this case integers in the interval [0, 1023]. Neighbouring ranks were then binned together to reduce the total number of bins down to 32 as necessary to give a trade-off between variance reduction and sensitivity of the histogram (<xref ref-type="bibr" rid="bib57">Talts et al., 2018</xref>). The horizontal lines on the histogram mark the (0.005, 0.5, 0.995) quantiles from a Binomial distribution that describes the variation expected of counts in any of the 32 rank-bins after N iterations:<disp-formula id="equ51"><label>(49)</label><mml:math id="m51"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>32</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p></sec></app><app id="appendix-8"><title>Appendix 8</title><sec sec-type="appendix" id="s18"><title>Participant posteriors</title><p>In a frequency-tagged experiment, it may be of interest to the researcher to look for effects of condition in individual participants. From posterior samples, it is possible to construct participant-specific posteriors over the mean resultant length or its difference between condition. In a similar manner to the calculation for electrodes in <xref ref-type="disp-formula" rid="equ24">Equation 22</xref>:<disp-formula id="equ52"><label>(50)</label><mml:math id="m52"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> EXP, <inline-formula><mml:math id="inf160"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> BL, and<disp-formula id="equ53"><label>(51)</label><mml:math id="m53"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><fig id="app8fig1" position="float"><label>Appendix 8—figure 1.</label><caption><title>Participant posteriors.</title><p>Highest density intervals containing 99% of the posterior probability for each participant and frequency over the difference in mean resultant length EXP-BL.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-app8-fig1-v3.tif"/></fig></sec></app><app id="appendix-9"><title>Appendix 9</title><sec sec-type="appendix" id="s19"><title>Headcap variation</title><p>In <xref ref-type="fig" rid="fig6">Figures 6B</xref> and <xref ref-type="fig" rid="fig7">7C</xref>, headcap plots are constructed by interpolating posterior means at each electrode across the skull. This is useful for summarising the result; however, it ignores the joint behaviour of the posterior and how its uncertainty describes a range of similar, but different responses. The plot below shows 25 samples from the AN-AV posterior distribution. Each headcap has been normalised through local z-scoring to prevent large magnitude differences from masking any individual behaviour.</p><fig id="app9fig1" position="float"><label>Appendix 9—figure 1.</label><caption><title>Joint headcap posterior.</title><p>Here we visualise uncertainty in the posterior over the difference AN-AV and how it captures a range of plausible activity patterns. As expected, samples demonstrate variation about the mean shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref> of a right parietal and left temporal activation. AN: adjective–noun; AV: adjective–verb.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84602-app9-fig1-v3.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84602.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><p>This important work advances the available statistical methods for estimating the degree to which the neural response is phase-locked to a stimulus. It does so by taking a compelling Bayesian approach that leverages the circular nature of the phase readout and demonstrates the added value of the approach in both simulated and empirical datasets.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84602.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Zoefel</surname><given-names>Benedikt</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Bayesian analysis of phase data in EEG and MEG&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Reviewing Editor Andrea Martin and Senior Editor Joshua Gold. The following individual involved in the review of your submission has agreed to reveal their identity: Benedikt Zoefel (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) All reviewers agree that a more extensive quantitative demonstration of the advantages of your methodological approach compared to existing approaches is needed. For example, first, quantifying the advantage of your analysis over the ITPC analysis in the manuscript would be more convincing than the current graphical approach.</p><p>Furthermore, it seems that using a <italic>simulation</italic> approach could be helpful. Simulation of common experimental and data situations, as well as extreme or tough cases where traditional methods run into problems (but your method does not, or is more robust), could be persuasive and help make the impact of the approach more demonstrable and quantifiable.</p><p>2) Comparison of data – More thorough and extensive quantitative comparison of the performance of your method compared to existing approaches, as all Reviewers mention, could be carried out on multiple (open) datasets of various sample sizes.</p><p>3) Reviewer 3 gives helpful concrete suggestions and concerns regarding the impact of this method for statistical inference (viz., mixed models). These, too, need to be addressed, ideally also quantitatively, but could also be addressed formally/mathematically.</p><p>4) Reviewer 1 helpfully explains how the perspective of experimentalists needs to be taken into approach in order for the work to have more impact. Similar to (3) above.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The study by Dimmock et al. proposes a Bayesian approach to measuring phase coherence. Although I'm familiar with the kind of EEG data analyzed here, I didn't figure out the purpose of the study. It seems like the aim of the study is neither to provide a more powerful statistical test nor to demonstrate some new neural phenomena. The only purpose seems to provide a Bayesian test, but why do we want it?</p><p>If the aim is to provide a more powerful test, it should be compared to classic tests for steady-state responses, such as the ones described in the following article.</p><p>Picton, Terence W., et al. &quot;The use of phase in the detection of auditory steady-state responses.&quot; Clinical Neurophysiology 112.9 (2001): 1698-1711.</p><p>The current article is certainly not written in a way that can be understood by an experimentalist. It doesn't matter too much if the methods are hard to follow, but it does matters if no interpretable results are shown. For example, the authors argue that the topographic plots using the new method have a clearer structure than the traditional ones. As an experimentalist, however, I can't figure out which structure is clearer and why it helps to answer scientific questions.</p><p>As a methodological paper, testing the method on multiple datasets is a basic requirement. More importantly, the method has to have a clear goal and clearly demonstrate how the goal is achieved.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This paper presents a novel Bayesian approach to testing for phase coherence in neurophysiological recordings. The approach is centred on probability distributions and therefore allows for more fine-grained conclusions about the existence of such phase consistency, in contrast to the often artificial yes/no decision on the acceptance of the alternative hypothesis that can be found in the literature.</p><p>I find this manuscript well written and the rationale well explained. The authors demonstrate that their approach can produce similar, but potentially clearer and less noisy results as compared to more commonly applied techniques (such as inter-trial coherence). It remains difficult to quantify differences between the two approaches (Bayesian vs frequentist) – for instance, the authors write that &quot;these graphs [from Bayesian analysis] show a clearer structure than the corresponding ITPC analysis&quot; without providing a quantification of the difference.</p><p>Together, this paper will be useful to the community, possibly opening up new ways of analysing phase-locked neural responses.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>This paper proposes a Bayesian take on the inter-trial phase coherence (ITPC) used to estimate how consistent the oscillatory phase is across trials for a given condition of interest. For standard ITPC the statistical power of the comparisons on the group level is determined by the sample size of the dataset since estimates are derived by first averaging across trials to derive a single condition-level estimate per subject. The advantage of the proposed Bayesian approach is that the resulting model is more robust as it is estimated from the trial level without averaging. It also allows us to derive subject-level estimates (slopes) and explore subject-variable noise. The authors illustrated this by replicating the ITPC analysis from the paper by Burroughs et al. (2021a) using the Bayesian ITPC and demonstrating perceivable noise reduction in the resulting estimates across frequencies and topographical EEG plots. Another key advantage of this method, as illustrated by the authors, is the ability to generate stable estimates for much smaller EEG datasets. While the authors show that Bayesian ITPC can replicate the findings obtained with the standard ITPC, it is not clear what advantages the proposed Bayesian approach offers over other previously proposed methods that allow for trial-level modelling such as linear mixed effects modelling. Secondly, a broader and more accessible description of the steps of the model settings, estimation, and the derivation of the summary statistics should be provided to enable the reader to replicate this method for their own dataset</p><p>Abstract</p><p>1) lines 12-17 please consider re-phrasing as the message here is not very clear. Please be more specific (based on your analysis findings) what Bayesian approach to coherence contributes more than the traditional one? 'More descriptive' and 'data-efficient' are vague descriptions.</p><p>Introduction</p><p>2) Lines 26-44. Here to help the readers I would recommend communicating your main point early in the paragraph – that measurement of coherence is an important methodological tool in M/EEG research that is used to answer a wide variety of scientific questions, yet there is room for improvement in how ITPC is estimated.</p><p>3) Line 84 – 'this plots', instead of 'this graphs'</p><p>4) Lines 96-107 – the main message from this section is not clear. Do authors argue that in the per-electrode ITPC approach the Bonferroni correction for multiple comparisons precludes finding meaningful spatial patterns in the data? In such cases, Bonferroni is rarely used, and spatial cluster-based permutation is a typically used approach that is less conservative and allows the finding of significant clusters of spatially connected electrodes.</p><p>5) Lines 126-128 – please unpack a bit more what is meant by 'a better description of the data' and 'a narrative phrased in terms of models and their consequence'.</p><p>6) Line 161 – here you mean Figure 4?</p><p>Methods section</p><p>7) Authors provide a detailed explanation and mathematical descriptions for the distributions from which the phase data can be modelled, and parameters are sampled when building up a Bayesian model of the ITPC. The supplementary materials then detail equations behind the full model used. Yet from these two sources of information, it is challenging for the reader to reconstruct the set of steps authors took to derive the results they plot in Figure 5. If the aim of the paper is to have the reader use the Bayesian approach to ITPC for their own datasets a more accessible step-by-step description of the model estimation is necessary – from calculating participant and electrode slopes/estimates to averaging steps used to produce Figure 5. This can be done by expanding relevant sections in the Methods.</p><p>8) Other methods such as Linear mixed models that likewise allow trial-level analysis and model subject slopes have been broadly applied to the EEG data and also ITPC. To increase the contribution of this paper, authors need to outline and demonstrate analytically the advantages of the Bayesian approach over these other non-Bayesian methods.</p><p>Discussion section</p><p>9) The section Model design choices seem to belong in the Results and not the Discussion section.</p><p>10) The Data efficiency section is very helpful in demonstrating the advantage of the Bayesian approach for smaller datasets. This section can be expanded by demonstrating further key advantages of the Bayesian approach over other non-Bayesian methods that use a trial-level approach (as proposed in point 8).</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Bayesian analysis of phase data in EEG and MEG&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Joshua Gold (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Reviewers 2 and 3 are satisfied with your revisions, however, given that <italic>eLife</italic> works on consensus and the fact that Reviewer 1 is not satisfied with a major concern of theirs from the first round of review, we ask that you directly address Reviewer 1's queries thoroughly. This includes those concerns regarding interpretability for experimentalists, and specifically, that you compare your method to the classic tests for steady-state responses as the Reviewer suggests. Please pay close attention to each of Reviewer 1's queries and address them in full.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. The authors did not address my all my comments and I copied them here.</p><p>If the aim is to provide a more powerful test, it should be compared to classic tests for steady-state responses, such as the ones described in the following article.</p><p>Picton, Terence W., et al. &quot;The use of phase in the detection of auditory steady-state responses.&quot; Clinical Neurophysiology 112.9 (2001): 1698-1711.</p><p>The current article is certainly not written in a way that can be understood by an experimentalist. It doesn't matter too much if the methods are hard to follow, but it does matter if no interpretable results are shown. For example, the authors argue that the topographic plots using the new method have a clearer structure than the traditional ones. As an experimentalist, however, I can't figure out which structure is clearer and why it helps to answer scientific questions.</p><p>2. I'm glad that the authors included a new dataset in the analysis. However, as an experimentalist, I still cannot see why the new method outperforms the traditional ITPC analysis in the newly added experiment. For the session &quot;Case study – statistical learning for an artificial language&quot;, we need at least a few conclusions, explicitly stating whether the new method or the traditional method gives a more reasonable result and why.</p><p>3. Simulation is also important. However, I can't really understand the &quot;Simulation study&quot; section. What is exactly R1 or R2? Why do we care about the bias? A more helpful simulation is probably just to simulate the time-domain EEG signal (e.g., using sinusoids and noise) and demonstrate that the new method, e.g., can yield statistical significance with fewer subjects.</p><p>&quot;We then use this modified model to generate fictive datasets with different numbers of participants and trials&quot;, but where are the results? It seems like Figure 11 does not show how the results change with the number of participants and trials.</p><p>For the new section on &quot;Data efficiency&quot;, why just one dataset and why only 4 participants? Testing two datasets and all possible numbers of participants are minimal requirements. Also, as an experimentalist, I really cannot understand what is shown in Figure 12.</p><p>4. &quot;the power is not a useful measure. Instead, the typical approach to frequency-tagged data for cognitive tasks is to use the inter-trial phase coherence.&quot; In fact, most of the studies cited in the introduction used power rather than phase analysis.</p><p>5. &quot;The Bayesian approach is more descriptive than traditional statistical approaches: it is a generative model of how the data arises and each component is interpretable and informative about data characteristics.&quot;</p><p>It's great. But why is the method more interpretable? Could you please summarize it in a way that can be understood by experimentalists?</p><p>&quot;It is also more data-efficient: it detects stimulus-related differences for smaller participant numbers than the standard approach.&quot;</p><p>How is this demonstrated in the two datasets? Is there a guideline about how many participants can be saved using the new approach?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84602.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>The study by Dimmock et al. proposes a Bayesian approach to measuring phase coherence. Although I'm familiar with the kind of EEG data analyzed here, I didn't figure out the purpose of the study. It seems like the aim of the study is neither to provide a more powerful statistical test nor to demonstrate some new neural phenomena. The only purpose seems to provide a Bayesian test, but why do we want it?</p><p>If the aim is to provide a more powerful test, it should be compared to classic tests for steady-state responses, such as the ones described in the following article.</p><p>Picton, Terence W., et al. &quot;The use of phase in the detection of auditory steady-state responses.&quot; Clinical Neurophysiology 112.9 (2001): 1698-1711.</p><p>The current article is certainly not written in a way that can be understood by an experimentalist. It doesn't matter too much if the methods are hard to follow, but it does matters if no interpretable results are shown. For example, the authors argue that the topographic plots using the new method have a clearer structure than the traditional ones. As an experimentalist, however, I can't figure out which structure is clearer and why it helps to answer scientific questions.</p><p>As a methodological paper, testing the method on multiple datasets is a basic requirement. More importantly, the method has to have a clear goal and clearly demonstrate how the goal is achieved.</p></disp-quote><p>We agree with this comment: our paper presents a novel approach to the analysis of phase data and so should be tested on additional datasets. Stress-testing a novel method is paramount to finding where it excels and perhaps where it does not, and is an necessary step to both increase confidence in the method and promote its adoption.</p><p>In our improved manuscript we have provided a detailed analysis of another dataset from a frequency tagged experiment that investigated the role of statistical learning in an artificial language (Pinto et al., 2022). As with the first dataset (Burroughs et al., 2021a, 2021b), we use our Bayesian model to compute statistics equivalent of the frequentist approach and compare the results. Please see: Materials and methods <italic>&gt;</italic> Data for a description of this new data, and Case study – statistical learning for an artificial language, for the results.</p><p>Additionally, we analysed the performance of our Bayesian model through a simulation study. This compared the two approaches on the following quantities:</p><p>– reported discovery of a difference when it exists (true-positive)</p><p>– reported discovery of a difference when it does not exist (false-positive)</p><p>We found that the Bayesian model can not only detect a true difference in mean resultant length between conditions with fewer data than the ITPC approach, it also has a greatly reduced false positive rate. These results are described in Simulation study, and appendices 5-6.</p><p>As a consequence of this simulation study we also discovered that our Bayesian model reduces bias in the estimation of mean resultant length. The value as calculated in the ITPC analysis is positively biased (Kutil, 2012), however, as demonstrated by our simulation study the Bayesian estimates do not show the same systematic bias as the ITPC. To provide some more rigour to this claim we also demonstrate through simulation based calibration (Talts et al., 2018), evidence of a non-biased estimation by the Bayesan model.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This paper presents a novel Bayesian approach to testing for phase coherence in neurophysiological recordings. The approach is centred on probability distributions and therefore allows for more fine-grained conclusions about the existence of such phase consistency, in contrast to the often artificial yes/no decision on the acceptance of the alternative hypothesis that can be found in the literature.</p><p>I find this manuscript well written and the rationale well explained. The authors demonstrate that their approach can produce similar, but potentially clearer and less noisy results as compared to more commonly applied techniques (such as inter-trial coherence). It remains difficult to quantify differences between the two approaches (Bayesian vs frequentist) – for instance, the authors write that &quot;these graphs [from Bayesian analysis] show a clearer structure than the corresponding ITPC analysis&quot; without providing a quantification of the difference.</p><p>Together, this paper will be useful to the community, possibly opening up new ways of analysing phase-locked neural responses.</p></disp-quote><p>Thank you for this encouraging review of our paper. As you have pointed out, it can be difficult to compare Bayesian and frequentist results, and this is certainly something we experienced throughout this project. Instead of strict one-to-one comparisons, that may not always be possible and which, in any case would be against the spirit of what a Bayesian analysis attempts to do, we pursued a qualitative approach, and compare the methods based on the description of the data they provide in their conclusions. For example, confidence intervals are not comparable to highest density intervals (HDIs), nevertheless their respective application is similar: to determine an interval and check its support for a hypothesis. We do feel that one of our contributions in our manuscript is to introduce figure types which are analogous to traditional frequentist results figures while sticking to the descriptive rather than hypothesis based spirit of Bayesian inference; examples of this are figures 6A, 10A and 12B.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>This paper proposes a Bayesian take on the inter-trial phase coherence (ITPC) used to estimate how consistent the oscillatory phase is across trials for a given condition of interest. For standard ITPC the statistical power of the comparisons on the group level is determined by the sample size of the dataset since estimates are derived by first averaging across trials to derive a single condition-level estimate per subject. The advantage of the proposed Bayesian approach is that the resulting model is more robust as it is estimated from the trial level without averaging. It also allows us to derive subject-level estimates (slopes) and explore subject-variable noise. The authors illustrated this by replicating the ITPC analysis from the paper by Burroughs et al. (2021a) using the Bayesian ITPC and demonstrating perceivable noise reduction in the resulting estimates across frequencies and topographical EEG plots. Another key advantage of this method, as illustrated by the authors, is the ability to generate stable estimates for much smaller EEG datasets. While the authors show that Bayesian ITPC can replicate the findings obtained with the standard ITPC, it is not clear what advantages the proposed Bayesian approach offers over other previously proposed methods that allow for trial-level modelling such as linear mixed effects modelling. Secondly, a broader and more accessible description of the steps of the model settings, estimation, and the derivation of the summary statistics should be provided to enable the reader to replicate this method for their own dataset</p></disp-quote><p>Thank you for this clear summary of our manuscript. While we are pleased that we have been able to convey the central results, we would like to improve on this by addressing your points.</p><p>We appreciate that the communication of the Bayesian model requires extensive mathematical treatment. Due to the nature of the model, the sequence of transformations that are required to change raw values to quantities of interest can become complicated. To help improve clarity, we have provided further detail on how different quantities are related to each other (see point 7 below).</p><p>We do not draw a strong distinction between our Bayesian model and Linear mixed models (LMMs). Our Bayesian model is, at its core, a LMM; with fixed effects for condition, and random effects for participants and electrodes. However, the power of the estimation method allows for increased model complexity, such as a custom link function, and a wrapped distribution for the likelihood. For example, bmrs (Bu¨rkner, 2017), is a package based on stan (Carpenter et al., 2017), that aims to provide greater flexibility to multilevel or mixed modelling than maximum likelihood based approaches such as lme4 (Bates et al., 2015), all while using a near-identical syntax for model specification.</p><disp-quote content-type="editor-comment"><p>Abstract</p><p>1) lines 12-17 please consider re-phrasing as the message here is not very clear. Please be more specific (based on your analysis findings) what Bayesian approach to coherence contributes more than the traditional one? 'More descriptive' and 'data-efficient' are vague descriptions.</p></disp-quote><p>It is important for us to communicate the proposed benefits of our Bayesian approach clearly and thoroughly. We have expanded on the abstract, taking these points into consideration.</p><p>“This Bayesian approach is illustrated using two examples from neurolinguistics and its properties are explored using simulated data. The Bayesian approach is more descriptive than traditional statistical approaches: it is a generative model of how the data arises and each component is interpretable and informative about data characteristics. It is also more data-efficient: it detects stimulus-related differences for smaller participant numbers than the standard approach.”</p><disp-quote content-type="editor-comment"><p>Introduction</p><p>2) Lines 26-44. Here to help the readers I would recommend communicating your main point early in the paragraph – that measurement of coherence is an important methodological tool in M/EEG research that is used to answer a wide variety of scientific questions, yet there is room for improvement in how ITPC is estimated.</p></disp-quote><p>Thank you for this recommendation. We agree, the main ideas of this manuscript had not been introduced adequately in the opening paragraphs. We have now changed the text to convey our motivations earlier.</p><p>“In an electroencephalography (EEG) or magnetoencephalography (MEG) frequency-tagged experiment the stimuli are presented at a specific frequency and the neural response is quantified at that frequency. This provides a more robust response than the typical event-related potential (ERP) paradigm because the response the brain makes to the stimuli occurs at the predefined stimulus frequency while noise from other frequencies, which will correspond to other cognitive and neurological processes, does not contaminate the response of interest. This provides a more robust response than the typical event-related potential (ERP) paradigm because the response the brain makes to the stimuli occurs at the predefined stimulus frequency while noise from other frequencies, which will correspond to other cognitive and neurological processes, does not contaminate the response of interest. This quantification is often approached by calculating the inter-trial phase coherence (ITPC). Indeed, estimating coherence is an important methodological tool in EEG and MEG research and is used to answer a wide variety of scientific questions. There is, however, scope for improving how the phase coherence is measured by building a Bayesian approach to estimation. This is a per-item analysis which gives a better description of uncertainty. In contrast, the ITPC discards information by averaging across trials. As a demonstration, both approaches are compared by applying them to two different frequency-tagged experimental datasets and through the use of simulated data.”</p><disp-quote content-type="editor-comment"><p>3) Line 84 – 'this plots', instead of 'this graphs'</p></disp-quote><p>Thank you – “This plots the ITPC measure for all six experimental conditions, …”</p><disp-quote content-type="editor-comment"><p>4) Lines 96-107 – the main message from this section is not clear. Do authors argue that in the per-electrode ITPC approach the Bonferroni correction for multiple comparisons precludes finding meaningful spatial patterns in the data? In such cases, Bonferroni is rarely used, and spatial cluster-based permutation is a typically used approach that is less conservative and allows the finding of significant clusters of spatially connected electrodes.</p></disp-quote><p>This was an interesting point, and one that we had overlooked. Bonferroni correcting such a large number of electrodes was a very extreme case, and as you have mentioned, is avoided in practise for an albeit weaker, but fairer cluster-based permutation test (CBPT). This has allowed us to form a more realistic comparison between the Bayesian and ITPC approaches; not a strictly one-to-one comparison, but nevertheless one that invites similar interpretations.</p><p>The headcap plots in figure 2B, and figure 9B (for the additional dataset), now include significant clusters of electrodes marked on the skull. The corresponding Bayesian results, figures 6/10, mark electrodes that did not contain zero in their highest density interval. Also, text (lines 119-130) has been added to the manuscript that discusses CBPT, how it applies to the ITPC, and how it compares with the Bayesian results.</p><disp-quote content-type="editor-comment"><p>5) Lines 126-128 – please unpack a bit more what is meant by 'a better description of the data' and 'a narrative phrased in terms of models and their consequence'.</p></disp-quote><p>In light of this comment we have expanded on the text:</p><p>“Furthermore, as a Bayesian approach, it supports a better description of the data, by describing a putative abstraction of the stochastic process that may have generated the data while explicitly stating the underpinning assumptions. This replaces a hypothesis-testing and significance-based account with a narrative phrased in terms of models and their consequence so, in place of an often contentious or Procrustean framework based on hypotheses, a Bayesian approach describes a putative model and quantifies the evidence for it.”</p><disp-quote content-type="editor-comment"><p>6) Line 161 – here you mean Figure 4?</p></disp-quote><p>Yes, thank you – Done.</p><disp-quote content-type="editor-comment"><p>Methods section</p><p>7) Authors provide a detailed explanation and mathematical descriptions for the distributions from which the phase data can be modelled, and parameters are sampled when building up a Bayesian model of the ITPC. The supplementary materials then detail equations behind the full model used. Yet from these two sources of information, it is challenging for the reader to reconstruct the set of steps authors took to derive the results they plot in Figure 5. If the aim of the paper is to have the reader use the Bayesian approach to ITPC for their own datasets a more accessible step-by-step description of the model estimation is necessary – from calculating participant and electrode slopes/estimates to averaging steps used to produce Figure 5. This can be done by expanding relevant sections in the Methods.</p></disp-quote><p>Thank you for this helpful comment. We agree that it is important for the reader to be clear on the series of steps taken to arrive at the plotted results. To improve clarity, we have added extra equations. Equations 12-13 have been added to show exactly how the circular variance relates to both the mean resultant R, and the scale of the wrapped Cauchy distribution <italic>γ</italic>. Equations 20-21 have been added to the Results section to make more apparent the calculation used to obtain the values plotted in figure 6 (previously figure 5). As an additional example of how one could go about extracting quantities of interest from the Bayesian model we have provided a further example in appendix 8 that shows posteriors over differences in mean resultant length for each participant.</p><disp-quote content-type="editor-comment"><p>8) Other methods such as Linear mixed models that likewise allow trial-level analysis and model subject slopes have been broadly applied to the EEG data and also ITPC. To increase the contribution of this paper, authors need to outline and demonstrate analytically the advantages of the Bayesian approach over these other non-Bayesian methods.</p></disp-quote><p>Our Bayesian model is very similar to a linear mixed model (LMM), we are essentially estimating fixed and random effects corresponding to conditions, participants, and electrodes. However, it affords us much more flexibility than a typical LMM or gLMM. Because of this we felt that in addition to the current frequentist analysis, the paper would not significantly benefit from including a LMM treatment of these data.</p><p>While we agree that a LMM may be suitable for modelling the ITPC, we are unaware how one could do this with phase angles, because this requires a simultaneous estimation of the mean resultant R and its circular variance <italic>S</italic>. As we have shown, these require a wrapped distribution. Modelling ITPC values, <italic>R<sub>pce</sub></italic>, which at the very least have been averaged over trials is a reasonable approach, but restricted compared to our model, both in terms of expressability—no joint model for R and <italic>S</italic>—and data efficiency, by working directly on a summary statistic of the data.</p><disp-quote content-type="editor-comment"><p>Discussion section</p><p>9) The section Model design choices seem to belong in the Results and not the Discussion section.</p></disp-quote><p>Yes, we agree. This section has now been moved to Results.</p><disp-quote content-type="editor-comment"><p>10) The Data efficiency section is very helpful in demonstrating the advantage of the Bayesian approach for smaller datasets. This section can be expanded by demonstrating further key advantages of the Bayesian approach over other non-Bayesian methods that use a trial-level approach (as proposed in point 8).</p></disp-quote><p>Providing examples where the Bayesian model outperforms the standard approach that we are comparing it to is important. However, including LMMs into this analysis is out of scope. Instead, to address this point from a different perspective we investigated the benefits of the Bayesian approach compared to the ITPC in a simulation study; this expands upon the data efficiency section to look at bias reduction, false-positive rates, and true-positive rates over various data sizes. We hope that this provides further, interesting, detail to the data-efficiency discussion.</p><disp-quote content-type="editor-comment"><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><p>Thank you for resubmitting your work entitled &quot;Bayesian analysis of phase data in EEG and MEG&quot; for further consideration by eLife. Your revised article has been evaluated by Joshua Gold (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Reviewers 2 and 3 are satisfied with your revisions, however, given that eLife works on consensus and the fact that Reviewer 1 is not satisfied with a major concern of theirs from the first round of review, we ask that you directly address Reviewer 1's queries thoroughly. This includes those concerns regarding interpretability for experimentalists, and specifically, that you compare your method to the classic tests for steady-state responses as the Reviewer suggests. Please pay close attention to each of Reviewer 1's queries and address them in full.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>1. The authors did not address all my comments and I copied them here.</p></disp-quote><p>We are sorry we did not address these comments; this was a mistake on our part, the comments in the original reviews which required the most additional research – the addition of a second data set and simulation studies – distracted us from other important comments.</p><disp-quote content-type="editor-comment"><p>If the aim is to provide a more powerful test, it should be compared to classic tests for steady-state responses, such as the ones described in the following article.</p><p>Picton, Terence W., et al. &quot;The use of phase in the detection of auditory steady-state responses.&quot; Clinical Neurophysiology 112.9 (2001): 1698-1711.</p></disp-quote><p>We have included a discussion of this approach in our introduction; this approach was developed for long stimulation by periodic stimuli recorded using a small number of scalp electrodes, our recordings are shorter, giving poorer frequency resolution, they are less temporally homogenous and are recorded from multiple electrodes; this makes the classic test unsuitable. The classic test are best suited to the classic application of steady-state methods, with simple tone-like stimuli; the hope is that it is becoming possible to apply frequency-tagging to richer, more temporally complex stimuli. We write:</p><p>“There are other classical tests of coherence which use phase information. One example is the Rayleigh test [Rayleigh (1880), Rayleigh (1919)], this test can be used to check for either significant departure from uniformity, or from the ‘expected phase’, that is a particular phase angle specified by the researcher based on some other insight into the behaviour. Other test, such as Hotelling’s <italic>T</italic><sup>2</sup> apply jointly to phase and amplitude [Hotelling (1931); Picton et al. (1987, 2001)]. These classical approaches are incompatible with the neurolinguistic study presented here. Firstly, it would be difficult to provide an expected phase; as demonstrated in Figure 4, the mean phase angle is highly variable across participants. There is also no substantive prior information available that could be used to supplement this value because language experiments vary from experiment to experiment. Secondly, because of the problem of semantic satiation the experiments we consider here are relatively short an lack the frequency resolution these classical approaches require.”</p><p>Beyond these details though, we believe that the Bayesian approach is better because it models the experiment; ultimately classical approaches depend on the skill of the researcher in statistical analysis whereas Bayesian inference will provide a set of methods which will make efficient use of the data by creating a model of a scientific understanding of the process that produces the data. In other words, although these Bayesian approaches are unfamiliar ultimately they will be easier to use for experimentalists! It seems to us that there are lots of intricate non-Bayesian “workarounds” to various data challenges, but in the long run this makes it harder and harder to understand the statistical framework while a Bayesian approach, though unfamiliar, will lead to more straightforward analyses. Even if this turns out not to be the case, it is a possibility that should be explored and we feel we are contributing to that exploration.</p><disp-quote content-type="editor-comment"><p>The current article is certainly not written in a way that can be understood by an experimentalist. It doesn't matter too much if the methods are hard to follow, but it does matter if no interpretable results are shown. For example, the authors argue that the topographic plots using the new method have a clearer structure than the traditional ones. As an experimentalist, however, I can't figure out which structure is clearer and why it helps to answer scientific questions.</p></disp-quote><p>We are very grateful for this comment, we realise we have “under-sold” one of the biggest advantages of our approach: the Bayesian method is less inclined to producing attractive looking but meaningless information, in frequentist statistics there is a danger of false discovery, or if that is corrected for, a risk of losing real evidence in a effort to remove fictive results. We have added a new figure focusing on a comparison of the topographic headmaps, both using the real data and using fictive data. It is clear that the Bayesian approach represents the underlying ground truth better. The new figure is Figure 8 and we write (lines 370 – 388):</p><p>“In Figure 2C we see that even for conditions, such as RR and RV, which contain no linguistic structure at the phase rate there are patterns of electrode activity in the topographics headcaps. In contrast, the analogous Bayesian headcaps in Figure 4C did not show similar patterns. We used simulated data to investigate whether the Bayesian model is correctly demonstrating that there is no phrase-level response for these conditions, rather that the other possibility: that the beguiling patterns seen in the ITPC headcaps represent a real activity invisible to the Bayesian analysis. In fact, the evidence points to the first alternative, Figure 8, presents evidence that the Bayesian model is more faithful to the data when there is no meaningful variation in electrode effects. Figure 8A shows the real data again, however, whereas previously the headcap was plotted for differences between conditions, here we fit directly to the RR condition. There is no effect visible for the Bayesian headcap but for the ITPC headcap there are variations that may suggest localised activity, even though this condition does not any structure at the phrase rate. In Figure 8B four datasets were simulated from the generative Bayesian model with electrode effects set to zero, the four simulations are marked as 1-4 in the figure. Except that there is only one condition the simulated data mimics the real data: it has 16 participants, 32 electrodes and 24 trials. These simulations are intended to represent four different iterations of the same experiment, apart from differing in any random numbers they are identical. The data resulting from these four simulations were fitted with both methods. Evidently, the Bayesian results are much closer to the ground truth. The ITPC results show variations that could easily be misinterpreted.”</p><p>We are pleased that the new figure so clearly demonstrates that advantage of our approach.</p><p>Overall, we have tried to find ways to make a Bayesian analysis interpretable and believe this is a useful contribution on our part, for example, in Figure 6B we introduce a novel Bayesian equivalent to the typical “significance bracket” style of graph.</p><disp-quote content-type="editor-comment"><p>2. I'm glad that the authors included a new dataset in the analysis. However, as an experimentalist, I still cannot see why the new method outperforms the traditional ITPC analysis in the newly added experiment. For the session &quot;Case study – statistical learning for an artificial language&quot;, we need at least a few conclusions, explicitly stating whether the new method or the traditional method gives a more reasonable result and why.</p></disp-quote><p>Thank you for this comment. We are pleased that you found the additional dataset useful, but regret that we did not make clear the respective performance of each method on it. To rectify this, we have added a new figure (Figure 14), that compares the two methods in terms of both confidence and highest density intervals, and <italic>p</italic>-values and posterior probability, as a function of the number of trials in the data. This figure shows that although the methods perform similarly on all the data (compare Figure 10 with Figure 11), when considering a lower number of trials the performance of the Bayesian model in detecting the signal in the data is much better: thus, although the original experiment was impressive in the size of its sample and, of course, as a published data set it is an example where the traditional statistical framework produced a result, a smaller experiment would have failed to produce a signal in cases where the Bayesian analysis would have succeeded. We also write (lines 581-594):</p><p>“Figure 14 is similar to Figure 13, however, it uses the statistical learning dataset Pinto et al. (2022), comparing conditions BL and EXP at the frequency 5.33Hz. In fact, for these data we saw little evidence that the Bayesian approach works better when the number of participants are reduced: we attribute this to the large number of trials, generally the extra efficiency of a Bayesian approach appears most apparent in low data regimes and the statistical learning data set is admirably well sampled. For this reason we used this data set to investigate data efficiency when the number of trials is reduced for a fixed number of participants. In Figure 14 data from the first 20 participants are considered and the analysis is repeated with different numbers of trials, discarding trials from the end. It is clear from Figure 14A that the Bayesian model can reliably detect the signal in the data with at least half the number of trials that the frequentist approach requires, this is potentially useful especially when because of the challenge semantic satiation posses to some neurolinguistic experiments. Figure 14B compares the <italic>p</italic>-values arising from the significance test with <italic>P</italic>(∆<italic>R &lt;</italic> 0) calculated from the posterior and shows the fast convergence of the posterior to the signal; the <italic>p</italic>-value is much slower and also more variable across trials.”</p><disp-quote content-type="editor-comment"><p>3. Simulation is also important. However, I can't really understand the &quot;Simulation study&quot; section. What is exactly R1 or R2? Why do we care about the bias? A more helpful simulation is probably just to simulate the time-domain EEG signal (e.g., using sinusoids and noise) and demonstrate that the new method, e.g., can yield statistical significance with fewer subjects.</p></disp-quote><p>R1 and R2 refer to the mean resultant lengths of two conditions 1 and 2 in the simulation study; we had failed to say that and have now fixed that silly error.</p><disp-quote content-type="editor-comment"><p>&quot;We then use this modified model to generate fictive datasets with different numbers of participants and trials&quot;, but where are the results? It seems like Figure 11 does not show how the results change with the number of participants and trials.</p></disp-quote><p>The results for different participants and trials were given in appendices 6 and 7, however, in light of this point, we felt that structure of this section was not helpful. It has now been re-arranged to flow in a more obvious way, and the importance of appendices 6 and 7 to the simulation results has been made clearer in the text (lines 467-476).</p><disp-quote content-type="editor-comment"><p>For the new section on &quot;Data efficiency&quot;, why just one dataset and why only 4 participants? Testing two datasets and all possible numbers of participants are minimal requirements. Also, as an experimentalist, I really cannot understand what is shown in Figure 12.</p></disp-quote><p>We go to a minimum of five participants to accommodate the parameters in the Bayesian model that estimate participant variance. With too little these parameters will not be informed. Similarly this is why a certain number of groups are recommended for random effects models, as it is important to obtain a reasonable estimate of their variance.</p><p>Our previous Figure 12 (now adapted into Figure 13) aims to show that the Bayesian model can detect a difference between conditions using fewer participants. We compare confidence intervals of widths 90/95/0.996 to Bayesian highest density intervals of the same. Depending on how conservative the analyst is about their significance level <italic>α</italic>, small to large differences arise between the methods in terms of the number of participants they require to convincingly detect the signal in the data.</p><disp-quote content-type="editor-comment"><p>4. &quot;the power is not a useful measure. Instead, the typical approach to frequency-tagged data for cognitive tasks is to use the inter-trial phase coherence.&quot; In fact, most of the studies cited in the introduction used power rather than phase analysis.</p></disp-quote><p>That is a good point; while power is frequently sufficient for some tasks, the ITPC becomes necessary for more difficult cognitive tasks, for example, power is not used in neurolinguistics since it rarely succeeds in identifying a signal. We have adjusted our text to reflect this:</p><p>”the induced power [...] does not work, empirically this proves too noisy a quantity for the stimulus-dependent signal to be easily detected and, indeed, although the frequency tag produces a more robust signal than an ERP, for more high-level or cognitive tasks, particularly neurolinguistic tasks, where frequency-tagging is now proving valuable, the power is not a useful measure; more needs to be done to remove the noise. Typically this is done by assuming the response is phase locked to the stimulus and so for frequency-tagged data in cognitive tasks it is common to use the inter-trial phase coherence.”</p><disp-quote content-type="editor-comment"><p>5. &quot;The Bayesian approach is more descriptive than traditional statistical approaches: it is a generative model of how the data arises and each component is interpretable and informative about data characteristics.&quot;</p><p>It's great. But why is the method more interpretable? Could you please summarize it in a way that can be understood by experimentalists?</p></disp-quote><p>We don’t believe the current approaches are easy to interpret; the issue of how to correct for multiple comparisons is very fraught for example, and there is no real clarity as to what the appropriate correction is. In the case of ITPC there is a lot of different standards used in claiming a response is significant, based on comparison with simulated data, or data at other nearby frequencies; it is very hard to decide what the correct approach is or how to interpret results that appear significant using one standard and not for another. As another ITPC example, often, two conditions are compared by t-test even though, since ITPC is bound by zero and one, the data is not Gaussian; interpreting the Gaussianity of data that passes a test of Gaussianity when it cannot actually be Gaussian is difficult! In contrast, a Bayesian analysis makes a more modest claim, it gives a description of a model and estimates the posterior of the model parameters based on the data. The familiarity we feel for the traditional methods are the result of years of exposure, we cannot hope to make the Bayesian approach equally familiar to experimentalists in a single paper, but we hope our use of multiple figures, two real data sets and two simulated data sets will be a step towards that goal.</p><disp-quote content-type="editor-comment"><p>&quot;It is also more data-efficient: it detects stimulus-related differences for smaller participant numbers than the standard approach.&quot;</p><p>How is this demonstrated in the two datasets? Is there a guideline about how many participants can be saved using the new approach?</p></disp-quote><p>We demonstrate the data efficiency of the Bayesian model on the two datasets in Figure 13 and Figure 14 respectively. Figure 13 shows that the number of participants can be reduced when using the Bayesian model, the amount depending on how cautious the analyst wants to be regarding false positives. Figure 14 is a new addition that looks at this problem as a function of trials instead of participants. Here it is shown that the Bayesian model detects the signal in the data with approximately half the number of trials required by the ITPC. Please also see our response to comment 2, and 3b.</p></body></sub-article></article>