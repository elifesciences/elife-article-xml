<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">100739</article-id><article-id pub-id-type="doi">10.7554/eLife.100739</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100739.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>DeePosit, an AI-based tool for detecting mouse urine and fecal depositions from thermal video clips of behavioral experiments</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Peles</surname><given-names>David</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0003-5543-1676</contrib-id><email>davidpelz@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Netser</surname><given-names>Shai</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4176-1124</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Ray</surname><given-names>Natalie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Suliman</surname><given-names>Taghreed</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-4177-5391</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Wagner</surname><given-names>Shlomo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7618-0752</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f009v59</institution-id><institution>Sagol Department of Neurobiology, Faculty of Natural Sciences, University of Haifa</institution></institution-wrap><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>08</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP100739</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-06-24"><day>24</day><month>06</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-06-27"><day>27</day><month>06</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.24.600419"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-02"><day>02</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100739.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-04-25"><day>25</day><month>04</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100739.2"/></event></pub-history><permissions><copyright-statement>© 2024, Peles et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Peles et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-100739-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-100739-figures-v1.pdf"/><abstract><p>In many mammals, including rodents, social interactions are often accompanied by active urination (micturition), which is considered a mechanism for spatial scent marking. Urine and fecal deposits contain a variety of chemosensory signals that convey information about the individual’s identity, genetic strain, social rank, and physiological or hormonal state. Furthermore, scent marking has been shown to be influenced by the social context and by the individual’s internal state and experience. Therefore, analyzing scent-marking behavior during social interactions can provide valuable insight into the structure of mammalian social interactions in health and disease. However, conducting such analyses has been hindered by several technical challenges. For example, the widely used void spot assay lacks temporal resolution and is prone to artifacts, such as urine smearing. To solve these issues, recent studies employed thermal imaging for the spatio-temporal analysis of urination activity. However, this method involved manual analysis, which is time-consuming and susceptible to observer bias. Moreover, defecation activity was hardly analyzed by previous studies. In the present study, we integrate thermal imaging with an open-source algorithm based on a transformer-based video classifier for automatic detection and classification of urine and fecal deposits made by male and female mice during various social behavior assays. Our results reveal distinct dynamics of urination and defecation in a test-, strain-, and sex-dependent manner, indicating two separate processes of scent marking in mice. We validate this algorithm, termed by us DeePosit, and show that its accuracy is comparable to that of a human annotator and that it is efficient in various setups and conditions. Thus, the method and tools introduced here enable efficient and unbiased automatic spatio-temporal analysis of scent-marking behavior in the context of behavioral experiments in small rodents.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Scientists conduct behavioral experiments on animals to study brain mechanisms that govern social behavior and how these may be affected by various conditions. For example, in rodents, urination and defecation are important social activities used for communication and territory marking, and they are influenced by the emotional state of an individual.</p><p>In the past, these activities were analyzed at the end of an experiment by shining ultraviolet light on a filter paper placed on the floor of the cages. However, this method does not provide information on when urination or defecation occurred. Also, in many cases, urine drops are smeared on the filter paper due to the animal's movement during the experiment, which reduces the accuracy of this method. To bridge this gap, Peles et al. developed a computer-vision algorithm – named DeePosit – to automatically track mice's urination and defecation activities during social behavior experiments recorded with a thermal camera.</p><p>To examine the efficiency of the tool, the researchers analyzed the urination and defecation activities of mice during several social behavior tests. They then tested whether these activities changed over time and if there were differences between male and female mice, or between different strains of laboratory mice.</p><p>The analysis revealed that the tool could identify the time and location of each urination and defecation event with an accuracy similar to that of a human observer. Using this tool, Peles et al. demonstrated that urination and defecation activities changed during a social encounter, for example, urination became more frequent. They observed that males urinated more often than females, which may be attributed to differences in their territorial behavior. It also revealed differences between laboratory strains.</p><p>Peles et al. are confident that this rapid, unbiased and cost-effective tool can improve the analysis of social behavior in animals, particularly rodents. This will be especially relevant for researchers investigating the effect of treatments in mouse models of various disorders. The tool can also be trained and adapted to different behavioral and experimental contexts. It may allow a comparison of an additional important aspect of social behavior in treated and non-treated animals, and in health and disease.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>social behavior</kwd><kwd>urination</kwd><kwd>micturition</kwd><kwd>fecal deposition</kwd><kwd>machine learning</kwd><kwd>thermal imaging</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>ISF-NSFC joint research program</institution></institution-wrap></funding-source><award-id>3459/20</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>1361/17</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>2220/22</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Ministry of Science, Technology and Space of Israel</institution></institution-wrap></funding-source><award-id>3-12068</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Ministry of Health of Israel</institution></institution-wrap></funding-source><award-id>3-18380</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>German Research Foundation</institution></institution-wrap></funding-source><award-id>GR 3619/16-1</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>German Research Foundation</institution></institution-wrap></funding-source><award-id>SH 752/2-1</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000090</institution-id><institution>Congressionally Directed Medical Research Programs</institution></institution-wrap></funding-source><award-id>AR210005</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001742</institution-id><institution>United States-Israel Binational Science Foundation</institution></institution-wrap></funding-source><award-id>2019186</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Shlomo</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Thermal imaging, computer vision tools, and an open-source algorithm incorporating a transformer-based video classifier are combined to automatically detect and classify urine and fecal deposits made by male and female mice and their spatio-temporal dynamics during behavioral tests.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In many mammalian species, including rodents, social interactions are accompanied or followed by events of active urination, also known as micturition or voiding activity (<xref ref-type="bibr" rid="bib1">Arakawa et al., 2008</xref>). Multiple studies have demonstrated that urine and fecal deposits comprise many chemosensory social signals that carry information about the individual, such as its species, sex, social rank, and identity, as well as its reproductive and health conditions (<xref ref-type="bibr" rid="bib3">Bigiani et al., 2005</xref>). These chemosensory signals include various metabolites, as well as many proteins, such as major urinary proteins (<xref ref-type="bibr" rid="bib4">Brennan, 2004</xref>). Thus, by depositing urine spots and feces in its environment, the individual also deposits social information, which may later be perceived by other individuals and modify their future social interactions with this individual (<xref ref-type="bibr" rid="bib12">Hurst and Beynon, 2004</xref>). In other words, the use of urine and fecal deposits allows individuals to advertise their availability to possible mates and communicate with other conspecifics. Moreover, in territorial species, urination is used to mark the territory of the individual, thus functioning as a spatio-social scent-marking activity (<xref ref-type="bibr" rid="bib5">Brennan and Kendrick, 2006</xref>). In rodents, urination was shown to be strongly influenced by the individual’s internal state, social rank, social context, and previous social experience (<xref ref-type="bibr" rid="bib9">Desjardins et al., 1973</xref>; <xref ref-type="bibr" rid="bib13">Hyun et al., 2021</xref>). Therefore, monitoring urination activity can provide valuable information on the individual’s social behavior and internal state. Specifically, deficits in urine depositing may reflect atypical social behavior in rodent models of various diseases (see <xref ref-type="bibr" rid="bib28">Wöhr et al., 2011</xref> for example), hence may be used for testing potential treatments in such models.</p><p>Urination during a given task is traditionally analyzed via the void spot assay, which uses filter paper placed on the arena floor to analyze, after the end of the experiment, the spatial distribution of urine spots (<xref ref-type="bibr" rid="bib29">Wolff and Powell, 1984</xref>; <xref ref-type="bibr" rid="bib11">Higuchi and Arakawa, 2022</xref>). However, this analysis usually lacks the temporal dimension, is distorted by urine smearing across the arena floor caused by the individual’s movement (see Figure 2d, e), and is limited in detecting overlapping urine spots. Another caveat is that the filter paper may be torn down by the mouse during the behavioral experiment. Recently, <xref ref-type="bibr" rid="bib8">Dalghi et al., 2023</xref> used a filter paper on the arena floor, UV light, several cameras, and a manual video annotation to analyze urination events. Several other studies (<xref ref-type="bibr" rid="bib26">Verstegen et al., 2020</xref>; <xref ref-type="bibr" rid="bib19">Miller et al., 2023a</xref>) used thermal imaging via infrared (IR) camera for such analysis, as urine deposits are emitted while being in body temperature, hence can be seen in the thermal image. However, fecal deposits are also emitted in body temperature, making it difficult to distinguish between feces and small urine spots by thermal imaging alone. Moreover, these studies relied on manual analysis of thermal video clips, which made the analysis process time-consuming and subjected to observer bias. To cope with these limitations, we have developed an open-source computer vision-based software to automatically detect and classify deposited urine and feces from thermal video clips. Our detection and classification algorithm is based on a combination of a heuristic algorithm used for the preliminary detection of bright (warm) blobs in the thermal video clip and a trainable video classifier used to classify the preliminary detections as either urine, feces, or background (BG, i.e., not urine or feces). We demonstrate the efficiency of this tool by analyzing the temporal dynamics of urination and defecation activities in male and female CD1 (ICR) mice while performing three social behavior tests, and further validate the algorithm by testing it with male C57BL/6J mice. We found that urination and defecation activities show distinct dynamics across the various tests in a sex-, strain-, and test-dependent manner.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Social discrimination</title><p>Each CD1 subject animal performed three different social discrimination tests, as previously described by <xref ref-type="bibr" rid="bib21">Mohapatra et al., 2024</xref>, on three consecutive days in the order described below. Each test consisted of a 15-min habituation stage, during which the subject mouse got used to an experimental arena containing empty chambers at randomly chosen opposite corners. After habituation, the empty chambers were replaced with similar chambers containing stimuli for a 5-min trial stage (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). In the <bold>Social Preference</bold> (SP) test, a novel (i.e., unfamiliar to the subject mouse) sex-matched stimulus mouse was placed in one chamber, while an object stimulus (a Lego toy) was placed in the opposite chamber. In the <bold>Sex Preference</bold> (SxP) test, a novel female mouse was placed in one chamber while a novel male was placed in the opposite chamber. In the stress version of the <bold>Emotional State Preference</bold> (ESPs) test, a novel stressed (restrained for 15 min before the test) mouse was introduced into one chamber while a naïve mouse was placed in the opposite chamber. We first analyzed the time spent by the subject mouse on investigating each stimulus during the three tests (<xref ref-type="fig" rid="fig1">Figure 1</xref>), using the video clips recorded via the visible light (VIS) camera. Both male and female mice showed the behavior expected from CD1 mice, as previously described by us (<xref ref-type="bibr" rid="bib17">Kopachev et al., 2022</xref>). Males showed a significantly higher investigation time toward the social stimulus, as compared to the object in the SP test, toward the opposite sex, as compared to the same sex stimulus mouse in the SxP test, and toward the stressed mouse, as compared to the naïve mouse in the ESPs test. Females showed similar behavior, except for the SxP test, where they exhibited no preference for any of the two stimuli. In accordance with our previous study (<xref ref-type="bibr" rid="bib22">Netser et al., 2017</xref>), in all cases, the preference toward a given stimulus was reflected only by long (&gt;6s), but not by short (≤ 6s) investigation bouts (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Thus, in terms of social behavior, the subject mice behaved as expected.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Investigation time across sexes and tests in CD1 mice.</title><p>Each of the tests (SP, SxP, and ESPs) is comprised of a 15-min habituation stage with empty chambers, followed by a 5-min trial stage in which the stimuli are present in the chambers (<bold>a</bold>). The setup row shows schematic representations of the arena for the (<bold>b</bold>) SP, (<bold>c</bold>) SxP, and (<bold>d</bold>) ESPs tests, while the males and females rows show the mean (± SEM) time dedicated by male (<italic>n</italic> = 36, blue bars) and female (<italic>n</italic> = 35, red bars) mice to investigate each stimulus during the various tests. The two leftmost bars in each panel show the total investigation time, while the two middle bars show the time spent on short (≤ 6 s) investigation bouts, and the two rightmost bars show the time spent on long (&gt;6 s) investigation bouts. A two-sided Wilcoxon rank sum test was used for statistical significance. ***p &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>Urine and feces detection</title><p>The experimental setup used for the detection of urine and fecal deposits, comprising VIS and IR cameras, as well as a black body, is schematically shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref>. Unlike the VIS camera (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), the IR camera captures the warm urine and feces drops soon after they were deposited (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). This allowed us to overcome several caveats of the void spot assay. For example, we could tolerate smeared urine spots (<xref ref-type="fig" rid="fig2">Figure 2d, e</xref>) and identify the exact time of each urine or fecal deposition event. Using the thermal video clips, we designed a detection algorithm (termed DeePosit) consisting of two main parts: (1) A preliminary heuristic detection algorithm detects warm blobs (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). (2) These blobs are then fed into a machine learning-based classifier (<xref ref-type="fig" rid="fig2">Figure 2g</xref>), which classifies them as urine, feces, or background (i.e., without detection) (see methods, <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>, and <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The experimental setup and analysis method.</title><p>The experimental setup (<bold>a</bold>) includes a visible light (VIS) camera, an infrared (IR) camera, and a blackbody set to 37°C. VIS (<bold>b</bold>) and IR (<bold>c</bold>) images that were captured at the same moment, a short time after a urine deposition, exemplify that, as the urine is still warm, it appears as highly contrasted blob in the IR image but not in the VIS one. Large urine spots, such as the one shown in (<bold>d</bold>), may be smeared across the arena’s floor (<bold>e</bold>), which is one limitation of the use of filter paper for quantifying urination at the end of the experiment. The preliminary detection algorithm is based on subtracting a background image from each frame in the video (<bold>f</bold>), which allows the detection of hot blobs reflecting the animal itself and urine and feces deposits. The detected blobs are then classified using a transformer-based artificial neural network (<bold>g</bold>), which gets as its input a time series of patches cropped around the detection and provides its classification as an output. Each three patches in that time series are merged into a single RGB image (see methods). In the confusion matrix presenting the accuracy of the full pipeline for test videos (<bold>h</bold>) in CD1 mice, the ‘Miss’ row counts the events that were not detected by the preliminary hot blobs detection and, hence, were not fed to the classifier. The BG (background) column counts the number of automatic detections for which no matching manually tagged event exists in the relevant space and time window. Test videos include videos from 60 experiments. See Methods for more details. The precision, recall, and F1 score for urine detection are 0.90, 0.86, 0.88 accordingly, and 0.91, 0.89, 0.90 for feces detection. The mean F1 score: <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>U</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>F</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$(F1_{Urine}+F1_{Feces})/2$\end{document}</tex-math></alternatives></inline-formula> is 0.89.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Accuracy for small and large detections in CD1 mice.</title><p>(<bold>a, b</bold>) Confusion matrices on test videos with separation between large and small automatic detections. The threshold for large detections is an area of 1cm<sup>2</sup> which is 47.3 pixels. Shown percents sum to 1 for each column in (<bold>a</bold>) and each row in (<bold>b</bold>). The Large Urination class is correct in 98.2% of the cases in which it was reported by the classifier while Small Urination is correct in only 84.5% as shown in (<bold>b</bold>). Most of the confusion between feces and urine spots is for small detections: 2.3% of the Ground Truth (GT) urine events were classified as Small Feces while 0% as Large Feces as shown in (<bold>a</bold>). Also, 2.4% of the GT feces events were classified as Small Urine while 0% as Large Urine. No GT feces event was classified as Large BG. While feces are usually small, Large Feces detection might occur when two adjacent feces are detected as a single segment or when the detected segment contains both urine and feces. The test set includes videos from 60 experiments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig2-figsupp1-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100739-fig2-video1.mp4" id="fig2video1"><label>Figure 2—video 1.</label><caption><title>Video for the events in the confusion matrix.</title><p>Each urine or feces event is shown in a 65 × 65 pixel window from –11 s before the event to +60 s afterward (similar to the classifier input). The video shows both the manual annotation and the automatic detection that was matched with it (side by side). Note that there are no automatic detections for ‘Miss’ and no manual annotation for ‘BG’. The video plays at X3 speed.</p></caption></media></fig-group><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100739-video1.mp4" id="video1"><label>Video 1.</label><caption><title>IR video of a single ESPs trial of a male mouse with an overlay of the automatic detections.</title><p>Automatic detections are overlayed in red for feces, green for urine, and blue for BG. The stressed mouse side of the arena is marked in green, and the object side is marked in red. Counters of the number and area of automatic detections in each side of the arena are written on the top left. The video plays at X8 speed.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100739-video2.mp4" id="video2"><label>Video 2.</label><caption><title>IR video of a single ESP habituation of a male mouse with an overlay of the automatic detections.</title><p>The video shows the habituation part of the experiment in <xref ref-type="video" rid="video1">Video 1</xref>.</p></caption></media><p>For the generation of training and testing datasets, a human annotator manually tagged urination and defecation events in 157 thermal video clips (about 20 min each), of which 97 were used for training and 60 for testing. The precision, recall, and F1 score of the DeePosit algorithm for the test video clips are 0.90, 0.86, 0.88 for urine deposits and 0.91, 0.89, 0.90 for feces, respectively. The mean F1 score: <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>U</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>F</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$(F1_{Urine}+F1_{Feces})/2$\end{document}</tex-math></alternatives></inline-formula> is 0.89 and the confusion matrix is shown in <xref ref-type="fig" rid="fig2">Figure 2h</xref>. Notably, for large urine deposits, the classification precision is higher (0.98), in comparison to small urine precision (0.85), most probably because large urine drops are more distinguishable from fecal deposits, which are always small <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. See <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref> and <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>, for examples of correct detections, as well as mistakes made by the detection algorithm in the test videos, which are further discussed in the Discussion section.</p></sec><sec id="s2-3"><title>Detection stability and consistency</title><p>We tested the algorithm’s accuracy across various stages of the experiment (<xref ref-type="fig" rid="fig3">Figure 3a, b</xref>), the various experiments (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), the two sexes (<xref ref-type="fig" rid="fig3">Figure 3d</xref>), and three equal spatial divisions of the arena (<xref ref-type="fig" rid="fig3">Figure 3e</xref>). We found that the accuracy was stable in all cases, with no significant difference between them. These results suggest that the accuracy level of the algorithm is uniform across all these instances, hence the algorithm’s mistakes should not create a bias that may affect the experimental results.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Validation of DeePosit accuracy.</title><p>Mean accuracy ± SEM of urine (<bold>a</bold>) and fecal (<bold>b</bold>) deposits detection by DeePosit, as measured by F1 score across various stages of the experiment. Each ‘+’ or ‘o’ marks the F1 accuracy for a single mouse in a single experiment. No significant difference was found. Similarly, DeePosit accuracy was not significantly affected by the experiment type (<bold>c</bold>), by the sex of the subject mouse (<bold>d</bold>), or by the spatial location of the deposition in the arena (arena’s floor was divided into three equal parts) (<bold>e</bold>). A two-sided Wilcoxon rank sum test was used. (<bold>a–c, e</bold>) are FDR corrected rank sum tests (<xref ref-type="bibr" rid="bib2">Benjamini and Hochberg, 1995</xref>). The # at (<bold>b</bold>) stands for FDR corrected p-value of 0.08. Sixty test videos (24 videos with a male subject mouse and 36 with a female) were used in (<bold>a, b, d</bold>). Forty-six test videos were used in (<bold>c, e</bold>) of which 18, 14, 14 videos were SP, SxP, and ESPs accordingly. Mice without manually annotated depositions of the relevant type (either urine or feces) during the relevant period, experiment, or spatial location were ignored (since F1 is not defined in such cases). Since differentiating small urine and feces in thermal videos can be a challenging task even for humans, we evaluated the accuracy of a second human annotator on 25 test videos of CD1 mice (a subset of the full test set) and reported both the accuracy achieved by DeePosit (<bold>f</bold>) and the second human annotator (<bold>g</bold>) on these test videos. The mean F1 score, <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>U</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>F</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$(F1_{Urine}+F1_{Feces})/2$\end{document}</tex-math></alternatives></inline-formula> is 0.86 for the second human annotator and 0.84 for the DeePosit algorithm. To compare our result with another popular object detection approach, we annotated 39 training videos of CD1 mice with bounding boxes to match the YOLOv8 framework. For fairness, we trained both algorithms on the same training set of videos. We tested the accuracy on the test set which includes 60 videos. (<bold>h</bold>) shows the confusion matrix for DeePosit, while (<bold>i, j</bold>) show the confusion matrices achieved using YOLOv8 with a single image as input (YOLOv8 Gray) and with three images as input representing time <italic>t</italic> + 0, <italic>t</italic> + 10, <italic>t</italic> + 30 s from each event (YOLOv8 RGB). DeePosit accuracy surpasses YOLOv8 results in both cases. YOLOv8 RGB accuracy surpasses YOLOv8 Gray, suggesting that temporal information is helpful in the detection of urine and feces.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Accuracy for small and large detections in C57BL/6 mice.</title><p>To check the robustness of our method for different strains of mice and experimental conditions, we tested our algorithm on black C57BL/6 male mice and a white arena (the arena is white in visible light but looks dark in long-wave infrared). (<bold>a</bold>) Confusion matrices reflecting the accuracy of DeePosit algorithm on 10 SP and 10 SxP videos that were not included in the training set. The mean F1 for C57BL/6 is 0.81. Interestingly, C57BL/6 mice do not produce small urine spots, and hence, all the ‘small urine’ detections were wrong. Ignoring the small urine detections improves the mean F1 score to 0.86.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Detection accuracy at various values of <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$\Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula>.</title><p>DeePosit accuracy was measured for several values of the preliminary heuristic detection temperature threshold <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$\Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula>. The best results were achieved with a threshold of <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$\Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula> = 1.6°C. However, a good accuracy level (F1 score between 0.88 and 0.89) was observed in all cases between 1.1 to 3.0°C. See Methods for more details.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Examples of detections in test videos.</title><p>(<bold>a, b</bold>) Examples of urination and defecation events that were detected and classified correctly. Each pair of columns includes a ground truth detection (to the left) next to the matched automatic detection (to the right), which includes the mask of the detected blob. The overlaid text mentions the video index and the frame index. (<bold>b</bold>) Urination events that were wrongly classified as background. (<bold>c</bold>) Urine depositions that were classified as feces. (<bold>d</bold>) Fecal depositions that were classified as urine.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig3-figsupp3-v1.tif"/></fig></fig-group><p>We further compared the accuracy level of DeePosit with that of a second human annotator, using the first human annotator as a ground truth to both. For that, we used a subset of 25 video clips from the entire test set. The accuracy achieved by DeePosit with this dataset was comparable to that of the second human annotator (mean F1 score of 0.84 and 0.86, respectively, <xref ref-type="fig" rid="fig3">Figure 3f, g</xref>). These results demonstrate the partial accuracy of urination and defecation annotation by human observers and show that DeePosit is comparable to a trained observer in tagging urine and fecal depositions.</p><p>We also compared the accuracy of DeePosit with the accuracy achieved by a classic object detection algorithm (YOLOv8) (<xref ref-type="bibr" rid="bib14">Jocher et al., 2023</xref>). For that, we annotated 39 training videos of CD1 mice with bounding boxes to match the YOLOv8 framework. For fairness, we compared YOLOv8 results with DeePosit algorithm that was trained on the same set of video clips. DeePosit was significantly better (mean F1 = 0.81) than YOLOv8, regardless whether we used a single image (YOLOv8 Gray, F1 = 0.58), or a sequence of three images (0, 10, and 30 s after each frame, YOLOv8 RGB, F1 = 0.68) as in input (see <xref ref-type="fig" rid="fig3">Figure 3h–j</xref>). The fact that using a sequence of images (YOLOv8 RGB) gave better results compared to a single one (YOLOv8 Gray) suggests that temporal information is important for the accurate detection and classification of deposition events.</p><p>Finally, to test the accuracy of DeePosit across different mouse strains and experimental arenas, we evaluated DeePosit accuracy for SP and SxP tests performed by C57BL/6 black mice (<italic>n</italic> = 10) in a white Plexiglass arena. DeePosit achieved good performance (mean F1 = 0.81), even though videos with black mice or with white arenas were not included in the training set (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Thus, DeePosit shows stable accuracy across experimental conditions.</p><p>Our code allows changing the main parameters of the algorithm in order to adjust them to the relevant settings. Therefore, we examined the sensitivity of DeePosit to changes in the parameters used by the algorithm. We first examined DeePosit accuracy as a function of the <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$\Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula> parameter of the preliminary heuristic detection (see methods). We found that <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$\Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula> = 1.6°C gave the best performance in our setting (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), although the accuracy was quite stable (mean F1 score of 0.88–0.89) for values between 1.1 and 3°C. We also trained the DeePosit classifier with an input time window of [–11..30] s instead of [–11..60] s and got no difference in the accuracy level (mean F1 score of 0.89 in both cases).</p></sec><sec id="s2-4"><title>Distinct dynamics of urination and defecation activities across the various tests</title><p><xref ref-type="fig" rid="fig4">Figure 4a, b</xref> shows the raw results of urine and fecal deposit detection by the DeePosit algorithm as a function of time across all three tests, for each male (blue symbols) and female (red symbols) subject mouse. The symbols representing the various deposit types are also labeled (with black dots) according to the arena side of each deposition (relative to the two stimuli). These raw results were further analyzed by computing the average number of urine or fecal deposits, per minute (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). The area of the deposits (cm<sup>2</sup>) is also plotted (<xref ref-type="fig" rid="fig4">Figure 4d</xref>), since urine deposit size might vary significantly between distinct events and conditions (<xref ref-type="bibr" rid="bib27">Wegner et al., 2018</xref>). In general, the event rate and deposit area showed similar trends. As for the side preference, females showed a slight tendency to a higher urination rate at the social stimulus side in the SP test, while males showed a tendency to a higher defecation rate at the social stimulus side (see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Importantly, urination and defecation activities showed distinct dynamics from each other: defecation exhibited a single clear peak in an early stage of the habituation, which appeared in all cases. In contrast, urination was characterized by two peaks, which were not visible in the SP test but appeared in the SxP and got even stronger in the ESPs test, thus showing a gradual increase across test days. The first urination peak occurred in males at the early habituation stage, parallel to the peak in defecation, while the second urination peak occurred in both males and females at the beginning of the trial stage, after stimuli insertion into the arena. For statistical analysis of these dynamics, we compared the mean urine and fecal deposition rates between three periods: the beginning of habituation (habituation minutes 1–4), the end of habituation (habituation minutes 11–14), and the trial - after stimuli introduction (trial minutes 1–4) (<xref ref-type="fig" rid="fig5">Figure 5a, b</xref>). The last minute of both the habituation and the trial stages was not included in the analysis since DeePosit uses 1 min of video after the deposition as input; hence, the accuracy may be lower in cases where we have less than 1 min of video after the deposition. However, including the missing minute of each stage in the analysis yielded similar results (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). For both males and females and across all tests (besides female SP, where only a trend was observed), we found a significantly higher level of fecal deposition at the beginning of habituation than at the habituation end and the trial stage. In contrast, a similar comparison of urination showed that its level was significantly higher during early habituation than at the end of it only for males in the SxP and ESPs tests. A similar elevation in urination was observed during the trial stage, as compared to the habituation end, for both males and females, again specifically during the SxP and ESPs tests. Interestingly, we found an opposite trend for fecal deposits, with a significant decrease in defecation rate during the trial, as compared to the end of habituation, in all the tests for males and in the SxP test for females (<xref ref-type="fig" rid="fig5">Figure 5a, b</xref>). Similar results were found for urine and fecal deposit areas (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Moreover, similar trends were observed when the proportion of mice actively depositing urine or feces during each stage was calculated for each case (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). These data reveal distinct dynamics for urination and defecation activities in a sex- and test-specific manner.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Urine and fecal deposition detection results across tests in CD1 mice.</title><p>Each o represents a single detection of urine deposition (<bold>a</bold>), while each + represents a single detection of fecal deposition (<bold>b</bold>). A black dot in the center of a circle or a + sign marks that this detection is on the side of the preferred stimulus, defined as the social stimulus in the SP trial, the female in the SxP trial, and the stressed mouse in the ESPs trial. Short green lines mark the start and end of the habituation stage and the end of the trial stage, while short vertical black lines mark the end of minute 14 of the habituation stage. The vertical black line at time = 0 marks the start of the trial stage after stimuli introduction to the arena, while the vertical dashed line marks 4 min after the beginning of the trial. Dynamics plots (right) show mean rate (<bold>c</bold>) and mean area (<bold>d</bold>) per minute for both urine and fecal deposits. Error bars represent ± SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Urine and fecal deposition detection results across tests in C57BL/6 mice.</title><p>DeePosit detections for 10 SP and 10 SxP tests performed by male C57BL/6 mice, that were not included in the training set are shown in (<bold>a–h</bold>) in a similar manner to <xref ref-type="fig" rid="fig4">Figure 4</xref>. We chose to ignore small urine detections (deposition area &lt;1 cm<sup>2</sup>) as we found that C57BL/6 males do not emit small urine depositions. Dynamic plots (<bold>c, d, g, h</bold>) show mean rate or mean deposition area, while error bars show ± SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Urine and fecal deposition side preference.</title><p>A comparison of the mean rate ± SEM (<bold>a, b</bold>) and area (<bold>c, d</bold>) of urine (two left bars in each panel) and fecal (two right bars in each panel) depositions made by male (blue bars) and female (red bars) subject mice in each side of the arena. For all three tests, two-sided Wilcoxon rank sum test equal to or smaller than 0.1, and 0.05 was marked with #, and * respectively. Mice with zero urine detections at the relevant test were ignored. The same was done for the feces analysis. For SP, <italic>n</italic> = 10 mice for male urine, <italic>n</italic> = 6 feces, <italic>n</italic> = 4 for female urine, and <italic>n</italic> = 4 for feces. For SxP, <italic>n</italic> = 24 for male urine and <italic>n</italic> = 11 for feces, <italic>n</italic> = 9 for female urine and <italic>n</italic> = 3 for feces. For ESPs, <italic>n</italic> = 17 for male urine and <italic>n</italic> = 6 for feces, <italic>n</italic> = 7 for female urine and <italic>n</italic> = 4 for feces.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig4-figsupp2-v1.tif"/></fig></fig-group><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Comparison between test stages.</title><p>Mean rate ± SEM of urination and defecation events detected during habituation start (minutes 1–4), habituation end (minutes 11–14), and trial (minutes 1–4) stages, for male CD1 mice (<bold>a</bold>), female CD1 mice (<bold>b</bold>) and male C57BL/6 mice (<bold>d</bold>). Percent of active mice (mice with at least one detection) across tests during habituation start, habituation end, and trial stages, for CD1 mice (<bold>c</bold>) and for male C57BL/6 mice (<bold>e</bold>). Two-sided Wilcoxon rank sum test equal to or smaller than 0.1, 0.05, 0.01, and 0.001 was marked with #, *, **, and ***, respectively. In (<bold>a, b, d</bold>), only mice with urination in at least one of the periods were included in the urine analysis. Same for feces. In (<bold>a, b</bold>), <italic>n</italic> = 13, 27, 19 male CD1 urination in SP, SxP, and ESPs, and <italic>n</italic> = 21, 28, 21 for defecation. Accordingly, for CD1 females, <italic>n</italic> = 5, 9, 8 for urination and <italic>n</italic> = 9, 14, 14 for defecation. In (<bold>d</bold>), <italic>n</italic> = 6, 6 for urination and <italic>n</italic> = 7, 9 defecation in SP and SxP. In (<bold>c</bold>), the total number of CD1 male mice is 24, 28, 21 in SP, SxP, and ESPs, and the total number of female mice is 15, 16, 17. In (<bold>e</bold>), the total number of male C57BL/6 mice is 10, 10 in SP and SxP.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Comparison of deposition events rate between test stages using 5-min periods.</title><p>Mean rate ± SEM of urination and defecation events during habituation start (minutes 1–5), habituation end (minutes 11–15), and trial (minutes 1–5) stages for male CD1 (<bold>a</bold>), female CD1 (<bold>b</bold>), and male C57BL/6 mice (<bold>d</bold>). Percent of active mice (mice with at least one detection) across tests during habituation start, habituation end, and trial for male and female CD1 mice (<bold>c</bold>) and for male C57BL/6 mice (<bold>e</bold>). Two-sided Wilcoxon rank sum test equal to or smaller than 0.1, 0.05, 0.01, and 0.001 was marked with #, *, **, and ***, respectively. In (<bold>a, b, d</bold>), only mice with urination in at least one of the periods were included in the urine analysis. Same for feces. In (<bold>a, b</bold>), <italic>n</italic> = 13, 27, 19 male CD1 urination in SP, SxP, and ESPs, and <italic>n</italic> = 23, 28, 21 for defecation. Accordingly, for CD1 females, <italic>n</italic> = 6, 10, 9 for urination and <italic>n</italic> = 12, 15, 15 for defecation. In (<bold>d</bold>), <italic>n</italic> = 6, 6 for urination and <italic>n</italic> = 7, 9 defecation in SP and SxP. In (<bold>c</bold>), the total number of CD1 male mice is 24, 28, 21 in SP, SxP, and ESPs, and the total number of female mice is 15, 16, 17. In (<bold>e</bold>), the total number of male C57BL/6 mice is 10, 10 in SP and SxP.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Comparison of deposition area between test stages using 4-min periods.</title><p>Mean area ± SEM of urine and fecal depositions per minute during habituation start (minutes 1–4), habituation end (minutes 11–14), and trial (minutes 1–4) stages. Statistical comparisons between the three periods (three pair-wise comparisons) were done separately for urine and fecal depositions. Mice with no urine or feces detection in these periods were ignored from the urine or feces analysis, respectively. Two-sided Wilcoxon rank sum test equal to or smaller than 0.1, 0.05, 0.01, and 0.001 was marked with #, *, **, and ***, respectively. Only mice with urination in at least one of the periods were included in the urine analysis. Same for feces. For males, <italic>n</italic> = 13, 27, 19 for urination in SP, SxP, and ESPs, and <italic>n</italic> = 21, 28, 21 for defecation. Accordingly, in females, <italic>n</italic> = 5, 9, 8 for urination and <italic>n</italic> = 9, 14,14 for defecation during SP, SxP, and ESPs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig5-figsupp2-v1.tif"/></fig></fig-group><p>When comparing the urination and defecation patterns of CD1 male mice with those observed in C57BL/6 male mice (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5">Figure 5d</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>), we found distinct characteristics. In contrast to CD1 mice, urination rate of C57BL/6 mice was higher at the beginning of habituation compared to the end of it already in the SP experiment. On the other hand, urination rate of C57BL/6 mice did not increase during the trial as compared to the end of habituation in any of the experiments. Notably, unlike CD1 mice, of C57BL/6 mice did not deposit urine spots smaller than 1 cm<sup>2</sup> (compare <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> with <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). As for the defecation rate of C57BL/6 mice, similarly to CD1 mice, it was higher at the beginning of habituation compared to the end of it. However, unlike the trend in CD1 mice, it was not reduced in the trial stage, as compared to end of habituation. Thus, the distinct dynamics of urination and defecation activities observed using DeePosit, are mouse strain specific.</p><sec id="s2-4-1"><title>Sex-dependent differences across the various stages</title><p>We used two types of statistical tests to compare between male and female CD1 mice. A two-sided Wilcoxon rank sum test (significance marked by *) was used for all pairwise comparisons. In addition, since some of the data was zero-inflated (many mice did not deposit urine or feces at all during the relevant period), we used a two-way Chi-square test (significance marked by +) to compare the distribution of zeros and non-zeros in the male group versus the female group. A test-dependent significant difference between males and females was found in the early stage of habituation (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). On the first day of experiments (the SP test), males and females showed a low urination rate at the first 4 min of habituation, with no significant difference between them. However, in the next two testing days (SxP and ESPs tests), when the mice were already familiar with the arena, we found a significantly higher rate and area of urine deposition in males compared to females (<xref ref-type="fig" rid="fig6">Figure 6a</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a</xref>). As for defecation events, males showed a significantly higher level in this period, in all tests. During the last stage of habituation (minutes 11–14), we found a significant difference between males and females only for the ESPs test, with males showing higher levels of both urination and defecation rate (<xref ref-type="fig" rid="fig6">Figure 6b</xref>) and area (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Comparison of deposition rates between sexes.</title><p>The mean rate ± SEM of urination and defecation events for males (blue bars) versus females (red bars) during early (minutes 1–4) (<bold>a</bold>) and late (minutes 11–14) (<bold>b</bold>) periods of the habituation stage and during the first minute (<bold>c</bold>) and minutes 2–4 of the trial stage (<bold>d</bold>) . A significant difference between the mean rate of urine or fecal depositions (two sided Wilcoxon rank sum test) with p value equal to or smaller than 0.1, 0.05, 0.01, 0.001 was marked with #, *, **, ***, respectively. A significant difference in the distribution of non-depositing animals (Chi-square test) with p value equal to or smaller than 0.1, 0.05, 0.01, 0.001 was marked with !,+,++,+++ respectively. For male mice, <italic>n</italic> = 24, 28, 21 for SP, SxP, and ESPs. For female mice, <italic>n</italic> = 15, 16, 17 accordingly.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Comparison of mean deposition areas between sexes.</title><p>The mean area ± SEM of urine and fecal depositions for males (blue bars) versus females (red bars) during early (minutes 1–4) (<bold>a</bold>) and late (minutes 11–14) (<bold>b</bold>) periods of the habituation stage and during the first minute (<bold>c</bold>) and minutes 2–4 (<bold>d</bold>) of the trial stage. A significant difference between the mean area of urine or fecal depositions (two sided Wilcoxon rank sum test) with p value equal to or smaller than 0.1, 0.05, 0.01, 0.001 was marked with #, *, **, ***, respectively. A significant difference in the distribution of non-depositing animals (Chi-square test) with p value equal to or smaller than 0.1, 0.05, 0.01, 0.001 was marked with !,+,++,+++ respectively. For male mice, <italic>n</italic> = 24, 28, 21 for SP, SxP, and ESPs. For female mice, <italic>n</italic> = 15, 16, 17 accordingly.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100739-fig6-figsupp1-v1.tif"/></fig></fig-group><p>For statistical comparison between males and females during the trial, where an initial peak was observed in some cases (<xref ref-type="fig" rid="fig4">Figure 4c, d</xref>), we divided the trial stage into two periods: the first minute and minutes 2–4, and averaged the results of each period separately. As apparent in <xref ref-type="fig" rid="fig6">Figure 6c, d</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1c, d</xref>, the urination rate during the first minute of the trial stage showed no sex-dependent difference in the SP test. In contrast, a significantly higher level was observed for males versus females in the SxP and ESPs tests. No sex-dependent difference in urination rate was observed For trial minutes 2–4, or in defacation rate for any of the trial periods.</p></sec><sec id="s2-4-2"><title>Male urine and fecal deposition rates are test dependent</title><p>Since the data so far suggest a dynamic change from the SP (first day) to the SxP (second day) and ESPs (third day) tests specifically for males, we checked the effect of test type (SP, SxP, and ESPs) on the dynamics of urination and defecation activities using Kruskal–Wallis test (<xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). The urination and defecation rates (<xref ref-type="table" rid="table1">Table 1</xref>) and deposits areas (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>) of males showed both a significant effect of the test type, with urination showing this effect during early habituation and during the trial, while defecation showing such effect at early habituation, but not during the trial stage. No significant effect was found for females.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>The effect of the test (SP, SxP, and ESPs) on urination or defecation events rates.</title><p>Kruskal–Wallis test was used to check if the test type affects the rate of urination or defecation events. p-value equal to or smaller than 0.1, 0.05, 0.01, 0.001 was marked with #, *, **, ***, respectively. For male mice, <italic>n</italic> = 24, 28, 21 for SP, SxP, and ESPs. For female mice, <italic>n</italic> = 15, 16, 17 accordingly.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Measurement</th><th align="left" valign="bottom">Habituation, minutes 1–4</th><th align="left" valign="bottom">Habituation, minutes 11–14</th><th align="left" valign="bottom">Trial, minute 1</th><th align="left" valign="bottom">Trial, minutes 2–4</th></tr></thead><tbody><tr><td align="left" valign="bottom">Male #Urine</td><td align="char" char="." valign="bottom">0.0004***</td><td align="char" char="." valign="bottom">0.3804</td><td align="char" char="." valign="bottom">0.0015**</td><td align="char" char="." valign="bottom">0.0301*</td></tr><tr><td align="left" valign="bottom">Female #Urine</td><td align="char" char="." valign="bottom">0.3777</td><td align="char" char="." valign="bottom">0.3943</td><td align="char" char="." valign="bottom">0.4287</td><td align="char" char="." valign="bottom">0.3918</td></tr><tr><td align="left" valign="bottom">Male #Feces</td><td align="char" char="." valign="bottom">0.0221*</td><td align="char" char="." valign="bottom">0.1178</td><td align="char" char="." valign="bottom">0.3054</td><td align="char" char="." valign="bottom">0.9251</td></tr><tr><td align="left" valign="bottom">Female #Feces</td><td align="left" valign="bottom">0.0635#</td><td align="char" char="." valign="bottom">0.2653</td><td align="char" char="." valign="bottom">0.1553</td><td align="char" char="." valign="bottom">0.5663</td></tr></tbody></table></table-wrap></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we present a new algorithm and an open-code trainable AI-based computational tool for detecting and classifying urination and defecation events from thermal video clips. This algorithm enables a detailed characterization of the dynamics of urination and defecation activities during social behavior of small rodents. One advantage of this tool is that it is automated, thus allowing a rapid and observer-unbiased analysis of urine and fecal deposition events and areas, with a good temporal and spatial resolution. Specifically, combining our algorithm with an IR camera for thermal imaging of behavioral experiments can replace the void spot test, which usually lacks any temporal resolution and is prone to mistakes caused by urine smearing and filter-paper tearing. Finally, our algorithm facilitates the analysis of defecation activity, which was rather unexplored so far but may contribute to scent-marking behavior, as discussed below. Our algorithm uses thermal video clips generated by an IR camera placed above the arena and does not require a camera placed below a clear arena floor, as used by a recent paper (see <xref ref-type="bibr" rid="bib16">Keller et al., 2018</xref> for example). Thus, it can be utilized for analyzing experiments conducted in standard experimental setups, such as those used for the three-chamber test. The computational tool and experimental method presented here may be useful for a detailed characterization of social behavior in mice, including murine models of autism spectrum disorder and other pathological conditions. It may also be used to explore urination and defecation activities in other scientific contexts, unrelated to social behavior. Finally, our experimental setup is cheap and easy to assemble, and the detection algorithm can run on a standard PC with a GPU card.</p><p>Analysis of the errors made by the algorithm in the test dataset (see <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref> for video clips of these events) raised several limitations, that might be addressed in future work. Urine or fecal deposits must be fully visible while the deposit is still warm. A close adjacency between the mouse and the deposit might cause the mouse mask to overlap the mask of the deposit, thus preventing its detection. Many of the ‘miss’ events in the test video clips were created by the mouse staying close to the urine or fecal deposits for a long period after their deposition. Few other ‘miss’ events were due to very small urine spots or due to repeated urination in the same position during a very short time period, which resulted in detecting these separate urination events as a single event by the algorithm. A wrong classification of urine as fecal deposition occurred in 2.3% of the urination events. In many of these events, the urination spot was small (and therefore harder to distinguish from a fecal deposition) (see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3c</xref>). Wrong classification of background as feces occurred 21 times in the test set. In most of these events, the mistake was due to feces that were moved by the mouse to a new location while still being warm. Such cases may be mitigated in future work by a tracking algorithm that continuously tracks the location of each fecal deposit. Wrong classification of background as urine occurred 33 times in the test set, with some of these errors caused by smearing of large warm urine spots.</p><p>We evaluated the accuracy of the algorithm and found it to be uniform across the various sexes, tests and session stages of the experiments used by us. This suggests that the low level of errors made by the algorithm should not create a bias during biological experiments. Moreover, the algorithm achieved a good and stable accuracy even for C57BL/6 mice examined in a while arena, a condition that was not represented in the training videos. Thus, the algorithm seems to be robust, with a low sensitivity to changing conditions. We also compared the algorithm’s accuracy to the accuracy achieved by a second human annotator on the same dataset and concluded that the algorithm accuracy is comparable to the accuracy of a human annotator while being much faster and unbiased. Finally, the algorithm showed superior performance over classic object detection algorithms, such as YOLOv8, which are based on a single image input. This is most likely due to the transformer-based architecture of our algorithm, which allows it to use the temporal information extracted from the thermal video clips.</p><p>Future work might improve DeePosit by extending the training set and including more challenging examples. Notably, comparing a small training set (<xref ref-type="fig" rid="fig3">Figure 3h</xref>) with a larger one (<xref ref-type="fig" rid="fig2">Figure 2h</xref>) shows that the larger training set improved the accuracy of DeePosit. Another way for future improvement in DeePosit accuracy may be by using a trainable detection and segmentation algorithm instead of heuristic preliminary detection. Note that our classifier currently does not get the mask of the preliminary detection as an input, making the classification task harder when there are adjacent deposition events. An end-to-end trainable detection, segmentation, and classification pipeline might address these limitations but will require a much larger training set. Future work might also adapt the algorithm for multi-animal experiments. Such adaptation might require detecting the mask of each of the animals, separating the identity of each of the animals, and associating each deposition with the relevant animal.</p><p>We validated our method and algorithm using experimental results from social discrimination tests conducted by male and female CD1 and male C57BL/6 mice. We demonstrated distinct dynamics of urination and defecation activities across the habituation and trial stages, with sex-, test-, and strain-dependent differences. Both male and female CD1 mice, as well as male C57BL/6 mice showed higher rates of defecation activity at the early stage of the habituation phase, as compared to later stages (<xref ref-type="fig" rid="fig5">Figure 5</xref>). This tendency may reflect a higher level of anxiety at the beginning of the habituation phase, caused by the novel context. Still, it may also serve for scent-marking activity, that labels the arena as a familiar environment. The latter explanation is supported by the fact that the peak in defecation activity was not reduced from the first-day test (SP) to the second and third-day tests (SxP and ESPs), when the subject is expected to be less anxious due to the familiar context. In contrast to defecation, urination activity at the beginning of the habitation phase in CD1 mice was test dependent. While no peak was observed during the SP test, the first time the animals were exposed to the experimental arena, it was observed in the second test (SxP) and got even stronger in the last test (ESPs). This development was statistically significant in CD1 males but not in females. Since these changes occur during the habituation phase, before the introduction of stimuli to the arena, they cannot reflect the type of test and thus seem to be induced by the order of the experiments. Notably, similar dynamics across experimental days were previously reported using the void spot assay for C57BL/6j mice (<xref ref-type="bibr" rid="bib15">Keil et al., 2016</xref>). This suggests that the induction of urination activity by males at the early stage of the habituation phase represents territorial scent-marking activity, which is positively correlated with the higher familiarity experienced by the subject in the arena as the experiments progressed between days. It should be noted that an early peak of urination upon entering an environment was reported by a recent study using a thermal camera for manual analysis of urination activity (<xref ref-type="bibr" rid="bib20">Miller et al., 2023b</xref>). A second peak of urination activity was observed at the beginning of the trial period, after stimuli insertion to the arena. This was observed in both male and female CD1 mice, but the test type significantly affected it only in males. In this case, we cannot dissect the effect of test type from the test order, as the urination activity occurred after stimuli insertion and, hence, may be induced by the presence of specific social stimuli. Since the subjects are already habituated to the arena at this stage, the elevated urination activity seems to serve as part of the subjects’ social behavior, most probably as a territorial scent-marking behavior induced by the presence of social stimuli, i.e., competitors. Interestingly, we found several differences in the dynamics of CD1 male mice and C57BL/6 male mice, suggesting that the scent-marking behavior is also strain specific. Unlike CD1 male mice, C57BL/6 male mice exhibited a peak in urination already at the beginning of the first (SP) habituation, a trend toward higher level of defecation activity in the SP trial stage, and no increase in urination activity during the SP and SxP trial stage, compared to the habituation end. However, several findings were common for both CD1 and C57BL/6 male mice, such as the higher feces rate at the beginning of habituation in comparison to the end of habituation and the higher levels of urination at the beginning of the SxP habituation stage.</p><p>We did not observe a consistent spatial distribution of the urine or fecal deposits between the arena sides of the preferred and non-preferred stimuli in CD1 mice. This seems to contradict a recent study (<xref ref-type="bibr" rid="bib20">Miller et al., 2023b</xref>), that reported opposite bias toward familiar versus unfamiliar stimuli in losers versus winners wild-derived mice following a social contest. This contradiction may be due to the distinct mouse strains or the distinct contexts of social behavior (presentation of a single stimulus animal in comparison to two simultaneously presented animals) used by both studies.</p><p>Overall, the novel algorithm and software presented here enable a cost-effective, rapid, and unbiased analysis of urination and defecation activities of behaving mice from thermal video clips. The algorithm is trainable and may be adapted to various behavioral and experimental contexts. Thus, it may pave the way for the integration of this important behavioral aspect in the analysis of small rodents’ social and non-social behaviors, in health and disease.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>Subject animals were adult (12–14 weeks old) male and female wild-type offspring derived from breeding couples of <italic>Gtf2i+/Dup</italic> with a CD1 (ICR) genetic background mice (<xref ref-type="bibr" rid="bib18">Mervis et al., 2012</xref>). We used this line of mice since, in parallel to this work, we are investigating the phenotype of the <italic>Gtf2i+/Dup</italic> mutant line. These results will be published separately. These mice were bred and grown in the SPF mouse facility of the University of Haifa. C57BL/6 mice were purchased from Envigo (Rehovot, Israel). Stimulus animals were adult (12–14 weeks old) male and female CD1 or C57BL/6 mice purchased from Envigo (Rehovot, Israel). All mice were housed in groups of 3–5 in a dark/light 12 hr cycle (lights on at 7 pm), with ad libitum food and water under veterinary inspection. Experiments were performed in the dark phase of the dark/light cycle. All experiments were approved by the University of Haifa ethics committee (Reference #: UoH-IL-2301-103-4).</p></sec><sec id="s4-2"><title>Setup and video acquisition</title><p>The experimental setup is based on the setup described in <xref ref-type="bibr" rid="bib23">Netser et al., 2019</xref>. Briefly, a black or white Plexiglass box arena (37 cm × 22 cm × 35 cm) was placed in a sound-attenuated chamber. A visible light (VIS) camera (both Flea3 and Grasshopper3 models manufactured by Teledyne FLIR were used, both with a wide-angle lens, rate of 30 frames per second, and USB3 interface) and a long wave IR camera (Opgal’s Thermapp MD with 6.8 mm lens, 384 × 288 pixels at a rate of 8.66 frames per second (FPS)) were placed about 70 cm above the arena’s floor. The IR camera was designed to measure human skin temperature and outputs the apparent temperature for each pixel. Raw pixel values were converted to Celsius degrees using the formula supplied by the manufacturer. We acquired the camera videos using custom-made Python software (code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit">https://github.com/davidpl2/DeePosit</ext-link> [copy archived at <xref ref-type="bibr" rid="bib25">Peles, 2025</xref>] and <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14754159">https://doi.org/10.5281/zenodo.14754159</ext-link>) that used the manufacturer’s SDK (SDK version: EyeR-op-SDK-x86-64-2.15.915.8688-MD). To improve the accuracy of and reduce possible drifts in the measured temperature, a high-emissivity blackbody (Nightingale BTR-03 blackbody by Santa Barbara Infrared, Inc) was placed in the camera’s field of view and was set to 37℃. During analysis, the offset between the blackbody apparent temperature and 37°C was subtracted from the image. To improve image quality, we turned on the camera at least 15 min before the beginning of the experiment (this allows the camera’s temperature to get stable). In addition, to reduce pixel non-uniformity, we captured 16 frames of a uniform surface (a piece of cardboard placed in front of the camera) before each test. These images were then averaged, and the average image’s mean was subtracted from it to get a non-uniformity image with zero mean. The non-uniformity image was then subtracted from each image in the video to achieve better pixel uniformity.</p></sec><sec id="s4-3"><title>Social behavior tests</title><p>We used three distinct social discrimination tests, as previously described in <xref ref-type="bibr" rid="bib21">Mohapatra et al., 2024</xref>. Briefly, all tests consisted of 15 min of habituation, during which the subject mouse got used to the arena with empty triangular chambers (12 cm isosceles, 35 cm height) located at randomly chosen opposite corners. Each triangular chamber had a metal mesh (18 mm × 6 cm; 1 cm × 1 cm holes) at its bottom, through which subject mice could interact with the stimuli. After habituation, the empty chambers were removed and new stimuli-containing chambers were introduced into the arena for the 5-min trial. In the SP test, a novel (i.e., unfamiliar to the subject mouse) sex-matched stimulus mouse was placed in one chamber, whereas an object stimulus (a Lego toy) was placed in the opposite chamber. In the Sex Preference (SxP) test, a novel female mouse was placed in one chamber while a novel male was placed in the opposite chamber. In the ESPs test, a novel stressed (restrained in a 50-ml plastic tube for 15 min before the test) sex-matched mouse was introduced to one chamber of the arena while a novel naïve mouse was placed in the opposite chamber.</p></sec><sec id="s4-4"><title>Behavioral analysis</title><p>VIS video clips were analyzed using TrackRodent (<xref ref-type="bibr" rid="bib24">Netser, 2020</xref>), as previously described in <xref ref-type="bibr" rid="bib22">Netser et al., 2017</xref>.</p></sec><sec id="s4-5"><title>Urine and feces detection algorithm</title><p>The detection algorithm consists of two main parts. A preliminary heuristic detection algorithm detects warm blobs. These blobs are then fed into a machine learning-based classifier, which classifies them as either urine, feces, or background (i.e., no detection). The algorithm’s code is available here: <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit">https://github.com/davidpl2/DeePosit</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14754159">https://doi.org/10.5281/zenodo.14754159</ext-link>.</p><sec id="s4-5-1"><title>Manual inputs</title><p>A graphical user interface was developed in Matlab to support all of the required manual annotations. Each video went through a manual annotation of the arena’s floor, the area of the blackbody, and a specification of the first and last frames of both the habituation and trial periods. These two periods were separated by a ~30-s period during which the stimuli were introduced to the arena, which was excluded from the analysis. Also, the arena side of each stimulus (e.g., the male and female sides in the SxP test) was defined as the half of the arena close to this stimulus’s chamber. To generate the train and test sets, a human annotator manually tagged urine and fecal deposition events in videos of 157 experiments with CD1 mice, of which 97 were used for training and 60 for testing. A single click was used to mark the center of each urine or fecal deposit in the first frame where it was clearly visible. The training set included 751 urine annotations and 637 feces annotations. The test set included 438 urine annotations and 374 feces annotations. Additional details can be found in the software’s manual.</p></sec><sec id="s4-5-2"><title>Preliminary detection of hot blobs</title><p>Urine and fecal deposits appear as hot (bright) blobs in the first seconds after deposition. After a cool-down period, which takes about 30–60 s for feces and small urine spots and up to ∼4 min for large urine spots, feces and urine appear as dark spots in the thermal image. The preliminary detection relies on these effects (see pseudo-code in Algorithm 1). It uses image subtraction to search for hot blobs that appear in the video and cool down later. We generate a background image <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$B_{i}$\end{document}</tex-math></alternatives></inline-formula> for each frame <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula> to detect new hot blobs. Subtraction of <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$B_{i}$\end{document}</tex-math></alternatives></inline-formula> from <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula> generates an image in which the mouse pixels and new (warm) urine and feces pixels appear bright. We set <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$B_{0}$\end{document}</tex-math></alternatives></inline-formula> as the per-pixel minimum of the first 20 s of video (note that habituation and trial videos are analyzed separately to account for possible minor shifts in the arena’s position). We assume that the mouse is brighter than the arena’s floor and that the mouse moves during the first 20 s, so each pixel will get the arena’s floor value at least once during this time.</p><p>For <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$i \gt 0$\end{document}</tex-math></alternatives></inline-formula> we compute <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$B_{i}$\end{document}</tex-math></alternatives></inline-formula> as the minimum of images <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$N_{j}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>44</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>36</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$j\in[i-44,..,i-36]$\end{document}</tex-math></alternatives></inline-formula> (this roughly matches time range <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>4</mml:mn><mml:mi>s</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$[i-5s,..,i-4s]$\end{document}</tex-math></alternatives></inline-formula>) where <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$N_{j}$\end{document}</tex-math></alternatives></inline-formula> is an image in which the mouse pixels were replaced by the last known values from before the time that the mouse occupied these pixels. We set <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>&lt;=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$N_{j \lt =0}=B_{0}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>To compute the mouse mask at frame i, <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$B_{i-1}$\end{document}</tex-math></alternatives></inline-formula> is subtracted from <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula>. The subtraction result is dilated by Matlab’s <italic>imdilate</italic> function with a structuring element of a disk of a radius of 2 pixels and then compared against a threshold of 1°C to get a binary mask of the pixels that are warmer than the arena’s floor. Connected regions are then computed using Matlab’s <italic>bwlabel</italic> function and the connected region with the largest intersection with the arena’s floor is considered as the mask of the mouse (denoted <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$M_{i}$\end{document}</tex-math></alternatives></inline-formula>).</p><p><inline-formula><alternatives><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$N_{i}$\end{document}</tex-math></alternatives></inline-formula> is then computed by taking <inline-formula><alternatives><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula> values for the pixels outside <inline-formula><alternatives><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft26">\begin{document}$M_{i}$\end{document}</tex-math></alternatives></inline-formula> and taking the values of <inline-formula><alternatives><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$N_{i-1}$\end{document}</tex-math></alternatives></inline-formula> for the mouse containing pixels: <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$N_{i}=N_{i-1}*M_{i}+F_{i}*(1-M_{i})$\end{document}</tex-math></alternatives></inline-formula> where * denotes pixel-wise multiplication.</p><p>The difference image <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$D_{i}$\end{document}</tex-math></alternatives></inline-formula> is computed by: <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$D_{i}=F_{i}-max(T,B_{i})$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> is the arena’s floor median temperature, computed by <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>F</mml:mi><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi mathvariant="normal">¬</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi mathvariant="normal">¬</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$T=median(B_{i}(AF\&amp;\neg M_{i}\&amp;\neg M_{i-1}))$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>F</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$AF$\end{document}</tex-math></alternatives></inline-formula> is a mask of the arena’s floor, <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">§amp;</mml:mi></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$\&amp;$\end{document}</tex-math></alternatives></inline-formula> is pixel-wise AND operation, and ¬ is pixel-wise NOT operations. Using <italic>T</italic> prevents higher detection sensitivity in darker regions of the arena floor (regions in the arena’s floor that are covered in cooled-down urine appear darker than dry regions of the arena’s floor, see <xref ref-type="fig" rid="fig2">Figure 2e</xref>).</p><p>The cooldown rate <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$CD_{i}$\end{document}</tex-math></alternatives></inline-formula> is computed by taking the per pixel minimum of the frames in the next 40 s following <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula> and subtracting it from <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>The hot blobs mask <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$BM_{i}$\end{document}</tex-math></alternatives></inline-formula> is computed by taking the pixels for which <inline-formula><alternatives><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft39">\begin{document}$D_{i} \gt \Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula> and not included in <inline-formula><alternatives><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft40">\begin{document}$M_{i}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$M_{i-1}$\end{document}</tex-math></alternatives></inline-formula> and for which the <inline-formula><alternatives><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>1.1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$CD_{i} \gt 1.1$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn><mml:mo>∗</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$CD_{i} \gt 0.5*D_{i}$\end{document}</tex-math></alternatives></inline-formula>. We explored several values for <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$\Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula> (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>) and chose <inline-formula><alternatives><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>1.6</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft45">\begin{document}$\Delta T_{Threshold}=1.6^{\circ}\rm C$\end{document}</tex-math></alternatives></inline-formula> as the default value for this parameter. We ask for the cooldown to be at least half of the increase in the temperature but not more than that since very large urinations cool down slower and might take more than 40 s to cool down fully. We excluded pixels in <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$M_{i-1}$\end{document}</tex-math></alternatives></inline-formula> (mouse containing pixels in frame i-1) and not just <inline-formula><alternatives><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$M_{i}$\end{document}</tex-math></alternatives></inline-formula> since the IR sensor has a response time that might causes pixels included in <inline-formula><alternatives><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft48">\begin{document}$M_{i-1}$\end{document}</tex-math></alternatives></inline-formula> to be slightly brighter.</p><p><inline-formula><alternatives><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$BM_{i}$\end{document}</tex-math></alternatives></inline-formula> goes through a morphological close operation using Matlab’s <italic>imclose</italic> function with a structure element of a disk with a radius of 4 pixels. This causes any nearby drops of urine to unify to a single detection. Blobs that overlap pixels outside the arena’s floor or touch the mouse mask are ignored to avoid detection on darker areas of the mouse (mostly the tail), reflections from the arena’s wall, and detections due to a stimulus mouse which sometimes sticks his nose throughout the barrier net of the chamber. Also, blobs with a size &lt;2 pixels or larger than 900 pixels are ignored (pixel size is roughly 0.02 cm<sup>2</sup>).</p><p>Blobs that intersect previously detected blobs are considered to be the same detection if no more than 30 s passed from the last frame in which the previous detection was last detected. A unified detection mask is computed each time a detection is associated with a previous detection. This allows reduction of false alarms which might be caused by the smearing of a still-hot urine drop. If no such intersection exists, a new preliminary detection is added to the list of detections. A blob should be detected in at least two frames to be included in the output detections. The selected frame ID for each blob is the frame that contains the maximum intensity for this blob out of all frames in which this blob was detected. The representative coordinates for each detected blob were chosen by taking the pixel with the maximum intensity inside the blob in the selected frame. Usually, the selected frame for each blob is the first frame of the detection (as the detection cools, the maximum intensity is usually in the first detected frame). Still, it might be another frame if the detection was partly occluded by the mouse tail or if a second urine event occurred in the same place during the relevant time frame. The output detections are fed into a classifier, which will be described next.</p><p>The detection threshold <inline-formula><alternatives><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$\Delta T_{Threshold}$\end{document}</tex-math></alternatives></inline-formula> is higher than the mouse detection threshold (1°C) to avoid false defections within the borders of the subject mouse body.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups" id="AL1"><tbody><tr><td align="left" valign="bottom">Algorithm 1 Preliminary Detection of Hot Blobs</td></tr><tr><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1..20</mml:mn><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$B_{0}(p)\leftarrow min_{i\in [1..20FPS]}(F_{i}(p))$\end{document}</tex-math></alternatives></inline-formula>                                        <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Background image at pixel p.<break/><inline-formula><alternatives><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&lt;=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$N_{i \lt =0}\leftarrow B_{0}$\end{document}</tex-math></alternatives></inline-formula> <break/> Let <inline-formula><alternatives><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft54">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula> be the i’th frame in the video<break/>Let <inline-formula><alternatives><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft55">\begin{document}$AF$\end{document}</tex-math></alternatives></inline-formula> be the mask of the arena’s floor (equals 1 for the arena’s floor pixels and 0 elsewhere)<break/><bold>for</bold> <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1..</mml:mn><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$i\in [1..n]$\end{document}</tex-math></alternatives></inline-formula> <bold>do</bold><break/>    𝑀<sub>𝑖</sub> ← 𝐵𝑙𝑜𝑏_𝑊𝑖𝑡ℎ_𝑀𝑎𝑥𝑖𝑚𝑎𝑙_𝐹𝑙𝑜𝑜𝑟_𝐼𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡𝑖𝑜𝑛(𝑖𝑚𝑑𝑖𝑙𝑎𝑡𝑒((<inline-formula><alternatives><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}${F_{i}}-{B_{i-1}}$\end{document}</tex-math></alternatives></inline-formula>) &gt; 1°C, 𝑟𝑎𝑑𝑖𝑢𝑠 = 2))<break/>    <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$N_{i}\leftarrow N_{i-1}M_{i}+F_{i}(1-M_{i})$\end{document}</tex-math></alternatives></inline-formula>                    <inline-formula><alternatives><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Mouse pixels are replaced with background pixels<break/>    <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>4</mml:mn><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$B_{i}(p)\leftarrow min_{j\in[i-5sec,...,i-4sec]}N_{j}(p)$\end{document}</tex-math></alternatives></inline-formula>             <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Background value for each pixel p<break/>    <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>F</mml:mi><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi mathvariant="normal">¬</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi mathvariant="normal">¬</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$T\leftarrow median(B_{i}(AF\&amp;\neg M_{i}\&amp;\neg M_{i-1}))$\end{document}</tex-math></alternatives></inline-formula>                <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Median temperature of the arena floor<break/>    <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$D_{i}\leftarrow F_{i}-max(T,B_{i})$\end{document}</tex-math></alternatives></inline-formula>                          <inline-formula><alternatives><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft65">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Difference image. max operation is pixel-wise<break/>    <inline-formula><alternatives><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>40</mml:mn><mml:mo>∗</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft66">\begin{document}$CD_{i}(p)\leftarrow F_{i}(p)-\min_{j\in[i..min(n,i+40*FPS)]}F_{j}(p)$\end{document}</tex-math></alternatives></inline-formula>      <inline-formula><alternatives><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft67">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Cooldown in the next 40 sec<break/>    <inline-formula><alternatives><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi mathvariant="normal">¬</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi mathvariant="normal">¬</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msup><mml:mn>1.1</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn><mml:mo>∗</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$BM_{i}\leftarrow(D_{i} \gt \Delta T_{Threshold})\&amp;\neg M_{i}\&amp;\neg M_{i-1}\&amp;(CD_{i } \gt 1.1 ^{\circ} C)\&amp;(CD_{i} \gt 0.5*D_{i})$\end{document}</tex-math></alternatives></inline-formula>   <inline-formula><alternatives><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft69">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Hot Blobs Mask<break/>    <inline-formula><alternatives><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft70">\begin{document}$BM_i\leftarrow\,imclose(BM_i,\, radius=4)$\end{document}</tex-math></alternatives></inline-formula>         <inline-formula><alternatives><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft71">\begin{document}$\triangleright$\end{document}</tex-math></alternatives></inline-formula> Filling small gaps in blobs mask<break/>     that are fully inside 𝐴𝐹 and not adjacent to mouse’s mask<break/>    with 𝑠𝑖𝑧𝑒 ∈ [2..900]<break/>    blobsList is updated. New blobs are associated with blobs that were detected up to<break/>    30 seconds ago if their masks intersect<break/><bold>end for</bold><break/>return blobs in blosbList that were detected in at least 2 frames</td></tr></tbody></table></table-wrap></sec><sec id="s4-5-3"><title>Classifying preliminary detections using an artificial neural network</title><p>Preliminary detections are fed to a trained artificial neural network classifier which classifies them as either: <italic>Urine</italic>, <italic>Feces</italic>, or <italic>Background</italic> (<xref ref-type="fig" rid="fig2">Figure 2g</xref>). We relied on the transformer-based architecture proposed by <xref ref-type="bibr" rid="bib6">Carion et al., 2020</xref>. This architecture was designed for object detection in RGB images. It receives an RGB image as input and outputs a set of bounding boxes around each detected object and the classification of each detection. In brief, this neural network architecture consists of a convolutional neural network based on the <italic>ResNet</italic> architecture proposed by <xref ref-type="bibr" rid="bib10">He et al., 2016</xref>, which serves as the backbone and extracts a set of feature vectors from each location in the input image. The feature vectors are attached with a position encoding, which is a second feature vector that describes the spatial location in the input image, associated with the backbone’s feature vector. For each spatial location, the feature vectors from the backbone and the positional encoding are summed and fed into an encoder transformer, which uses an attention mechanism to share information between the feature vectors from various spatial locations. A decoder block is fed with the output of the encoder, and an additional set of vectors is denoted as queries. The decoder uses several layers of self and cross-attention to share information between queries (self-attention) and between the queries and the decoder output (cross-attention). Finally, the encoder outputs a feature vector for each input query. This vector is fed into a feed-forward network to compute each query’s bounding box and classification. One of the possible classification outputs for each query is ‘no object’. We relied on the popular open-source code published by <xref ref-type="bibr" rid="bib6">Carion et al., 2020</xref> and made a few adjustments. Instead of feeding a single RGB image as input, for each detection in <inline-formula><alternatives><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula> we used a series of 78 grayscale image patches cropped around the detection pixel (65 × 65 pixels patch) and representing a time window of about [–11s .. 60s] around the detection. For detection in <inline-formula><alternatives><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}$F_{i}$\end{document}</tex-math></alternatives></inline-formula> we used the frames [<inline-formula><alternatives><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>12</mml:mn><mml:mo>∗</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>11</mml:mn><mml:mo>∗</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>0</mml:mn><mml:mo>∗</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>65</mml:mn><mml:mo>∗</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$F_{i-12*8},F_{i-11*8},...F_{i-0*8},...,F_{i+65*8}$\end{document}</tex-math></alternatives></inline-formula>] for classification. We used this relatively large time window to capture the cooldown of the feces and urine, movement of feces (which are frequently moved by the mouse), or smearing of urine. Additionally, this time window allows for capturing the moment of the deposition of the urine or feces, which sometimes occurs a few seconds before the preliminary detection (since the mouse may fully or partly occlude the detection in the first seconds). In case one or more frames in this sequence are not available (exceeds the time limits of the video), a uniform image with a temperature of 22℃ was used instead. Each of the three consequent patches in this set was combined into a single RGB patch and was fed to the backbone. This allows the use of pre-trained backbone weights as well as reduced run-time in comparison to the option of feeding each patch separately to the backbone. Similarly to <xref ref-type="bibr" rid="bib6">Carion et al., 2020</xref>, each of the backbone’s output feature vectors was attached with a positional encoder. However, we adjusted the positional encoding to include additional information on the time of each feature vector (in addition to its spatial location). To do that, we computed time encoding in the same way it was computed by <xref ref-type="bibr" rid="bib6">Carion et al., 2020</xref> for encoding the <italic>x</italic> or <italic>y</italic> coordinate and concatenated it to the <italic>x</italic>, <italic>y</italic> position encoding vector. To keep the length of the joint position and time encoding the same, we added a fully connected trainable layer that gets the (<italic>x</italic>, <italic>y</italic>, <italic>t</italic>) embedding as input (dim = 128*3 = 384) and outputs a feature vector with dim = 256 which allows using the rest of the neural network and pre-trained weights without additional changes. Lastly, instead of using 100 queries as in <xref ref-type="bibr" rid="bib6">Carion et al., 2020</xref>, we used just a single query to get just the classification of the input set of patches and disabled the computation of a bounding box. Since our training set is relatively small, we used transfer learning and initialized the learnable weights with the weights published by <xref ref-type="bibr" rid="bib6">Carion et al., 2020</xref> (weight file: detr-r50-dc5-f0fb7ef5.pth). We used the dc5 (dilated C5 stage) option proposed by <xref ref-type="bibr" rid="bib6">Carion et al., 2020</xref>, which increases the spatial resolution of the backbone’s output by a factor of 2 as it may be more suitable for classifying small objects, and used ResNet-50 as the backbone. We first trained the classifier using 39 train videos (each video contains a single experiment and includes both the habituation and trial periods and is of length of roughly 20 min). A second round of training used the weights of the first round as initial weights and included an additional 58 training videos (a total of 97 training videos).</p><p>Training database generation included extraction of (a) Positive examples of urine and feces that were manually marked. (b) Forty negative examples (labeled as background) per video in randomly selected positions and time (half during habituation and half during trial) that are not close in space and time to any manual annotation. (c) Hard negative examples consist of preliminary detected blobs (detected by the heuristic detection algorithm) that are not close in space and time to any manual detection. For both types of negative examples, a negative example in position <inline-formula><alternatives><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft75">\begin{document}$x_{d}$\end{document}</tex-math></alternatives></inline-formula> and time <inline-formula><alternatives><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft76">\begin{document}$t_{d}$\end{document}</tex-math></alternatives></inline-formula> was considered to be close to a manual detection of position <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$x_{m}$\end{document}</tex-math></alternatives></inline-formula> in time <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$t_{m}$\end{document}</tex-math></alternatives></inline-formula> if <inline-formula><alternatives><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>25</mml:mn><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:math><tex-math id="inft79">\begin{document}$distance(x_{d},x_{m}) \lt 25pixels$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mn>10</mml:mn><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>30</mml:mn><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$-10sec\leq t_{d}-t_{m}\leq 30sec$\end{document}</tex-math></alternatives></inline-formula>. For the positive examples, we augmented the data by a time shift of [–3s..6s], compensating for possible differences between the manual tagging and the preliminary detection time, as well as increasing the training set size. Data augmentation for all examples included a random spatial shift of +–2 pixels, random flip, and rotation of 90°, 180°, and 270°. Input data was normalized to contain values between [0..255] using linear mapping that mapped 10°C to 0 and 40°C to 255. Values that exceeded 0 or 255 were trimmed. The first training round (39 training videos) was done for 230 epochs with a learning rate of 1e−5 for the backbone and 1e−4 for the rest of the weights and a factor 10 learning rate drop after 200 epochs. The second training round (97 training videos) was done for 50 epochs with a learning rate of 1e−5 for the backbone and 1e−4 for the rest of the weights and a factor 10 learning rate drop after 40 epochs.</p></sec></sec><sec id="s4-6"><title>Accuracy measurement</title><p>The accuracy of automatic detections was evaluated using the following principles: (1) Manually tagged urine or fecal deposition is considered correctly detected by the algorithm, if an automatic detection with the same label exists at a distance of up to 20 pixels (2.9 cm) and in a time difference of up to 15 s. Spatial tolerance is required due to inherent ambiguity in the manual urine tagging process, as different observers often mark large spots or long traces of urine differently (see <xref ref-type="fig" rid="fig2">Figure 2d</xref> for an example of such a trace). Specifically, the detection algorithm might unify adjacent urine spots, tagged as multiple urine depositions by human annotators (see e.g. <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref> and <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Temporal tolerance is required as the mouse body may cover the deposit or be very close to it for a while, thus delaying the time the preliminary detection algorithm detects it. (2) In the case described in 1, all automatic detections in this time and space window that got a correct label by the algorithm as the manual tagging are not counted as false alarms. (3) In contrast, if only automatic detections carrying labels different from the manually tagged deposition exist in the relevant space and time around it, then the closest one will be associated with this manual annotation and will be counted as misclassification (i.e., urine that was classified as feces or BG and feces that was classified as urine or BG), while the others will be counted as false alarms (will be counted in the BG column of the confusion matrix).</p></sec><sec id="s4-7"><title>Comparison with a second human annotator</title><p>The task of detecting and correctly classifying urine and feces in thermal videos is also challenging for a human annotator. To assess the performance of the DeePosit algorithm and compare it to a human annotator, 25 test videos were manually annotated by a second human annotator that marked a polygon surrounding each feces or urine spot. The detections of the DeePosit algorithm and of the second human annotator were compared to the annotation of the first human annotator (see <xref ref-type="fig" rid="fig3">Figure 3f, g</xref>).</p></sec><sec id="s4-8"><title>Comparison with YOLOv8 object detector</title><p>We compared our algorithm with a YOLOv8 (<xref ref-type="bibr" rid="bib14">Jocher et al., 2023</xref>) based algorithm (YOLOv8n architecture). We trained YOLOv8 on 39 thermal video clips that were manually tagged with bounding boxes around each feces or urine spot. An additional 25 videos were annotated with bounding boxes for validation. OpenLabeling annotation tool was used for bounding boxes annotation (<xref ref-type="bibr" rid="bib7">Cartucho et al., 2018</xref>). The training was done for 10,000 epochs with default parameters. Weights were initialized with YOLOv8n.pt pre-trained weight file, which was published by <xref ref-type="bibr" rid="bib14">Jocher et al., 2023</xref>. Output weight file with the best accuracy on the validation videos was chosen. As YOLOv8 expects the pixel values to be between 0 and 255, temperatures between 10 and 40℃ were linearly mapped to values between 0 and 255. As YOLOv8 is designed for 3-channel RGB images, we compared two training approaches. The first approach (termed YOLOv8 Gray) used the same thermal image for the R, G, and B channels. The second approach used three thermal images from time <italic>t</italic>, <italic>t</italic> + 10 s, and <italic>t</italic> + 30 s, where <italic>t</italic> is the time of the deposition tagging, and fed them to the YOLOv8 classifier as the R, G, and B channels. This gives the classifier relevant temporal information that might capture the cool-down process, smearing of urine or shift of feces. Training examples included all frames in which a manual detection was labeled. Bounding boxes were annotated around all warm and clearly visible urine or feces in each of these frames (including old urine and feces that are still warm and clearly visible). In addition, 40 randomly selected images (from each training video) with no manual detection in a time period of −60…+10 s were added to the training set. During inference, YOLOv8 Gray or YOLOv8 RGB was activated on each frame of the thermal video. To prevent the same deposition from being detected many times, overlapping detections with the same label were unified if no more than 30 s passed between them. We compared the accuracy achieved by YOLOv8 Gray and YOLOv8 RGB with the DeePosit algorithm that was trained on the same 39 training videos. The results are shown in <xref ref-type="fig" rid="fig3">Figure 3h–j</xref>.</p></sec><sec id="s4-9"><title>Model evaluation on mice of a different strain (C57BL/6)</title><p>To evaluate the usability of our method in a different strain of mice and a different setting, we conducted 10 SP and 10 SxP experiments with C57BL/6 black mice using a white Plexiglass box arena (37 cm × 22 cm × 35 cm). We used the same classifier and the same preliminary detection parameters. Note that the training set does not include C57BL/6 mice videos or videos with white arenas (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5">Figure 5d, e</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d, e</xref> for results).</p></sec><sec id="s4-10"><title>Statistical analysis</title><p>We used a two-sided Wilcoxon rank sum test (Matlab’s <italic>ranksum</italic> function) for all pairwise comparisons. Rank sum p-value equal to or smaller than 0.1, 0.05, 0.01, and 0.001 was marked with #, *, **, and ***, respectively. In addition, since some of the data is zero-inflated (many mice do not deposit urine or feces in the relevant measured period), we used a two-way Chi-square test to compare the distribution of zeros and non-zeros in the male group versus the female group in <xref ref-type="fig" rid="fig6">Figure 6</xref> and in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. The two-way Chi-square test was implemented using Matlab (see code in Appendix 1). p-value equal or smaller than 0.1, 0.05, 0.01, and 0.001 was marked with !, +, ++, and +++, respectively, and was mentioned to the left side of the ranksum p-value symbol (i.e, the notation +/** means that two-way Chi-square test resulted in p-value ≤0.05 and the ranksum test resulted in p-value ≤0.01). For the habituation versus trial comparison (<xref ref-type="fig" rid="fig5">Figure 5a, b</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), and the side preference analysis (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), mice with zero urine detections across all periods of the same test were ignored. The same was done for the feces analysis. Lastly, we used Matlab’s <italic>kruskalwallis</italic> function for the Krusukal–Wallis test, which was used to examine the effect of test type (SP, SxP, and ESPs) on the dynamics of the urine and feces rate (<xref ref-type="table" rid="table1">Table 1</xref>) and area (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). Additional statistical data for the figures is <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit/tree/main/FigStat/PostRevision">available</ext-link>. No mouse selection was done, but several videos in which the arena was not positioned well below the camera were excluded. Two cohorts of CD1 mice were used, each of which included at least eight mice and was tested by a different experimenter.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Formal analysis, Writing – original draft, Algorithm development, Video annotation</p></fn><fn fn-type="con" id="con2"><p>Writing – review and editing, Algorithm development, Experimental setup building</p></fn><fn fn-type="con" id="con3"><p>Data curation, Performing social behaviour tests, Video annotation</p></fn><fn fn-type="con" id="con4"><p>Data curation, Performing social behaviour tests, Video annotation</p></fn><fn fn-type="con" id="con5"><p>Resources, Funding acquisition, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were approved by the University of Haifa ethics committee (Reference #: UoH-IL-2301-103-4).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-100739-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Raw video files will be shared with any academic PI upon a reasonable request from the corresponding author (davidpelz@gmail.com) and within a reasonable time. The data is free to use for academic purposes. No project proposal is required. For commercial use, please contact Shlomo Wagner (shlomow@research.haifa.ac.il). The code, trained classifier weights, statistical data, and an example of a raw video are shared in <ext-link ext-link-type="uri" xlink:href="https://github.com/davidpl2/DeePosit">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib25">Peles, 2025</xref>) and <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14754159">Zenodo</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>David</surname><given-names>P</given-names></name><name><surname>Shai</surname><given-names>S</given-names></name><name><surname>Natalie</surname><given-names>R</given-names></name><name><surname>Taghreed</surname><given-names>S</given-names></name><name><surname>Shlomo</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>DeePosit: an AI-based tool for detecting mouse urine and fecal depositions from thermal video clips of behavioral experiments</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.14754159</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Prof Lucy Osborne and Prof Giuseppe Testa for providing the <italic>Gtf2i+/Dup</italic> mutant mice line. We also thank Yaniv Goldstein, Janet Tabakova, Wjdan Awaisy, and Shorook Amara for their help in annotating the videos and Sara Sheikh for drawing the experiment setup illustration. This study was supported by ISF-NSFC joint research program (Grant No. 3459/20), the Israel Science Foundation (Grants No. 1361/17 and 2220/22), the Ministry of Science, Technology and Space of Israel (Grant No. 3-12068), the Ministry of Health of Israel (Grant #3-18380 for EPINEURODEVO), the German Research Foundation (DFG) (GR 3619/16-1 and SH 752/2-1), the Congressionally Directed Medical Research Programs (CDMRP) (Grant No. AR210005) and the United States-Israel Binational Science Foundation (Grant No. 2019186).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arakawa</surname><given-names>H</given-names></name><name><surname>Blanchard</surname><given-names>DC</given-names></name><name><surname>Arakawa</surname><given-names>K</given-names></name><name><surname>Dunlap</surname><given-names>C</given-names></name><name><surname>Blanchard</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Scent marking behavior as an odorant communication in mice</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>32</volume><fpage>1236</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2008.05.012</pub-id><pub-id pub-id-type="pmid">18565582</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bigiani</surname><given-names>A</given-names></name><name><surname>Mucignat-Caretta</surname><given-names>C</given-names></name><name><surname>Montani</surname><given-names>G</given-names></name><name><surname>Tirindelli</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Pheromone Reception Inmammals Berlin</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/s10254-004-0038-0</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The nose knows who’s who: chemosensory individuality and mate recognition in mice</article-title><source>Hormones and Behavior</source><volume>46</volume><fpage>231</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.yhbeh.2004.01.010</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>PA</given-names></name><name><surname>Kendrick</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mammalian social odours: attraction and individual recognition</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>361</volume><fpage>2061</fpage><lpage>2078</lpage><pub-id pub-id-type="doi">10.1098/rstb.2006.1931</pub-id><pub-id pub-id-type="pmid">17118924</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Carion</surname><given-names>N</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Synnaeve</surname><given-names>G</given-names></name><name><surname>Usunier</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>End-to-end object detection with transformers</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>213</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-58452-8_13</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cartucho</surname><given-names>J</given-names></name><name><surname>Ventura</surname><given-names>R</given-names></name><name><surname>Veloso</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Robust object recognition through symbiotic deep learning in mobile robots</article-title><conf-name>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</conf-name><conf-loc>Madrid</conf-loc><elocation-id>ages</elocation-id><pub-id pub-id-type="doi">10.1109/IROS.2018.8594067</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalghi</surname><given-names>MG</given-names></name><name><surname>Montalbetti</surname><given-names>N</given-names></name><name><surname>Wheeler</surname><given-names>TB</given-names></name><name><surname>Apodaca</surname><given-names>G</given-names></name><name><surname>Carattino</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Real-time void spot assay</article-title><source>Journal of Visualized Experiments</source><volume>01</volume><elocation-id>e64621</elocation-id><pub-id pub-id-type="doi">10.3791/64621</pub-id><pub-id pub-id-type="pmid">36847378</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desjardins</surname><given-names>C</given-names></name><name><surname>Maruniak</surname><given-names>JA</given-names></name><name><surname>Bronson</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Social rank in house mice: differentiation revealed by ultraviolet visualization of urinary marking patterns</article-title><source>Science</source><volume>182</volume><fpage>939</fpage><lpage>941</lpage><pub-id pub-id-type="doi">10.1126/science.182.4115.939</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higuchi</surname><given-names>Y</given-names></name><name><surname>Arakawa</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Contrasting central and systemic effects of arginine-vasopressin on urinary marking behavior as a social signal in male mice</article-title><source>Hormones and Behavior</source><volume>141</volume><elocation-id>105128</elocation-id><pub-id pub-id-type="doi">10.1016/j.yhbeh.2022.105128</pub-id><pub-id pub-id-type="pmid">35180615</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurst</surname><given-names>JL</given-names></name><name><surname>Beynon</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Scent wars: the chemobiology of competitive signalling in mice</article-title><source>BioEssays</source><volume>26</volume><fpage>1288</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1002/bies.20147</pub-id><pub-id pub-id-type="pmid">15551272</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyun</surname><given-names>M</given-names></name><name><surname>Taranda</surname><given-names>J</given-names></name><name><surname>Radeljic</surname><given-names>G</given-names></name><name><surname>Miner</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Ochandarena</surname><given-names>N</given-names></name><name><surname>Huang</surname><given-names>KW</given-names></name><name><surname>Osten</surname><given-names>P</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Social isolation uncovers a circuit underlying context-dependent territory-covering micturition</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2018078118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2018078118</pub-id><pub-id pub-id-type="pmid">33443190</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jocher</surname><given-names>G</given-names></name><name><surname>Chaurasia</surname><given-names>A</given-names></name><name><surname>Qiu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Ultralytics yolov8</data-title><version designator="8.3.159">8.3.159</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keil</surname><given-names>KP</given-names></name><name><surname>Abler</surname><given-names>LL</given-names></name><name><surname>Altmann</surname><given-names>HM</given-names></name><name><surname>Bushman</surname><given-names>W</given-names></name><name><surname>Marker</surname><given-names>PC</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Ricke</surname><given-names>WA</given-names></name><name><surname>Bjorling</surname><given-names>DE</given-names></name><name><surname>Vezina</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Influence of animal husbandry practices on void spot assay outcomes in C57BL/6J male mice</article-title><source>Neurourology and Urodynamics</source><volume>35</volume><fpage>192</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1002/nau.22692</pub-id><pub-id pub-id-type="pmid">25394276</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>JA</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>EH-J</given-names></name><name><surname>Lilascharoen</surname><given-names>V</given-names></name><name><surname>George</surname><given-names>O</given-names></name><name><surname>Lim</surname><given-names>BK</given-names></name><name><surname>Stowers</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Voluntary urination control by brainstem neurons that relax the urethral sphincter</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1229</fpage><lpage>1238</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0204-3</pub-id><pub-id pub-id-type="pmid">30104734</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopachev</surname><given-names>N</given-names></name><name><surname>Netser</surname><given-names>S</given-names></name><name><surname>Wagner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Sex-dependent features of social behavior differ between distinct laboratory mouse strains and their mixed offspring</article-title><source>iScience</source><volume>25</volume><elocation-id>103735</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2022.103735</pub-id><pub-id pub-id-type="pmid">35098101</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mervis</surname><given-names>CB</given-names></name><name><surname>Dida</surname><given-names>J</given-names></name><name><surname>Lam</surname><given-names>E</given-names></name><name><surname>Crawford-Zelli</surname><given-names>NA</given-names></name><name><surname>Young</surname><given-names>EJ</given-names></name><name><surname>Henderson</surname><given-names>DR</given-names></name><name><surname>Onay</surname><given-names>T</given-names></name><name><surname>Morris</surname><given-names>CA</given-names></name><name><surname>Woodruff-Borden</surname><given-names>J</given-names></name><name><surname>Yeomans</surname><given-names>J</given-names></name><name><surname>Osborne</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Duplication of GTF2I results in separation anxiety in mice and humans</article-title><source>American Journal of Human Genetics</source><volume>90</volume><fpage>1064</fpage><lpage>1070</lpage><pub-id pub-id-type="doi">10.1016/j.ajhg.2012.04.012</pub-id><pub-id pub-id-type="pmid">22578324</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CH</given-names></name><name><surname>Haxhillari</surname><given-names>K</given-names></name><name><surname>Hillock</surname><given-names>MF</given-names></name><name><surname>Reichard</surname><given-names>TM</given-names></name><name><surname>Sheehan</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2023">2023a</year><article-title>Scent mark signal investment predicts fight dynamics in house mice</article-title><source>Proceedings. Biological Sciences</source><volume>290</volume><elocation-id>20222489</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2022.2489</pub-id><pub-id pub-id-type="pmid">36787797</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CH</given-names></name><name><surname>Hillock</surname><given-names>MF</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Carlson-Clarke</surname><given-names>B</given-names></name><name><surname>Haxhillari</surname><given-names>K</given-names></name><name><surname>Lee</surname><given-names>AY</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Sheehan</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2023">2023b</year><article-title>Dynamic changes to signal allocation rules in response to variable social environments in house mice</article-title><source>Communications Biology</source><volume>6</volume><elocation-id>297</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-023-04672-x</pub-id><pub-id pub-id-type="pmid">36941412</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohapatra</surname><given-names>AN</given-names></name><name><surname>Peles</surname><given-names>D</given-names></name><name><surname>Netser</surname><given-names>S</given-names></name><name><surname>Wagner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Synchronized LFP rhythmicity in the social brain reflects the context of social encounters</article-title><source>Communications Biology</source><volume>7</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-023-05728-8</pub-id><pub-id pub-id-type="pmid">38168971</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Netser</surname><given-names>S</given-names></name><name><surname>Haskal</surname><given-names>S</given-names></name><name><surname>Magalnik</surname><given-names>H</given-names></name><name><surname>Wagner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A novel system for tracking social preference dynamics in mice reveals sex- and strain-specific characteristics</article-title><source>Molecular Autism</source><volume>8</volume><elocation-id>53</elocation-id><pub-id pub-id-type="doi">10.1186/s13229-017-0169-1</pub-id><pub-id pub-id-type="pmid">29026510</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Netser</surname><given-names>S</given-names></name><name><surname>Haskal</surname><given-names>S</given-names></name><name><surname>Magalnik</surname><given-names>H</given-names></name><name><surname>Bizer</surname><given-names>A</given-names></name><name><surname>Wagner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A system for tracking the dynamics of social preference behavior in small rodents</article-title><source>Journal of Visualized Experiments</source><volume>01</volume><elocation-id>e60336</elocation-id><pub-id pub-id-type="doi">10.3791/60336-v</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Netser</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>TrackRodent</data-title><version designator="7d91b09">7d91b09</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/shainetser/TrackRodent">https://github.com/shainetser/TrackRodent</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Peles</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>DeePosit</data-title><version designator="swh:1:rev:be73ff51549f95ad7cc4b67dfc497ba03650a8d2">swh:1:rev:be73ff51549f95ad7cc4b67dfc497ba03650a8d2</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2f68a84e482cac6aa72dde94e47fa3a07615faf0;origin=https://github.com/davidpl2/DeePosit;visit=swh:1:snp:0ecb1949aed9d5608e1fdc2669a89188e9f226a2;anchor=swh:1:rev:be73ff51549f95ad7cc4b67dfc497ba03650a8d2">https://archive.softwareheritage.org/swh:1:dir:2f68a84e482cac6aa72dde94e47fa3a07615faf0;origin=https://github.com/davidpl2/DeePosit;visit=swh:1:snp:0ecb1949aed9d5608e1fdc2669a89188e9f226a2;anchor=swh:1:rev:be73ff51549f95ad7cc4b67dfc497ba03650a8d2</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verstegen</surname><given-names>AM</given-names></name><name><surname>Tish</surname><given-names>MM</given-names></name><name><surname>Szczepanik</surname><given-names>LP</given-names></name><name><surname>Zeidel</surname><given-names>ML</given-names></name><name><surname>Geerling</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Micturition video thermography in awake, behaving mice</article-title><source>Journal of Neuroscience Methods</source><volume>331</volume><elocation-id>108449</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108449</pub-id><pub-id pub-id-type="pmid">31812917</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wegner</surname><given-names>KA</given-names></name><name><surname>Abler</surname><given-names>LL</given-names></name><name><surname>Oakes</surname><given-names>SR</given-names></name><name><surname>Mehta</surname><given-names>GS</given-names></name><name><surname>Ritter</surname><given-names>KE</given-names></name><name><surname>Hill</surname><given-names>WG</given-names></name><name><surname>Zwaans</surname><given-names>BM</given-names></name><name><surname>Lamb</surname><given-names>LE</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bjorling</surname><given-names>DE</given-names></name><name><surname>Ricke</surname><given-names>WA</given-names></name><name><surname>Macoska</surname><given-names>J</given-names></name><name><surname>Marker</surname><given-names>PC</given-names></name><name><surname>Southard-Smith</surname><given-names>EM</given-names></name><name><surname>Eliceiri</surname><given-names>KW</given-names></name><name><surname>Vezina</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Void spot assay procedural optimization and software for rapid and objective quantification of rodent voiding function, including overlapping urine spots</article-title><source>American Journal of Physiology. Renal Physiology</source><volume>315</volume><fpage>F1067</fpage><lpage>F1080</lpage><pub-id pub-id-type="doi">10.1152/ajprenal.00245.2018</pub-id><pub-id pub-id-type="pmid">29972322</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wöhr</surname><given-names>M</given-names></name><name><surname>Roullet</surname><given-names>FI</given-names></name><name><surname>Hung</surname><given-names>AY</given-names></name><name><surname>Sheng</surname><given-names>M</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Communication impairments in mice lacking Shank1: reduced levels of ultrasonic vocalizations and scent marking behavior</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e20631</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0020631</pub-id><pub-id pub-id-type="pmid">21695253</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>PR</given-names></name><name><surname>Powell</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Urine patterns in mice: An analysis of male/female counter-marking</article-title><source>Animal Behaviour</source><volume>32</volume><fpage>1185</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/S0003-3472(84)80235-3</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><p>Code for computing Two Way Chi-Square Test which was used to compare the distribution of active mice (with at least one detection) in males vs females.</p><p><code>% Compute two way chi square test for 2x2 table
%
% Hypothseis H0: there is no relation between gender and the distribution of zeros .
% Hypothseis H1: there is a relation between gender and the distribution of zeros .
%
% Inputs :
% valsMales - a vector of length 2 that contains : [zeros count , non zeros count] for males .
% valsFemales - a vector of length 2 that contains : [zeros count , non zeros count] for females.
%
% Outputs :
% pVal - p value . A value lower than 0.05 suggests that the hypothesis H0 should be rejected .
% chiStat - statistic of the chi square test .
% df - degree of freedom (equals 1 for 2x2 tables).
%
function [pVal , chiStat ,df] = TwoWayChiSqrTest (valsMales , valsFemales)
if length (valsMales)~=2 || length (valsFemales)~=2
‍‎error (’input vectors should have length =2 ’)
end
sumMales = sum (valsMales);
sumFemales = sum (valsFemales);
sumAll = sumMales + sumFemales ;
sum1 = valsMales (1) + valsFemales (1) ;
sum2 = valsMales (2) + valsFemales (2) ;
expectedFreqMales = [sumMales *(sum1 / sumAll), sumMales *(sum2 / sumAll)];
expectedFreqFemales = [sumFemales *(sum1 / sumAll), sumFemales *(sum2 / sumAll)];
chiStatMales = sum ((valsMales - expectedFreqMales).^2 ./ expectedFreqMales);
chiStatFemales = sum ((valsFemales - expectedFreqFemales).^2 ./ expectedFreqFemales);
chiStat = chiStatMales + chiStatFemales ;
df = 1;
pVal = 1- chi2cdf (chiStat ,df);</code></p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>The effect of the test on the urine and feces area.</title><p>Kruskal–Wallis test was used to check if the test type (SP, SxP, and ESPs) affects the area of urine or feces. p-value equal to or smaller than 0.1, 0.01, 0.001 was marked with #, **, ***, respectively. For male mice, <italic>n</italic> = 24, 28, 21 for SP, SxP, and ESPs. For female mice, <italic>n</italic> = 15, 16, 17 accordingly.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Measurement</th><th align="left" valign="bottom">Habituation, minutes 1–4</th><th align="left" valign="bottom">Habituation, minutes 11–14</th><th align="left" valign="bottom">Trial, minute 1</th><th align="left" valign="bottom">Trial, minutes 2–4</th></tr></thead><tbody><tr><td align="left" valign="bottom">Male urine area</td><td align="left" valign="bottom">0.0003***</td><td align="left" valign="bottom">0.3436</td><td align="left" valign="bottom">0.0011**</td><td align="left" valign="bottom">0.0614#</td></tr><tr><td align="left" valign="bottom">Female urine area</td><td align="left" valign="bottom">0.3847</td><td align="left" valign="bottom">0.374</td><td align="left" valign="bottom">0.399</td><td align="left" valign="bottom">0.3124</td></tr><tr><td align="left" valign="bottom">Male feces area</td><td align="left" valign="bottom">0.0098**</td><td align="left" valign="bottom">0.3315</td><td align="left" valign="bottom">0.2738</td><td align="left" valign="bottom">0.8938</td></tr><tr><td align="left" valign="bottom">Female feces area</td><td align="left" valign="bottom">0.2352</td><td align="left" valign="bottom">0.5138</td><td align="left" valign="bottom">0.1553</td><td align="left" valign="bottom">0.571</td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100739.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This manuscript presents an <bold>important</bold> machine-learning-based approach to the automated detection of urine and fecal deposits by rodents, key ethological behaviors that have traditionally been very poorly studied. The strength of evidence for the claim is <bold>solid</bold>, showing accuracy near 90% across several contexts. Training and testing for the specific contexts used by other experimenters, however, is probably warranted to make the model most relevant to the data that may be analyzed.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100739.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The manuscript provides a novel method for the automated detection of scent marks from urine and feces in rodents. Given the importance of scent communication in these animals and their role as model organisms, this is a welcome tool.</p><p>Strengths:</p><p>The method uses a single video stream to allow for the distinction between urine and feces. It is automated.</p><p>Weaknesses:</p><p>The accuracy is decent but not perfect and may be too low to detect some effects that are biologically real but subtle (e.g. less than 10% differences). For many assays, however, this tools will be useful.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100739.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peles</surname><given-names>David</given-names></name><role specific-use="author">Author</role><aff><institution>University of Haifa</institution><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Netser</surname><given-names>Shai</given-names></name><role specific-use="author">Author</role><aff><institution>University of Haifa</institution><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Ray</surname><given-names>Natalie</given-names></name><role specific-use="author">Author</role><aff><institution>University of Haifa</institution><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Suliman</surname><given-names>Taghreed</given-names></name><role specific-use="author">Author</role><aff><institution>University of Haifa</institution><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Wagner</surname><given-names>Shlomo</given-names></name><role specific-use="author">Author</role><aff><institution>University of Haifa</institution><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><p>We thank the reviewers for their constructive and helpful comments, which led us to make major changes in the model and manuscript, including adding the results of new experiments and analyses. We believe that the revised manuscript is much better than the previous version and that it addresses all issued raised by the reviewers.</p><p>Summary of changes made in the revised manuscript:</p><p>(1) We increased the training set size from 39 video clips to 97 video clips and the testing set size from 25 video clips to 60 video clips. The increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p><p>(2) We further evaluated the accuracy of the DeePosit algorithm in comparison to a second human annotator and found that the algorithm accuracy is comparable to human-level accuracy.</p><p>(3) The additional test videos allowed us to test the consistency of the algorithm performance across gender, space, time, and experiment type (SP, SxP, and ESPs). We found consistent levels of performance across all categories (see Figure 3), suggesting that errors made by the algorithm are uniform across conditions, hence should not create any bias of the results.</p><p>(4) In addition, we tested the algorithm performance on a second strain of mice (male C57BL/6) in a different environmental condition (white arena instead of a black one) and found that the algorithm achieves comparable accuracy, even though C57BL/6 mice and white arena were not included in the training set. Thus, the algorithm seems to be robust and efficient across various experimental conditions.</p><p>(5) Analyzing urination and defecation dynamics in an additional strain of mice revealed interesting strain-specific features, as discussed in the revised manuscript.</p><p>(6) Overall, we found DeePosit accuracy to be stable with no significant bias across stages of the experiment, types of the experiment, gender of the mice, strain of mice, and across experimental conditions.</p><p>(7) We also compared the performance of DeePosit to a classic object detection algorithm: YOLOv8. We trained YOLOv8 both on a single image input (YOLOv8 Gray) and on 3 image inputs representing a sequence of three time points around the ground truth event (t): t+0, t+10, and t+30 seconds (YOLOv8 RGB). DeePosit achieved significantly better accuracy over both YOLOv8 alternatives. YOLOv8 RGB achieved better accuracy than YOLOv8 Gray, suggesting that temporal information is important for this task. It's worth mentioning that while YOLOv8 requires the annotator to draw rectangles surrounding each urine spot or feces as part of the training set, our algorithm training set used just a single click inside each spot, allowing faster generation of training sets.</p><p>(8) As for the algorithm parameters, we tested the effect of the main parameter of the preliminary detection (the temperature threshold for the detection of a new blob) and found that a threshold of 1.6°C gave the best accuracy and used this parameter for all of the experiments instead of 1.1°C which was used in the original manuscript. It's worth mentioning that the performance is quite stable (mean F1 score of 0.88-0.89) for the thresholds between 1.1°C and 3°C (Figure 3—Figure Supplement 2).</p><p>(9) We also checked if changing the input length of the video clip that is fed to the classifier affects the accuracy by training the classifier with -11.30 seconds video clips (41 seconds in total) instead of -11.60 seconds (71 seconds in total) and found no difference in accuracy.</p><p>(10) In the revised paper, we report recall, precision, and F1 scores in the caption of the relevant figures and also supply Excel files with the full statistics for each of the figures.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The manuscript provides a novel method for the automated detection of scent marks from urine and feces in rodents. Given the importance of scent communication in these animals and their role as model organisms, this is a welcome tool.</p></disp-quote><p>We thank the reviewer for the positive assessment of our tool</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>The method uses a single video stream (thermal video) to allow for the distinction between urine and feces. It is automated.</p><p>Weaknesses:</p><p>The accuracy level shown is lower than may be practically useful for many studies. The accuracy of urine is 80%.</p></disp-quote><p>We have trained the model better, using a larger number of video clips. The increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p><disp-quote content-type="editor-comment"><p>This is understandable given the variability of urine in its deposition, but makes it challenging to know if the data is accurate. If the same kinds of mistakes are maintained across many conditions it may be reasonable to use the software (i.e., if everyone is under/over counted to the same extent). Differences in deposition on the scale of 20% would be challenging to be confident in with the current method, though differences of the magnitude may be of biological interest. Understanding how well the data maintain the same relative ranking of individuals across various timing and spatial deposition metrics may help provide further evidence for the utility of the method.</p></disp-quote><p>The additional test videos allowed us to test the consistency of the algorithm performance across gender, space, time and experiment type (SP, SxP, and ESP). We found consistent levels of performance across all categories (see Figure 3), suggesting that errors made by the algorithm are uniform across conditions, hence should not create any bias of the results.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors built a tool to extract the timing and location of mouse urine and fecal deposits in their laboratory set up. They indicate that they are happy with the results they achieved in this effort.</p></disp-quote><p>Yes, we are.</p><disp-quote content-type="editor-comment"><p>The authors note urine is thought to be an important piece of an animal's behavioral repertoire and communication toolkit so methods that make studying these dynamics easier would be impactful.</p></disp-quote><p>We thank the reviewer for the positive assessment of our work.</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>With the proposed method, the authors are able to detect 79% of the urine that is present and 84% of the feces that is present in a mostly automated way.</p><p>Weaknesses:</p><p>The method proposed has a large number of design choices across two detection steps that aren't investigated. I.e. do other design choices make the performance better, worse, or the same?</p></disp-quote><p>We chose to use a heuristic preliminary detection algorithm for the detection of warm blobs, since warm blobs can be robustly detected with heuristic algorithms without the need for a training set. This design selection might allow easier adaptation of our algorithm for different types of arenas. Another advantage of using a heuristic preliminary detection is the easy control of the preliminary detection parameters such as the minimum temperature difference for detecting a blob, size limits of the detected blob, cooldown rate and so on that may help in adopting it to new conditions. As for the classifier, we chose to feed it with a relatively small window surrounding each preliminary detection, and hence it is not affected by the arena’s appearance outside of its region of interest. This should allow lower sensitivity to the arena’s appearance.</p><p>As for the algorithm parameters, we tested the effect of the main parameter of the preliminary detection (the temperature threshold for the detection of a new blob) and found that a threshold of 1.6°C gave the best accuracy and used this parameter for all of the experiments instead of 1.1°C which was used in the original manuscript. It's worth mentioning that the performance is quite stable (mean F1 score of 0.88-0.89) for the thresholds between 1.1°C and 3°.</p><p>We also checked if changing the input length of the video clip fed to the classifier affects the accuracy by training the classifier with -11.30 seconds video clips (41 seconds in total) instead of -11.60 seconds (71 seconds in total) and found no difference in accuracy.</p><p>Overall, the algorithm's accuracy seems to be rather stable across various choices of parameters.</p><disp-quote content-type="editor-comment"><p>Are these choices robust across a range of laboratory environments?</p></disp-quote><p>We tested the algorithm performance on a second strain of mice (male C57BL/6) in a different environmental condition (white arena instead of a black one) and found that the algorithm achieves comparable accuracy, even though C57BL/6 mice and white arena were not included in the training set. Thus, the algorithm seems to be robust and efficient across various experimental conditions.</p><disp-quote content-type="editor-comment"><p>How much better are the demonstrated results compared to a simple object detection pipeline (i.e. FasterRCNN or YOLO on the raw heat images)?</p></disp-quote><p>We compared the performance of DeePosit to a classic object detection algorithm: YOLOv8. We trained YOLOv8 both on a single image input (YOLOv8 Gray) and on 3 image inputs representing a sequence of three time points around the ground truth event (t): t+0, t+10, and t+30 seconds (YOLOv8 RGB). DeePosit achieved significantly better accuracy over both YOLOv8 alternatives. YOLOv8 RGB achieved better accuracy than YOLOv8 Gray, suggesting that temporal information is important for this task. It's worth mentioning that while YOLOv8 requires annotator to draw rectangles surrounding each urine spot or feces as part of the training set, our algorithm training set used just a single click inside each spot, allowing faster generation of a training sets.</p><disp-quote content-type="editor-comment"><p>The method is implemented with a mix of MATLAB and Python.</p></disp-quote><p>That is right.</p><disp-quote content-type="editor-comment"><p>One proposed reason why this method is better than a human annotator is that it &quot;is not biased.&quot; While they may mean it isn't influenced by what the researcher wants to see, the model they present is still statistically biased since each object class has a different recall score. This wasn't investigated. In general, there was little discussion of the quality of the model.</p></disp-quote><p>We tested the consistency of the algorithm performance across gender, space, time and experiment type (SP, SxP, and ESP). We found consistent levels of performance across all categories (see Figure 3), suggesting that errors made by the algorithm are uniform across conditions, hence should ne create any bias of the results. Specifically, the detection accuracy is similar between urine and feces, hence should not impose a bias between the various object classes.</p><disp-quote content-type="editor-comment"><p>Precision scores were not reported.</p></disp-quote><p>In the revised paper we report recall, precision, and F1 scores in the caption of the relevant figures and also supply Excel files with the full statistics for each of the figures.</p><disp-quote content-type="editor-comment"><p>Is a recall value of 78.6% good for the types of studies they and others want to carry out? What are the implications of using the resulting data in a study?</p></disp-quote><p>We have trained the model better, using a larger number of video clips. The increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p><disp-quote content-type="editor-comment"><p>How do these results compare to the data that would be generated by a &quot;biased human?&quot;</p></disp-quote><p>We further evaluated the accuracy of the DeePosit algorithm in comparison to a second human annotator and found that the algorithm accuracy is comparable to human-level accuracy (Figure 3).</p><disp-quote content-type="editor-comment"><p>5 out of the 6 figures in the paper relate not to the method but to results from a study whose data was generated from the method. This makes a paper, which, based on the title, is about the method, much longer and more complicated than if it focused on the method.</p></disp-quote><p>We appreciate the reviewer's comment, but the analysis of this new dataset by DeePosit demonstrates how the algorithm may be used to reveal novel and distinguishable dynamics of urination and defecation activities during social interactions, which were not yet reported.</p><disp-quote content-type="editor-comment"><p>Also, even in the context of the experiments, there is no discussion of the implications of analyzing data that was generated from a method with precision and recall values of only 7080%. Surely this noise has an effect on how to correctly calculate p-values etc. Instead, the authors seem to proceed like the generated data is simply correct.</p></disp-quote><p>As mentioned above, the increase in training set size improved the overall accuracy from a mean F1 score of 0.81 in the previous version to a mean F1 score of 0.891 (see Figure 2 and Figure 3) in the current version. Specifically, the F1 score for urine detection was improved from 0.79 to 0.88.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>The authors introduce a tool that employs thermal cameras to automatically detect urine and feces deposits in rodents. The detection process involves a heuristic to identify potential thermal regions of interest, followed by a transformer network-based classifier to differentiate between urine, feces, and background noise. The tool's effectiveness is demonstrated through experiments analyzing social preference, stress response, and temporal dynamics of deposits, revealing differences between male and female mice.</p><p>Strengths:</p><p>The method effectively automates the identification of deposits</p><p>The application of the tool in various behavioral tests demonstrates its robustness and versatility.</p><p>The results highlight notable differences in behavior between male and female mice</p></disp-quote><p>We thank the reviewer for the positive assessment of our work.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The definition of 'start' and 'end' periods for statistical analysis is arbitrary. A robustness check with varying time windows would strengthen the conclusions.</p></disp-quote><p>In all the statistical tests conducted in the revised manuscript, we have used a time period of 4 minutes for the analysis. We did not used the last minute of each stage for the analysis since the input of DeePosit requires 1 minute of video after the event. Nevertheless, we also conducted the same tests using a 5-minute period and found similar results (Figure 5—Figure Supplement 1).</p><disp-quote content-type="editor-comment"><p>The paper could better address the generalizability of the tool to different experimental setups, environments, and potentially other species.</p></disp-quote><p>As mentioned above, we tested the algorithm performance on a second strain of mice (male C57BL/6) in a different environmental condition (white arena instead of a black one) and found that the algorithm achieves comparable accuracy, even though C57BL/6 mice and white arena were not included in the training set. Thus, the algorithm seems to be robust and efficient across various experimental conditions.</p><disp-quote content-type="editor-comment"><p>The results are based on tests of individual animals, and there is no discussion of how this method could be generalized to experiments tracking multiple animals simultaneously in the same arena (e.g., pair or collective behavior tests, where multiple animals may deposit urine or feces).</p></disp-quote><p>At the moment, the algorithm cannot be applied for multiple animals freely moving in the same arena. However, in the revised manuscript we explicitly discussed what is needed for adapting the algorithm to perform such analyses.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p>- Add a note and/or perform additional calculations to show that the results do not depend on the specific definitions of 'start' and 'end' periods. For instance, vary the time window thresholds and recalculate the statistics using different windows (e.g., 1-5 minutes instead of 1-4 minutes).</p></disp-quote><p>In all the statistical tests conducted in the revised manuscript, we have used a time period of 4 minutes for the analysis. We did not use the last minute of each stage for the analysis since the input of DeePosit requires 1 minute of video after the event. Nevertheless, we also conducted the same tests using a 5-minute period and found similar results (Figure 5—Figure Supplement 1).</p><disp-quote content-type="editor-comment"><p>- Condense Figures 4, 5, and 6 to simplify the presentation. Focus on demonstrating the effectiveness of the tool rather than detailed experimental outcomes, as the primary contribution of this paper is methodological.</p></disp-quote><p>We have added to the revised manuscript one technical figure (Figure 3) comparing the accuracy of the algorithm performance across gender, space, time, and experiment type (SP, SxP, and ESP) as well as comparing its performance to a second human annotator and to YOLOv8. One more partially technical figure (Figure 5) compares the results of the algorithm between white ICR mice in the black arena and black C57BL/6 mice in the white arena. Thus, only Figures 4 and 6 show detailed experimental outcomes.</p><disp-quote content-type="editor-comment"><p>- Provide more detail on how the preliminary detection procedure and parameters might need adjustment for different experimental setups or conditions. Discuss potential adaptations for field settings or more complex environments.</p></disp-quote><p>As for the algorithm parameters, we tested the effect of the main parameter of the preliminary detection (the temperature threshold for the detection of a new blob) and found that a threshold of 1.6°C gave the best accuracy and used this parameter for all of the experiments instead of 1.1°C which was used in the original manuscript. It's worth mentioning that the performance is quite stable (mean F1 score of 0.88-0.89) for the thresholds between 1.1°C and 3°.</p><p>We also checked if changing the input length of the video clip that is fed to the classifier affects the accuracy by training the classifier with -11.30 seconds video clips (41 seconds in total) instead of -11.60 seconds (71 seconds in total) and found no difference in accuracy.</p><p>Overall, the algorithm's accuracy seems to be rather stable across various choices of parameters.</p><disp-quote content-type="editor-comment"><p>Editor's note:</p><p>Should you choose to revise your manuscript, please ensure your manuscript includes full statistical reporting including exact p-values wherever possible alongside the summary statistics (test statistic and df) and 95% confidence intervals. These should be reported for all key questions and not only when the p-value is less than 0.05 in the main manuscript.</p></disp-quote><p>We have deposited the detailed statistics of each figure in https://github.com/davidpl2/DeePosit/tree/main/FigStat/PostRevision</p></body></sub-article></article>