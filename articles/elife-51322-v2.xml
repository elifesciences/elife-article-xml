<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">51322</article-id><article-id pub-id-type="doi">10.7554/eLife.51322</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Wireless recording from unrestrained monkeys reveals motor goal encoding beyond immediate reach in frontoparietal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-112735"><name><surname>Berger</surname><given-names>Michael</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7239-1675</contrib-id><email>mberger@dpz.eu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-156368"><name><surname>Agha</surname><given-names>Naubahar Shahryar</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-138485"><name><surname>Gail</surname><given-names>Alexander</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1165-4646</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Cognitive Neuroscience Laboratory, German Primate Center – Leibniz-Institute for Primate Research</institution><addr-line><named-content content-type="city">Goettingen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Faculty of Biology and Psychology, University of Goettingen</institution><addr-line><named-content content-type="city">Goettingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Leibniz-ScienceCampus Primate Cognition</institution><addr-line><named-content content-type="city">Goettingen</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution>Bernstein Center for Computational Neuroscience</institution><addr-line><named-content content-type="city">Goettingen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Pesaran</surname><given-names>Bijan</given-names></name><role>Reviewing Editor</role><aff><institution>Center for Neural Science, New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>04</day><month>05</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e51322</elocation-id><history><date date-type="received" iso-8601-date="2019-08-28"><day>28</day><month>08</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-05-02"><day>02</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Berger et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Berger et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-51322-v2.pdf"/><abstract><p>System neuroscience of motor cognition regarding the space beyond immediate reach mandates free, yet experimentally controlled movements. We present an experimental environment (Reach Cage) and a versatile visuo-haptic interaction system (<italic>MaCaQuE</italic>) for investigating goal-directed whole-body movements of unrestrained monkeys. Two rhesus monkeys conducted instructed walk-and-reach movements towards targets flexibly positioned in the cage. We tracked 3D multi-joint arm and head movements using markerless motion capture. Movements show small trial-to-trial variability despite being unrestrained. We wirelessly recorded 192 broad-band neural signals from three cortical sensorimotor areas simultaneously. Single unit activity is selective for different reach and walk-and-reach movements. Walk-and-reach targets could be decoded from premotor and parietal but not motor cortical activity during movement planning. The Reach Cage allows systems-level sensorimotor neuroscience studies with full-body movements in a configurable 3D spatial setting with unrestrained monkeys. We conclude that the primate frontoparietal network encodes reach goals beyond immediate reach during movement planning.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>arm movements</kwd><kwd>wireless neurophysiology</kwd><kwd>motion capture</kwd><kwd>motor cortex</kwd><kwd>premotor cortex</kwd><kwd>parietal cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>DFG RU-1847-C1</award-id><principal-award-recipient><name><surname>Gail</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>EC-H2020-FETPROACT-16 732266 WP1</award-id><principal-award-recipient><name><surname>Gail</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The novel Reach Cage allows neurophysiology studies of structured behavior with unrestrained Rhesus macaques showing that the frontoparietal reach network is selective for reach goals outside the immediately reachable space.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Cognitive sensorimotor neuroscience investigates how the brain processes sensory information, develops an action plan, and ultimately performs a corresponding action. Experimental setups with non-human primates typically make use of physical restraint, such as a primate chair, to control for spatial parameters like head position, gaze direction, and body and arm posture. This approach led to numerous important insights into neural correlates of visually guided hand and arm movements. The frontoparietal reach network, in particular, including the posterior parietal cortex, premotor cortex, and motor cortex, has been studied in terms of force encoding (<xref ref-type="bibr" rid="bib24">Cheney and Fetz, 1980</xref>), direction encoding (<xref ref-type="bibr" rid="bib40">Georgopoulos et al., 1986</xref>), spatial reference frames of reach goal encoding (<xref ref-type="bibr" rid="bib5">Batista et al., 1999</xref>; <xref ref-type="bibr" rid="bib17">Buneo et al., 2002</xref>; <xref ref-type="bibr" rid="bib58">Kuang et al., 2016</xref>; <xref ref-type="bibr" rid="bib83">Pesaran et al., 2006</xref>), context integration (<xref ref-type="bibr" rid="bib39">Gail and Andersen, 2006</xref>; <xref ref-type="bibr" rid="bib65">Martínez-Vázquez and Gail, 2018</xref>; <xref ref-type="bibr" rid="bib78">Niebergall et al., 2011</xref>; <xref ref-type="bibr" rid="bib104">Westendorff et al., 2010</xref>), obstacle avoidance (<xref ref-type="bibr" rid="bib56">Kaufman et al., 2013</xref>; <xref ref-type="bibr" rid="bib73">Mulliken et al., 2008</xref>), bimanual coordination (<xref ref-type="bibr" rid="bib35">Donchin et al., 1998</xref>; <xref ref-type="bibr" rid="bib69">Mooshagian et al., 2018</xref>), eye-hand coordination (<xref ref-type="bibr" rid="bib52">Hwang et al., 2012</xref>; <xref ref-type="bibr" rid="bib70">Mooshagian and Snyder, 2018</xref>; <xref ref-type="bibr" rid="bib90">Sayegh et al., 2017</xref>; <xref ref-type="bibr" rid="bib106">Wong et al., 2016</xref>), and decision making (<xref ref-type="bibr" rid="bib26">Christopoulos et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Cisek, 2012</xref>; <xref ref-type="bibr" rid="bib57">Klaes et al., 2011</xref>; <xref ref-type="bibr" rid="bib97">Suriya-Arunroj and Gail, 2019</xref>). Because of the physical restraint, the scope of previous studies was mostly limited to hand or arm movements, and those were restricted to the immediately reachable space. Well-controlled planning and execution of spatially and temporally structured goal-directed movements in larger workspaces, including reach goals beyond immediate reach, could not be investigated in monkeys.</p><p>Neuropsychological and neurophysiological evidence suggest that frontoparietal areas encode the space near the body differently from the space far from the body (see <xref ref-type="bibr" rid="bib36">Farnè et al., 2016</xref> for review). Visuospatial neglect can be restricted to the near or far space as shown by patients with large-scale lesions comprising also parietal cortex (<xref ref-type="bibr" rid="bib47">Halligan and Marshall, 1991</xref>; <xref ref-type="bibr" rid="bib102">Vuilleumier et al., 1998</xref>) and transcranial magnetic stimulation over the parietal cortex (<xref ref-type="bibr" rid="bib10">Bjoertomt et al., 2002</xref>). Bimodal neurons in premotor cortex and the posterior parietal cortex of non-human primates respond to visual and somatosensory stimulation, with visual receptive fields being congruent with somatosensory receptive fields and thereby covering the space near the body (<xref ref-type="bibr" rid="bib28">Colby and Goldberg, 1999</xref>; <xref ref-type="bibr" rid="bib44">Graziano et al., 1997</xref>; <xref ref-type="bibr" rid="bib86">Rizzolatti et al., 1981</xref>; <xref ref-type="bibr" rid="bib87">Rizzolatti et al., 1997</xref>). In addition, mirror neurons in the ventral premotor cortex can respond differently to an observed reach if the reach goal is within its own reach or not. (<xref ref-type="bibr" rid="bib12">Bonini et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Caggiano et al., 2009</xref>). These findings indicate that encoding of bimodal sensory information and information about observed actions seems to be dependent on one’s own body boundaries. Moreover, those findings suggest that premotor and parietal cortex are affected by this distinction. The frontoparietal network encodes motor goals within immediate reach, but it is unclear if this also holds true for motor goals beyond immediate reach. Because of the physical restraint of conventional setups, it has not been possible to investigate naturalistic goal-directed movements that require the monkey to walk towards targets at variable positions and, thus, to investigate how monkeys plan to acquire a reach goal beyond the immediately reachable space.</p><p>In conventional experiments, tethered connections prohibit recording from freely moving primates, at least in the case of larger species such as macaques. Tethered recordings in freely moving smaller primate species, such as squirrel monkeys (<xref ref-type="bibr" rid="bib61">Ludvig et al., 2004</xref>) or marmosets (<xref ref-type="bibr" rid="bib30">Courellis et al., 2019</xref>; <xref ref-type="bibr" rid="bib79">Nummela et al., 2017</xref>) have been demonstrated. One study also showed tethered recordings in Japanese macaques; however these were in an environment with no obstacles and with low channel count (<xref ref-type="bibr" rid="bib49">Hazama and Tamura, 2019</xref>). Using wireless recording technology in combination with chronically implanted arrays, recent studies achieved recordings of single unit activity in nonhuman primates investigating vocalization (<xref ref-type="bibr" rid="bib46">Hage and Jurgens, 2006</xref>; <xref ref-type="bibr" rid="bib88">Roy and Wang, 2012</xref>), simple uninstructed behavior (<xref ref-type="bibr" rid="bib92">Schwarz et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Talakoub et al., 2019</xref>), treadmill locomotion (<xref ref-type="bibr" rid="bib21">Capogrosso et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Foster et al., 2014</xref>; <xref ref-type="bibr" rid="bib92">Schwarz et al., 2014</xref>; <xref ref-type="bibr" rid="bib108">Yin et al., 2014</xref>), chair-seated translocation (<xref ref-type="bibr" rid="bib85">Rajangam et al., 2016</xref>), sleep (<xref ref-type="bibr" rid="bib108">Yin et al., 2014</xref>; <xref ref-type="bibr" rid="bib110">Zhou et al., 2019</xref>), and simple movements to a food source (<xref ref-type="bibr" rid="bib21">Capogrosso et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Chestek et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Fernandez-Leon et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Hazama and Tamura, 2019</xref>; <xref ref-type="bibr" rid="bib92">Schwarz et al., 2014</xref>; <xref ref-type="bibr" rid="bib94">Shahidi et al., 2019</xref>). An alternative to wireless transmission can be data logging for which the data are stored separately from behavioral data on the headstage (<xref ref-type="bibr" rid="bib109">Zanos et al., 2011</xref>). This has been used in investigations of simple uninstructed behavior and sleep (<xref ref-type="bibr" rid="bib54">Jackson et al., 2006</xref>; <xref ref-type="bibr" rid="bib55">Jackson et al., 2007</xref>; <xref ref-type="bibr" rid="bib107">Xu et al., 2019</xref>). However, none of the experiments with neural recordings in unrestrained monkeys presented an experimental environment that instructs temporally and spatially precise movement behavior (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). To study goal-directed motor planning and spatial encoding of motor goals, we developed the Reach Cage in which we can instruct precise movement start times and multiple distributed movement goals independent from the food source.</p><p>Here, we present an experimental environment, the Reach Cage, which is equipped with a visuo-haptic interaction system (<italic>MaCaQuE</italic>) and allows investigating movement planning and goal-directed movements of unrestrained rhesus monkeys while recording and analyzing in real-time cortical single-unit activity. We trained monkeys to perform controlled memory-guided reach movements with instructed delay to targets within and beyond the immediately reachable space. Using an open source markerless video-based motion capture software (<xref ref-type="bibr" rid="bib66">Mathis et al., 2018</xref>), we measured 3-dimensional head, shoulder, elbow, and wrist trajectories. We used wireless recording technology to extract single unit activity in real-time from three cortical areas (parietal reach region PRR, dorsal premotor cortex PMd, and primary motor cortex M1) at a bandwidth suitable for brain-machine interface (BMI) applications. We show that the Reach Cage is suitable for sensorimotor neuroscience with physically unrestrained rhesus monkeys providing a richer set of motor tasks, including walk-and-reach movements. With the Reach Cage we were able to study motor goal encoding beyond the immediate reach and during ongoing walking movements. We show that PRR and PMd, but not M1, already contain target location information of far-located walk-and-reach targets during the planning period before and during the walk-and-reach movement.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We developed the Reach Cage to expand studies of visual guided reaching movements to larger workspaces and study movements of rhesus monkeys performing structured whole-body movement tasks while being physically unrestrained. We report on quantitative assessment of the animals’ behavior in the Reach Cage, and neuroscientific analysis of walk-and-reach goal encoding. The timing of the monkeys’ reaching behavior can be precisely controlled and measured with the touch and release times of our touch-sensitive cage-mounted targets (1<sup>st</sup> section). Additionally, multi-joint 3-dimensional reach kinematics can be measured directly with the video-based motion capture system (2<sup>nd</sup> section). We will show that high channel count wireless neural recording is possible in the Reach Cage and report on single-unit activity during such structured task performance (3<sup>rd</sup> section). Finally, we demonstrate the suitability of the Reach Cage for studying motor goal encoding beyond the immediate reach and show that premotor and parietal cortical activity contain information about far-located walk-and-reach targets position during movement planning (4<sup>th</sup> section).</p><sec id="s2-1"><title>Real-time control of instructed behavior in physical unrestrained rhesus monkeys in the Reach Cage</title><p>The core element of our newly developed Reach Cage (<xref ref-type="fig" rid="fig1">Figure 1</xref>) is the <italic>Macaque Cage Query Extension</italic> (<italic>MaCaQuE</italic>). Using this interaction device, we were able to train two fully unrestrained rhesus monkeys to conduct spatially and temporally well-structured memory-guided reaches, a behavioral task common to sensorimotor neuroscience in primates. Here we report the technical details of <italic>MaCaQuE</italic> and its use with physically unrestrained rhesus monkeys; however, we also used <italic>MaCaQuE</italic> successfully in a study with human participants (<xref ref-type="bibr" rid="bib7">Berger et al., 2019</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The Reach Cage setup.</title><p>(<bold>A</bold>) Monkey K performing a reach task on the <italic>Macaque Cage Query Extension</italic> (<italic>MaCaQuE</italic>), touching one of the illuminated <italic>MaCaQuE</italic> Cue and Target boxes (<italic>MCTs</italic>) inside the Reach Cage. (<bold>B</bold>) An <italic>MCT</italic> contains a proximity sensor to make the translucent front cover touch-sensitive and four RGB LEDs to color-illuminate it. (<bold>C</bold>) Schematic drawing of <italic>MaCaQuE</italic> showing the electronic components with the microcontroller interfacing between <italic>MCTs</italic> and an external computer for experimental control. (<bold>D</bold>) Sketch of the Reach Cage with 10 <italic>MCTs</italic> inside, two on the floor pointing upwards serving as a starting position for the monkey and two rows of four (near and far) pointing towards the starting position. The far <italic>MCTs</italic> were positioned to the back of the cage such that the animals needed to walk first. An 11<sup>th</sup><italic>MCT</italic> is positioned outside the cage for providing additional visual cues. The universal <italic>MCTs</italic> can be arranged flexibly to serve different purposes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig1-v2.tif"/></fig><p>Both animals learned within a single first session that touching a target presented on a <italic>MaCaQuE Cue and Target box</italic> (<italic>MCT</italic>, <xref ref-type="fig" rid="fig1">Figure 1B</xref>) leads to a liquid reward. The computer-controlled precise timing and dosage of reward (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) meant that we could employ <italic>MaCaQuE</italic> for positive reinforcement training (PRT) to teach both animals a memory-guided target acquisition task with instructed delay (see Materials and methods). Unlike chair-based setups, <italic>MaCaQuE</italic> allows for target placement beyond the immediate reach of the monkeys (<xref ref-type="fig" rid="fig1">Figure 1D</xref><xref ref-type="video" rid="video1">,Video 1</xref>) Monkey K performed the final version of the walk-and-reach task (<xref ref-type="fig" rid="fig2">Figure 2A/B</xref>) with 77% correct trials on average (s.d. 9%, 19 sessions) with up to 412 correct trials per session (mean 208, s.d. 93). The sessions lasted on average 40 min (s.d. 15 min). Monkey L performed the final version of the task with 55% correct trials on average (s.d. 5%, 10 sessions) with up to 326 correct trials per session (mean 219, s.d. 55). Sessions lasted on average 65 min (s.d. 15 min). The majority of errors resulted from premature release of the start buttons prior to the go cue. Trials with properly timed movement initiation were 92% correct in monkey K and 78% correct in monkey L.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-51322-video1.mp4"><label>Video 1.</label><caption><title>Three-dimensional animation of the Reach Cage.</title></caption></media><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Walk-and-reach task.</title><p>(<bold>A</bold>) Timeline of the walk-and-reach task. Yellow <italic>MCTs</italic> indicate illumination. Only near targets are shown to illustrate this example trial. The second left-most near target was indicated as target and had to be reached after an instructed memory period in response to the go cue (isoluminant color change on the <italic>MCT</italic> outside the cage). (<bold>B</bold>) An example trial to a far target for monkey K (left) and monkey L (right). The frames of the video correspond to the time periods of the trial illustrated in (<bold>A</bold>). (<bold>C</bold>) Times between go cue and start button release (button release time), and between start button release and target acquisition (movement time) were distributed narrowly in most cases for reaching movements to near (red) and far (blue) targets demonstrating the temporally well-structured behavior. Dashed vertical lines indicate averages and corresponding numbers indicate averages and standard deviations (s.d.) in ms.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig2-v2.tif"/></fig><p>While the animals were not physically restricted to a specific posture, the strict timing of the task encouraged them to optimize their behavior. As the <italic>MaCaQuE</italic> system makes information about <italic>MCT</italic> touches and releases available with minimal delay (&lt;20 ms), it is possible to enforce an exact timing of the movements when solving a reaching task in the Reach Cage. <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows the distribution of button release times and movement times towards near and far targets for the task (monkey K/L: 19/10 sessions, 3956/2194 correct trials). As a whole-body translocation is required to approach far targets, movement times were longer than for near targets in both monkeys (t-test, p&lt;0.001). Movement time distributions were narrow (s.d. ≤ 76 ms), indicating that the monkeys optimized their behavior for consistent target acquisition. Button release time indicates the onset of the hand movement, not necessarily the whole-body movement. In monkey K, the button release times were higher for far compared to near targets (t-test, p&lt;0.001). In contrast, button release times in monkey L were lower for far compared to near targets (p&lt;0.001), reflecting a different behavioral strategy for movement onset (monkey K was sitting during the delay period whereas monkey L was standing).</p><p>The behavioral results as directly measured with <italic>MaCaQuE</italic> via the proximity sensors of the <italic>MCTs</italic> demonstrate that the Reach Cage is suitable to train animals on goal-directed reaching tasks with target positions not being constrained by the immediately reachable space of the animal. The temporally well-structured task performance at the same time allows behavioral and neurophysiological analyses as applied in more conventional settings.</p></sec><sec id="s2-2"><title>Time-continuous 3-dimensional arm kinematics during walk-and-reach behavior</title><p>As we do not impose physical restraint, the monkeys have more freedom to move than in conventional setups. This allows for testing new behavioral paradigms such as the walk-and-reach task but also provides more freedom in how to solve the task. We used DeepLabCut (<xref ref-type="bibr" rid="bib66">Mathis et al., 2018</xref>) for video-based motion capture and analyzed kinematics and their variability.</p><p>We measured the 3-dimensional posture of the reaching arm during the reach and walk-and-reach behavior of 2/3 sessions with a total of 469/872 successful trials in monkey K/L. Specifically, we tracked the monkeys’ headcap, left shoulder, elbow, and wrist (<xref ref-type="video" rid="video2">Videos 2</xref>–<xref ref-type="video" rid="video5">5</xref>). <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the side-view of the body part positions for each trial and video frame between 100 ms before button release and 100 ms after target acquisition for the reach (red) and walk-and-reach (blue) movements to the mid-left target.</p><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-51322-video2.mp4"><label>Video 2.</label><caption><title>The video shows reaching movements by monkey K with motion capture labels from all four cameras.</title><p>One example trial for each near target is depicted.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-51322-video3.mp4"><label>Video 3.</label><caption><title>The video shows reaching movements by monkey L with motion capture labels from all four cameras.</title><p>One example trial for each near target is depicted.</p></caption></media><media id="video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-51322-video4.mp4"><label>Video 4.</label><caption><title>The video shows walk-and-reach movements by monkey K with motion capture labels from all four cameras.</title><p>One example trial for each far target is depicted.</p></caption></media><media id="video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-51322-video5.mp4"><label>Video 5.</label><caption><title>The video shows walk-and-reach movements by monkey L with motion capture labels from all four cameras.</title><p>One example trial for each far target is depicted.</p></caption></media><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Structured behavior during task performance in unrestrained animals.</title><p>(<bold>A</bold>) Motion tracking of the left wrist, elbow, shoulder, and the headcap implant during reach and walk-and-reach movements for monkey K (left) and monkey L (right). Video-based markers are tracked in three dimensions and projected to a side-view. Trial-by-trial marker positions for the reach (red) and walk-and-reach (blue) movements to the mid-left targets are shown for a sampling frequency of 60 Hz, overlaid for multiple sessions (light-dark colors). (<bold>B</bold>) Small trial-to-trial variability of movement trajectories, even across sessions, demonstrates spatially well-structured and consistent behavior. For each trial and marker, the average Euclidean distance to the trial-averaged trajectory at corresponding times is shown (see Materials and methods). For reference, neighboring near targets were mounted at approximately 130 mm distance (dashed line) in this experiment. The <italic>MCT</italic> diameter is 75 mm. (<bold>C</bold>) Reconstructed 3-dimensional arm posture as function of time during reach and walk-and-reach movements based on the video motion capture separately for each monkey and session. The lines connect the marker (wrist to elbow to shoulder to headcap) for each marker position averaged across trials. Gray rectangles show target and start button <italic>MCTs</italic>. Pictures below show snapshots of characteristic postures during an example reach and walk-and-reach trial.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig3-v2.tif"/></fig><p>Within each animal, reach kinematics were highly consistent from trial to trial and from session to session. To quantify the variability in arm posture, we calculated for each target and marker separately and at corresponding times the Euclidean distance between the single-trial trajectories and the across sessions trial-averaged trajectory. <xref ref-type="fig" rid="fig3">Figure 3B</xref> shows the distributions of Euclidean distance averaged over time for each trial, marker, and monkey. The highest variability was seen in the wrist during walk-and-reach movements with a median of 37/46 mm and 0.75-quartile of 50/67 mm for monkeys K and L, respectively. Within a session these median deviations are 1–6 mm smaller. As a reference, the transparent front plate of the targets has a diameter of 75 mm. The center-to-center distance between neighboring targets is around 130 mm (near; shown as dashed line in the plot) and 210 mm (far). This shows that even across sessions, the arm posture during the movements towards the same target at a given time varied only by a few centimeters.</p><p>The movement patterns between monkey K (left) and monkey L (right) were different. <xref ref-type="fig" rid="fig3">Figure 3C</xref> shows the trial-averaged arm posture for each time point during the reach and walk-and-reach movements. Monkey K was sitting during the memory period and then used its left forelimb for walking and reaching. Monkey L was standing during the memory period and walked bipedally to the far targets. Both animals used this strategy consistently in all trials.</p><p>The kinematic analyses demonstrate that the animals not only complied with the spatial and temporal task requirements in terms of starting and endpoint acquisition but also adopted reliable repetitive behavior in terms of overall reach kinematics despite the lack of physical restraint. The animals used different behavioral strategies. However, the video-based motion capture allowed us to quantify the arm and head kinematics.</p></sec><sec id="s2-3"><title>Multi-channel single unit activity can be recorded in the Reach Cage using wireless technology</title><p>The Reach Cage provides an adequate setting for studying well-isolated single neuron activity from multiple areas of the cerebral cortex of monkeys during movement planning and execution of goal-directed behavior in minimally constrained settings. We here provide simultaneous recordings from three different sensorimotor areas, including non-surface areas inside sulci, during the goal-directed memory-guided walk-and-reach task.</p><p>We chronically implanted a total of 192 electrodes in primary motor cortex (M1), dorsal premotor cortex (PMd), and the parietal reach region (PRR) in the posterior parietal cortex of both monkeys using six 32-channel floating microwire arrays (FMA) with various lengths (see Materials and methods). We recorded broadband (30 ksps per channel) neural data from all arrays simultaneously (i.e. up to 192 channels) while the monkeys performed the walk-and-reach task (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The animals moved through the cage wearing the wireless electronics and protective cap without banging the implant into the cage equipment and performed the behavioral task as without the wireless gear.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Wireless recording in the Reach Cage.</title><p>Four example units from the frontoparietal reach network of monkeys K and L recorded wirelessly while the monkeys performed the memory-guided walk-and-reach task. The figure shows for each unit averaged spike densities with corresponding raster plots (top left), the waveform (top right), and the unfiltered broadband signal during reach and walk-and-reach example movements. Vertical dashed lines indicate task events in order of appearance: target cue (on and off), go cue, start button release, and target acquisition. Error bars indicate bootstrapped 95% confidence interval for the spike densities and s.d. for the waveform. Color indicates near (red) and far (blue) targets, lightness level indicates right (light) to left (dark) target positions.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Data loss rate differences across targets for all trials.</title><p>Two-way ANOVA table; ‘*” indicates significance with p&lt;0.05.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51322-fig4-data1-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>Data loss rate differences across targets for trials below 5% data loss.</title><p>Two-way ANOVA table; ‘*” indicates significance with p&lt;0.05.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51322-fig4-data2-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Data loss rate per target.</title><p>The figure shows the data loss rate of all trials per target position and distance (color coded). The left plot indicates all successful trials of all 12 sessions (two monkey K and 10 monkey L). Bars indicate the mean and error bars the bootstrapped confidence interval. Data loss rate is slightly modulated by target position and distance x position (left panel and <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>). This bias is introduced by a small fraction of trials with high data loss. We removed all trials with data loss of 5% or higher (right plot) for the data analyses presented in the main manuscript. When we did this, no bias by target position was observed within near or within far trials (right panel; <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig4-figsupp1-v2.tif"/></fig></fig-group><p>We recorded in monkey K/L 2/10 sessions from all six arrays simultaneously using two 96-channel wireless headstages. Our custom-designed implants can house both headstages and protect them and the array connectors against dirt and physical impact. The implants are designed to be used with different commercially available wireless systems, with the 2 × 96 channel digital systems presented here or with a 31- or 127-channel analog wireless system, dependent on the need of the experiment. Implant development and methodological details will be discussed below (Material and methods).</p><p>The wireless signal transmission was stable during walking movements. To quantify the stability, we calculated the rate of data loss from lost connection to the wireless system. We checked for each time point if either of the two headstages did not receive data. As a conservative measure, we only considered correctly performed trials, as in these trials it is guaranteed that the animal moved the full stretch from start to goal. The best sessions showed loss rates of 3.18%/1.03% of all time bins for monkey K/L, and worst sessions of 6.59%/6.34%, respectively. On average across sessions and monkeys, the loss rate was 3.32% (s.d. 1.7%). Data loss was spread over all targets with a slight spatial bias (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref> and <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>, 2 -way ANOVA position F(3, 2657)=3.48, p=0.015; position x distance F(3, 2657)=4.81, p=0.002). The spatial bias was introduced by trials with high data loss rates. When removing trials with a loss rate of above 5% there was no significant spatial bias anymore (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref> and <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>, 2-way ANOVA position F(3, 2657)=0.88, p=0.45; position x distance F(3, 2657)=2.36, p=0.07). From here on, we only consider correct trials with a loss rate of less than 5%. Note, walk-and-reach trials showed different loss rates from reach trials (F(3, 2657)=279.96, p&lt;0.001); however, this does not influence further results that focus on movement direction of reach or walk-and-reach movements separately.</p><p>The wireless signal quality was stable during walking movements and allowed us to isolate single- and multi-unit activity during the walk-and-reach task. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows four example neurons from the frontoparietal reach network of both monkeys while performing the task. Trial-averaged spike densities (top left) show that units were modulated by task condition. All four example neurons are significantly modulated by target distance, left-to-right target position, time during the trial, and interactions of distance x position and distance x time (ANOVA p&lt;0.05). Units A and C are mostly active during the memory period while units B and D are active during memory period and movement. Waveforms of the isolated example neurons are shown on the top right of each panel. Unfiltered broadband data of one near (red) and one far (blue) example trial are shown below. Spiking activity can be identified in the broadband signal also during the reach and walk-and-reach movement. We performed the same ANOVA for the activity in each channel of all 12 recorded sessions. Three sessions revealed task-responsive activity on all 192 channels, that is showed at least one effect in distance, position, time, or one of the interactions; across all sessions the mean number of task-responsive channels was 189 (s.d. five channels). Up to 179 channels were position-responsive, that is showing at least one effect in position or one of the interactions (mean: 162, s.d. 17 channels).</p><p>In summary, the Reach Cage proved to be suitable for addressing neuroscientific questions based on single and multi-unit recordings. Broadband wireless neural signals showed excellent spike isolation and modulation of spike frequency correlated with behavioral events.</p></sec><sec id="s2-4"><title>Premotor and parietal cortex encode movement goals beyond immediate reach</title><p>The Reach Cage allows us for the first time to test the spatial encoding of movement goals at larger distances to the animal. We wanted to know whether the frontoparietal reach network encodes motor goals only within the immediate reach or also beyond. For this, we computed separately in near and far trials the performance for decoding goal direction (left vs. right) with a support vector machine (SVM) decoder based on multi-unit firing rates.</p><p>We analyzed the session with the highest number of trials for each animal to avoid biasing our results by reusing repeated measures of the same neurons on channels that showed stable signals across multiple sessions. <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows the movement paths of the wrist (top) and head (bottom) of the animals for the reach (orange) and walk-and-reach (blue) behavior towards the different targets. <xref ref-type="fig" rid="fig5">Figure 5B</xref> (line plots) shows 20-fold cross validation of decoding accuracy in 300 ms time windows at 100 ms time steps. To test if there is reach goal encoding during movement planning prior to onset of movement, we analyzed the time window during the memory period starting 100 ms after target cue offset. To test if there is reach goal encoding during reaching (near) and during ongoing walking-and-reaching (far), we analyzed the 300 ms immediately before target acquisition. We performed a one-tailed permutation test to determine whether decoding accuracy is significantly above chance. In PMd and PRR, decoding is significant for both memory and movement period for reach and walk-and-reach movements (<xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>). In M1, decoding accuracy only reached significance for walk-and-reach movements in monkey L during the movement period. We then analyzed the population average of the firing rate modulation between the preferred and anti-preferred direction, again left vs. right (<xref ref-type="fig" rid="fig5">Figure 5B</xref> bar plots). We performed a one-tailed permutation test to determine significance. As in the decoding analysis, PMd and PRR show significant firing rate modulation for both memory and movement period for reach and walk-and-reach movements (<xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). Here, M1 shows significant firing rate modulation during the walk-and-reach movement period for both monkeys but, as for the decoding analysis, not during the memory period.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Direction decoding in the walk-and-reach task.</title><p>(<bold>A</bold>) Wrist (top) and head (bottom) position during reach (orange) and walk-and-reach (blue) movements towards the eight targets projected to the top-view. Each point corresponds to one location in one trial sampled at 60 Hz. (<bold>B</bold>) Decoding accuracy of 20-fold cross validation of a linear SVM decoder in 300 ms bins at 100 ms time steps (line plots). We decoded if a trial was towards one of the two left or one of the two right targets. Premotor and parietal cortex but not motor cortex showed significant decoding walk-and-reach targets even during the memory period. Statistical testing was done on one bin in the memory period 100–400 ms after the cue and movement period 300–0 ms before target acquisition (black dashed line). The colored dashed curve indicates the significance threshold based on a one-tailed permutation test. The population average of the firing rate modulation between preferred and anti-preferred direction (left vs. right) during the memory and movement bin is shown in the bar plots. The black lines indicate the significance threshold based on a one-tailed permutation test. In both plots, an asterisk corresponds to a significant increase with Bonferroni correction.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Test for significant SVM decoding accuracy.</title><p>One-tailed permutation test; ‘*” indicates Bonferroni corrected significance with p&lt;0.0042.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51322-fig5-data1-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Test for significant firing rate modulation.</title><p>One-tailed permutation test; ‘*” indicates Bonferroni corrected significance with p&lt;0.0042.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51322-fig5-data2-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig5sdata3"><label>Figure 5—source data 3.</label><caption><title>Test for change in decoding accuracy between trials with and without passage.</title><p>Two-tailed permutation test on SVM decoding accuracy differences between with and without passage; ‘*” indicates Bonferroni corrected significance with p&lt;0.0083.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51322-fig5-data3-v2.xlsx"/></supplementary-material></p><p><supplementary-material id="fig5sdata4"><label>Figure 5—source data 4.</label><caption><title>Test for change in firing rate modulation between trials with and without passage.</title><p>Two-tailed permutation test; ‘*” indicates Bonferroni corrected significance with p&lt;0.0083.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51322-fig5-data4-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Decoding walk-and-reach goals with different walking paths.</title><p>To test if target direction decoding accuracy (<xref ref-type="fig" rid="fig5">Figure 5</xref>) of walk-and-reach trials depends on the walking path, we recorded one session for each monkey. The sessions contained reach and walk-and-reach trials; however, we only perform the decoding analysis on walk-and-reach trials here. The sessions contained two blocks: one as presented in the results part of the manuscript (red color, ‘open’) and another with a narrow passage introduced in the middle of the walking path in a 40 cm distance to the wrist starting position (blue color, ‘passage’). See Materials and methods for more details. (<bold>A</bold>) Top view of wrist (top) and head (bottom) trajectories for the eight reach and walk-and-reach targets during trials with and without the passage. The horizontal axis is the same for wrist and head. Histogram plots show the marker position distribution on the vertical axis at the point of the dashed line. The distribution with the passage is different from the one without (Kolmogorow-Smirnow test p&lt;0.001 for wrist and head of both animals) with a smaller range (open/passage for monkey K: wrist 276 mm / 74 mm; head: 250 mm/ 106 mm and monkey L: wrist 218 mm / 61 mm; head: 311 mm / 117 mm) and s.d. (open/passage for monkey K: wrist 88 mm / 17 mm; head: 82 mm/ 32 mm and monkey L: wrist 64 mm / 15 mm ; head: 83 mm / 26 mm). (<bold>B</bold>) Line plots show SVM decoding accuracy of left vs. right walk-and-reach target direction with 10-fold cross validation on 300 ms overlapping bins. There was no significant difference between trials with and without the narrow passage in the memory (100–400 ms after target cue, dashed line) or movement period (300–0 ms before target acquisition, dashed line) of any monkey in any brain area. The confidence intervals for decoding accuracy difference (gray area) are plotted around the curve without passage. The bar plots show the population average of the difference in firing rate modulation for trials with and without the passage during the memory and movement period. The error bars indicate confidence intervals. Again, there was no significant difference between trials with and without the narrow passage. In both cases statistical tests were two-tailed permutation tests with Bonferroni correction (see <xref ref-type="supplementary-material" rid="fig5sdata3">Figure 5—source data 3</xref> and <xref ref-type="supplementary-material" rid="fig5sdata4">4</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig5-figsupp1-v2.tif"/></fig></fig-group><p>From the horizontal fanning out of the unconstrained movement patterns (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), it became evident that both animals directed their walking movement towards the goal from early on in the movement. To confirm that the motor goal information decodable from PMd and PRR correlates with the reach goal location rather than initial walking movement direction, we introduced a passage in the middle of the walk-and-reach path (a transparent divider between near and far targets with a narrow opening cut out). Although movement trajectories for the different motor goal locations collapsed onto very similar initial walking directions because of the passage (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>), the decoding accuracy and firing rate modulation was not affected by this measure, that is was independent of the movement path (<xref ref-type="fig" rid="fig5s1">Figure 5 – figure supplement 1B</xref> and <xref ref-type="supplementary-material" rid="fig5sdata3">Figure 5—source data 3</xref> and <xref ref-type="supplementary-material" rid="fig5sdata4">4</xref>).</p><p>Taken together, the Reach Cage environment allows us to study sensorimotor neuroscience questions within an unrestrained spatial setting. Here, we show that target location information is present in premotor and parietal cortex of far-located targets beyond the immediate reach.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We introduced the Reach Cage as a novel experimental environment for sensorimotor neuroscience with physically unrestrained rhesus monkeys. As a core interactive element, we developed <italic>MaCaQuE</italic>, a new experimental control system for sensorimotor tasks in cage environments. We trained two monkeys to conduct spatially and temporally structured memory-guided reach tasks that required them to reach to targets near or far from them with a walk-and-reach movement. With <italic>MaCaQuE</italic>, we could measure button release and movement times in response to visual cues with the same if not higher temporal precision as in touch screen experiments. Using markerless video-based motion capture, we could track 3-dimensional head and multi-joint arm kinematics for reach and walk-and-reach movements. Trajectories had low spatial variability over trials, showing that monkeys perform instructed movement consistently even when no physical restraint is applied. Variations in movement pattern between task conditions or monkeys could well be quantified in detail with this motion capture approach. In parallel, we wirelessly recorded broadband neural signals of 192 channels from three brain areas (M1, PMd, and PRR) simultaneously, an approach suitable for BMI applications. Isolated single-neuron activities were clearly modulated by the task events and encoded information about the location of immediate reach targets and also of remote walk-and-reach targets. Moreover, we could identify walk-and-reach target location information in premotor and parietal cortex, but not motor cortex, during movement and even during the memory period before the movement. This suggests that premotor and parietal cortex encodes motor goals beyond immediate reach. With our Reach Cage approach, we provide an experimental environment that allows testing fully unrestrained monkeys on spatially and temporally controlled behavior. With wireless intra-cortical recordings and markerless motion capture experimental spatial configurations are possible that are not restricted to the vicinity of the animals but allow studying complex full-body movement patterns.</p><sec id="s3-1"><title>Far-space motor goal encoding in the frontoparietal reach network</title><p>We showed that during the memory period of the walk-and-reach task, target location information of near-located reach and far-located walk-and-reach trials was present in PRR and PMd. Reducing the initial walk-and-reach movement path to a minimum variability between the different target directions by introducing a passage did not change decoding accuracy. This indicates that PRR and PMd activity contains spatial information about the reach goal beyond the immediate reach.</p><p>PMd (e.g. <xref ref-type="bibr" rid="bib32">Crammond and Kalaska, 1994</xref>) and PRR (e.g. <xref ref-type="bibr" rid="bib96">Snyder et al., 1998</xref>) activity are known to encode reach related spatial information during planning of reaches within immediate reach. We now show that this is also true beyond reach when walking behavior is needed to approach the reach target. Monkey K even used its reaching arm for walking by making ground contact, while monkey L was swinging its reaching arm during the locomotion without putting it down. This result might seem surprising in view of 1) neuropsychological studies showing that a near space specific neglect can arise from parietal lesions (<xref ref-type="bibr" rid="bib47">Halligan and Marshall, 1991</xref>; <xref ref-type="bibr" rid="bib102">Vuilleumier et al., 1998</xref>) or parietal transcranial stimulation (<xref ref-type="bibr" rid="bib10">Bjoertomt et al., 2002</xref>) and 2) the existence of bimodal neurons in premotor and posterior parietal cortex that have visual receptive fields centered on body surface and only covering its vicinity (<xref ref-type="bibr" rid="bib28">Colby and Goldberg, 1999</xref>; <xref ref-type="bibr" rid="bib44">Graziano et al., 1997</xref>; <xref ref-type="bibr" rid="bib86">Rizzolatti et al., 1981</xref>; <xref ref-type="bibr" rid="bib87">Rizzolatti et al., 1997</xref>). Yet, none of these studies explicitly show nor disregard PMd or PRR being involved in far space encoding. It could be, for example, that such a near or far space specificity is located in separate parts of premotor or parietal cortex. However, we propose an alternative explanation. The extent of the near space, often called peripersonal space (<xref ref-type="bibr" rid="bib87">Rizzolatti et al., 1997</xref>), is variable. Neurophysiological and neuropsychological studies have shown that it can expand around tools (<xref ref-type="bibr" rid="bib9">Berti and Frassinetti, 2000</xref>; <xref ref-type="bibr" rid="bib41">Giglia et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Holmes, 2012</xref>; <xref ref-type="bibr" rid="bib53">Iriki et al., 1996</xref>; <xref ref-type="bibr" rid="bib62">Maravita et al., 2002</xref>; <xref ref-type="bibr" rid="bib64">Maravita and Iriki, 2004</xref>) or fake arms (<xref ref-type="bibr" rid="bib11">Blanke et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Botvinick and Cohen, 1998</xref>; <xref ref-type="bibr" rid="bib45">Graziano et al., 2000</xref>; <xref ref-type="bibr" rid="bib63">Maravita et al., 2003</xref>; <xref ref-type="bibr" rid="bib81">Pavani et al., 2000</xref>). There is evidence from human psychophysics that the peripersonal space, here defined by the spatial extent of visuo-tactile integration, expands towards reach goals (<xref ref-type="bibr" rid="bib15">Brozzoli et al., 2009</xref>; <xref ref-type="bibr" rid="bib16">Brozzoli et al., 2010</xref>). Correspondingly, we could show that peripersonal space, as defined by the occurrence of visuo-tactile integration, in human participants expands to reach goals beyond immediate reach when subjects performed a walk-and-reach task similar to here (<xref ref-type="bibr" rid="bib7">Berger et al., 2019</xref>). While previous research suggested selective encoding of near space in parts of parietal and premotor cortex, goal-directed behavior might lead to an expansion of so-called near space even beyond immediate reach. Far-located walk-and-reach goals hence might effectively be within the ‘near space’ and be encoded similar to near-located reach goals in parietal and premotor regions known for reach goal selectivity during planning and movement.</p></sec><sec id="s3-2"><title>Neuroscience of goal-directed behavior in unrestrained non-human primates</title><p>As the example of far-space encoding above demonstrates, our understanding of motor cognition and spatial cognition in the primate brain might underestimate the true complexity of cortical representations as experimental needs previously prevented the study of more involved goal-directed full-body movements. While the limitations imposed by tethered recording techniques have been overcome with wireless technologies or data-logging in several neurophysiological studies with unrestrained non-human primates, the investigation of sensorimotor behavior has so far mostly focused on locomotion behavior, such as treadmill or corridor walking, or immediate collection of food items with the forelimb (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for an overview). In none of these previous studies, was precisely timed and spatially well-structured goal-directed behavior, or even movement planning, investigated in unrestrained monkeys. If behavior was ‘instructed’, it was always a direct movement towards a food source. Our Reach Cage made it possible to have multiple movement targets dislocated from the food source and placed at variable locations within the cage. Also, it allowed provision of strict temporal instructions to the animals on when to start or when to finish a movement.</p><p>With the Reach Cage we aimed for an experimental setting which allows us to study spatial cognitive and full-body sensorimotor behavior with levels of experimental control and behavioral analysis equivalent to conventional chair-seated experiments. We aimed for maximal freedom of the animal to move, and combined this with the conventional approach of a highly trained and structured task that (1) allows us to control movement timing to introduce certain periods, such as movement preparation; (2) ensures that the animal focuses on the specific behavior required by the task demand; and (3) provides repetition for a statistical analysis. With this combination, we were able to train the animals to conduct goal-directed memory-guided walk-and-reach movements upon instruction, a behavior which cannot be studied in chair-based settings or on treadmills.</p><p>The animals’ movement behavior was only constrained by the task and the overall cage volume. Nonetheless, reach trajectories revealed fast straight movements with little trial-to-trial variability even across sessions. Apparently, over the course of training, the animals had optimized their movement behavior and adopted consistent starting postures and stereotyped movement sequences. We were able to use the interaction device <italic>MaCaQuE</italic> to reveal narrow distributions of hand release time of the start button as response to the go signal and the movement time from the start button to the reach target. This spatiotemporal consistency of the behavior over many trials allows analytical approaches to both the behavioral and the neural data equivalent to conventional settings.</p><p><italic>MaCaQuE</italic> can serve as a robust cage-based equivalent to illuminated push-buttons (<xref ref-type="bibr" rid="bib5">Batista et al., 1999</xref>; <xref ref-type="bibr" rid="bib18">Buneo and Andersen, 2012</xref>) or a touch screen (<xref ref-type="bibr" rid="bib57">Klaes et al., 2011</xref>; <xref ref-type="bibr" rid="bib104">Westendorff et al., 2010</xref>) in conventional experiments, or as an alternative to wall-mounted touch screens in the housing environment (<xref ref-type="bibr" rid="bib6">Berger et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Calapai et al., 2017</xref>). Yet, the <italic>MaCaQuE</italic> system is more flexible in terms of spatial configuration. Targets and cues are vandalism-proof and can be placed at any position in large enclosures, allowing for 3-dimensional arrangements and an arbitrarily large workspace. If more explorative, less stereotyped behavior is of interest, the trial-repetitive nature of the current task can easily be replaced by alternative stimulus and reward protocols, for example for foraging tasks. Our reach goal decoding analysis performed on a single trial basis showed that single trial quantification is possible. This would allow for the analyses of unstructured behavior. In another study, we used <italic>MaCaQuE</italic> with humans and expanded it to deliver vibro-tactile stimuli to the subjects’ fingers and to receive additional input from push buttons in parallel to the reach target input and output (<xref ref-type="bibr" rid="bib7">Berger et al., 2019</xref>). It would also be straightforward to implement continuous interaction devices such as a joystick or motors to control parts of the cage, for instance doors. Similar to other systems for neuroscience experimentation and training (<xref ref-type="bibr" rid="bib60">Libey and Fetz, 2017</xref>; <xref ref-type="bibr" rid="bib84">Ponce et al., 2016</xref>; <xref ref-type="bibr" rid="bib100">Teikari et al., 2012</xref>), we used low-cost off-the-shelf components with an easy-to-program microcontroller platform as a core.</p></sec><sec id="s3-3"><title>Wireless recordings for BMI applications</title><p>An important translational goal of sensorimotor neuroscience with non-human primates is the development of BMI based on intracortical extracellular recordings to aid patients with severe motor impairments. Intracortical signals can be decoded to control external devices, as demonstrated in non-human primates (<xref ref-type="bibr" rid="bib22">Carmena, 2013</xref>; <xref ref-type="bibr" rid="bib48">Hauschild et al., 2012</xref>; <xref ref-type="bibr" rid="bib74">Musallam et al., 2004</xref>; <xref ref-type="bibr" rid="bib89">Santhanam et al., 2006</xref>; <xref ref-type="bibr" rid="bib93">Serruya et al., 2002</xref>; <xref ref-type="bibr" rid="bib99">Taylor, 2002</xref>; <xref ref-type="bibr" rid="bib101">Velliste et al., 2008</xref>; <xref ref-type="bibr" rid="bib103">Wessberg et al., 2000</xref>), and suited to partially restore motor function in quadriplegic human patients (<xref ref-type="bibr" rid="bib1">Aflalo et al., 2015</xref>; <xref ref-type="bibr" rid="bib14">Bouton et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Collinger et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Gilja et al., 2015</xref>; <xref ref-type="bibr" rid="bib50">Hochberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib105">Wodlinger et al., 2015</xref>). The results from the Reach Cage allow relevant insight towards BMI applications in two ways. First, we show encoding of reach goals during other ongoing movement behavior (locomotion). A previous study showed that when monkeys perform an arm movement task in parallel to a BMI cursor task based on decoding arm movement related neural activity, the BMI performance decreases (<xref ref-type="bibr" rid="bib80">Orsborn et al., 2014</xref>). Little was known before about the stability of forelimb decoding performance when other body movements are performed in parallel, such as walking. For partially movement-impaired patients, such as arm amputees, existence of reach goal signals, as demonstrated here, is a prerequisite for restoring the lost function with a prosthesis while still conducting the healthy movements, for example walking. Second, the Reach Cage in its current form with its discrete lights and targets provides a useful environment for BMI studies that follow a different approach, namely to control smart devices or a smart home with ambient assisted living environments reacting to discrete sets of commands. While the user only needs to choose among a discrete set of programs, the smart device or home would take care of the continuous control of the addressed actuators. The Reach Cage is a useful tool to develop such a BMI that makes temporally precise and correct decisions on which program to activate. Importantly, the Reach Cage allows to test if, and in which brain areas, such decisions are encoded invariant to body position in the room, important also for patients incapable of walking but using assistdevices such as a wheelchair to relocate (<xref ref-type="bibr" rid="bib85">Rajangam et al., 2016</xref>).</p><p>We show that our recording bandwidth and quality is sufficiently high for analyzing neural spiking activity in multiple brain areas simultaneously. Further, we show that there is enough information in the population activity to be detected by a decoder on a single trial basis. This is an important prerequisite for BMI applications, and also for the analysis of free behavior, for which structured repetitive behavior is neither given nor wanted. To our knowledge, 192 channels is the highest channel count of recording full broadband (30 ksps per channel) intracortical recordings in unrestrained non-human primates. Previous studies presented simultaneous recordings of 96 channels broadband data; when higher channel counts were used, for example spiking activity from 512 channels (<xref ref-type="bibr" rid="bib92">Schwarz et al., 2014</xref>), automatic spike detection on the headstage was applied and only spike times and waveforms were transmitted and recorded. This is sufficient for spike time analyses but full broadband data would be necessary to extract local field potentials and to change spike detection post-hoc.</p><p>An alternative to wireless recordings is data logging, which can be used to store the recorded data on a head-mounted device (<xref ref-type="bibr" rid="bib54">Jackson et al., 2006</xref>; <xref ref-type="bibr" rid="bib55">Jackson et al., 2007</xref>; <xref ref-type="bibr" rid="bib109">Zanos et al., 2011</xref>). While the logging device is detached from any behavioral monitoring or task instruction system, additional measures can be taken to ensure offline synchronization of behavioral data with the logged neural data. Yet, real-time spike sorting and data processing for closed-loop BMI applications are limited to the head-mounted computational capacity when using loggers, which is usually low, while a wireless transmission provides access to powerful processors outside the animal.</p></sec><sec id="s3-4"><title>Three-dimensional markerless motion capture in the Reach Cage</title><p>In addition to <italic>MaCaQuE</italic> for experimental control, we demonstrated the usefulness of 3-dimensional video-based multi-joint motion tracking during the walk-and-reach movements. Reliable motion capture with unrestrained monkeys provides a technical challenge. At least two cameras need to see a marker or body part to reconstruct a 3-dimensional position. Occlusion by objects or the animal itself is usually an issue (<xref ref-type="bibr" rid="bib23">Chen and Davis, 2000</xref>; <xref ref-type="bibr" rid="bib68">Moeslund et al., 2006</xref>). When using systems based on physical markers (active LEDs or passive reflectors), rhesus monkeys tend to rip off the markers attached to their body, unless excessively trained. An alternative is fluorescent or reflective markers directly painted to the skin of the animal (<xref ref-type="bibr" rid="bib31">Courtine et al., 2005</xref>; <xref ref-type="bibr" rid="bib82">Peikon et al., 2009</xref>), which also require continuously repeated shaving, or markers that cannot be removed, such as collars (<xref ref-type="bibr" rid="bib3">Ballesta et al., 2014</xref>). Video-based marker-free system models designed for use with monkeys were recently reported (<xref ref-type="bibr" rid="bib2">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib76">Nakamura et al., 2016</xref>); however, these are not yet reported with neurophysiological recordings. We used the recently introduced open source toolbox DeepLabCut (<xref ref-type="bibr" rid="bib66">Mathis et al., 2018</xref>), which provides markerless tracking of visual features in a video, such as body parts but also objects. DeepLabCut provides excellent tracking of body parts from different species such as monkeys (<xref ref-type="bibr" rid="bib59">Labuguen et al., 2019</xref>), mice, flies, humans, fish, horses, and cheetahs (<xref ref-type="bibr" rid="bib77">Nath et al., 2019</xref>), While we focus on instructed behavior, the current motion capture setting would allow quantification of 3-dimensional free behavior of non-human primates given an appropriate number of camera views.</p></sec><sec id="s3-5"><title>Conclusion</title><p>Systems neuroscience can benefit substantially from the possibility of quantifying free behavior and simultaneously recording large-scale brain activity, particularly, but not only in sensorimotor research. This possibility opens a range of new opportunities, for example to study motor control of multi-joint and whole-body movements, spatial cognition in complex workspaces, or social interactive behavior. With the opportunities that wireless technology offers, a desirable approach would be to let the monkey freely decide on its behavior to obtain neural correlates of most natural behavior (<xref ref-type="bibr" rid="bib42">Gilja et al., 2010</xref>) while motion capture provides the related movement kinematics (<xref ref-type="bibr" rid="bib2">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Ballesta et al., 2014</xref>; <xref ref-type="bibr" rid="bib4">Bansal et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Nakamura et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Peikon et al., 2009</xref>). In fact, we consider it an important next step in systems neuroscience to demonstrate that the important and detailed knowledge that has been gained from tightly controlled experimental settings generalizes well to more naturalistic behaviors. Here, with the Reach Cage we present an experimental environment in combination with high-channel count wireless recording from multiple brain areas and with multi-joint markerless motion capture. We demonstrated that we can use this setting to study instructed behavior, for which it is easier to isolate different behavioral aspects of interest (movement planning, walking, and reaching). This allowed us to isolate movement planning related activity to reach targets outside of the immediate reach. We could show that the frontoparietal reach network encodes such far-located reach goals.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>Two male rhesus monkeys (Macaca mulatta K age: 6 years; and L age: 15 years) were trained in the Reach Cage. Both animals were behaviorally trained with positive reinforcement learning to sit in a primate chair. Monkey K did not participate in any research study before but was trained on a goal-directed reaching task on a touch screen device in the home enclosure (<xref ref-type="bibr" rid="bib6">Berger et al., 2018</xref>). Monkey L was experienced with goal-directed reaching on a touch screen and with a haptic manipulandum in a conventional chair-seated setting before entering the study (<xref ref-type="bibr" rid="bib71">Morel et al., 2016</xref>). Both monkeys were chronically implanted with a transcutaneous titanium head post, the base of which consisted of four legs custom-fit to the surface of the skull. The animals were trained to tolerate periods of head fixation, during which we mounted equipment for multi-channel wireless recordings. We implanted six 32-channel floating microelectrode arrays (Microprobes for Life Science, Gaithersburg, Maryland) with custom electrode lengths in three areas in the right hemisphere of cerebral cortex. Custom-designed implants protected electrode connectors and recording equipment. The implant design and implantation procedures are described below.</p><p>Both animals were housed in social groups with one (monkey L) or two (monkey K) male conspecifics in facilities of the German Primate Center. The facilities provide cage sizes exceeding the requirements by German and European regulations, and access to an enriched environment including wooden structures and various toys (<xref ref-type="bibr" rid="bib20">Calapai et al., 2017</xref>). All procedures have been approved by the responsible regional government office [Niedersächsisches Landesamt für Verbraucherschutz und Lebensmittelsicherheit (LAVES)] under permit numbers 3392 42502-04-13/1100 and comply with German Law and the European Directive 2010/63/EU regulating use of animals in research.</p></sec><sec id="s4-2"><title>MaCaQuE</title><p>We developed the <italic>Macaque Cage Query Extension</italic> (<italic>MaCaQuE</italic>) to provide computer-controlled visual cues and reach targets at freely selectable individual positions in a monkey cage (<xref ref-type="fig" rid="fig1">Figure 1</xref>). <italic>MaCaQuE</italic> comprises a microcontroller-based interface, controlled via a standard PC, plus a variable number of <italic>MaCaQuE</italic> Cue and Target boxes (<italic>MCT</italic>).</p><p>The <italic>MCT</italic> cylinder is made of PVC plastic and has a diameter of 75 mm and a length of 160 mm. At one end of the cylinder the <italic>MCTs</italic> contain a capacitive proximity sensor (EC3016NPAPL, Carlo Gavazzi, Steinhausen, Switzerland) and four RGB-LEDs (WS2812B, Worldsemi Co., Daling Village, China), both protected behind a clear polycarbonate cover. With the LEDs, light stimuli of different color (8-bit color resolution) and intensity can be presented to serve as visual cues (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The LEDs surround the proximity sensor, which registers when the monkey touches the middle of the polycarbonate plate with at least one finger. This way the <italic>MCT</italic> acts as a reach target. LEDs, sensor plus a custom-printed circuit board for controlling electronics and connectors are mounted to a custom-designed 3D-printed frame made out of PA2200 (Shapeways, New York City, New York). A robust and lockable RJ45 connector (etherCON, Neutrik AG, Schaan, Liechtenstein) connects the <italic>MCT</italic> to the interface unit from the opposite side of the cylinder via standard Ethernet cables mechanically protected inside flexible metal tubing. The RGB-LEDs require an 800 kHz digital data signal. For noise reduction, we transmit the signal with a differential line driver (SN75174N, SN74HCT245N, Texas Instruments Inc, Dallas, Texas) via twisted-pair cabling in the Ethernet cable to a differential bus transreceiver (SN75176B, Texas Instruments Inc) on the <italic>MCT</italic>. Ethernet cables are CAT 6; however, any other category would be suitable (CAT 1 up to 1 MHz). This setting allows us to use cables at least up to 15 m. Hence, there are no practical limits on the spatial separation between <italic>MCTs</italic> and from the interface for applications even in larger animal enclosures. We did not test longer cables. Apart from the one twisted-pair for the data stream of the RGB-LEDs, the Ethernet cable transmits 12 V power from the interface unit and the digital touch signal from the proximity sensor to the interface unit. The proximity sensor is directly powered by the 12 V line. The LEDs receive 5 V power from a voltage regulator (L7805CV, STMicroelectronics, Geneva, Switzerland) that scales the 12 V signal down. The PVC and polycarbonate enclosure of the <italic>MCT</italic> as well as the metal cable protection are built robustly enough to be placed inside a rhesus monkey cage. <italic>MaCaQuE</italic> incorporates up to two units to deliver precise fluid rewards (<xref ref-type="bibr" rid="bib20">Calapai et al., 2017</xref>). Each unit consists of a fluid container and a peristaltic pump (OEM M025 DC, Verderflex, Castleford, UK). MOSFET-transistors (BUZ11, Fairchild Semiconductor, Sunnyvale, California) on the interface unit drive the pumps.</p><p>The <italic>MCTs</italic> and reward systems are controlled by the Arduino-compatible microcontroller (Teensy 3.x, PJRC, Sherwood, Oregon) placed on a custom-printed circuit board inside the interface unit (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). To operate a high number of <italic>MCTs</italic> the microcontroller communicates with the proximity sensor and LEDs using two serial data streams, respectively. For the proximity sensor, we used shift registers (CD4021BE, Texas Instruments) that transform the parallel output from the <italic>MCTs</italic> to a single serial input to the microcontroller. The LEDs have an integrated control circuit to be connected in series. An additional printed circuit board connected to the main board contained 16 of the RGB-LEDs that receive the serial LED data stream from the microcontroller. We use this array of LEDs to convert the serial stream into parallel input to the <italic>MCTs</italic> by branching each input signal to the differential line drivers that transmit the signal to each <italic>MCT</italic>. To optimize the form factor of the interface unit, we made a third custom-printed circuit board that contains all connectors. In our current experiments, we assembled a circuit for connecting up to 16 <italic>MCTs</italic> but the <italic>MaCaQuE</italic> system would be easily expandable to a larger number. To set the transistors to drive the pumps of the reward systems, the 3.3V logic signal from the microcontroller is scaled up to 5V by a buffer (SN74HCT245N, Texas Instruments Inc, Dallas, Texas). As <italic>MaCaQuE</italic> incorporates parts operating at 3.3V (microcontroller), 5V (LED array), and 12V (peristaltic pump and <italic>MCT</italic>), we used a standard PC-power supply (ENP-7025B, Jou Jye Computer GmbH, Grevenbroich, Germany) as power source. Additionally, 12 digital general-purpose-input-output (GPIO) pins are available on the interface, which were used to 1) send and receive synchronizing signals to other behavioral or neural recording hardware (strobe); 2) add a button to manually control reward units; and 3) add a switch to select which reward unit is addressed by the manual reward control. Further options such as sending test signals or adding sensors or actuators are possible. Custom-printed circuit boards are designed with EAGLE version 6 (Autodesk Inc, San Rafael, California).</p><p>We used Arduino-C to program the microcontroller firmware. <italic>MaCaQuE</italic> was accessed by a USB connection from a computer using either Windows or Mac OS. A custom-written C++ software package (MoRoCo) operated the behavioral task and interfaced with <italic>MaCaQuE</italic> via the microcontroller. We developed hardware testing software using Processing and C++. <italic>MaCaQuE</italic> was also used in another study involving human participants (<xref ref-type="bibr" rid="bib7">Berger et al., 2019</xref>). Schematics and software are available online (<xref ref-type="bibr" rid="bib8">Berger and Gail, 2020</xref> and <ext-link ext-link-type="uri" xlink:href="https://github.com/sensorimotorgroupdpz/MaCaQuE">https://github.com/sensorimotorgroupdpz/MaCaQuE</ext-link>).</p></sec><sec id="s4-3"><title>Reach Cage</title><p>The Reach Cage is a cage-based training and testing environment for sensorimotor experiments with a physically unrestrained rhesus monkey (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). Inner cage dimensions are 170 cm x 80 cm x 85 cm (W x D x H) with a metal mesh grid at the top and bottom, a solid metal wall on one long side (back), and clear polycarbonate walls on all other sides. The idea of the experiment was to implement a memory-guided goal-directed reach task with instructed delay, equivalent to common conventional experiments (<xref ref-type="bibr" rid="bib33">Crammond and Kalaska, 2000</xref>), to compare neural responses during planning and execution of reaches towards targets at different positions in space.</p><p>We used <italic>MaCaQuE</italic> to provide 10 visual cues and reach targets (<italic>MCTs</italic>) inside the cage (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Two <italic>MCTs</italic> were positioned on the floor pointing upwards. Eight were placed 25 cm below the ceiling in two rows of four each, pointing toward the middle position between the two <italic>MCTs</italic> on the floor. The floor <italic>MCTs</italic> provided the starting position for the behavioral task (start buttons). The monkey could comfortably rest its hands on the start buttons while sitting or standing in between. The row of ceiling <italic>MCTs</italic> closer to the starting position was placed with a 10 cm horizontal distance and 60 cm vertical distance to the starting position (near targets). We chose this configuration to provide a comfortable position for a rhesus monkey to reach from the starting positions to the near targets without the need to relocate its body. The second row of <italic>MCTs</italic> was positioned at 100 cm horizontal distance from the starting positions (far targets) requiring the animal to make steps towards the targets (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). An 11<sup>th</sup><italic>MCT</italic> was placed outside the cage in front of the monkey (in the starting position and facing the opposite wall) to provide an additional visual cue. For positive reinforcement training, <italic>MaCaQuE’s</italic> reward systems can provide fluid reward through protected silicon and metal pipes into one of two small spoon-size stainless steel bowls mounted approx. 20 cm above the floor in the middle of either of the two long sides of the Reach Cage.</p></sec><sec id="s4-4"><title>Behavioral task</title><p>We trained both monkeys on a memory-guided walk-and-reach task with instructed delay (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). When the <italic>MCT</italic> outside lit up, the monkeys were required to touch and hold both start buttons (hand fixation). After 400–800 ms, one randomly chosen reach target lit up for 400 ms indicating the future reach goal (cue). The animals had to remember the target position and wait for 400–2000 ms (memory period) until the light of the <italic>MCT</italic> outside changed its color to red without changing the luminance (go cue). The monkeys then had a 600 ms time window starting 200 ms after the go cue to release at least one hand from the start buttons. We introduced the 200 ms delay to discourage the animals from anticipating the go cue and triggering a reach prematurely. After releasing the start buttons, the animals needed to reach to the remembered target within 600 ms or walk-and-reach within 1200 ms dependent on whether the target was near or far. Provided the animals kept touching for 300 ms, the trial counted as correct indicated by a high pitch tone and reward. A lower tone indicated an incorrect trial. The reward was delivered as juice in one of two randomly assigned drinking bowls. We used unpredictable sides for reward delivery to prevent the animal from planning the movement to the reward before the end of the trial.</p><p>In the beginning, we did not impose the choice of hand on the monkeys in this study but let them freely pick their preferred hand. While monkey K reached to the targets with the right hand, monkey L used the left hand. Both animals consistently used their preferred hand and never switched. For the walk-and-reach task we trained monkey K to use its left hand using positive reinforcement training. Once trained, the monkey used consistently its left hand.</p><p>In a control session (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) we added a passage in the middle of the walk-and-reach movements. The session was split into two blocks with (160/100 trials for monkey K/L) and without (154/178 trials for monkey K/L) this passage. The passage had an opening of 31 cm horizontally that constrained the animal’s walk-and-reach movements to a narrower path. Reach movements were unaffected.</p><p>All data presented in this manuscript were collected after animals were trained on the behavioral task.</p></sec><sec id="s4-5"><title>Motion capture and analysis of behavior</title><p>The animals’ behavior was analyzed in terms of accuracy (percent correct trials), timing (as registered by the proximity sensors), and arm kinematics (video-based motion capture).</p><p>We analyzed start button release and movement times of both monkeys based on the <italic>MCT</italic> signals when they performed the walk-and-reach task (monkey K: 19 sessions; monkey L: 10 sessions). Button release time is the time between the go cue and the release of one of the start buttons. Movement time is the time between the release of one of the start buttons and target acquisition. We analyzed the timing separately for each monkey and separately for all near and all far targets.</p><p>Additionally, we tracked continuous head and arm kinematics in detail offline (). We recorded four video streams in parallel from different angles together with the <italic>MCT</italic> signals and the neural data. For these synchronized multi-camera video recordings, we used a commercial video capture system (Cineplex Behavioral Research System, Plexon Inc, Dallas, Texas) incorporating four Stingray F-033/C color cameras (Allied Vision Technologies GmbH, Stadtroda, Germany). Videos were recorded with 60 fps frame rate in VGA resolution. Video processing on camera and host PC takes less than 20 ms (camera shutter opening time not included). The system uses a central trigger to synchronize all cameras. For synchronization with all other data, the system sent a sync pulse every 90 frames to <italic>MaCaQuE</italic>.</p><p>To quantify the movement trajectories, we tracked the 3-dimensional position of the left wrist, elbow, shoulder, and headcap (part of the head implant, see below and <xref ref-type="fig" rid="fig6">Figure 6C</xref>, no 10) frame-by-frame when the monkeys performed the walk-and-reach task correctly. To do so, we first tracked the 2-dimensional position in each video and then reconstructed the 3-dimensional position out of the 2-dimensional data. For 2-dimensional markerless body-part tracking we used DeepLabCut (DLC), based on supervised deep neural networks to track visual features consistently in different frames of a video (<xref ref-type="bibr" rid="bib66">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Mathis et al., 2019</xref>; <xref ref-type="bibr" rid="bib77">Nath et al., 2019</xref>). We trained a single network based on a 101-layer ResNet for all four cameras and both monkeys. Using DLC’s own tools, we labeled in total 7507 frames from 12 sessions (four monkey K and eight monkey L). All training frames were randomly extracted from times at which the monkeys performed the walk-and-reach task correctly. We not only trained the model to track headcap, left wrist, elbow, and shoulder, but also snout, left finger, right finger, wrist, elbow, shoulder, tail, and four additional points on the headcap. While those additional body parts were less often visible with this specific camera setting and not of interest for our current study, the tracking of certain desired features can be improved by training DLC models to additional other features (see <xref ref-type="bibr" rid="bib66">Mathis et al., 2018</xref> for details). We used cross validation to estimate the accuracy of DLC in our situation, using 95% of our labeled data as training data for the model and 5% as test data. The model provides a likelihood estimate for each data point. We removed all results with a likelihood of less than 0.9. For the remaining data points of all 10 features, the root mean squared error was 2.57 pixels for the training and 4.7 pixels for test data. With this model we estimated the position of the body parts in each video. Then we reconstructed the 3-dimensional position using the toolbox pose3d (<xref ref-type="bibr" rid="bib95">Sheshadri et al., 2020</xref>). First, we captured images from a checkerboard with defined length on all four cameras at the same time. Using the Computer Vision Toolbox from Matlab (Mathworks Inc, Natick, Massachusetts), we estimated the camera calibration parameters for each camera and for each camera pair. Pose3d uses those parameters to triangulate the 3-dimensional position from at least two camera angles. If feature positions from more than two cameras are available, pose3d will provide the least-squares estimate. By projecting the 3-dimensional position back into 2-dimensional camera coordinates we could measure the reprojection error. We excluded extreme outliers with a reprojection error above 50 pixels for at least one camera.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Implant system design.</title><p>(<bold>A</bold>) Three-dimensional computer models of the implants and electronics. The skull model of monkey L (beige) is extracted from a CT scan including the titanium implant for head fixation (4, headpost), which was already implanted before this study. Further implants are colored for illustrative purposes and do not necessarily represent the actual colors. (<bold>B</bold>) Image of microelectrode array placement during the surgery of monkey L (top) and monkey K (bottom). Anatomical landmark descriptions: IPS – intraparietal sulcus; CS – central sulcus; PCD – postcentral dimple; AS – arcuate suclus. (<bold>C</bold>) Image of the implants on monkey L’s head. (<bold>D</bold>) Different configurations of wireless headstages and protective headcaps temporally mounted on the implants. Numbers indicate: 1 – chamber; 2 – adapter board holder; 3 – array connector holder; 4 – headpost (from CT scan); 5 – flat headcap; 6 – W32 headstage; 7 – W128 headstage; 8 - Exilis headstage (two used in parallel); 9 – headcap for W128 headstage; 10 - headcap for W32 or Exilis headstages.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51322-fig6-v2.tif"/></fig><p>After the reconstruction of the 3-dimensional positions of the body parts, we performed an outlier analysis. First, we applied a boundary box with the size of 132 cm x 74 cm x 75 cm (W x D x H) and removed data points that lay outside the box. Second, we looked for outliers based on discontinuity over time (aka speed). We calculated the Euclidean distance between each consecutive time point for each body part trajectory and applied a threshold to detect outliers. We only rejected the first and every second outlier as a single outlier will lead to two ‘jumps’ in the data. Then we reiterated the process until all data points were below threshold. We applied different thresholds for each body part, dependent on whether the frame was during a movement (between start button release and target acquisition) or not. Specifically, we used 12 mm/frame and 80 mm/frame for the wrist and 15 mm/frame and 40 mm/frame for the other body parts with the higher threshold during the movement. With a frame rate of 60 fps, 100 mm/frame corresponds to 6 m/s. After rejecting all outliers (DeepLabCut low likelihood, reprojection error, boundary box, and discontinuity), the percentage of valid data points of all seven analyzed sessions during correctly performed trials for Monkey K/L was: wrist 94.93%; elbow 92.51%; shoulder 94.98%; headcap 97.58%. We interpolated the missing data points using phase preserving cubic interpolation.</p><p>We analyzed the movement trajectories of the four body parts during reach and walk-and-reach movements. For the behavioral analysis (2/3 sessions, 469/872 successful trials monkey K/L), we chose a time window of between 100 ms before start button release and 100 ms after target acquisition (<xref ref-type="fig" rid="fig3">Figure 3</xref>). For the analysis with neural data (231/326 successful trials monkey K/L one session each), we chose the time window of between 300 ms before start button release and 300 ms after target acquisition (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In both cases, we used linear interpolation for temporal alignment of the data between trials and relative to the neural data in the latter case. For trial-averaging, we averaged the data across trials on each aligned time point for each dimension. The 3-dimensional data are presented from a side-view (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and top-view (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>) of the movement. The side-view is defined by one of the four cameras directly facing the side of the Reach Cage. Arm posture plots are straight lines connecting wrist with elbow, elbow with shoulder, and shoulder with headcap. For the variability analysis, we calculated the Euclidean distance at each time point and trial to the trial-averaged trajectory for each target and body part. We then averaged the distances over all time points for each trial and present the median and 0.75-quartile for each body part and target distance pooled over the target position. For the control session with a narrow passage (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>, 314/278 successful trials monkey K/L one session each), we additionally analyzed the spread of the wrist and head position of the walk-and-reach movements over trials at a 40 cm distance from the animals’ average wrist starting position. We report range and s.d. over the axis orthogonal to the side-view, that is the target axis, and used the Kolmogorow-Smirnow test to determine whether distributions with and without narrow passage differed.</p><p>The behavioral analyses were performed using Matlab with the data visualization toolbox <italic>gramm</italic> (<xref ref-type="bibr" rid="bib72">Morel, 2018</xref>). The 2-dimensional feature tracking with DeepLabCut was done in Python (Python Software Foundation, Beaverton, Oregon).</p></sec><sec id="s4-6"><title>Implant system design</title><p>Wireless neural recordings from the cerebral cortex of rhesus monkeys during minimally restrained movements require protection of the electrode array connectors and the headstage electronics of the wireless transmitters. We designed a protective multi-component implant system to be mounted on the animal skull (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The implant system and implantation technique was designed to fulfill the following criteria: 1) Electrode connectors need to be protected against dirt and moisture; 2) While the animal is not in the experiment, the implants need to be small and robust enough for the animal to live unsupervised with a social group in an enriched large housing environment; 3) During the experiment, the wireless headstage needs to be protected against manipulation by the animal and potential physical impacts from bumping the head; 4) The head-mounted construction should be as lightweight as possible; 5) Placing of the electrode arrays and their connectors during the surgery needs to be possible without the risk of damaging electrodes, cables, or the brain; 6) Implant components in contact with organic tissue need to be biocompatible; 7) Temporary fixation of the animal’s head in a primate chair needs to be possible for access to implants and for wound margin cleaning; 8) Implants must not interfere with wireless signal transmission; 9) Optionally, the implant may serve as trackable object for motion capture.</p><p>We designed the implant system for two main configurations: first, a home configuration containing only permanently implanted components and being as small as possible when the animal is not in a recording session but in its group housing (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, top left); second, a recording configuration with removable electronic components being attached. This configuration should fit a 31-channel headstage, a 127-channel headstage (W32/W128, Triangle BioSystems International, Durham, North Carolina), or two 96-channel headstages (CerePlex Exilis, Blackrock Microsystems LLC, Salt Lake City, Utah). Headstage placement is illustrated in <xref ref-type="fig" rid="fig6">Figure 6D</xref>. The implant system consists of four custom-designed components: a skull-mounted outer encapsulation (chamber; <xref ref-type="fig" rid="fig6">Figure 6A/C</xref>, no 1), a mounting base for holding a custom-designed printed circuit board (adaptor board holder, no 2), a mounting grid to hold the connectors of the electrode arrays (connector holder, no 3), and a set of different-sized caps to contain (or not) the different wireless headstages (no 5–10). Dimensions of the wireless headstages are W32: 17.9 mm x 25 mm x 14.2 mm (W x D x H), 4.5 g weight; W128: 28.7 mm x 34.3 mm x 14.2 mm (W x D x H), 10 g weight; Exilis: 25 mm x 23 mm x 14 mm (W x D x H), 9.87 g weight.</p><p>We designed the implants custom-fit to the skull using CT and MRI scans. Using 3D Slicer (Brigham and Women's Hospital Inc, Boston, Massachusetts), we generated a skull model out of the CT scan (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and a brain model out of the MRI scan (T1-weighted; data not shown). In the MRI data we identified the target areas for array implantation based on anatomical landmarks (intraparietal, central, and arcuate sulci; pre-central dimple), and defined Horsley-Clarke stereotactic coordinates for the craniotomy necessary for array implantation (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). We used stereotactic coordinates extracted from the MRI scan to mark the planned craniotomy on the skull model from the CT scan. We then extracted the mesh information of the models and used Inventor (Autodesk Inc, San Rafael, California) and CATIA (Dassault Systèmes, Vélizy-Villacoublay, France) to design virtual 3-dimensional models of the implant components which are specific to the skull geometry and planned craniotomy. Both monkeys already had a titanium headpost implanted, of which the geometry, including subdural legs, was visible in the CT (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, no 4), and, therefore, could be incorporated into our implant design.</p><p>We built the chamber to surround the planned craniotomy and array connectors (<xref ref-type="fig" rid="fig6">Figure 6A/C</xref>, no 1). The chamber was milled out of polyether ether ketone (TECAPEEK, Ensinger GmbH, Nufringen, Germany) to be lightweight (monkey K/L: 10/14 grams; 65/60.3 mm max. length, 50/49.5 mm max. width, 24.9/31.2 mm max. height; wall thickness: 2/2 mm) and biocompatible. For maximal stability despite low diameter, stainless-steel M2 threads (Helicoil, Böllhoff, Bielefeld, Germany) were inserted into the wall for screwing different protective headcaps onto the chamber. The built-in eyelets at the outside bottom of the chamber wall allow mounting of the chamber to the skull using titanium bone screws (2.7 mm corticalis screws, 6–10 mm length depending on bone thickness, DePuy Synthes, Raynham, Massachusetts). Fluting of the lower half of the inner chamber walls let dental cement adhere to the chamber wall.</p><p>The subdural 32-channel floating microelectrode arrays (FMA, Microprobes for Life Science) are connected by a stranded gold wire to an extra-corporal 36-pin nano-strip connector (Omnetics Connector Corporation, Minneapolis, Minnesota). We constructed an array connector holder to hold up to six of the Omnetics connectors inside the chamber (<xref ref-type="fig" rid="fig6">Figure 6A/C</xref>, no 3). The connector holder was 3D-printed in a very lightweight but durable and RF-invisible material (PA2200 material, Shapeways). The holding grid of the array connector holder is designed such that it keeps the six connectors aligned in parallel with 2 mm space between. The spacing allows for: 1) connecting six 32-channel Cereplex (Blackrock Microsystems LLC) headstages for tethered recording simultaneously on all connectors, 2) directly plugging a 31-channel wireless system onto one of the array connectors, or 3) flexibly connecting four out of six arrays with adaptor cables to an adaptor board, linking the arrays to a 127-channel wireless system. The total size of the array connector is 27 mm x 16.2 mm incorporating all six connectors. The bottom of the array connector holder fits the skull geometry with a cut-out to be placed above an anchor screw in the skull for fixation with bone cement (PALACOS, Heraeus Medical GmbH, Hanau, Germany). This is needed as the array connector is placed on the skull next to the craniotomy during insertion of the electrode arrays, that is before implantation of the surrounding chamber (see below). The medial side of the holding grid, pointing to the craniotomy, is open so that we can slide in the array connectors from the side during the surgery. On the lateral side small holes are used to inject dental cement with a syringe to embed and glue the connectors to the grid.</p><p>The 31-channel wireless headstage can be directly plugged into a single Omnetics nano-strip array connector. The 127-channel wireless headstage instead has Millmax strip connectors (MILL-MAX MFG. CORP., Oyster Bay, New York) as input. A small adapter board (electrical interface board, Triangle BioSystems International) builds the interface to receive up to four Omnetics nano-strip connectors from the implanted arrays via adaptor cables (Omnetics Connector Corporation). We constructed a small holder with two M3 Helicoils for permanent implantation to later screw-mount the adaptor board when needed during recording (<xref ref-type="fig" rid="fig6">Figure 6A/C</xref>, no 2). Fluting on the sides of the adaptor board holder helps embedding of the holder into dental cement. Like the array connector holder, the adaptor board holder was 3D-printed in PA2200. The 96-channel Exilis headstages have three Omnetics nano-strip connectors which would fit into the array connectors; however, precise alignment was very difficult because of the small size of the connector. Instead we relied on adapter cables, as with the 127-channel headstage, to connect headstage and array connectors. The two headstages fit perfectly in the protective headcap (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, no 10), which also prevents movements of the headstage itself.</p><p>Depending on the experiment and space needed, we used three different protective headcaps. While the animal was not in an experiment, a flat 4 mm machine-milled transparent polycarbonate headcap with rubber sealing protected the connectors against moisture, dirt, and manipulations (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, no 5). During experiments, we used two specifically designed protective headcaps for the two different wireless headstages. Both were 3D-printed in PA2200 in violet color to aid motion capture. As the 31-channel wireless headstage is connected to the array connectors directly, it extends over the chamber walls when connected to one of the outermost connectors (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, no 6). We designed the respective protective headcap to cover this overlap (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, no 10). The 127-channel wireless headstage (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, no 7) with its adapter board is higher and overlaps the chamber on the side opposite to the connectors. We designed the respective headcap accordingly (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, no 9). The two 96-channel Exilis Headstages were used with the smaller headcap (no 10). For Monkey L, we 3D-printed a version with slightly larger inner dimensions in green polylactic acid (PLA) using fused deposit modeling.</p><p>As the 3D-printed headcaps were only used during recording sessions, that is for less than 2 h, without contact to other animals, and under human observation, we did not add extra sealing against moisture. However, by adding rubber sealing, the internal electronics would be safe even for longer periods of time in a larger and enriched social-housing environment without human supervision.</p></sec><sec id="s4-7"><title>Surgical procedure</title><p>The intracortical electrode arrays and the permanent components of the chamber system were implanted during a single sterile surgery under deep gas anesthesia and analgesia via an IV catheter. Additionally, the animals were prophylactically treated with phenytoin (5–10 mg/kg) for seizure prevention, starting from 1 week before surgery and continuing until 2 weeks post-surgery (fading-in over 1 week), and with systemic antibiotics (monkey K: cobactan 0.032 ml/kg and synolux 0.05 ml/kg 1 day pre-surgery and 2 days post-surgery; monkey L: duphamox, 0.13 ml/kg, 1 day pre-surgery to 1 day post-surgery). During craniotomy, brain pressure was regulated with mannitol (monkey K/L: 16/15.58 ml/kg; on demand). Analgesia was refreshed on a 5 h cycle continuously for 4 post-surgical days using levomethadon (0.28/0.26 mg/kg), daily for 1/3post-surgical days using metacam (0.24/0.26 mg/kg) and for another 4 days (rimadyl, 2.4/1.94 mg/kg) according to demand.</p><p>We implanted six FMAs in the right hemisphere of both monkeys. Each FMA consists of 32 parylene-coated platinum/iridium electrodes and four ground electrodes arranged in four rows of nine electrodes (covering an area of 1.8 mm x 4 mm) staggered in length row-wise, with the longest row opposite the cable and the shortest row closest to the cable. Two FMAs were placed in each of the three target areas: parietal reach region (PRR), dorsal premotor cortex (PMd), and arm-area of primary motor cortex (M1). MRI scans were used to define desired array positions and craniotomy coordinates. As we did not know the location of blood vessels beforehand, the final placing of the arrays was done based on the visible anatomical landmarks. PRR arrays were positioned along the medial wall of the intraparietal sulcus (IPS) starting about 7 mm away from the parieto-occipital sulcus (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), with electrode lengths of 1.5–7.1 mm. M1 arrays were positioned along the frontal wall of the central sulcus, at a laterality between precentral dimple and arcuate spur, with electrode lengths of 1.5–7.1 mm. The longer electrodes of PRR and M1 arrays were located on the side facing the sulcus. PMd arrays were positioned, between arcuate spur, precentral dimple, and the M1 arrays as close to the arcuate spur, with electrode lengths of 1.9–4.5 mm.</p><p>Except for the steps related to our novel chamber system, the procedures for FMA implantation were equivalent to those described in <xref ref-type="bibr" rid="bib91">Schaffelhofer et al., 2015</xref>. The animal was placed in a stereotaxic instrument to stabilize the head and provide a Horsley-Clarke coordinate system. We removed skin and muscles from the top of the skull as much as needed based on our pre-surgical craniotomy planning. Before the craniotomy, we fixed the array connector holder to the skull with a bone screw serving as anchor and embedded in dental cement on the hemisphere opposite to the craniotomy. After removing the bone with a craniotome (DePuy Synthes) and opening the dura in a U-shaped flap for later re-suturing, we oriented and lowered the microelectrode arrays one-by-one using a manual micro-drive (Narishige International Limited, London, UK), which was mounted to the stereotaxic instrument on a ball-and-socket joint. Before insertion, the array connector was put into our array connector holder and fixed with a small amount of dental cement. During insertion, the array itself was held at its back plate under-pressure in a rubber-coated tube connected to a vacuum pump which was attached to the microdrive. We slowly lowered the electrodes about 1 mm every 30 s until the back plate touched the dura mater. We let the array rest for 4 min before removing first the vacuum and then the tube.</p><p>After implanting all arrays, we arranged the cables for minimal strain and closed the dura with sutures between the cables. We placed Duraform (DePuy Synthes) on top, returned the leftover bone from the craniotomy and filled the gaps with bone replacement material (BoneSource, Stryker, Kalamazoo, Michigan). We sealed the craniotomy and covered the exposed bone surface over the full area of the later chamber with Super-Bond (Sun Medical Co Ltd, Moriyama, Japan). We secured the array cables at the entry point to the connectors and filled all cavities in the array connector holder with dental cement. We mounted the chamber with bone screws surrounding implants and craniotomy, positioned the adaptor board holder, and filled the inside of the chamber with dental cement (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Finally, we added the flat protective headcap on the chamber.</p></sec><sec id="s4-8"><title>Neural recordings</title><p>Neural recordings were conducted in both monkeys during the walk-and-reach task in the Reach Cage. We recorded wirelessly from all six arrays simultaneously using the two 96-channel Exilis Headstages. To remove interference between the two headstages, we placed a small metal plate between the two headstages which was connected to the ground of one headstage. We used seven antennas in the cage, which were all connected to both receivers for the respective headstage. The headstages used carrier frequencies of 3.17 GHz and 3.5 GHz, respectively. The signal was digitized on the headstages and sent to two recordings systems, one for each headstage. We used a 128-channel Cerebus system and a 96-channel CerePlex Direct system (both Blackrock Microsystems LLC) for signal processing. For the control session (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), we sued a single 256-channel CerePlex Direct system (Blackrock Microsystems LLC) receiving input from both receivers. <italic>MaCaQuE</italic> sent the trial number at the beginning of each trial to the parallel port of each system. We connected an additional shift register M74HC595 (STMicroelectronics) to the GPIO port of MaCaQuE for interfacing the parallel ports. The recording systems recorded the trial number along with a time stamp for offline data synchronization.</p><p>We calculated data loss rate per trial on the broadband data. The headstage transmits digital data. When it loses connection the recording system repeats the latest value. As wireless data are transmitted in series, a connection loss affects all channels. We looked in the first 32 channels of the broadband data at least four consecutive times for which the data did not change. Then we labeled all consecutive time points as ‘data lost’ for which the data did not change. We did this for both 96-channel recordings separately. As we wanted to estimate the reliability of the 192-channel recording, we considered data loss at times where even only one of the two headstages showed data loss. Then we calculated the percentage of time points with data loss for each session only considering times within trials for which the monkey performed the task correctly. We also calculated the data loss for each trial separately. Only trials with data loss smaller than 5% were considered for further analysis.</p><p>We performed the preprocessing of broadband data and the extraction of waveforms as previously described (<xref ref-type="bibr" rid="bib34">Dann et al., 2016</xref>). First, the raw signal was high-pass filtered using a sliding window median with a window length of 91 samples (~3 ms). Then, we applied a 5000 Hz low-pass using a zero-phase second order Butterworth filter. To remove common noise, we transformed the signal in PCA space per array, removed principle components that represented common signals, and transformed it back (<xref ref-type="bibr" rid="bib75">Musial et al., 2002</xref>). On the resulting signal, spikes were extracted by threshold crossing using a negative or positive threshold. We sorted the extracted spikes manually using Offline Sorter V3 (Plexon Inc, Dallas, Texas). If single-unit isolation was not possible, we assigned the non-differentiable cluster as multi-unit, but otherwise treated the unit the same way in our analysis. We performed offline sorting for the example units (<xref ref-type="fig" rid="fig4">Figure 4</xref>), decoding and encoding analysis (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) but not for the control session (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>). The spike density function for the example units (<xref ref-type="fig" rid="fig4">Figure 4</xref>) wereas computed by convolving spike trains per trial and per unit with a normalized Gaussian with standard deviation of 50 ms. The spike density function was sampled at 200 Hz. The exemplary broadband data in <xref ref-type="fig" rid="fig4">Figure 4</xref> show the data before preprocessing.</p><p>We analyzed the firing rate of all 192 channels in the 12 sessions and of four example units with respect to four different temporal alignments: target cue onset, go cue, start button release, and target acquisition. To quantify neural activity during the delay period and the movement, we analyzed time windows of 500 ms either immediately before or after a respective alignment. We analyzed the modulation of firing rate relative to the position of the reach targets and time window for each unit. We calculated an ANOVA with factors: distance (near, far), position (outer left, mid left, mid right, outer right), and time (before and after the respective alignments, eight time windows). We considered a channel/unit task modulated if there was a significant effect on any factor or interaction. We considered it position modulated if there was a significant main effect on position or an effect on position x distance, position x time, or position x distance x time.</p><p>For the population decoding analysis, we used a linear support vector machine (SVM) on the firing rate within 300 ms time windows. We decoded left vs. right side, that is grouped left-outer and left-mid targets as well as right-outer and right-mid targets. Reach and walk-and-reach movements were analyzed separately. Decoding accuracy was estimated by 20-fold cross validation. The 20-folds always referred to the same trials in each window throughout the timeline. For statistical testing we focused on one time window during memory and one during movement period, respectively. As the shortest trials have a memory period of 400 ms we selected 100–400 ms after the cue as the window for the memory period. For the movement period, we selected 300–0 ms before target acquisition. To determine decoding accuracy above chance level, we performed a one-tailed permutation test as follows. We generated a null distribution of 500 samples by permuting the target direction (left vs. right category) in all trials independently for each monkey and distance, and calculated the 20-fold cross validated decoding accuracy as described before.</p><p>For the population encoding analysis, we calculated the population average of the firing rate modulation individually for each monkey, distance, and movement period as follows: First, we calculated the average firing rate for each unit and target direction; then we took the absolute difference between left and right target directions (same grouping as in the decoding analysis) and averaged the absolute difference over all units per area. We performed a one-tailed permutation test to test if this firing rate difference is significantly higher than expected by chance. For this, we generated a null distribution of 1000 samples and then calculated the population average of the firing rate modulation for each sample. For both permutation tests, a p-value was calculated by the fraction of the null distribution above the value to test. We used Bonferroni multiple comparison correction with a multiplier of 12 (3 areas x 2 distance x two time periods).</p><p>For the control session with and without a narrow passage for walk-and-reach movements (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>), we performed an SVM decoding analysis and calculated the population averaged firing rate modulation. We used 10-fold cross validation and tested if the decoding accuracy changed depending on whether or not the passage was present. To test if the passage had an effect, we used a two-tailed permutation test with 500 surrogate samples by permuting the passage label in all trials independently for each monkey. For the population encoding analysis, we calculated the firing rate modulation by target direction per channel, subtracted the modulation without passage from the modulation with passage, and calculated the average over the population per area. To test if the passage changed the left vs. right firing rate modulation, we generated a null distribution of 1000 samples and calculated the difference in firing rate modulation for each sample. For the permutation tests of the control session, we considered both tails of the null distribution to calculate the p-value. We applied Bonferroni multiple comparison correction with a multiplier of 6 (3 areas x two time periods).</p><p>Raw data and spike data processing was performed with Matlab and visualized using the toolbox <italic>gramm</italic> (<xref ref-type="bibr" rid="bib72">Morel, 2018</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Sina Plümer for help with data collection and technical support, Klaus Heisig and Marvin Kulp for help with mechanical constructions, Swathi Sheshadri, Benjamin Dann, Mariana Eggert Martínez, and Baltasar Rüchardt for help with motion capture, Peer Strogies for help with implant design, Attila Trunk and Ole Fortmann for help with data collection, Pierre Morel, Enrico Ferrea, Michael Fauth, Jan-Matthias Braun, Christian Tetzlaff, and Florentin Wörgötter for helpful discussions, Leonore Burchardt for help with animal training, and Janine Kuntze, Luisa Klotz, and Dirk Prüße for technical support.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con2"><p>Investigation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: Both animals were housed in social groups with one (monkey L) or two (monkey K) male conspecifics in facilities of the German Primate Center. The facilities provide cage sizes exceeding the requirements by German and European regulations, access to an enriched environment including wooden structures and various toys (Calapai et al. 2017). All procedures have been approved by the responsible regional government office [Niedersächsisches Landesamt für Verbraucherschutz und Lebensmittelsicherheit (LAVES)] under permit numbers 3392 42502-04-13/1100 and comply with German Law and the European Directive 2010/63/EU regulating use of animals in research.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Overview of neurophysiology studies with unrestrained monkeys.</title><p>This table presents an overview of current neurophysiology studies with unrestrained monkeys. The Reach Cage provides the only environment capable of instructing the animal to control start and end times of a desired movement, which for example allows training of animals to withhold a movement and study movement planning. Also, while previous studies studied a variety of behavior, instructed goal-directed movements were always direct food (source) directed movements. Only the Reach Cage can dissociate motor goals from food sources. There are four other studies that present multiple movement goals. There are locomotion studies that incorporate 3D motion capture, but these are not markerless and none showed 3D kinematics of reaching behavior. Note that other studies have shown 3D markerless motion capture of freely behaving monkeys (<xref ref-type="bibr" rid="bib2">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib76">Nakamura et al., 2016</xref>); however, without neurophysiological recordings.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51322-supp1-v2.xlsx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-51322-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data (schematics, soft- and hardware documentation) for constructing the MaCaQuE or equivalent systems is made available via GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/sensorimotorgroupdpz/MaCaQuE">https://github.com/sensorimotorgroupdpz/MaCaQuE</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>M</given-names></name><name><surname>Gail</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>sensorimotorgroupdpz/MaCaQuE</data-title><source>Zenodo</source><pub-id assigning-authority="Zenodo" pub-id-type="doi">10.5281/zenodo.3685793</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aflalo</surname> <given-names>T</given-names></name><name><surname>Kellis</surname> <given-names>S</given-names></name><name><surname>Klaes</surname> <given-names>C</given-names></name><name><surname>Lee</surname> <given-names>B</given-names></name><name><surname>Shi</surname> <given-names>Y</given-names></name><name><surname>Pejsa</surname> <given-names>K</given-names></name><name><surname>Shanfield</surname> <given-names>K</given-names></name><name><surname>Hayes-Jackson</surname> <given-names>S</given-names></name><name><surname>Aisen</surname> <given-names>M</given-names></name><name><surname>Heck</surname> <given-names>C</given-names></name><name><surname>Liu</surname> <given-names>C</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neurophysiology decoding motor imagery from the posterior parietal cortex of a tetraplegic human</article-title><source>Science</source><volume>348</volume><fpage>906</fpage><lpage>910</lpage><pub-id pub-id-type="doi">10.1126/science.aaa5417</pub-id><pub-id pub-id-type="pmid">25999506</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bala</surname> <given-names>PC</given-names></name><name><surname>Eisenreich</surname> <given-names>BR</given-names></name><name><surname>Bum</surname> <given-names>S</given-names></name><name><surname>Yoo</surname> <given-names>M</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Park</surname> <given-names>HS</given-names></name><name><surname>Zimmermann</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>OpenMonkeyStudio : automated markerless pose estimation in freely moving macaques</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.31.928861</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballesta</surname> <given-names>S</given-names></name><name><surname>Reymond</surname> <given-names>G</given-names></name><name><surname>Pozzobon</surname> <given-names>M</given-names></name><name><surname>Duhamel</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A real-time 3D video tracking system for monitoring primate groups</article-title><source>Journal of Neuroscience Methods</source><volume>234</volume><fpage>147</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.05.022</pub-id><pub-id pub-id-type="pmid">24875622</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bansal</surname> <given-names>AK</given-names></name><name><surname>Truccolo</surname> <given-names>W</given-names></name><name><surname>Vargas-Irwin</surname> <given-names>CE</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decoding 3D reach and grasp from hybrid signals in motor and premotor cortices: spikes, multiunit activity, and local field potentials</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>1337</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1152/jn.00781.2011</pub-id><pub-id pub-id-type="pmid">22157115</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batista</surname> <given-names>AP</given-names></name><name><surname>Buneo</surname> <given-names>CA</given-names></name><name><surname>Snyder</surname> <given-names>LH</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Reach plans in eye-centered coordinates</article-title><source>Science</source><volume>285</volume><fpage>257</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1126/science.285.5425.257</pub-id><pub-id pub-id-type="pmid">10398603</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname> <given-names>M</given-names></name><name><surname>Calapai</surname> <given-names>A</given-names></name><name><surname>Stephan</surname> <given-names>V</given-names></name><name><surname>Niessing</surname> <given-names>M</given-names></name><name><surname>Burchardt</surname> <given-names>L</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Standardized automated training of rhesus monkeys for neuroscience research in their housing environment</article-title><source>Journal of Neurophysiology</source><volume>119</volume><fpage>796</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1152/jn.00614.2017</pub-id><pub-id pub-id-type="pmid">29142094</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname> <given-names>M</given-names></name><name><surname>Neumann</surname> <given-names>P</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Peri-hand space expands beyond reach in the context of walk-and-reach movements</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>3013</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-39520-8</pub-id><pub-id pub-id-type="pmid">30816205</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Berger</surname> <given-names>M</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>sensorimotorgroupdpz/MaCaQuE</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo.3685793">http://doi.org/10.5281/zenodo.3685793</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berti</surname> <given-names>A</given-names></name><name><surname>Frassinetti</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>When far becomes near: remapping of space by tool use</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>415</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1162/089892900562237</pub-id><pub-id pub-id-type="pmid">10931768</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bjoertomt</surname> <given-names>O</given-names></name><name><surname>Cowey</surname> <given-names>A</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Spatial neglect in near and far space investigated by repetitive transcranial magnetic stimulation</article-title><source>Brain</source><volume>125</volume><fpage>2012</fpage><lpage>2022</lpage><pub-id pub-id-type="doi">10.1093/brain/awf211</pub-id><pub-id pub-id-type="pmid">12183347</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanke</surname> <given-names>O</given-names></name><name><surname>Slater</surname> <given-names>M</given-names></name><name><surname>Serino</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Behavioral, neural, and computational principles of bodily Self-Consciousness</article-title><source>Neuron</source><volume>88</volume><fpage>145</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.029</pub-id><pub-id pub-id-type="pmid">26447578</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonini</surname> <given-names>L</given-names></name><name><surname>Maranesi</surname> <given-names>M</given-names></name><name><surname>Livi</surname> <given-names>A</given-names></name><name><surname>Fogassi</surname> <given-names>L</given-names></name><name><surname>Rizzolatti</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Space-dependent representation of objects and other's action in monkey ventral premotor grasping neurons</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>4108</fpage><lpage>4119</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4187-13.2014</pub-id><pub-id pub-id-type="pmid">24623789</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname> <given-names>M</given-names></name><name><surname>Cohen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Rubber hands 'feel' touch that eyes see</article-title><source>Nature</source><volume>391</volume><elocation-id>756</elocation-id><pub-id pub-id-type="doi">10.1038/35784</pub-id><pub-id pub-id-type="pmid">9486643</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouton</surname> <given-names>CE</given-names></name><name><surname>Shaikhouni</surname> <given-names>A</given-names></name><name><surname>Annetta</surname> <given-names>NV</given-names></name><name><surname>Bockbrader</surname> <given-names>MA</given-names></name><name><surname>Friedenberg</surname> <given-names>DA</given-names></name><name><surname>Nielson</surname> <given-names>DM</given-names></name><name><surname>Sharma</surname> <given-names>G</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name><name><surname>Glenn</surname> <given-names>BC</given-names></name><name><surname>Mysiw</surname> <given-names>WJ</given-names></name><name><surname>Morgan</surname> <given-names>AG</given-names></name><name><surname>Deogaonkar</surname> <given-names>M</given-names></name><name><surname>Rezai</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Restoring cortical control of functional movement in a human with quadriplegia</article-title><source>Nature</source><volume>533</volume><fpage>247</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1038/nature17435</pub-id><pub-id pub-id-type="pmid">27074513</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brozzoli</surname> <given-names>C</given-names></name><name><surname>Pavani</surname> <given-names>F</given-names></name><name><surname>Urquizar</surname> <given-names>C</given-names></name><name><surname>Cardinali</surname> <given-names>L</given-names></name><name><surname>Farnè</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Grasping actions remap peripersonal space</article-title><source>NeuroReport</source><volume>20</volume><fpage>913</fpage><lpage>917</lpage><pub-id pub-id-type="doi">10.1097/WNR.0b013e32832c0b9b</pub-id><pub-id pub-id-type="pmid">19512951</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brozzoli</surname> <given-names>C</given-names></name><name><surname>Cardinali</surname> <given-names>L</given-names></name><name><surname>Pavani</surname> <given-names>F</given-names></name><name><surname>Farnè</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Action-specific remapping of peripersonal space</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>796</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.10.009</pub-id><pub-id pub-id-type="pmid">19837102</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buneo</surname> <given-names>CA</given-names></name><name><surname>Jarvis</surname> <given-names>MR</given-names></name><name><surname>Batista</surname> <given-names>AP</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Direct visuomotor transformations for reaching</article-title><source>Nature</source><volume>416</volume><fpage>632</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1038/416632a</pub-id><pub-id pub-id-type="pmid">11948351</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buneo</surname> <given-names>CA</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Integration of target and hand position signals in the posterior parietal cortex: effects of workspace and hand vision</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>187</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1152/jn.00137.2011</pub-id><pub-id pub-id-type="pmid">22457457</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caggiano</surname> <given-names>V</given-names></name><name><surname>Fogassi</surname> <given-names>L</given-names></name><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Thier</surname> <given-names>P</given-names></name><name><surname>Casile</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Mirror neurons differentially encode the peripersonal and extrapersonal space of monkeys</article-title><source>Science</source><volume>324</volume><fpage>403</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1126/science.1166818</pub-id><pub-id pub-id-type="pmid">19372433</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calapai</surname> <given-names>A</given-names></name><name><surname>Berger</surname> <given-names>M</given-names></name><name><surname>Niessing</surname> <given-names>M</given-names></name><name><surname>Heisig</surname> <given-names>K</given-names></name><name><surname>Brockhausen</surname> <given-names>R</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A cage-based training, cognitive testing and enrichment system optimized for rhesus macaques in neuroscience research</article-title><source>Behavior Research Methods</source><volume>49</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.3758/s13428-016-0707-3</pub-id><pub-id pub-id-type="pmid">26896242</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Capogrosso</surname> <given-names>M</given-names></name><name><surname>Milekovic</surname> <given-names>T</given-names></name><name><surname>Borton</surname> <given-names>D</given-names></name><name><surname>Wagner</surname> <given-names>F</given-names></name><name><surname>Moraud</surname> <given-names>EM</given-names></name><name><surname>Mignardot</surname> <given-names>JB</given-names></name><name><surname>Buse</surname> <given-names>N</given-names></name><name><surname>Gandar</surname> <given-names>J</given-names></name><name><surname>Barraud</surname> <given-names>Q</given-names></name><name><surname>Xing</surname> <given-names>D</given-names></name><name><surname>Rey</surname> <given-names>E</given-names></name><name><surname>Duis</surname> <given-names>S</given-names></name><name><surname>Jianzhong</surname> <given-names>Y</given-names></name><name><surname>Ko</surname> <given-names>WK</given-names></name><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Detemple</surname> <given-names>P</given-names></name><name><surname>Denison</surname> <given-names>T</given-names></name><name><surname>Micera</surname> <given-names>S</given-names></name><name><surname>Bezard</surname> <given-names>E</given-names></name><name><surname>Bloch</surname> <given-names>J</given-names></name><name><surname>Courtine</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A brain-spine interface alleviating gait deficits after spinal cord injury in primates</article-title><source>Nature</source><volume>539</volume><fpage>284</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1038/nature20118</pub-id><pub-id pub-id-type="pmid">27830790</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmena</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Advances in Neuroprosthetic learning and control</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001561</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001561</pub-id><pub-id pub-id-type="pmid">23700383</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Davis</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Camera Placement Considering Occlusion for Robust Motion Capture</source><publisher-name>Stanford University</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheney</surname> <given-names>PD</given-names></name><name><surname>Fetz</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Functional classes of primate corticomotoneuronal cells and their relation to active force</article-title><source>Journal of Neurophysiology</source><volume>44</volume><fpage>773</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1152/jn.1980.44.4.773</pub-id><pub-id pub-id-type="pmid">6253605</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chestek</surname> <given-names>CA</given-names></name><name><surname>Gilja</surname> <given-names>V</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Kier</surname> <given-names>RJ</given-names></name><name><surname>Solzbacher</surname> <given-names>F</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Harrison</surname> <given-names>RR</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>HermesC: low-power wireless neural recording system for freely moving primates</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>17</volume><fpage>330</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2009.2023293</pub-id><pub-id pub-id-type="pmid">19497829</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christopoulos</surname> <given-names>VN</given-names></name><name><surname>Bonaiuto</surname> <given-names>J</given-names></name><name><surname>Kagan</surname> <given-names>I</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inactivation of parietal reach region affects reaching but not saccade choices in internally guided decisions</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11719</fpage><lpage>11728</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1068-15.2015</pub-id><pub-id pub-id-type="pmid">26290248</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Making decisions through a distributed consensus</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>927</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.05.007</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colby</surname> <given-names>CL</given-names></name><name><surname>Goldberg</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>SPACE AND ATTENTION IN PARIETAL CORTEX</article-title><source>Annual Review of Neuroscience</source><volume>22</volume><fpage>319</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.22.1.319</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collinger</surname> <given-names>JL</given-names></name><name><surname>Wodlinger</surname> <given-names>B</given-names></name><name><surname>Downey</surname> <given-names>JE</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name><name><surname>Weber</surname> <given-names>DJ</given-names></name><name><surname>McMorland</surname> <given-names>AJC</given-names></name><name><surname>Velliste</surname> <given-names>M</given-names></name><name><surname>Boninger</surname> <given-names>ML</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>High-performance neuroprosthetic control by an individual with tetraplegia</article-title><source>The Lancet</source><volume>381</volume><fpage>557</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(12)61816-9</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Courellis</surname> <given-names>HS</given-names></name><name><surname>Nummela</surname> <given-names>SU</given-names></name><name><surname>Metke</surname> <given-names>M</given-names></name><name><surname>Diehl</surname> <given-names>GW</given-names></name><name><surname>Bussell</surname> <given-names>R</given-names></name><name><surname>Cauwenberghs</surname> <given-names>G</given-names></name><name><surname>Miller</surname> <given-names>CT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spatial encoding in primate Hippocampus during free navigation</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000546</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000546</pub-id><pub-id pub-id-type="pmid">31815940</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Courtine</surname> <given-names>G</given-names></name><name><surname>Roy</surname> <given-names>RR</given-names></name><name><surname>Hodgson</surname> <given-names>J</given-names></name><name><surname>McKay</surname> <given-names>H</given-names></name><name><surname>Raven</surname> <given-names>J</given-names></name><name><surname>Zhong</surname> <given-names>H</given-names></name><name><surname>Yang</surname> <given-names>H</given-names></name><name><surname>Tuszynski</surname> <given-names>MH</given-names></name><name><surname>Edgerton</surname> <given-names>VR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Kinematic and EMG determinants in quadrupedal locomotion of a non-human primate (Rhesus)</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>3127</fpage><lpage>3145</lpage><pub-id pub-id-type="doi">10.1152/jn.01073.2004</pub-id><pub-id pub-id-type="pmid">15647397</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crammond</surname> <given-names>DJ</given-names></name><name><surname>Kalaska</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Modulation of preparatory neuronal activity in dorsal premotor cortex due to stimulus-response compatibility</article-title><source>Journal of Neurophysiology</source><volume>71</volume><fpage>1281</fpage><lpage>1284</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.71.3.1281</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crammond</surname> <given-names>DJ</given-names></name><name><surname>Kalaska</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Prior information in motor and premotor cortex: activity during the delay period and effect on pre-movement activity</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>986</fpage><lpage>1005</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.2.986</pub-id><pub-id pub-id-type="pmid">10938322</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dann</surname> <given-names>B</given-names></name><name><surname>Michaels</surname> <given-names>JA</given-names></name><name><surname>Schaffelhofer</surname> <given-names>S</given-names></name><name><surname>Scherberger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Uniting functional network topology and oscillations in the fronto-parietal single unit network of behaving primates</article-title><source>eLife</source><volume>5</volume><elocation-id>e15719</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.15719</pub-id><pub-id pub-id-type="pmid">27525488</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donchin</surname> <given-names>O</given-names></name><name><surname>Gribova</surname> <given-names>A</given-names></name><name><surname>Steinberg</surname> <given-names>O</given-names></name><name><surname>Bergman</surname> <given-names>H</given-names></name><name><surname>Vaadia</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Primary motor cortex is involved in bimanual coordination</article-title><source>Nature</source><volume>395</volume><fpage>274</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1038/26220</pub-id><pub-id pub-id-type="pmid">9751054</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farnè</surname> <given-names>A</given-names></name><name><surname>Serino</surname> <given-names>A</given-names></name><name><surname>van der Stoep</surname> <given-names>N</given-names></name><name><surname>Spence</surname> <given-names>C</given-names></name><name><surname>Di Luca</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Depth: the forgotten dimension <italic>multisensory Research</italic></article-title><source>Psychology</source><volume>29</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1163/22134808-00002525</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez-Leon</surname> <given-names>JA</given-names></name><name><surname>Parajuli</surname> <given-names>A</given-names></name><name><surname>Franklin</surname> <given-names>R</given-names></name><name><surname>Sorenson</surname> <given-names>M</given-names></name><name><surname>Felleman</surname> <given-names>DJ</given-names></name><name><surname>Hansen</surname> <given-names>BJ</given-names></name><name><surname>Hu</surname> <given-names>M</given-names></name><name><surname>Dragoi</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A wireless transmission neural interface system for unconstrained non-human primates</article-title><source>Journal of Neural Engineering</source><volume>12</volume><elocation-id>056005</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/12/5/056005</pub-id><pub-id pub-id-type="pmid">26269496</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>JD</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Freifeld</surname> <given-names>O</given-names></name><name><surname>Gao</surname> <given-names>H</given-names></name><name><surname>Walker</surname> <given-names>R</given-names></name><name><surname>I Ryu</surname> <given-names>S</given-names></name><name><surname>H Meng</surname> <given-names>T</given-names></name><name><surname>Murmann</surname> <given-names>B</given-names></name><name><surname>J Black</surname> <given-names>M</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A freely-moving monkey treadmill model</article-title><source>Journal of Neural Engineering</source><volume>11</volume><elocation-id>046020</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/11/4/046020</pub-id><pub-id pub-id-type="pmid">24995476</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gail</surname> <given-names>A</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural dynamics in monkey parietal reach region reflect context-specific sensorimotor transformations</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>9376</fpage><lpage>9384</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1570-06.2006</pub-id><pub-id pub-id-type="pmid">16971521</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname> <given-names>AP</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name><name><surname>Kettner</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Neuronal population coding of movement direction</article-title><source>Science</source><volume>233</volume><fpage>1416</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1126/science.3749885</pub-id><pub-id pub-id-type="pmid">3749885</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giglia</surname> <given-names>G</given-names></name><name><surname>Pia</surname> <given-names>L</given-names></name><name><surname>Folegatti</surname> <given-names>A</given-names></name><name><surname>Puma</surname> <given-names>A</given-names></name><name><surname>Fierro</surname> <given-names>B</given-names></name><name><surname>Cosentino</surname> <given-names>G</given-names></name><name><surname>Berti</surname> <given-names>A</given-names></name><name><surname>Brighina</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Far space remapping by tool use: a rTMS study over the right posterior parietal cortex</article-title><source>Brain Stimulation</source><volume>8</volume><fpage>795</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1016/j.brs.2015.01.412</pub-id><pub-id pub-id-type="pmid">25732371</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilja</surname> <given-names>V</given-names></name><name><surname>Chestek</surname> <given-names>CA</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Foster</surname> <given-names>J</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Autonomous head-mounted electrophysiology systems for freely behaving primates</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>676</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.06.007</pub-id><pub-id pub-id-type="pmid">20655733</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilja</surname> <given-names>V</given-names></name><name><surname>Pandarinath</surname> <given-names>C</given-names></name><name><surname>Blabe</surname> <given-names>CH</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Simeral</surname> <given-names>JD</given-names></name><name><surname>Sarma</surname> <given-names>AA</given-names></name><name><surname>Sorice</surname> <given-names>BL</given-names></name><name><surname>Perge</surname> <given-names>JA</given-names></name><name><surname>Jarosiewicz</surname> <given-names>B</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Henderson</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Clinical translation of a high-performance neural prosthesis</article-title><source>Nature Medicine</source><volume>21</volume><fpage>1142</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1038/nm.3953</pub-id><pub-id pub-id-type="pmid">26413781</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname> <given-names>MS</given-names></name><name><surname>Hu</surname> <given-names>XT</given-names></name><name><surname>Gross</surname> <given-names>CG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Visuospatial properties of ventral premotor cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>2268</fpage><lpage>2292</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.5.2268</pub-id><pub-id pub-id-type="pmid">9163357</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname> <given-names>MS</given-names></name><name><surname>Cooke</surname> <given-names>DF</given-names></name><name><surname>Taylor</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Coding the location of the arm by sight</article-title><source>Science</source><volume>290</volume><fpage>1782</fpage><lpage>1786</lpage><pub-id pub-id-type="doi">10.1126/science.290.5497.1782</pub-id><pub-id pub-id-type="pmid">11099420</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hage</surname> <given-names>SR</given-names></name><name><surname>Jurgens</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>On the role of the pontine brainstem in vocal pattern generation: a telemetric Single-Unit recording study in the squirrel monkey</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>7105</fpage><lpage>7115</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1024-06.2006</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halligan</surname> <given-names>PW</given-names></name><name><surname>Marshall</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Left neglect for near but not far space in man</article-title><source>Nature</source><volume>350</volume><fpage>498</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1038/350498a0</pub-id><pub-id pub-id-type="pmid">2014049</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauschild</surname> <given-names>M</given-names></name><name><surname>Mulliken</surname> <given-names>GH</given-names></name><name><surname>Fineman</surname> <given-names>I</given-names></name><name><surname>Loeb</surname> <given-names>GE</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cognitive signals for brain-machine interfaces in posterior parietal cortex include continuous 3D trajectory commands</article-title><source>PNAS</source><volume>109</volume><fpage>17075</fpage><lpage>17080</lpage><pub-id pub-id-type="doi">10.1073/pnas.1215092109</pub-id><pub-id pub-id-type="pmid">23027946</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hazama</surname> <given-names>Y</given-names></name><name><surname>Tamura</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effects of self-locomotion on the activity of place cells in the Hippocampus of a freely behaving monkey</article-title><source>Neuroscience Letters</source><volume>701</volume><fpage>32</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2019.02.009</pub-id><pub-id pub-id-type="pmid">30738872</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Bacher</surname> <given-names>D</given-names></name><name><surname>Jarosiewicz</surname> <given-names>B</given-names></name><name><surname>Masse</surname> <given-names>NY</given-names></name><name><surname>Simeral</surname> <given-names>JD</given-names></name><name><surname>Vogel</surname> <given-names>J</given-names></name><name><surname>Haddadin</surname> <given-names>S</given-names></name><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Cash</surname> <given-names>SS</given-names></name><name><surname>van der Smagt</surname> <given-names>P</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reach and grasp by people with tetraplegia using a neurally controlled robotic arm</article-title><source>Nature</source><volume>485</volume><fpage>372</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1038/nature11076</pub-id><pub-id pub-id-type="pmid">22596161</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname> <given-names>NP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Does tool use extend peripersonal space? A review and re-analysis</article-title><source>Experimental Brain Research</source><volume>218</volume><fpage>273</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1007/s00221-012-3042-7</pub-id><pub-id pub-id-type="pmid">22392444</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname> <given-names>EJ</given-names></name><name><surname>Hauschild</surname> <given-names>M</given-names></name><name><surname>Wilke</surname> <given-names>M</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Inactivation of the parietal reach region causes optic ataxia, impairing reaches but not saccades</article-title><source>Neuron</source><volume>76</volume><fpage>1021</fpage><lpage>1029</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.030</pub-id><pub-id pub-id-type="pmid">23217749</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iriki</surname> <given-names>A</given-names></name><name><surname>Tanaka</surname> <given-names>M</given-names></name><name><surname>Iwamura</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Coding of modified body schema during tool use by macaque postcentral neurones</article-title><source>Neuroreport</source><volume>7</volume><fpage>2325</fpage><lpage>2330</lpage><pub-id pub-id-type="doi">10.1097/00001756-199610020-00010</pub-id><pub-id pub-id-type="pmid">8951846</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname> <given-names>A</given-names></name><name><surname>Mavoori</surname> <given-names>J</given-names></name><name><surname>Fetz</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Long-term motor cortex plasticity induced by an electronic neural implant</article-title><source>Nature</source><volume>444</volume><fpage>56</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1038/nature05226</pub-id><pub-id pub-id-type="pmid">17057705</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname> <given-names>A</given-names></name><name><surname>Mavoori</surname> <given-names>J</given-names></name><name><surname>Fetz</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Correlations between the same motor cortex cells and arm muscles during a trained task, free behavior, and natural sleep in the macaque monkey</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>360</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1152/jn.00710.2006</pub-id><pub-id pub-id-type="pmid">17021028</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The roles of monkey M1 neuron classes in movement preparation and execution</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>817</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1152/jn.00892.2011</pub-id><pub-id pub-id-type="pmid">23699057</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klaes</surname> <given-names>C</given-names></name><name><surname>Westendorff</surname> <given-names>S</given-names></name><name><surname>Chakrabarti</surname> <given-names>S</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Choosing goals, not rules: deciding among rule-based action plans</article-title><source>Neuron</source><volume>70</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.053</pub-id><pub-id pub-id-type="pmid">21555078</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuang</surname> <given-names>S</given-names></name><name><surname>Morel</surname> <given-names>P</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Planning movements in visual and physical space in monkey posterior parietal cortex</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>731</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu312</pub-id><pub-id pub-id-type="pmid">25576535</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Labuguen</surname> <given-names>R</given-names></name><name><surname>Bardeloza</surname> <given-names>DK</given-names></name><name><surname>Negrete</surname> <given-names>SB</given-names></name><name><surname>Matsumoto</surname> <given-names>J</given-names></name><name><surname>Inoue</surname> <given-names>K</given-names></name><name><surname>Shibata</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Primate markerless pose estimation and movement analysis using DeepLabCut</article-title><conf-name>Joint 8th International Conference on Informatics, Electronics &amp; Vision (ICIEV) And 2019 3rd International Conference on Imaging, Vision &amp; Pattern Recognition (IcIVPR)</conf-name><fpage>297</fpage><lpage>300</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Libey</surname> <given-names>T</given-names></name><name><surname>Fetz</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Open-Source, low cost, Free-Behavior monitoring, and reward system for neuroscience research in Non-human primates</article-title><source>Frontiers in Neuroscience</source><volume>11</volume><elocation-id>265</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2017.00265</pub-id><pub-id pub-id-type="pmid">28559792</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludvig</surname> <given-names>N</given-names></name><name><surname>Tang</surname> <given-names>HM</given-names></name><name><surname>Gohil</surname> <given-names>BC</given-names></name><name><surname>Botero</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Detecting location-specific neuronal firing rate increases in the Hippocampus of freely-moving monkeys</article-title><source>Brain Research</source><volume>1014</volume><fpage>97</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2004.03.071</pub-id><pub-id pub-id-type="pmid">15212996</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maravita</surname> <given-names>A</given-names></name><name><surname>Spence</surname> <given-names>C</given-names></name><name><surname>Kennett</surname> <given-names>S</given-names></name><name><surname>Driver</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Tool-use changes multimodal spatial interactions between vision and touch in normal humans</article-title><source>Cognition</source><volume>83</volume><fpage>B25</fpage><lpage>B34</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(02)00003-3</pub-id><pub-id pub-id-type="pmid">11869727</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maravita</surname> <given-names>A</given-names></name><name><surname>Spence</surname> <given-names>C</given-names></name><name><surname>Driver</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Multisensory integration and the body schema: close to hand and within reach</article-title><source>Current Biology</source><volume>13</volume><fpage>R531</fpage><lpage>R539</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(03)00449-4</pub-id><pub-id pub-id-type="pmid">12842033</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maravita</surname> <given-names>A</given-names></name><name><surname>Iriki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Tools for the body (schema)</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>79</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.12.008</pub-id><pub-id pub-id-type="pmid">15588812</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martínez-Vázquez</surname> <given-names>P</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Directed interaction between monkey premotor and posterior parietal cortex during Motor-Goal retrieval from working memory</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>1866</fpage><lpage>1881</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy035</pub-id><pub-id pub-id-type="pmid">29481586</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Yüksekgönül</surname> <given-names>M</given-names></name><name><surname>Rogers</surname> <given-names>B</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pretraining boosts out-of-domain robustness for pose estimation </article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.11229">https://arxiv.org/abs/1909.11229</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeslund</surname> <given-names>TB</given-names></name><name><surname>Hilton</surname> <given-names>A</given-names></name><name><surname>Krüger</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A survey of advances in vision-based human motion capture and analysis</article-title><source>Computer Vision and Image Understanding</source><volume>104</volume><fpage>90</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2006.08.002</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mooshagian</surname> <given-names>E</given-names></name><name><surname>Wang</surname> <given-names>C</given-names></name><name><surname>Holmes</surname> <given-names>CD</given-names></name><name><surname>Snyder</surname> <given-names>LH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Single units in the posterior parietal cortex encode patterns of bimanual coordination</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>1549</fpage><lpage>1567</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx052</pub-id><pub-id pub-id-type="pmid">28369392</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mooshagian</surname> <given-names>E</given-names></name><name><surname>Snyder</surname> <given-names>LH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial eye-hand coordination during bimanual reaching is not systematically coded in either LIP or PRR</article-title><source>PNAS</source><volume>115</volume><fpage>E3817</fpage><lpage>E3826</lpage><pub-id pub-id-type="doi">10.1073/pnas.1718267115</pub-id><pub-id pub-id-type="pmid">29610356</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morel</surname> <given-names>P</given-names></name><name><surname>Ferrea</surname> <given-names>E</given-names></name><name><surname>Taghizadeh-Sarshouri</surname> <given-names>B</given-names></name><name><surname>Audí</surname> <given-names>JM</given-names></name><name><surname>Ruff</surname> <given-names>R</given-names></name><name><surname>Hoffmann</surname> <given-names>KP</given-names></name><name><surname>Lewis</surname> <given-names>S</given-names></name><name><surname>Russold</surname> <given-names>M</given-names></name><name><surname>Dietl</surname> <given-names>H</given-names></name><name><surname>Abu-Saleh</surname> <given-names>L</given-names></name><name><surname>Schroeder</surname> <given-names>D</given-names></name><name><surname>Krautschneider</surname> <given-names>W</given-names></name><name><surname>Meiners</surname> <given-names>T</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Long-term decoding of movement force and direction with a wireless myoelectric implant</article-title><source>Journal of Neural Engineering</source><volume>13</volume><elocation-id>016002</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/13/1/016002</pub-id><pub-id pub-id-type="pmid">26643959</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morel</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gramm: grammar of graphics plotting in matlab</article-title><source>The Journal of Open Source Software</source><volume>3</volume><elocation-id>568</elocation-id><pub-id pub-id-type="doi">10.21105/joss.00568</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulliken</surname> <given-names>GH</given-names></name><name><surname>Musallam</surname> <given-names>S</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Forward estimation of movement state in posterior parietal cortex</article-title><source>PNAS</source><volume>105</volume><fpage>8170</fpage><lpage>8177</lpage><pub-id pub-id-type="doi">10.1073/pnas.0802602105</pub-id><pub-id pub-id-type="pmid">18499800</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musallam</surname> <given-names>S</given-names></name><name><surname>Corneil</surname> <given-names>BD</given-names></name><name><surname>Greger</surname> <given-names>B</given-names></name><name><surname>Scherberger</surname> <given-names>H</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Cognitive control signals for neural prosthetics</article-title><source>Science</source><volume>305</volume><fpage>258</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1126/science.1097938</pub-id><pub-id pub-id-type="pmid">15247483</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musial</surname> <given-names>PG</given-names></name><name><surname>Baker</surname> <given-names>SN</given-names></name><name><surname>Gerstein</surname> <given-names>GL</given-names></name><name><surname>King</surname> <given-names>EA</given-names></name><name><surname>Keating</surname> <given-names>JG</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Signal-to-noise ratio improvement in multiple electrode recording</article-title><source>Journal of Neuroscience Methods</source><volume>115</volume><fpage>29</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/S0165-0270(01)00516-7</pub-id><pub-id pub-id-type="pmid">11897361</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakamura</surname> <given-names>T</given-names></name><name><surname>Matsumoto</surname> <given-names>J</given-names></name><name><surname>Nishimaru</surname> <given-names>H</given-names></name><name><surname>Bretas</surname> <given-names>RV</given-names></name><name><surname>Takamura</surname> <given-names>Y</given-names></name><name><surname>Hori</surname> <given-names>E</given-names></name><name><surname>Ono</surname> <given-names>T</given-names></name><name><surname>Nishijo</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A markerless 3D computerized motion capture system incorporating a skeleton model for monkeys</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0166154</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0166154</pub-id><pub-id pub-id-type="pmid">27812205</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname> <given-names>T</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>A</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title><source>Nature Protocols</source><volume>14</volume><fpage>2152</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><pub-id pub-id-type="pmid">31227823</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niebergall</surname> <given-names>R</given-names></name><name><surname>Khayat</surname> <given-names>PS</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name><name><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multifocal attention filters targets from distracters within and beyond primate MT neurons' receptive field boundaries</article-title><source>Neuron</source><volume>72</volume><fpage>1067</fpage><lpage>1079</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.10.013</pub-id><pub-id pub-id-type="pmid">22196340</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummela</surname> <given-names>SU</given-names></name><name><surname>Jovanovic</surname> <given-names>V</given-names></name><name><surname>de la Mothe</surname> <given-names>L</given-names></name><name><surname>Miller</surname> <given-names>CT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Social Context-Dependent activity in marmoset frontal cortex populations during natural conversations</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7036</fpage><lpage>7047</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0702-17.2017</pub-id><pub-id pub-id-type="pmid">28630255</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orsborn</surname> <given-names>AL</given-names></name><name><surname>Moorman</surname> <given-names>HG</given-names></name><name><surname>Overduin</surname> <given-names>SA</given-names></name><name><surname>Shanechi</surname> <given-names>MM</given-names></name><name><surname>Dimitrov</surname> <given-names>DF</given-names></name><name><surname>Carmena</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Closed-loop decoder adaptation shapes neural plasticity for skillful neuroprosthetic control</article-title><source>Neuron</source><volume>82</volume><fpage>1380</fpage><lpage>1393</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.048</pub-id><pub-id pub-id-type="pmid">24945777</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pavani</surname> <given-names>F</given-names></name><name><surname>Spence</surname> <given-names>C</given-names></name><name><surname>Driver</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Visual capture of touch: out-of-the-body experiences with rubber gloves</article-title><source>Psychological Science</source><volume>11</volume><fpage>353</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00270</pub-id><pub-id pub-id-type="pmid">11228904</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peikon</surname> <given-names>ID</given-names></name><name><surname>Fitzsimmons</surname> <given-names>NA</given-names></name><name><surname>Lebedev</surname> <given-names>MA</given-names></name><name><surname>Nicolelis</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Three-dimensional, automated, real-time video system for tracking limb motion in brain-machine interface studies</article-title><source>Journal of Neuroscience Methods</source><volume>180</volume><fpage>224</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2009.03.010</pub-id><pub-id pub-id-type="pmid">19464514</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesaran</surname> <given-names>B</given-names></name><name><surname>Nelson</surname> <given-names>MJ</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dorsal premotor neurons encode the relative position of the hand, eye, and goal during reach planning</article-title><source>Neuron</source><volume>51</volume><fpage>125</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.05.025</pub-id><pub-id pub-id-type="pmid">16815337</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponce</surname> <given-names>CR</given-names></name><name><surname>Genecin</surname> <given-names>MP</given-names></name><name><surname>Perez-Melara</surname> <given-names>G</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Automated chair-training of rhesus macaques</article-title><source>Journal of Neuroscience Methods</source><volume>263</volume><fpage>75</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.01.024</pub-id><pub-id pub-id-type="pmid">26854396</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajangam</surname> <given-names>S</given-names></name><name><surname>Tseng</surname> <given-names>PH</given-names></name><name><surname>Yin</surname> <given-names>A</given-names></name><name><surname>Lehew</surname> <given-names>G</given-names></name><name><surname>Schwarz</surname> <given-names>D</given-names></name><name><surname>Lebedev</surname> <given-names>MA</given-names></name><name><surname>Nicolelis</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Wireless cortical Brain-Machine interface for Whole-Body navigation in primates</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>22170</elocation-id><pub-id pub-id-type="doi">10.1038/srep22170</pub-id><pub-id pub-id-type="pmid">26938468</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Scandolara</surname> <given-names>C</given-names></name><name><surname>Matelli</surname> <given-names>M</given-names></name><name><surname>Gentilucci</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Afferent properties of periarcuate neurons in macaque monkeys. II. visual responses</article-title><source>Behavioural Brain Research</source><volume>2</volume><fpage>147</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(81)90053-X</pub-id><pub-id pub-id-type="pmid">7248055</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Fadiga</surname> <given-names>L</given-names></name><name><surname>Fogassi</surname> <given-names>L</given-names></name><name><surname>Gallese</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The space around Us</article-title><source>Science</source><volume>277</volume><fpage>190</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1126/science.277.5323.190</pub-id><pub-id pub-id-type="pmid">9235632</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname> <given-names>S</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Wireless multi-channel single unit recording in freely moving and vocalizing primates</article-title><source>Journal of Neuroscience Methods</source><volume>203</volume><fpage>28</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2011.09.004</pub-id><pub-id pub-id-type="pmid">21933683</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Afshar</surname> <given-names>A</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A high-performance brain-computer interface</article-title><source>Nature</source><volume>442</volume><fpage>195</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1038/nature04968</pub-id><pub-id pub-id-type="pmid">16838020</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sayegh</surname> <given-names>PF</given-names></name><name><surname>Gorbet</surname> <given-names>DJ</given-names></name><name><surname>Hawkins</surname> <given-names>KM</given-names></name><name><surname>Hoffman</surname> <given-names>KL</given-names></name><name><surname>Sergio</surname> <given-names>LE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The contribution of different cortical regions to the control of spatially decoupled Eye-Hand coordination</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>1194</fpage><lpage>1211</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01111</pub-id><pub-id pub-id-type="pmid">28253075</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaffelhofer</surname> <given-names>S</given-names></name><name><surname>Agudelo-Toro</surname> <given-names>A</given-names></name><name><surname>Scherberger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Decoding a wide range of hand configurations from macaque motor, premotor, and parietal cortices</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>1068</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3594-14.2015</pub-id><pub-id pub-id-type="pmid">25609623</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname> <given-names>DA</given-names></name><name><surname>Lebedev</surname> <given-names>MA</given-names></name><name><surname>Hanson</surname> <given-names>TL</given-names></name><name><surname>Dimitrov</surname> <given-names>DF</given-names></name><name><surname>Lehew</surname> <given-names>G</given-names></name><name><surname>Meloy</surname> <given-names>J</given-names></name><name><surname>Rajangam</surname> <given-names>S</given-names></name><name><surname>Subramanian</surname> <given-names>V</given-names></name><name><surname>Ifft</surname> <given-names>PJ</given-names></name><name><surname>Li</surname> <given-names>Z</given-names></name><name><surname>Ramakrishnan</surname> <given-names>A</given-names></name><name><surname>Tate</surname> <given-names>A</given-names></name><name><surname>Zhuang</surname> <given-names>KZ</given-names></name><name><surname>Nicolelis</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Chronic, wireless recordings of large-scale brain activity in freely moving rhesus monkeys</article-title><source>Nature Methods</source><volume>11</volume><fpage>670</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2936</pub-id><pub-id pub-id-type="pmid">24776634</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serruya</surname> <given-names>MD</given-names></name><name><surname>Hatsopoulos</surname> <given-names>NG</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Fellows</surname> <given-names>MR</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Brain-machine interface: instant neural control of a movement signal</article-title><source>Nature</source><volume>416</volume><fpage>141</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/416141a</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shahidi</surname> <given-names>N</given-names></name><name><surname>Schrater</surname> <given-names>P</given-names></name><name><surname>Wright</surname> <given-names>T</given-names></name><name><surname>Pitkow</surname> <given-names>X</given-names></name><name><surname>Dragoi</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Population coding of strategic variables during foraging in freely-moving macaques</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/811992</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheshadri</surname> <given-names>S</given-names></name><name><surname>Dann</surname> <given-names>B</given-names></name><name><surname>Hueser</surname> <given-names>T</given-names></name><name><surname>Scherberger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>3d reconstruction toolbox for behavior tracked with multiple cameras</article-title><source>Journal of Open Source Software</source><volume>5</volume><elocation-id>1849</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01849</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname> <given-names>LH</given-names></name><name><surname>Batista</surname> <given-names>AP</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Change in motor plan, without a change in the spatial locus of attention, modulates activity in posterior parietal cortex</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>2814</fpage><lpage>2819</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.5.2814</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suriya-Arunroj</surname> <given-names>L</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Complementary encoding of priors in monkey frontoparietal network supports a dual process of decision-making</article-title><source>eLife</source><volume>8</volume><elocation-id>e47581</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47581</pub-id><pub-id pub-id-type="pmid">31612855</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Talakoub</surname> <given-names>O</given-names></name><name><surname>Sayegh</surname> <given-names>PF</given-names></name><name><surname>Womelsdorf</surname> <given-names>T</given-names></name><name><surname>Zinke</surname> <given-names>W</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Lewis</surname> <given-names>CM</given-names></name><name><surname>Hoffman</surname> <given-names>KL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hippocampal and neocortical oscillations are tuned to behavioral state in freely-behaving macaques</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/552877</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Direct cortical control of 3D neuroprosthetic devices</article-title><source>Science</source><volume>296</volume><fpage>1829</fpage><lpage>1832</lpage><pub-id pub-id-type="doi">10.1126/science.1070291</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teikari</surname> <given-names>P</given-names></name><name><surname>Najjar</surname> <given-names>RP</given-names></name><name><surname>Malkki</surname> <given-names>H</given-names></name><name><surname>Knoblauch</surname> <given-names>K</given-names></name><name><surname>Dumortier</surname> <given-names>D</given-names></name><name><surname>Gronfier</surname> <given-names>C</given-names></name><name><surname>Cooper</surname> <given-names>HM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An inexpensive Arduino-based LED stimulator system for vision research</article-title><source>Journal of Neuroscience Methods</source><volume>211</volume><fpage>227</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2012.09.012</pub-id><pub-id pub-id-type="pmid">23000405</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Velliste</surname> <given-names>M</given-names></name><name><surname>Perel</surname> <given-names>S</given-names></name><name><surname>Spalding</surname> <given-names>MC</given-names></name><name><surname>Whitford</surname> <given-names>AS</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical control of a prosthetic arm for self-feeding</article-title><source>Nature</source><volume>453</volume><fpage>1098</fpage><lpage>1101</lpage><pub-id pub-id-type="doi">10.1038/nature06996</pub-id><pub-id pub-id-type="pmid">18509337</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuilleumier</surname> <given-names>P</given-names></name><name><surname>Valenza</surname> <given-names>N</given-names></name><name><surname>Mayer</surname> <given-names>E</given-names></name><name><surname>Reverdin</surname> <given-names>A</given-names></name><name><surname>Landis</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Near and far visual space in unilateral neglect</article-title><source>Annals of Neurology</source><volume>43</volume><fpage>406</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1002/ana.410430324</pub-id><pub-id pub-id-type="pmid">9506563</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wessberg</surname> <given-names>J</given-names></name><name><surname>Stambaugh</surname> <given-names>CR</given-names></name><name><surname>Kralik</surname> <given-names>JD</given-names></name><name><surname>Beck</surname> <given-names>PD</given-names></name><name><surname>Laubach</surname> <given-names>M</given-names></name><name><surname>Chapin</surname> <given-names>JK</given-names></name><name><surname>Kim</surname> <given-names>J</given-names></name><name><surname>Biggs</surname> <given-names>SJ</given-names></name><name><surname>Srinivasan</surname> <given-names>MA</given-names></name><name><surname>Nicolelis</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Real-time prediction of hand trajectory by ensembles of cortical neurons in primates</article-title><source>Nature</source><volume>408</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/35042582</pub-id><pub-id pub-id-type="pmid">11099043</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westendorff</surname> <given-names>S</given-names></name><name><surname>Klaes</surname> <given-names>C</given-names></name><name><surname>Gail</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The cortical timeline for deciding on reach motor goals</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>5426</fpage><lpage>5436</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4628-09.2010</pub-id><pub-id pub-id-type="pmid">20392964</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wodlinger</surname> <given-names>B</given-names></name><name><surname>Downey</surname> <given-names>JE</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name><name><surname>Boninger</surname> <given-names>ML</given-names></name><name><surname>Collinger</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ten-dimensional anthropomorphic arm control in a human brain-machine interface: difficulties, solutions, and limitations</article-title><source>Journal of Neural Engineering</source><volume>12</volume><elocation-id>016011</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/12/1/016011</pub-id><pub-id pub-id-type="pmid">25514320</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname> <given-names>YT</given-names></name><name><surname>Fabiszak</surname> <given-names>MM</given-names></name><name><surname>Novikov</surname> <given-names>Y</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Pesaran</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Coherent neuronal ensembles are rapidly recruited when making a look-reach decision</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>327</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1038/nn.4210</pub-id><pub-id pub-id-type="pmid">26752158</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>W</given-names></name><name><surname>de Carvalho</surname> <given-names>F</given-names></name><name><surname>Jackson</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sequential neural activity in primary motor cortex during sleep</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>3698</fpage><lpage>3712</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1408-18.2019</pub-id><pub-id pub-id-type="pmid">30842250</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname> <given-names>M</given-names></name><name><surname>Borton</surname> <given-names>DA</given-names></name><name><surname>Komar</surname> <given-names>J</given-names></name><name><surname>Agha</surname> <given-names>N</given-names></name><name><surname>Lu</surname> <given-names>Y</given-names></name><name><surname>Li</surname> <given-names>H</given-names></name><name><surname>Laurens</surname> <given-names>J</given-names></name><name><surname>Lang</surname> <given-names>Y</given-names></name><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Bull</surname> <given-names>C</given-names></name><name><surname>Larson</surname> <given-names>L</given-names></name><name><surname>Rosler</surname> <given-names>D</given-names></name><name><surname>Bezard</surname> <given-names>E</given-names></name><name><surname>Courtine</surname> <given-names>G</given-names></name><name><surname>Nurmikko</surname> <given-names>AV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Wireless neurosensor for full-spectrum electrophysiology recordings during free behavior</article-title><source>Neuron</source><volume>84</volume><fpage>1170</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.11.010</pub-id><pub-id pub-id-type="pmid">25482026</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zanos</surname> <given-names>S</given-names></name><name><surname>Richardson</surname> <given-names>AG</given-names></name><name><surname>Shupe</surname> <given-names>L</given-names></name><name><surname>Miles</surname> <given-names>FP</given-names></name><name><surname>Fetz</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The Neurochip-2: an autonomous head-fixed computer for recording and stimulating in freely behaving monkeys</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>19</volume><fpage>427</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2011.2158007</pub-id><pub-id pub-id-type="pmid">21632309</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>A</given-names></name><name><surname>Santacruz</surname> <given-names>SR</given-names></name><name><surname>Johnson</surname> <given-names>BC</given-names></name><name><surname>Alexandrov</surname> <given-names>G</given-names></name><name><surname>Moin</surname> <given-names>A</given-names></name><name><surname>Burghardt</surname> <given-names>FL</given-names></name><name><surname>Rabaey</surname> <given-names>JM</given-names></name><name><surname>Carmena</surname> <given-names>JM</given-names></name><name><surname>Muller</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A wireless and artefact-free 128-channel neuromodulation device for closed-loop stimulation and recording in non-human primates</article-title><source>Nature Biomedical Engineering</source><volume>3</volume><fpage>15</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1038/s41551-018-0323-x</pub-id><pub-id pub-id-type="pmid">30932068</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51322.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Pesaran</surname><given-names>Bijan</given-names></name><role>Reviewing Editor</role><aff><institution>Center for Neural Science, New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Santacruz</surname><given-names>Samantha R</given-names></name><role>Reviewer</role><aff><institution>The University of Texas at Austin</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>In this manuscript, the authors present an experimental environment and approach to instruct and quantify behavior in unrestrained macaque monkeys while conducting high-bandwidth wireless recordings from multiple brain areas simultaneously. The results demonstrate how the novel approach and methodology can be used to resolve neural mechanisms of behavior in more naturalistic settings.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;The Reach Cage environment for wireless neurophysiology during structured goal-directed behavior of unrestrained monkeys&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Samantha Santacruz (Reviewer #2); Andrew Jackson (Reviewer #3).</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic> at this time. Our assessment is that the revisions required may require a substantial reworking of the study to overcome what might be significant technical limitations. However, the reviewers found enough merit in this study that we would be happy to consider a new submission if you are able to adequately address the concerns raised.</p><p>In particular, the reviewers found several aspects of the work of value, such as the timeliness, the design and use of behavioral cues in the cage, and the combination of remote neurophysiological and behavioral tracking. This led to broad support for the goals and methods of the work. However, all three reviewers highlighted major weaknesses in the methodological advance presented that substantially undercut the significance of the work. The fundamental concern is that the behavioral apparatus alone seems more suited to a more specialized methods journal and it is not clear what kind of important scientific advance the wireless and motion capture components can support.</p><p>More clarification and quantification of the motion tracking performance is needed. The successful frame capture rate needs to be quantified over intervals of fixed duration and how this compares with reported performance of similar motion tracking methods that are not in-cage needs to be presented. Data for the second subject, Monkey L, also needs to be presented to give a second animal that would strengthen the claims of overall system performance. The inability to monitor more than 1 DOF is also considered a major limitation for a system meant ultimately to examine modulation of activity in an unconstrained animal, and this needs to be addressed.</p><p>The wireless recordings also need to be analyzed to quantify in detail wireless performance for the 31 and 127 channel recording setups using standard metrics, such as BER. Since the work is done in a non-ideal, highly reflective environment, we would be satisfied if the performance was close to the worst-case reported BER value of 10^-2 (taken from Yin et al., 2014, from which the current equipment is derived).</p><p>More detailed comments from the individual reviewers are provided below.</p><p><italic>Reviewer #1:</italic></p><p>This manuscript represents behavioral paradigms and data collection methods that are certain to become the focus of attention in the next few years. The study of the relations between single-neuron activity and highly constrained motor behaviors has largely given way in the past ten years to the study of large numbers of simultaneously recorded neurons, albeit, with largely the same behavioral paradigms that have been in place for the past several decades. The advent of the study of more natural behaviors has only more recently begun, given the attendant difficulties of designing useful behavioral paradigms, monitoring the movements themselves, and recording large number of neurons wirelessly.</p><p>This study is an initial attempt in that direction, and is described frequently as a proof-of-concept. The MaCaQuE (Macaque Cage Query Extension) component, in addition to its clever name, is the most original and important part of the methods. It combines proximity touch sensors and multi-color cue LEDs with a well-designed network communication method that interconnects the remote components with a microcontroller. The system is expandable and flexible, and forms the basis of a nice behavioral control system for eliciting innovative, more natural behaviors. I also found the flexible head-mounted chamber and connector-mounting system for neural recordings to be innovative and potentially quite useful. It is designed for an Omnetics-based headstage, but could presumably be modified without too great effort for the Cerebus connector.</p><p>Unfortunately, the cage itself appears to have been designed with too much metal for good RF transmission. While the 32-channel transmitter functioned adequately, the 128-channel version did not, suffering unacceptable artifacts and signal loss. While 32 channels will certainly be useful for many studies, it is a good bit less than the ~100 channels that have become something of the norm for multi-channel studies these days. Beyond that, passing reference to illumination levels and perhaps other restrictions seem to have severely limited motion tracking. Although they used four cameras to track a single colored marker (dye on the fur) at the monkey's wrist), the percentage of tracked frames seems to have been very low. Although this too, was described in less detail than would have been ideal, apparently one-third of trials tracked fewer than ~150ms of the trajectory (&quot;5 data points&quot;). These limitations of data collection strike me as being pretty substantial.</p><p>In addition to the methodological considerations reported here, this study also includes basic results for recordings from three cortical motor areas during &quot;stretch and reach&quot; and &quot;walk and reach&quot; paradigms. These results are presented in sufficiently limited form that they serve primarily to illustrate the paradigms themselves. I am sure that a fuller-length treatment of these data is planned, and anticipate that it will be of great interest. Beyond the behavioral and data collection methods described here, these kinds of experiments will also require a range of more sophisticated analytical approaches to take full advantage of the data.</p><p>Consequently, despite its potential, this paper is not adequately compelling either in term of methodological or scientific contributions. Addressing the former will be difficult, unless I have misunderstood the basic limitations of the wireless neural recordings and motion tracking. I assume the latter is a distinct possibility. There are a fair number of grammatical errors that should be addressed, not all of which I attempted to mark.</p><p><italic>Reviewer #2:</italic></p><p>The authors present a behavior system (the Reach Cage in combination with MaCaQuE interaction device) for performing sensorimotor tasks with nonhuman primates while wirelessly recording neural data. Overall, I find the system design to be novel and a clear demonstration of a system that may be used for the suggested purposes. My two main concerns are: (1) the motivation for the need of such a system, and (2) the limitation of kinematic tracking to one position (wrist). Regarding the former concern, I think the authors could improve their introductory discussion on how such a system is lacking and provides greater insight than, for example, using an existing commercial wireless recording system (e.g. from Blackrock) with something as simple as a cage-mounted touchscreen (essentially porting standard experimental setups with head-fixed animals to a the homecage environment). Regarding the latter concern, the authors do address this point late in the paper (Discussion section) but unconvincingly. There are sleeves/bodysuits that could be used with LED markers for tracking multiple positions that subjects do tolerate. The authors should at least make an argument regarding potential ways they could envision future revisions of their system to allow for tracking of more than one point. Additionally, I have minor concerns regarding the statistical analysis of single-unit data (present in Figure 4). For the analysis of Example B, statistical tests are performed separately for the two time points (cue onset and before go cue) and for the two distances of targets (near and far). It is more appropriate to do a combined statistical analysis, such as a two-way ANOVA with firing rate as the dependent variable, and timepoint (2 levels) and distances (2 levels) are independent variables. Similarly, for example D, a combined statistical analysis should be performed for the distances (near and far) and sides (left and right).</p><p><italic>Reviewer #3:</italic></p><p>This methods paper by Berger and Gail describes an experimental set-up for studying goal-directed reaching in unrestrained non-human primates with associated wireless neural recording. This work is timely, since the use of chronically-implanted electrodes and wireless recording is allowing neuroscientific experiments to be conducted in less constrained environments. However, the lack of constraints brings both advantages (behaviours may be more naturalistic than conventional restrained tasks such as centre-out reaching) and disadvantages (behaviours are less controlled and therefore data is less amenable to common analysis techniques such as trial averaging). The approach here appears to be a reasonable trade-off between the two, in that movements are unrestrained but trained/cued so as to be fairly stereotyped. I can certainly envisage some scientific questions that would be amenable to such an approach, although with the caveat that such stereotyped behaviours may not be only marginally more naturalistic than the traditional tasks. I also see this work as valuable as a stepping stone to studying fully unconstrained, free behaviour, although in this regard it may be important to incorporate more detailed motion capture in order to make sense of the recordings.</p><p>The neural recording uses a commercially available system, and only a couple of exemplar neurons are reported, so the principle novelty of this work comprises a description of the behavioural set-up, task design and kinematic recordings. The system comprises a number of cue/target boxes placed within an arena, with camera-based tracking of movement, and the operation of this is well described in the paper. However, it would be useful to include more description of how easily the animals were trained on this task (e.g. how many sessions, how long did it take for consistent behaviour to emerge). The authors should also assess how stereotyped was the behaviour from trial-to-trial vs. session-to-session. The collection of motion kinematics seems to be fairly limited – only the position of the wrist is tracked via red paint markings, in only one of the animals. Why was this not done for the other animal? No attempt is made to characterise the accuracy of this tracking – it appears that quite a large region of the wrist was painted so I can envisage that determining the centre location from multiple cameras may be quite dependent on arm/hand orientation etc. It would also be interesting to discuss whether other colours on, for example, the elbow and shoulder could allow a more detailed reconstruction of limb kinematics.</p><p>The modular design of the headpiece is nice. However, please add a scale bar to Figure 5C and also give dimensions for the total implant height. It is not clear whether the wireless recording system is battery- or inductively-powered. If the former, please give details of battery capacity, life-time, weight etc. If the latter, please include details of how power coils were positioned etc.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;The Reach Cage environment for wireless neural recordings during goal-directed behavior of unrestrained monkeys&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Samantha R Santacruz (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors present an updated version of their behavior system (the Reach Cage in combination with MaCaQuE) which has been demonstrated in a sensorimotor task (&quot;walk-and-reach&quot; task) with nonhuman primates while wirelessly recording neural data as a proof-of-principle of the overall functionality of the system. The primary strength of this work, is perceived by the reviewers to be the reduced constraints on behavior (towards naturalistic settings) while still affording systematic study via controlled tasks and quantification. Previous concerns regarding the motivation for the study and limitations of kinematic tracking have been addressed with a marked significant improvement in the system. However, since the individual pieces of the system have been previously demonstrated (e.g. wireless recordings, in-cage behavior), whether the combined effort represents a novel contribution in a way that will facilitate new experiment needs more direct support. Moreover, more detailed analyses of the behavioral and neurophysiological data are needed.</p><p>Essential revisions:</p><p>1) The direction of the work (unconstrained task electrophysiology exploration) is of great interest, but the significance of this manuscript toward those goals is still not sufficiently clear. A fundamental concern is that this content seems more appropriate for a specialized methods journal. Although the authors have addressed in their rebuttal and through revisions in their paper the potential for scientific impact of this system, this does not address the point that ultimately they are working in an area of technology/methodology that has been previously reported. Claimed novelty is not correct: other groups have recorded neural data and behavior wirelessly from custom cage designs (Powell, 2017, J. Neuroscience Meth., for example). The authors also claim in the Introduction that goal-directed behaviors that involve walking have not previously been possible, but then cite Yin. Et al., 2014 and Capogrosso et al., 2016 which both involve walking behaviors and are goal (treat) directed walking behavior.</p><p>In revision, a deeper comparison to the current manuscript results is needed. First, the MaCaQue system was already published (Berger, 2019). That paper describes the behavioral collection applied to humans. The current report presents a combination of neural data collection with that behavior, and in monkey, which is interesting, but no novel behavioral or neuroscience results are provided. Please describe clearly what is unique in the system from the human version. Please present (e.g., via a table) a comparison of benchmarks across previous reports not limited to the following:</p><p>Roy and Wang, 2012; Chestek et al., 2009; Yin et al., 2014; Foster et al., 2014; Schwarz et al., 2014; Capogrosso et al., 2016.</p><p>2) The work is overly descriptive, and at some points, misleading. The neuroscience and electrophysiology leave substantial questions about the interpretation of the results and could be described more clearly. For example, the authors appear to present conclusions based on 4 neurons (out of 192) and almost no population analysis is completed (trajectories are not analysis in and of themselves). While statistical tests on those neurons is appropriate, the results do not substantiate the claims that 192 channels are recorded and convey information about goal-directed behavior.</p><p>3) Quantification of behavioral variability is also limited. Movement analysis does not describe the kinematics that are said to be collected. For example, acquisition times are given, but not variability in kinematic variables. Additional documentation of the data analyzed for the behavioral variability (Figure 3) is needed. Please report analysis of variability in movement kinematics. How many trials and behavioral sessions are presented? At what point in behavioral training are these data from? Claims of consistency/variability are difficult to assess without these details.</p><p>4) Wireless coverage rates merits more careful quantification. Wireless transmission efficacy can often vary across the cage volume due to non-uniform receiver coverage. An analysis of data loss rates that quantifies volume of the cage spanned and presence or absence of spatial trends in where data loss occurs would be significantly more compelling than aggregate data loss rates. Drop-out with strong spatial bias would hamper and complicate task analyses. This is important for the authors to quantify because their other analyses highlight that animals exhibit relatively stereotyped behaviors during tasks (Figure 3). Are the low data loss rates because the animals occupy a restricted volume of the cage during task behavior? Some quantification of coverage volume will also be valuable for reporting and demonstrating the tool's capabilities.</p><p>5) Claims for BMI utility require more careful elaboration. The authors argue their system, with real-time neural data recording and behavioral control, would be useful for BMI studies. While this is true, their system as currently designed has constraints on the possible BMI experiments that could be done which should be elaborated. Their behavioral task system, with discrete targets, would have limited applications to discrete decoding behaviors. Elaboration on specific utility and limitations/future directions is needed.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Wireless recording from unrestrained monkeys reveals motor goal encoding beyond immediate reach in frontoparietal cortex&quot; for consideration by <italic>eLife</italic>. Your revised article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Samantha R Santacruz (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission to clear up a few remaining points.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>The authors present a cohesive and compelling piece of work 1) demonstrating the utility of their in-cage behavioral system and wireless recordings, and 2) providing novel findings enabled by their work. The reviewers find the manuscript is greatly strengthened by the new analyses and re-framing to fully place their work in context. The combination of in-cage, untethered neural recording with well-controlled sensorimotor tasks is both powerful and a niche that has not been addressed to-date. the manuscript is suitable for publication in <italic>eLife</italic> subject to revisions.</p><p>Essential revisions:</p><p>1) The authors state in the rebuttal &quot;We now added an analysis on all 12 recorded session and test for each of the 192 channels with an ANOVA if the activity is task ( = time, target distance or position) modulated and if it is target position modulated (Results section), which was not the scope of the analysis in the previous version of the manuscript. We demonstrate that there are sessions that show 192 task modulated channels and provide statistics on the population activity in the newly written part of the Results section.&quot; I believe this quoted text actually refers to: &quot;movement. Of all twelve recorded sessions three sessions revealed task responsive activity on all 192 channels, i.e. showed at least one effect in distance, position, time or one of the interactions; across all sessions the mean number of task-responsive channels was 189 (s.d. 5 channels). Up to 179 channels were position responsive, i.e. showing at least one effect in position or one of the interactions (mean: 162, s.d. 17 channels).&quot; It is unclear how this analysis was performed. Was &quot;task responsive activity&quot; determined from another ANOVA? Please clarify.</p><p>2) In the section &quot;Premotor and parietal cortex encode movement goals beyond immediate reach&quot;. Here the authors perform SVM classification using neural spiking activity to decode reach to either a near or far target. This analysis is problematic in that they determine significant decoding accuracy by comparing performance to that of a classifier using activity prior to cue onset and also assume chance levels of 50% (based on plots in Figure 5B). It would be more appropriate to compare accuracy to shuffled data in order to determine true chance levels (particularly if the near and far trials aren't balanced) and significance. Please perform the permutation-based test of significance.</p><p>3) The authors state: &quot;Moreover, we could decode walk-and-reach target location information from premotor and parietal cortex, but not motor cortex, during movement and even during the memory period before the movement. This suggests that premotor and parietal cortex encodes motor goals beyond immediate reach.&quot; A decoding model and encoding model are complimentary, but not the same. High decoding accuracy from the SVM simply means that the population-level neural activity during these timepoints is separable enough in a high dimensional space. This may be due to many factors. To more completely demonstrate that premotor and parietal cortex encode motor goals beyond immediate reach, an encoding model (e.g. regression or GLM) describing how neural activity co-varies with the motor goals would be valuable.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51322.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>[…]</p><p>The fundamental concern is that the behavioral apparatus alone seems more suited to a more specialized methods journal and it is not clear what kind of important scientific advance the wireless and motion capture components can support.</p></disp-quote><p>We are not aware of previous work studying large-scale neural network activity in freely moving monkeys trained on goal-directed naturalistic walk-and-reach movements that are temporally and spatially structured under the control of the experimenter. We provide new wireless neurophysiology and motion capture data. We show broadband intracortical recordings from 192 channel at 30 ksps from three brain areas simultaneously. Motion capture is now fully markerless and provides 3D tracking of four body-parts namely head, left wrist, elbow and shoulder. We show with an additional analysis that single-trial neural dynamics of multiple areas can be analyzed and correlated with detailed multi-joint kinematics of on unrestrained behavior. Real-time read-out of the neural data makes the setting suitable for BMI applications. Details are described below.</p><disp-quote content-type="editor-comment"><p>More clarification and quantification of the motion tracking performance is needed. The successful frame capture rate needs to be quantified over intervals of fixed duration and how this compares with reported performance of similar motion tracking methods that are not in-cage needs to be presented. Data for the second subject, Monkey L, also needs to be presented to give a second animal that would strengthen the claims of overall system performance. The inability to monitor more than 1 DOF is also considered a major limitation for a system meant ultimately to examine modulation of activity in an unconstrained animal, and this needs to be addressed.</p></disp-quote><p>Now, we use a different approach for markerless motion tracking based on a deep neural network. We present new data from both monkeys performing a memory-guided walk-and-reach task. The head, left wrist, elbow and shoulder are tracked in 3D during performance of the walk-and reach task. We increased the framerate to 60 Hz and quantified the successful frame capture rate after outlier rejection (97.58% – 99.95%).</p><p>The main goal of the presented research approach is to provide an experimental setting that allows studying structured and goal-oriented behavior beyond what is possible in a chair-based setup, for instance walk-and-reach behavior towards targets outside the immediate peripersonal space. We tried to make our rational clearer throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>The wireless recordings also need to be analyzed to quantify in detail wireless performance for the 31 and 127 channel recording setups using standard metrics, such as BER. Since the work is done in a non-ideal, highly reflective environment, we would be satisfied if the performance was close to the worst-case reported BER value of 10^-2 (taken from Yin et al., 2014, from which the current equipment is derived).</p></disp-quote><p>We now use 2x 96-channel wireless headstages simultaneously to records from all 192 electrodes at 30 ksps sampling rate at the same time. Our former wireless system was not derived from Yin et al., however, our current system distinctly is. Since we do not have direct access to the digital data stream of this proprietary system, we cannot perform BER measures as shown by Yin et al. or other publications about wireless systems. Also, we do not believe that a standard BER measure is suitable to judge the relevant performance, since it is a property of the system itself and measured under optimal conditions for wireless transmission. Here, we report an experimental environment and not a wireless recording system. Thus, we report transmission stability, i.e. the fraction of time points for which we received data (around 96.68%). This is an equivalent metric to BER, in our application, as instead of measuring the quality of the wireless transmission signal itself, we measure the ability to receive the signal within our recording environment during a recording session. We are not aware of corresponding stability measures for comparable setups in which monkeys relocate themselves. This includes Yin et al., who provide data of monkeys sleeping or walking on a treadmill. In both cases, the head of the monkey, and consequently the wireless transmitter, stays approximately in the same position, which is technically less challenging.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The direction of the work (unconstrained task electrophysiology exploration) is of great interest, but the significance of this manuscript toward those goals is still not sufficiently clear. A fundamental concern is that this content seems more appropriate for a specialized methods journal. Although the authors have addressed in their rebuttal and through revisions in their paper the potential for scientific impact of this system, this does not address the point that ultimately they are working in an area of technology/methodology that has been previously reported. Claimed novelty is not correct: other groups have recorded neural data and behavior wirelessly from custom cage designs (Powell, 2017, J. Neuroscience Meth., for example). The authors also claim in the Introduction that goal-directed behaviors that involve walking have not previously been possible, but then cite Yin. et al., 2014 and Capogrosso et Al., 2016, which both involve walking behaviors and are goal (treat) directed walking behavior.</p></disp-quote><p>Our manuscript is not primarily about wireless transmission, it is about a conceptual approach in cognitive and sensorimotor neuroscience bearing on, among other things, wireless recordings. In this sense the study of Powell et al., 2017, which presents an RF transparent cage, i.e. equipment for wireless recording, has a very different scope. We present an experimental environment and approach for which precisely guided and quantifiable spatially and temporally instructed behavior is possible with unrestrained macaques while conducting high-bandwidth wireless recordings from multiple brain areas simultaneously. The combined features of the Reach Cage allow experimental designs that former neurophysiology studies/environments with unrestrained monkeys did not provide:</p><p>– Precise computer-controlled timing of temporal instructions (programmable color illumination of targets) to the animal and real-time control of the experimental flow based on the animals registered behavior (touch-sensitivity of the targets).</p><p>– As a consequence, investigation of motor planning is possible and demonstrated here. No other study known to us dealt with motor planning in unrestrained monkeys.</p><p>– Multiple distributed, light identifiable and instructable reach goals that are independent of the food source (important in cognitive neuroscience) and can be configured variably in number and position.</p><p>– As a consequence, a defined starting position with the ability to pause the animal’s movement allows to provide visual cues at a defined locations relative to the body, necessary for motor-cognitive aspects of sensorimotor science. For example, trained goal directed behavior led to a distinct number of behavioral categories (here movements to 8 different targets) and repetitive trials as previously possible only in more conventional chair-seated paradigms (see also response to comment 3). We are not aware of a study showing that pure training of monkeys without any physical restraint leads to such low trial-to-trail variability in freely moving animals.</p><p>We have revised the Introduction and Discussion to make the idea behind our study more clear. We have added an overview of previous studies, Supplementary file 1, as suggested below.</p><p>As requested by the editor’s summary, we provide more detail on our neurophysiology data as a proof-of-concept. For this, we replaced the last subsection in the Results section with a new analysis of the neurophysiology data that shows that premotor and parietal cortex encode motor goals beyond immediate reach during planning and locomotion. We are not aware of existing experimental environments or published data that would show this or would have been able to show this.</p><disp-quote content-type="editor-comment"><p>In revision, a deeper comparison to the current manuscript results is needed. First, the MaCaQue system was already published (Berger, 2019). That paper describes the behavioral collection applied to humans. The current report presents a combination of neural data collection with that behavior, and in monkey, which is interesting, but no novel behavioral or neuroscience results are provided. Please describe clearly what is unique in the system from the human version.</p></disp-quote><p>We are surprised by this argument given <italic>eLife</italic>’s support for preprints and find it in fact a highly problematic argument. The MaCaQuE system was first published as the preprint of this manuscript. In Berger et al., 2019, we cited the preprint accordingly. We will cite this manuscript, not Bergeret al.,2019, when referring to the MaCaQuE System, since only here we present an extensive description of the MaCaQuE system. Since the current manuscript was already in the pipeline at <italic>eLife</italic> and available as preprint, we think this is the only appropriate way of dealing with such situation. Since the publication history can easily be tracked in the archive system, the genesis is transparent. Regarding novelty, Berger et al., 2019, is all about a human psychophysics result showing congruency effects in the interference of cross-modal sensory integration with spatial distractor stimuli, not about the MaCaQuE system and not about non-human primates, as evident from the paper. The novelty here is explained in our response to the above comment, for which MaCaQuE is an important but not the sole component. Specifically, the novelty here is directed towards studies with non-human primates. The current and previous studies have a completely independent scope.</p><p>We find the argument highly problematic since it effectively means that throughout the review process, during which authors – like in our case – invest massively to follow reviewer requests for significant modifications and sometimes have to wait many months to get feedback on their submission, authors would not be allowed to make reference to their published preprints in other studies. This defeats the purpose of preprints and in our view contradicts <italic>eLife</italic>’s policy.</p><disp-quote content-type="editor-comment"><p>Please present (e.g., via a table) a comparison of benchmarks across previous reports not limited to the following:</p><p>Roy and Wang, 2012; Chestek et al., 2009; Yin et al., 2014; Foster et al., 2014; Schwarz et al., 2014; Capogrosso et al., 2016.</p></disp-quote><p>We present this table as Supplementary file 1.</p><disp-quote content-type="editor-comment"><p>2) The work is overly descriptive, and at some points, misleading. The neuroscience and electrophysiology leave substantial questions about the interpretation of the results and could be described more clearly. For example, the authors appear to present conclusions based on 4 neurons (out of 192) and almost no population analysis is completed (trajectories are not analysis in and of themselves). While statistical tests on those neurons is appropriate, the results do not substantiate the claims that 192 channels are recorded and convey information about goal-directed behavior.</p></disp-quote><p>Since the focus of the manuscript, based on the reviewer suggestions, has now partly shifted towards more neuroscientific content, we also adapted the style and it should be less descriptive. The methods-oriented parts, which were originally more in the foreground since we submitted to the “Tools and Resources” rubric of the journal, remain descriptive. The example units are shown to demonstrate that it is possible to record clearly modulated, well-isolated single-unit activity in much detail, with modulations of the neural responses following closely the precisely controlled different stages of the trials over time and at the same time being selective for the spatial parameters of the task. We now added an analysis on all 12 recorded session and test for each of the 192 channels with an ANOVA if the activity is task ( = time, target distance or position) modulated and if it is target position modulated (Results section), which was not the scope of the analysis in the previous version of the manuscript. We demonstrate that there are sessions that show 192 task modulated channels and provide statistics on the population activity in the newly written part of the Results section.</p><disp-quote content-type="editor-comment"><p>3) Quantification of behavioral variability is also limited. Movement analysis does not describe the kinematics that are said to be collected. For example, acquisition times are given, but not variability in kinematic variables. Additional documentation of the data analyzed for the behavioral variability (Figure 3) is needed. Please report analysis of variability in movement kinematics. How many trials and behavioral sessions are presented? At what point in behavioral training are these data from? Claims of consistency/variability are difficult to assess without these details.</p></disp-quote><p>We do report an analysis of variability of movement kinematics (Figure 3B and third paragraph in this subsection). This is the across time points averaged Euclidean distance for each trial to a trial-averaged trajectory. We show that trials deviate only few centimeters from the averaged trajectory. Even for the most variable marker (wrist) the averaged Euclidean distance is below 67 mm for 75% of the trials.</p><p>We now added more sessions to this analysis. Now the analysis is based on 2 (monkey K) and 3 sessions (monkey L) with a total of 469 and 872, respectively, successful trials. We added this information in the Results section and Figure 3.</p><p>All data presented in the manuscript is after behavioral training. We added this information in the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>4) Wireless coverage rates merits more careful quantification. Wireless transmission efficacy can often vary across the cage volume due to non-uniform receiver coverage. An analysis of data loss rates that quantifies volume of the cage spanned and presence or absence of spatial trends in where data loss occurs would be significantly more compelling than aggregate data loss rates. Drop-out with strong spatial bias would hamper and complicate task analyses. This is important for the authors to quantify because their other analyses highlight that animals exhibit relatively stereotyped behaviors during tasks (Figure 3).</p></disp-quote><p>We added an analysis of data loss rates across targets (Results section and Figure 4—figure supplement 1). While the raw data shows some dependency of data loss rates on the target position in the range of few percent, we did not observe significant differences between trials towards different target positions in the left-right dimension after applying our 5% criterion for rejecting poor trials. We do see a difference between reach and walk-and-reach movements; however, this does not affect our results since these quantify left-right selectivity for reach and walk-and-reach movements independently.</p><disp-quote content-type="editor-comment"><p>Are the low data loss rates because the animals occupy a restricted volume of the cage during task behavior? Some quantification of coverage volume will also be valuable for reporting and demonstrating the tool's capabilities.</p></disp-quote><p>Since monkeys performed a spatially and temporally highly structured behavior during the recording sessions, we cannot draw conclusions on the coverage beyond the space covered in Figure 4—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>5) Claims for BMI utility require more careful elaboration. The authors argue their system, with real-time neural data recording and behavioral control, would be useful for BMI studies. While this is true, their system as currently designed has constraints on the possible BMI experiments that could be done which should be elaborated. Their behavioral task system, with discrete targets, would have limited applications to discrete decoding behaviors. Elaboration on specific utility and limitations/future directions is needed.</p></disp-quote><p>We thank the reviewer for pointing this out. We used the opportunity to rewrite the corresponding Discussion section. On the one hand, we are specifically interested in the control of discrete events, e.g. for smart home applications, for which spatial selectivity/invariance regarding the subjects position in space is important. On the other hand, robustness of prosthetic control during performance of other movements in parallel, like walking, is critical for applications in partially impaired patients (e.g. arm amputees). The new neural analyses show that target location information can be decoded during walking, as a first step to show that information for BMIs can be harnessed from the neural activity recorded in the Reach Cage. We now elaborate on this in the Discussion section, but since BMI application is not the main focus of this study, we removed the paragraph from the Introduction.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors state in the rebuttal &quot;We now added an analysis on all 12 recorded session and test for each of the 192 channels with an ANOVA if the activity is task ( = time, target distance or position) modulated and if it is target position modulated (Results section), which was not the scope of the analysis in the previous version of the manuscript. We demonstrate that there are sessions that show 192 task modulated channels and provide statistics on the population activity in the newly written part of the Results section.&quot; I believe this quoted text actually refers to: &quot;movement. Of all twelve recorded sessions three sessions revealed task responsive activity on all 192 channels, i.e. showed at least one effect in distance, position, time or one of the interactions; across all sessions the mean number of task-responsive channels was 189 (s.d. 5 channels). Up to 179 channels were position responsive, i.e. showing at least one effect in position or one of the interactions (mean: 162, s.d. 17 channels).&quot; It is unclear how this analysis was performed. Was &quot;task responsive activity&quot; determined from another ANOVA? Please clarify.</p></disp-quote><p>We understand why this paragraph is unclear and added an explanatory sentence: “We performed the same ANOVA for the activity in each channel of all twelve recorded sessions.“</p><disp-quote content-type="editor-comment"><p>2) In the section &quot;Premotor and parietal cortex encode movement goals beyond immediate reach&quot;. Here the authors perform SVM classification using neural spiking activity to decode reach to either a near or far target. This analysis is problematic in that they determine significant decoding accuracy by comparing performance to that of a classifier using activity prior to cue onset and also assume chance levels of 50% (based on plots in Figure 5B). It would be more appropriate to compare accuracy to shuffled data in order to determine true chance levels (particularly if the near and far trials aren't balanced) and significance. Please perform the permutation-based test of significance.</p></disp-quote><p>We replaced the statistics on the decoding analysis with a permutation-based test of significance for our main analysis (Results, Figure 5 and Figure 5—source data 1) and for our supplementary analysis (Results, Figure 5—figure supplement 1 and Figure 5—source data 3). The permutation-based test revealed the same results as the test in the previous version of the manuscript, apart from now one additional significant decoding accuracy in M1 of monkey L during movement (but not within the memory period) of walk-and-reach movements. All previously reported effects are still valid. This minimally different result does not change the conclusions of our study. We revised the Materials and methods section accordingly.</p><disp-quote content-type="editor-comment"><p>3) The authors state: &quot;Moreover, we could decode walk-and-reach target location information from premotor and parietal cortex, but not motor cortex, during movement and even during the memory period before the movement. This suggests that premotor and parietal cortex encodes motor goals beyond immediate reach.&quot; A decoding model and encoding model are complimentary, but not the same. High decoding accuracy from the SVM simply means that the population-level neural activity during these timepoints is separable enough in a high dimensional space. This may be due to many factors. To more completely demonstrate that premotor and parietal cortex encode motor goals beyond immediate reach, an encoding model (e.g. regression or GLM) describing how neural activity co-varies with the motor goals would be valuable.</p></disp-quote><p>We added an encoding (“tuning”) analysis. As we are interested in population and not single unit effects, we calculated the population average of the firing rate modulation, i.e. absolute difference between average firing rate for left and right trials and then averaged over all units after aligning to the preferred side (“population tuning”). As for the decoding analysis, we calculated a permutation-based test to test for significant encoding (Figure 5 and Figure 5—source data 2) or significant differences between passage and open trials (Figure 5—figure supplement 1 and Figure 5—source data 4). We describe this accordingly in the Results and Materials and methods. The results for the encoding analysis do not differ from the decoding analyses, hence the conclusions of the study remain the same.</p></body></sub-article></article>