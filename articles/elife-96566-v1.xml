<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">96566</article-id><article-id pub-id-type="doi">10.7554/eLife.96566</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.96566.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Homeostatic synaptic normalization optimizes learning in network models of neural population codes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mayzel</surname><given-names>Jonathan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-2237-3880</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Schneidman</surname><given-names>Elad</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8653-9848</contrib-id><email>elad.schneidman@weizmann.ac.il</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0316ej306</institution-id><institution>Department of Brain Sciences, Weizmann Institute of Science</institution></institution-wrap><addr-line><named-content content-type="city">Rehovot</named-content></addr-line><country>Israel</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP96566</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-03-11"><day>11</day><month>03</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-12-18"><day>18</day><month>12</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.05.530392"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-01"><day>01</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96566.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-18"><day>18</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96566.2"/></event></pub-history><permissions><copyright-statement>© 2024, Mayzel and Schneidman</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Mayzel and Schneidman</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-96566-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-96566-figures-v1.pdf"/><abstract><p>Studying and understanding the code of large neural populations hinge on accurate statistical models of population activity. A novel class of models, based on learning to weigh sparse nonlinear Random Projections (RP) of the population, has demonstrated high accuracy, efficiency, and scalability. Importantly, these RP models have a clear and biologically plausible implementation as shallow neural networks. We present a new class of RP models that are learned by optimizing the randomly selected sparse projections themselves. This ‘reshaping’ of projections is akin to changing synaptic connections in just one layer of the corresponding neural circuit model. We show that Reshaped RP models are more accurate and efficient than the standard RP models in recapitulating the code of tens of cortical neurons from behaving monkeys. Incorporating more biological features and utilizing synaptic normalization in the learning process, results in accurate models that are more efficient. Remarkably, these models exhibit homeostasis in firing rates and total synaptic weights of projection neurons. We further show that these sparse homeostatic reshaped RP models outperform fully connected neural network models. Thus, our new scalable, efficient, and highly accurate population code models are not only biologically plausible but are actually optimized due to their biological features. These findings suggest a dual functional role of synaptic normalization in neural circuits: maintaining spiking and synaptic homeostasis while concurrently optimizing network performance and efficiency in encoding information and learning.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>population coding</kwd><kwd>spiking models</kwd><kwd>sparse coding</kwd><kwd>homeostatic synaptic plasticity</kwd><kwd>network models</kwd><kwd>efficient coding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>137628</award-id><principal-award-recipient><name><surname>Schneidman</surname><given-names>Elad</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>542997</award-id><principal-award-recipient><name><surname>Schneidman</surname><given-names>Elad</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000936</institution-id><institution>Gordon and Betty Moore Foundation</institution></institution-wrap></funding-source><award-id>2919.02</award-id><principal-award-recipient><name><surname>Schneidman</surname><given-names>Elad</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>DFG, German Research Foundation</institution></institution-wrap></funding-source><award-id>SFB 1528</award-id><principal-award-recipient><name><surname>Schneidman</surname><given-names>Elad</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Israeli Council for Higher Education/Weizmann Data Science Research Center</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schneidman</surname><given-names>Elad</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Martin Kushner Schnur, and Mr. &amp; Mrs. Lawrence Feis</institution></institution-wrap></funding-source><award-id>Weizmann Institute</award-id><principal-award-recipient><name><surname>Schneidman</surname><given-names>Elad</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>The Joseph and Bessie Feinberg Chair</institution></institution-wrap></funding-source><award-id>Weizmann Institute</award-id><principal-award-recipient><name><surname>Schneidman</surname><given-names>Elad</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An accurate and efficient biologically plausible statistical model of the spiking activity of neural populations shows computational benefits of homeostatic synaptic scaling in learning large neural population codes.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The potential ‘vocabulary’ of spiking patterns of a population of neurons scales exponentially with the size of the population, and so, mapping the rules of neural population codes and their semantic organization, cannot rely on direct sampling of the vocabulary for more than a handful of neurons. Moreover, the stochastic nature of neural activity implies that the characterization of neural codes must rely on probability distributions over population activity patterns. Therefore, to describe and analyze the structure and content of the code with which neural circuits respond to stimuli, process information, and direct action – we must learn statistical models of their activity. Such models have been used to study neural population codes in different systems: Models of the directional coupling between neurons, such as Generalized Linear Models, have been used to replicate the stimulus-dependent rates of populations of tens of neurons (<xref ref-type="bibr" rid="bib51">Truccolo et al., 2005</xref>; <xref ref-type="bibr" rid="bib36">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib5">Calabrese et al., 2011</xref>; <xref ref-type="bibr" rid="bib55">Weber et al., 2012</xref>). Maximum entropy models have accurately captured the joint activity patterns of more than 100 neurons, using simple statistical features of the population, like firing rates, pairwise correlations, synchrony, and other low-order statistics (<xref ref-type="bibr" rid="bib39">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib43">Shlens et al., 2006</xref>; <xref ref-type="bibr" rid="bib45">Tang et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Tkačik et al., 2014</xref>; <xref ref-type="bibr" rid="bib11">Ganmor et al., 2011</xref>; <xref ref-type="bibr" rid="bib28">Marre et al., 2009</xref>; <xref ref-type="bibr" rid="bib31">Ohiorhenuan et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Granot-Atedgi et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Meshulam et al., 2017</xref>). These models have further been used to characterize the semantic organization of population codes (<xref ref-type="bibr" rid="bib12">Ganmor et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Tkačik et al., 2013a</xref>). Auto-encoder models have been employed to replicate the detailed structure of population activity – yielding generative models that can be used to study the code, but their design is difficult to interpret (<xref ref-type="bibr" rid="bib33">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib2">Barrett et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Gonçalves et al., 2020</xref>). Importantly, scaling of these models to hundreds of neurons is computationally demanding (<xref ref-type="bibr" rid="bib49">Tkačik et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Meshulam et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Ganmor et al., 2011</xref>), which has been a major challenge in modeling large neural systems.</p><p>While statistical models are invaluable for describing and studying neural codes, it is not clear whether the brain relies on such models or implements them when representing or processing information (<xref ref-type="bibr" rid="bib40">Schneidman, 2016</xref>; <xref ref-type="bibr" rid="bib21">Karpas et al., 2019</xref>). Consequently, much of the analysis of neural codes has focused on decoding population activity, typically using simple decoders (<xref ref-type="bibr" rid="bib34">Panzeri et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Tkačik et al., 2013b</xref>; <xref ref-type="bibr" rid="bib36">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib4">Botella-Soler et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Shi et al., 2019</xref>; <xref ref-type="bibr" rid="bib56">Whiteway et al., 2020</xref>), or metrics over the structure of population activity patterns (<xref ref-type="bibr" rid="bib12">Ganmor et al., 2015</xref>; <xref ref-type="bibr" rid="bib10">Gallego et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Chaudhuri et al., 2019</xref>). Yet, if neural circuits do implement such statistical models, and in particular, ones that compute the likelihood of their inputs – this would present a realizable mechanism for real neural circuits to carry Bayesian computation and decision making (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Vertes and Sahani, 2018</xref>; <xref ref-type="bibr" rid="bib59">Zemel et al., 1998</xref>). Such network models are, therefore, of interest not only as a way to study neural codes, but also as a potential way for biological neural networks to implement efficient learning and overcome the credit assignment problem. In addition, they may be useful for improving learning in artificial neural networks using biological features (<xref ref-type="bibr" rid="bib3">Bengio et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib37">Poirazi et al., 2003</xref>; <xref ref-type="bibr" rid="bib38">Richards et al., 2019</xref>; <xref ref-type="bibr" rid="bib61">Zhong et al., 2022</xref>; <xref ref-type="bibr" rid="bib8">Chavlis and Poirazi, 2021</xref>).</p><p>Both structured architectural features of neural circuits and random connectivity patterns have been suggested to shape the computation carried out by neural circuits (<xref ref-type="bibr" rid="bib26">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>; <xref ref-type="bibr" rid="bib16">Haber and Schneidman, 2022a</xref>; <xref ref-type="bibr" rid="bib25">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Pechuk et al., 2022</xref>; <xref ref-type="bibr" rid="bib17">Haber and Schneidman, 2022b</xref>). These computations rely on the nature of synaptic connectivity and the coupling between synapses in terms of how they change during learning. Competition mechanisms between synapses or other regularization mechanisms have also been suggested to be important components of computation and learning in artificial neural networks as well as in cortical circuits (<xref ref-type="bibr" rid="bib18">Heeger, 1992</xref>; <xref ref-type="bibr" rid="bib6">Carandini and Heeger, 2011</xref>). One such mechanism is the homeostatic scaling of synaptic plasticity, which has been observed in vitro and in vivo at the level of incoming synapses to a neuron and outgoing ones (<xref ref-type="bibr" rid="bib52">Turrigiano et al., 1998</xref>; <xref ref-type="bibr" rid="bib22">Keck et al., 2013</xref>; <xref ref-type="bibr" rid="bib19">Hengen et al., 2013</xref>; <xref ref-type="bibr" rid="bib53">Turrigiano, 2008</xref>). This mechanism has been commonly attributed to the regulation of firing rates, while its functional implications remain mostly unclear, but of interest computationally and mechanistically (<xref ref-type="bibr" rid="bib9">El-Boustani et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Wu et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Keck et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Zenke and Gerstner, 2017</xref>; <xref ref-type="bibr" rid="bib50">Toyoizumi et al., 2014</xref>). A related computational feature has been presented by network models that include divisive normalization, suggested as an important component of computations performed by cortical circuits (<xref ref-type="bibr" rid="bib44">Simoncelli and Heeger, 1998</xref>).</p><p>Here, we bring these ideas together to present a biologically-inspired variant of a new family of statistical models for large neural population codes. Adding biological features to these population models enabled us to improve the models, and to explore designs that real neural circuits could employ to implement such models. Specifically, we expand the Random Projections (RP) model (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>), which was shown to be highly accurate in recapitulating the detailed spiking patterns of more than 100 neurons in different neural systems. Importantly, in addition to being accurate and requiring little amounts of training data, these RP models can be readily implemented by a simple neural circuit model – suggesting how real neural circuits can learn a statistical model of their own inputs and compute the likelihood of the inputs. We show that we can make these models better by ‘reshaping’ the randomly chosen sparse non-linear projections that they rely on, achieving highly accurate models using significantly fewer projections. We further show that reshaping of projections that incorporates normalization of synaptic weights during learning, results in more accurate models that are also more efficient, and makes the models homeostatic in terms of neural activity and total synaptic weights. Thus, we present a new class of accurate and efficient statistical models for large neural population codes that also suggests a clear computational benefit of homeostatic synaptic normalization and its potential role in biological neural networks and artificial ones.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The RP model is a class of highly accurate, scalabale, and efficient statistical models of the joint activity patterns of large populations of neurons (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Vertes and Sahani, 2018</xref>). These models are based on random and sparse nonlinear functions, or ‘projections’, of the population: Given a recording of the spiking activity of a population of neuorns, the model is a probability distribution over discrete activity patterns (quantized into small time bins, e.g. 10–20ms), that relies on a set of random non-linear functions of the population activity,<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>∑</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are randomly sampled coefficients such that most of them for any <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> are zero (i.e. the set is sparse), <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> are thresholds, and <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> are nonlinear functions (e.g. the Heaviside step function). The RP model is the maximum entropy distribution <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib20">Jaynes, 1957</xref>), which is consistent with the observed average values of the random projections, <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> (see Materials and methods). Thus, it is the least structured distribution that retains the average values of the projections, is mathematically unique, and is given by<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are Lagrange multipliers, and <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Z</mml:mi></mml:mstyle></mml:math></inline-formula> is a normalization factor or the ‘partition function’, which can be found numerically. Applied to cortical data from multiple areas (see, e.g. <xref ref-type="fig" rid="fig1">Figure 1A</xref>), this model proved to be highly accurate in predicting individual activity patterns, using small amounts of training data (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>). Importantly, unlike many other statistical models of population activity, RP models have a simple, biologically plausible neural circuit that can implement them (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>): <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows such a feed-forward circuit with one intermediate layer and an output neuron, where the random coefficients of the sparse projections, <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, are the synaptic weights connecting the input neurons <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> to an intermediate layer of neurons <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each intermediate neuron implements one projection of the input population. The Lagrange multipliers, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, are the synaptic weights connecting the intermediate layer to the output neuron, whose membrane potential or output gives the log-likelihood of the activity pattern of <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, up to a normalization factor.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Reshaped Random Projections (RP) models outperform RP models.</title><p>(<bold>A</bold>) A short segment of the spiking activity of 100 cortical neurons used for the analysis and comparison of different statistical models of population activity (see Materials and methods). (<bold>B</bold>) Schematics of the neural circuits that implement the different RP models we compared: The ‘standard’ RP model, where the coefficients <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> that define the projections are randomly selected and fixed whereas the factors <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are learned (see text). The Reshaped RP model, where the coefficients <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> that define the projections are tuned and the factors <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> s are fixed. The backpropagation model, where we tune both the <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> s and <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> s. (<bold>C</bold>) The predicted probability of individual populations’ activity patterns as a function of the observed probability for RP, Reshaped, and backpropagation models. Gray funnels denote 99% confidence interval. (<bold>D</bold>) Average performance of models of the three classes is shown as a function of the number of projections, measured by log-likelihood, over 100 sets of randomly selected groups of 50 neurons. Reshaped models outperform RP models and are on par with backpropagation; the shaded area denotes the standard error over 100 models. (<bold>E–F</bold>) Mean firing rates of projection neurons and mean correlation between projections. Reshaped models show lower correlations and lower firing rates compared to RP and backpropagation models. Standard errors are smaller than marker’s size, hence invisible.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Comparison of different Random Projections (RP) model variants .</title><p>(<bold>A</bold>) Average performance measured by log-likelihood (top), projections mean firing rates (bottom left), and mean correlation between projections (bottom right) of step and sigmoid RP models as a function of the number of projections. Sigmoid RP models outperform step RP models, and show higher mean firing rates and correlations. (<bold>B</bold>) Average performance measured by log-likelihood (top), projections mean firing rates (bottom left), and mean correlation between projections (bottom right) of RP and Reshaped RP models compared to RP models after pruning and replacement (P&amp;R). RP models after P&amp;R are very accurate even though a small number of parameters is optimized at each iteration. In all panels shaded area denotes standard error over 100 models.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Comparison of different Reshaped Random Projections (RP) model variants.</title><p>(<bold>A</bold>) Average performance measured by log-likelihood (top), projections mean firing rates (bottom left), and mean correlation between projections (bottom right) of Reshaped RP and backpropagation models with optimized threshold <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, compared to models with fixed thresholds. Threshold optimization has little to non effect on the models performance, mean firing rates, and mean correlations. (<bold>B</bold>) Average performance measured by log-likelihood (top), projections mean firing rates (bottom left), and mean correlation between projections (bottom right) of Reshaped RP models with different fixed values of λ. Models with higher values of λ show lower mean firing rates and mean correlation values. In all panels, shaded areas denote standard error over 100 models.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig1-figsupp2-v1.tif"/></fig></fig-group><p>The model in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> harbors a duality between the projections, <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and their coefficients, <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>: In the maximum entropy formalism of the model, the projections are randomly sampled and then fixed, and their corresponding weights, <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>’s, are tuned to maximize the entropy and satisfy the constraints. Alternatively, we may consider the case of training the model by keeping the <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>’s fixed and changing or tuning the projections <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to maximize the likelihood. In the corresponding neural circuit, this would imply that we would learn a circuit that implements the statistical model by training the sparse set of synaptic connections, <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, which define the projections, instead of training the synapses that weigh the projections, <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>Notably, a variant of the RP model in which projections that were weighted by a low value of <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are pruned and replaced with new RP proved to be more accurate than the original RP model, while using fewer projections (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>). This procedure of pruning and replacement is a crude form of learning of the model through changing the projections, and finding more efficient ones. We, therefore, asked here whether instead of the heuristic pruning and replacement, we can directly learn more accurate and efficient models by tuning the projections.</p><sec id="s2-1"><title>Reshaping RP gives more accurate and compact models</title><p>We first learned a new class of RP models for populations of tens of cortical neurons from the prefrontal cortex of monkeys performing a visual classification task (<xref ref-type="bibr" rid="bib24">Kiani et al., 2014</xref>) by tuning their randomly selected projections. Specifically, given an initial draw of sparse projections, the random weights that define the projections, <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, are then changed to maximize the likelihood of the model:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>η</mml:mi></mml:mstyle></mml:math></inline-formula> is the learning rate. We note that unlike the RP model presented in <xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>, here we used a sigmoid function for the nonlinearity of the projections,<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> sets the slope of the sigmoid. In this formulation, the model ranges from an independent model of the population for <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>, to the original RP model of <xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref> for <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mstyle></mml:math></inline-formula>. The rule for changing the projections (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) means that the specific set of inputs to each projection neuron is retained, but their relative weights are changed, and so the projections are ‘reshaped’.</p><p>We compared the RP and the Reshaped RP models by quantifying their performance on the same set of initial projections. We first learned the RP model as in <xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>, using a Heaviside non-linearity for the projections, and RP models that used a sigmoid non-linearity, where both models used the same set of RP, and found the latter models to be be more accurate (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). We then learned Reshaped RP models in which we optimize the same initial projections while keeping all <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>. We note that while in its maximum entropy formulation, the RP model is the unique solution to a convex optimization problem, the Reshaped RP models are not guaranteed to reach a global optimum. We also considered another class of models, in which the projections and the Lagrange multipliers <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are optimized simultaneously, similar to backpropagation-based learning used to train feed-forward neural networks (see Materials and methods). <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows an example of the accuracy of the sigmoid RP models, Reshaped RP models, and backpropagation-based models in predicting the probability of individual activity patterns for one group of 20 neurons, recorded from the cortex of behaving monkeys (<xref ref-type="bibr" rid="bib24">Kiani et al., 2014</xref>). The activity patterns are predicted by the reshaped RP model to an accuracy that is within the sampling noise (denoted by the 99% confidence interval funnel), and is similar to the performance of the full backpropagation model. The standard RP model, in comparison, has many more patterns that are outside the 99% confidence interval funnel. We quantified the performance of the three classes of models by calculating the mean log-likelihood of the models over 100 groups of 50 neurons on held out datasets, as a function of the number of projections that we used (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The reshaped models outperform the RP ones for a low number of projections, whereas the performances of all three models converge to a similar value for large number of projections.</p><p>Because reshaping may change all the existing synapses of each projection, the number of parameters is the number of projections times the projections in-degree. While this is much larger than the number of parameters that we learn for the RP model (one for each projection), we submit that the performance of the reshaped models is not a mere result of having more parameters. In particular, we have seen that RP models that use a small set of projections can be very accurate when the projections are optimized using the pruning and replacement process (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>; see also <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). Thus, it is really the nature of the projections that shapes the performance. Indeed, our results here show that a small fixed connectivity projection set with weight tuning is enough for accurate performance which is on par or better than an RP model with more projections.</p><p>To compare the ‘mechanistic’ nature of these different models, we calculated the mean correlation between the projections within each model class, and the average values of each projection (where the average is over the population activity patterns), which correspond to the mean firing rates of the neurons in the intermediate layer. Interestingly, the firing rates of the neurons in the intermediate layer are considerably lower for the reshaped models, and this sparseness in activity becomes more pronounced as a function of the number of projections (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). We further find that the correlations between the projections in the reshaped models are considerably lower compared to RP and backpropagation models (<xref ref-type="fig" rid="fig1">Figure 1F</xref>).</p><p>The projections’ thresholds <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, which are analogous to the spiking thresholds of the projection neurons, may affect the performance of the models. We, therefore, asked how optimizing <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, in addition to reshaping the coefficients of each projection, affect the reshaped RP and the backpropagation models. We find that this addition has a small effect on the performance of the models in terms of their likelihood (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). We also find that this has a small effect on the firing rates of the projection neurons: backpropagation models with tuned thresholds show lower firing rates compared to backpropagation models with fixed threshold, whereas reshaped RP models with optimized thresholds show higher firing rates compared to models with fixed threshold. Yet, both versions of the reshaped RP models show lower firing rates compared to both versions of the backpropagation models. Given the small effect of tuning threshold on models’ performance and their internal properties, we henceforth focus on Reshaped RP models with fixed thresholds.</p><p>An additional set of parameters that might affect the Reshaped RP models are the coefficients <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, that weigh each of the projections. Above, we used <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> for all projections, here we investigated the effect of the value of <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula> on the performance of the Reshaped RP models (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>). We find that for models with a small set of projections, high values of <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula> result in better performance than models with low values. We find an opposite relation for models with large number of projections. (We submit that the performance decrease of Reshaped RP models with high value of <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula>, as the number of projections grows, is a reflection of the non-convex nature of the Reshaped RP optimization problem). The mean firing rates of the projection neurons for models with different values of <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula> show a clear trend, where higher values result in lower mean firing rates. Thus, we conclude that there is an interplay between the number of projections and the value of <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula> one should pick. For the population sizes and projection sets we have used here, <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> is a good choice, but, we note that in general, one should seek the appropriate value of <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula> for different population sizes or data sets.</p><p>Thus, the reshaped projection models suggest a way to learn more accurate models of population activity, by tuning of projections. These models are also more efficient, requiring fewer projections. These projections also have lower firing rates (i.e. reshaped projections use fewer spikes), and they are less correlated. Given their accuracy and efficiency, we next asked how adding biological features or constraints to a Reshaped RP circuit may affect its performance and efficiency.</p></sec><sec id="s2-2"><title>Normalized reshaping of RP gives more accurate and efficient models</title><p>We studied the effect of adding two classes of biological features or constraints on the performance and nature of the Reshaped RP circuit model. The first constraint stems from the biophysical limits on individual synapses, and so we bound the maximal strength of individual synapses such that the strength of all synaptic weights are smaller than a ‘ceiling’ value: <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula>. The other is a normalization of the synaptic weights during the reshaping, inspired by the synaptic re-scaling that has been observed experimentally (<xref ref-type="bibr" rid="bib53">Turrigiano, 2008</xref>), and divisive normalization of synaptic weights (<xref ref-type="bibr" rid="bib18">Heeger, 1992</xref>). We consider multiple mechanisms of this kind later, but begin here with fixing the total sum of the incoming synaptic strength of each projection such that <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, when the strength of one synapse increases (decreases), the strength of the rest of the incoming synapses decreases (increases) such that the total synaptic weight incoming into the projection is kept constant. We term this constraint ‘homeostatic synaptic normalization’. We emphasize that the notion of homeostatic mechanisms is commonly reserved for designating regulation processes that retain a functional property of neurons, whereas normalization of synaptic weights might seem more mechanistic than functional. But, as we show later, learning with synaptic normalization also regulates the firing rate of the projection neurons, and so, we use this name henceforth.</p><p>To compare the effect of these constraints, we used the same set of initial RP, and then learn by reshaping them, each time with a different value of their corresponding parameters, <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula>. We estimated the likelihood of each of the models on 100 groups of 50 neurons, over 100 random sets of 150 projections. To quantify the ‘synaptic budget’ of each model, we measured the total sum of the absolute values of synaptic weights available to each model in units of the total synaptic strength of the initial set of projections (this is equivalent to defining the total sum of the synaptic weights of the initial set of projections as ‘1’, and then measuring total synaptic weights in these units). For the models with bounded synapses, the total available synaptic budget is given by the number of synapses times <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula>, whereas for the homeostatic constraint, it equals <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula> times the number of projections in the model. <xref ref-type="fig" rid="fig2">Figure 2B</xref> shows the log-likelihood of each model class vs. the total available synaptic budget of the different models: For a wide range of synaptic budgets, the homeostatic models outperform the bounded models, and only for very high values of available synaptic budget, the performance of the bounded models is on par with the homeostatic models.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Reshaped RP models that use homeostatic synaptic normalization outperform RP models and bounded RP models.</title><p>(<bold>A</bold>) Schematic drawing of the different models we studied: standard RP model, unconstrained reshaped model, and two types of constrained reshaped models: Bounded models in which each synapse separately obeys <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>ω</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> during learning, and normalized input reshaped models, where we fix the total synaptic weight of incoming synapses <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>B</bold>) The mean log-likelihood of the models is shown as a function of the total available budget. The normalized input reshaped RP models give optimal results for a wide range of values of available synaptic budget, outperforming the bounded models and the RP model. (<bold>C</bold>) The mean log-likelihood of the models is shown as a function of the total used budget. Aside from the bounded models, all other models are the same as in (<bold>B</bold>) by construction. For high available budget values, bounded models show better performance while utilizing a lower synaptic budget, similar to the unconstrained reshape model. (<bold>D</bold>) Comparison of the performance of 100 individual examples of each model class and their corresponding RP models, where all models relied on the same set of initial projections. Normalized input models outperformed the RP models in all cases (all points are above the diagonal), while all bounded models were worse (points below the diagonal). (<bold>E–F</bold>) The mean correlation and firing rates of projections as a function of the model’s cost. Normalized reshape models show low correlations and mean firing rates, similar to unconstrained reshaped models. Note that in panels B, C, E, and F, the standard errors are smaller than the marker size, and are therefore invisible.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Homeostatic models are superior to Reshaped RP models for every choice of <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula>.</title><p>Mean log-likelihood of homeostatic and reshaped RP models with different fixed values of <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula>, as a function total used synaptic cost. For every choice of <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula>, there exists a homeostatic model which is more efficient and preforms similarly or better than the corresponding reshaped RP model. Standard error is smaller than marker size, hence cannot be seen.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig2-figsupp1-v1.tif"/></fig></fig-group><p>The differences between the homeostatic normalization models and the bounded synaptic strength models are further reflected in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, which shows the performance of each model class as a function of the total sum of synaptic weights that is used by that model at the end of the training, <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. We note that the curve of the homeostatic model is identical to the one from <xref ref-type="fig" rid="fig2">Figure 2B</xref> by definition; the curve of the bounded models shows that at a certain value of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula> the sum of the synaptic weights starts to decrease and converges to the unconstrained reshaped model. The poor performance of the bounded models compared to the homeostatic ones suggests that the coupled changes in the synaptic weights improve learning. Specifically, during reshaping, the homeostatic models move synaptic “mass&quot; from less important synapses to more important ones. This redistribution of resources results in accurate models even for relatively low values of synaptic weights – making them more efficient in terms of the total synaptic weight needed.</p><p>The dominance of the homeostatic learning over the bounded synaptic weights is clear not just for the average over models, but also at the level of individual models: <xref ref-type="fig" rid="fig2">Figure 2D</xref> shows the performance of the homeostatic and bounded models that are initialized with the same set of RP; all bounded constraint models are inferior to the corresponding RP ones, whereas all the homeostatic constraint models are superior to the RP models (and clearly all the homeostatic models are superior to the corresponding bounded models).</p><p>We further find that the mean firing rates of the reshaped projection neurons, as well as the correlations between them, are lower in the homeostatic models compared to the bounded models (<xref ref-type="fig" rid="fig2">Figure 2E–F</xref>), making them more energetically efficient (in terms of spiking activity). We recall that this is consistent with the notions of efficient coding by decorrelated neural populations (<xref ref-type="bibr" rid="bib1">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib32">Olshausen and Field, 1997</xref>).</p><p>Exploring the effect of synaptic normalization on models with different values of <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), we find that homeostatic Reshaped RP models are superior to the non-homeostatic Reshaped RP models: For low values of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula>, the homeostatic and Reshaped RP models show similar performance in terms of log-likelihood, whereas the homeostatic models are more efficient. Importantly, for high values of <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> homeostatic models are not only more efficient but also show better performance. We conclude that the benefit of the homeostatic model is insensitive to the specific choice of <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s2-3"><title>Normalized reshaping of RP results in more efficient codes and homeostasis of firing rates</title><p>The experimental characterization of synaptic re-scaling has shown it to be a homeostatic mechanism that regulates the firing rates of neurons (<xref ref-type="bibr" rid="bib53">Turrigiano, 2008</xref>). We therefore asked whether the synaptic normalization we employ for the Reshaped RP models has a similar effect. <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows that the overall performance of the model in terms of capturing the population codebook is similar between the ‘free’ reshape model and different values of synaptic normalization. Similarly, reshaping with normalization or without it drives the projection neurons to converge to similar average firing rate values (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). However, the distribution of firing rates over the different neurons becomes narrower with tighter normalization values (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Importantly, while different normalization values imply very different initial firing rates of the projection neurons, after reshaping the values converge to similar average values (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Moreover, reshaping with normalization implies smaller changes in the reshaping process (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Thus, normalized reshaping results in homeostatic regulation of the firing rates, which validates the naming of these models as homeostatic normalization reshaping of RP.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Homeostatic models’ performance and firing rates.</title><p>(<bold>A</bold>) Likelihood values of the different homeostatic models we consider here; Error bars denote standard error over 100 different models. (<bold>B</bold>) Projections’ mean firing rates during reshaping; Standard error is smaller than marker size, hence cannot be seen (<bold>C</bold>) A histogram of the projections’ firing rates of unconstrained models and different homeostatic input models. (<bold>D</bold>) Projections’ mean firing rates after reshaping vs. the projection’s initial firing rate, for different homeostatic models and unconstrained model; Error bars denote standard deviation over 100 different models. (<bold>E</bold>) The cumulative density function of the projections’ firing rate relative change during reshape, <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig3-v1.tif"/></fig><p>Having established the computational benefits and efficiency of the homeostatic reshaped projection models that rely on synaptic normalization, we turned to ask how the connectivity itself, rather than the synaptic weights, may affect the performance of the models.</p></sec><sec id="s2-4"><title>Optimal sparseness of Reshaped Projections models under homeostatic constraints</title><p>The benefits of reshaping a given set of projections, reflected in the figures above, raise the question of the importance of the nature of the RP we choose (which are then reshaped). We, therefore, asked how the initial random ‘wiring’ of the projections affects the performance of the model, and whether non-RP would result in even better models. To quantify the effects of the projections’ connectivity on the performance and efficiency of reshaped models, we used simulated population activity that we generated using RP models that were trained on real data. By using synthetic data that was generated by a known model, we can compare the learned models to the ‘ground truth’ in terms of connectivity, as well as extensively sampling of activity patterns from the model.</p><p>We learned homeostatic reshaped models for the synthetic data, using different initial connectivity structures (<xref ref-type="fig" rid="fig4">Figure 4A–B</xref>): (i) A ‘true’ connectivity model in which we reshaped a RP model that has the same connectivity as the projections of the model that generated the data. (ii) A Random connectivity model in which we reshaped projections with sparse and connectivity that is randomly sampled and is independent of the model that generated the synthetic data. (iii) A full connectivity model in which we reshaped RP with full connectivity, that is all input neurons are connected to all the projections, but with random initial weights. We carried out homeostatic reshaping of the projections in all three models with different values of <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula>. Surprisingly, the true and random connectivity models performed very similarly (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Although the full connectivity model contains the ‘ground truth’ connectivity, and could recreate the true connectivity by canceling out unnecessary synapses during reshaping – we find that the full connectivity models are inferior to the other models, except for the case of high model costs.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Models that rely on projections that use random connectivity show similar performance to models that use the correct connectivity.</title><p>(<bold>A</bold>) Synthetic population activity data is sampled from an RP model with known connectivity (i.e. the ‘ground truth’ model; see Materials and methods). (<bold>B</bold>) Homeostatic Reshaped random projections models that differ in their connectivity are learned to fit the synthetic data. The ‘True connectivity’ model uses projections whose connectivity is identical to the ‘ground truth’ model. The ‘Random connectivity’ model uses projections that are randomly sampled using sparse random connectivity. The ‘Full connectivity’ model is a homeostatic reshaped model that uses projections with full connectivity. (<bold>C</bold>) The mean log-likelihood of the models is shown as a function of the model’s cost. The true connectivity model is only slightly better than the random connectivity model, with both outperforming the full connectivity model for low model budget values. (<bold>D</bold>) The firing rates of the projection neurons, shown as a function of the model cost. (<bold>E</bold>) The mean correlation between the activity of the projection neurons, shown as a function of the model cost. We note that true and random connectivity models are indistinguishable. (<bold>F</bold>) The performance of homeostatic reshaped RP models, shown as a function of their normalized in-degree of the projections (0–disconnected, 1–fully connected), for different normalization values, shown by the model’s synaptic cost. (<bold>G</bold>) The performance of the homeostatic Reshaped RP models, shown as a function of the synaptic cost normalized by the in-degree of the projections. Curves of different cost values coincide, suggesting a fixed optimal cost/activity ratio. Note that in panels D-G the standard errors over 100 models are smaller than the size of markers and are, therefore, invisible.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig4-v1.tif"/></fig><p>The mean correlations between projections at the end of reshaping and the mean firing rates of the models that use the true and random connectivity were also very similar (<xref ref-type="fig" rid="fig4">Figure 4D–E</xref>), whereas the full connectivity models showed, again, very different behavior. These results reflect another computational benefit of homeostatic reshaping: there is no need to know the optimal circuit connectivity, and there is no apparent benefit to all-to-all connectivity, which would be expensive in terms of the energetic cost, the space needed, and the biological construction. Thus, starting from random connectivity and optimizing the circuit under homeostatic constraints seems to provide optimal results.</p><p>Given the inefficiency of the fully connected reshaped projections model, we also quantified the effect of the sparseness of the projections on reshaped RP models. We recall that for the standard RP model, sparse projections were optimal for a wide range of network sizes (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>), and so we measured the performance of homeostatic reshaped RP models for different values of in-degree of the projections, while keeping the total synaptic budget of the models fixed. We found that different synaptic budgets have a different optimal in-degree (<xref ref-type="fig" rid="fig4">Figure 4F</xref>), and that the value of the optimal in-degree seems to grow with the total synaptic budget.</p><p>We further estimated the efficiency of the models by the synaptic cost per connection in the projections (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). We find that curves for different total synaptic costs seem to coincide and have a similar peak value – suggesting an optimal ratio between the total available resources and the number of synapses.</p></sec><sec id="s2-5"><title>Different homeostatic mechanisms for reshaping RP models result in different projection sets</title><p>We explored two other forms of synaptic normalization rules for the reshaping of projections (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). In the first, we fixed and normalized the outgoing synapses from each neuron, such that <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the second, we kept the total synaptic weight of the whole circuit fixed, namely, <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="fig" rid="fig5">Figure 5B</xref> shows that the performance of the models that use these other homeostatic mechanisms is surprisingly similar in terms of the model’s likelihood over the test data, as well as the firing rates of the projection neurons (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>), and correlations between them (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Different homeostatic synaptic normalization mechanisms result in similar model behavior, for a wide range of random projection parameters.</title><p>(<bold>A</bold>) Schematic drawings of the different homeostatic models we compared: Homeostatic input models in which we fixed the total synaptic weight of the incoming synapses <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>; Homeostatic output models in which we fixed the total synaptic weight of the outgoing synapses <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>; and Homeostatic circuit models in which we fixed the total synaptic weight of the whole synaptic circuit <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>B</bold>) The mean log-likelihood of models, shown as a function of the total used synaptic cost. All three homeostatic model variants show similar behavior. (<bold>C</bold>) Schematic drawing of how projections rotate during reshaping: starting from the initial projections (grey lines), they rotate to their reshaped orientation (black lines) by angle <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. (<bold>D</bold>) Rotation angles after reshaping, shown for different pairs of models. All four panels show models that initialized with the same set of projections. The different labels specify the constraint type and strength, namely, the specific value of <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula>. (<bold>E</bold>) The mean rotation angle of the projections due to reshaping, shown as a function of the model synaptic budget.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Homeostatic model variants show similar firing rates and correlation, but different rotation angles and distribution of weights.</title><p>(<bold>A</bold>) and (<bold>B</bold>) Mean correlation between the projection neurons and mean firing rates as a function of the model cost of the three different homeostatic models we compared. All three homeostatic models show a similar behavior. Note that the standard errors over 100 models are smaller than the marker’s size and are, therefore, invisible. (<bold>C</bold>) Histograms of rotation angles of the projections after learning in the case of the unconstrained reshape models and different homeostatic input (top) and homeostatic circuit (bottom) models. (<bold>D</bold>) Histograms of weight distribution after learning in the case of the unconstrained reshape models and different homeostatic input (top) and homeostatic circuit (bottom) models. The legend specifies the constraint type and strength, namely, the specific value of Φ.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96566-fig5-figsupp1-v1.tif"/></fig></fig-group><p>As the homeostatic reshaping of RP proved to be similarly accurate and efficient for the three homeostatic model variants, we asked which features of normalized reshaping might differentiate between homeostatic models in terms of their performance. Since each projection defines a hyperplane in the space of population activity patterns, reshaping can be interpreted as a rotation or a change of the angle of these hyperplanes, depicted schematically in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. We, therefore, compared the different homeostatic variants of the reshaped projections models by initializing them from the same set of RP, and evaluating the corresponding rotation angles, <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula>, of all of the projections due to the reshaping.</p><p><xref ref-type="fig" rid="fig5">Figure 5D</xref> shows an example of the rotations of the same initial projections for one model under different reshaping constraints. While the rotation angles of the bounded model with a high value of <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula> is almost identical to the rotation angels of the unconstrained Reshaped RP model (<xref ref-type="fig" rid="fig5">Figure 5D</xref> top left), as one would expect, the other three panels in 5D reflect substantial differences between models reshaped under different conditions: unconstrained Reshaped RP model vs. a homeostatic one (bottom left), different homeostatic model variants with the same synaptic cost (top right), and homeostatic models with different synaptic cost (bottom right).</p><p><xref ref-type="fig" rid="fig5">Figure 5E</xref> shows the mean rotation angle over 100 homeostatic models as a function of synaptic cost – reflecting that the different forms of homeostatic regulation results in different reshaped projections. We show in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref> the histogram of the rotation angles of several different homeostatic models, as well as the unconstrained Reshape model. Interestingly, although the three homeostatic variants show unique rotation angle histograms, they all show a similar minimal mean rotation angle at the same value of synaptic cost. We note that while there is dependency or even redundancy between these different homeostatic mechanisms, it is not immediately clear why their minimal values would be so similar. Analyzing the distribution of the synaptic weights <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> after learning leads to a similar conclusion (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D</xref>): The peak of the histograms is at <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>, implying that during reshaping most synapses are effectively pruned. While the distribution is broader for models with higher synaptic budget, it is asymmetric, showing local maxima at different values of <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p><p>The diversity of solutions that the different model classes and parameters show imply a form of redundancy in model choice or learning procedure. This reflects a multiplicity of ways to learn or optimize such networks, that biology could use to shape or tune neural population codes.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We presented a new family of statistical models for large neural populations that is based on sparse and random non-linear projections of the population, which are adapted during learning. This new family of models proved to be more accurate than the highly accurate RP class of models, using fewer projections and incurring a lower ‘synaptic cost’ in terms of the total sum of synaptic weights of the model. Moreover, we found that reshaping of the projections gave even more accurate and efficient models in terms of synaptic weights of the neural circuit that implements the model, and was optimal for random and sparse initial connectivity, surpassing fully connected network models. The synaptic normalization mechanism resulted in homeostatic regulation of the firing rates of neurons in the model.</p><p>Our results suggest a computational role for the experimentally observed scaling or normalization of synapses during learning: In addition to ‘regularizing’ the firing rates in neural circuits, in our Reshaped RP models, homeostatic plasticity optimizes the efficiency of network models in scenarios of limited resources and random connectivity. Moreover, the similarity of the performance of models that use different homeostatic synaptic mechanisms suggests a possible universal role for homeostatic mechanisms in computation.</p><p>We note that while homeostatic synaptic scaling regulates the firing rates of neurons (<xref ref-type="bibr" rid="bib53">Turrigiano, 2008</xref>), it is not immediately clear what ‘sets’ the desired firing rate of each neuron. The synaptic normalization constraints we used here offer a simple solution: a universal value of the total incoming synaptic weights for the neurons in the circuit (or outgoing ones), results in a widely distributed firing rates of neurons (which may change considerably during the learning), but converge to a similar average value. Thus, rather than requiring some mechanism to define and balance the firing rates of individual neurons, our model suggest a single global synaptic feature that would set this for the RP.</p><p>The shallowness of the circuit implementation of the Reshaped RP model implies that the learning of these models does not require the backpropagation of information over many layers, which distinguishes deep artificial networks from biological ones. Moreover, the locality of the reshaping process itself points to the feasibility of this model in terms of real biological circuits. The biological plausibility is further supported by the robustness of the model to the specific connectivity used for the reshaped models, and to the specific choice of the homeostatic mechanism we used.</p><p>A key remaining issue for the biological feasibility of the RP family of models is the feedback signal from the readout neuron to the intermediate neurons. The noise-dependent learning mechanism for RP models presented in <xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref> and for other local feedback and synaptic learning mechanisms that approximate backprogapation (<xref ref-type="bibr" rid="bib37">Poirazi et al., 2003</xref>) offers clear directions for future study. Our results may also be relevant for learning in artificial neural networks, whose training relies on non-convex approaches that necessitate different regularization techniques (<xref ref-type="bibr" rid="bib14">Goodfellow et al., 2016</xref>). The homeostatic mechanism we focused on here is a form of ‘hard’ L1 regularization, but on the sum of the weights. This approach limits the search space, compared to regularization over the weights themselves, but defines coupled changes in weights, in a manner highly effective for the cortical data we studied. We, therefore, hypothesize that homeostatic normalization may be beneficial for artificial architectures (see, e.g. <xref ref-type="bibr" rid="bib61">Zhong et al., 2022</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental data</title><p>Extra-cellular recordings were performed using Utah arrays from populations of neurons in the prefrontal cortex of macaque monkeys performing a direction discrimination task with random dots. For more details see <xref ref-type="bibr" rid="bib24">Kiani et al., 2014</xref>.</p></sec><sec id="s4-2"><title>Data pre-processing</title><p>Neural activity was discretized using 20ms bins, such that in each time bin a neuron was active (‘1’) if it emitted a spike in that bin and silent (‘0’) if not. Recorded data was split randomly into training sets and held-out test sets: 100 different random splits were generated for each model setup, consisting of 160,000 samples in the training set and 40,000 in the test set.</p></sec><sec id="s4-3"><title>Constructing sparse RP</title><p>Following (<xref ref-type="bibr" rid="bib27">Maoz et al., 2020</xref>), the coefficients <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the RP are set using a two stage process. First, the connectivity of the projections is set such that the average in-degree (<inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) of the projections matches a predetermined sparsity value: each input neuron connects to each projection with a probability <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of neurons in the input layer. The corresponding <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> coefficients are then sampled from a Gaussian distribution, <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, and the remaining <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> values are set to zero. The threshold of each projection, <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, was set to 1.</p><p>The average in-degree of sparse models used here was 5, unless specified otherwise in the text. For the fully connected models <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> (i.e. sparsity =0).</p></sec><sec id="s4-4"><title>Training RP models</title><p>Given empirical data <bold>X</bold> and a set of projections defined by <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, we train the RP models by searching for the parameters <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> that maximize the log-likelihood of the model given the data, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is a convex function whose gradient is given by<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We found the values <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> that maximize the log-likelihood by gradient descent with momentum or ADAM algorithms. We computed the empirical expectation in <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> by summing over the training data, and the expectation over the probability model <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> by summing over synthetic data generated from <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> using Metropolis–Hasting sampling.</p><p>For each of the empirical marginals <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, we used the Clopper–Pearson method to estimate the distribution of possible values for the real marginal given the empirical observation. We set the convergence threshold of the numerical solver such that each of the marginals in the model distribution falls within a Confidence Interval of one Standard Deviation under this distribution, from its empirical marginal.</p></sec><sec id="s4-5"><title>Reshaping RP models</title><p>Given empirical data <bold>X</bold>, we optimize the RP models by modifying the coefficients <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> such that the log-likelihood of the model is maximized, <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. Starting from an initial set of projections, <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, using the update rule of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, we optimize the projections by applying the gradient descent with momentum algorithm. Importantly, only non-zero elements of <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are optimized.</p></sec><sec id="s4-6"><title>Optimizing backpropagation models</title><p>Full backpropagation models are optimized using the learning rules of the trained RP models and the reshaped models simultaneously in each gradient descent step, that is <xref ref-type="disp-formula" rid="equ3 equ5">Equations 3 and 5</xref>.</p></sec><sec id="s4-7"><title>Homeostatic reshaping of RP models</title><p>The homeostatic RP models are reshaped as follows: We first define a set of unconstrained projections where the coefficients <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are randomly sampled. Each of the projections is then normalized homeostatically, such that <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are a function of this unconstrained set: <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϕ</mml:mi></mml:mstyle></mml:math></inline-formula> is the available synaptic budget for each projection. We then optimize <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to maximize the log-likelihood of the model given the empirical data <bold>X</bold>: <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The computed constrained projections <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are then used in the resulting homeostatic RP model.</p></sec><sec id="s4-8"><title>Bounded reshaping of RP models</title><p>Similar to reshaping homeostatic RP models, we define a set of unconstrained projections <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, where the projections are a function of this unconstrained set: <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi></mml:mstyle></mml:math></inline-formula> is the ‘ceiling’ value of each synapse.</p></sec><sec id="s4-9"><title>Generating synthetic data from RP models with known connectivity</title><p>Synthetic neural activity patterns were obtained by training RP models on real neural recordings as described above and then generating data from these models using Metropolis–Hastings sampling.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-96566-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. The software that was developed for this work can be found on <ext-link ext-link-type="uri" xlink:href="https://github.com/schneidmanlab/maximum_entropy_package">GitHub</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib41">Schneidman, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Adam Haber, Tal Tamir, Udi Karpas, and the rest of the Schneidman lab members for discussions, comments, and ideas. This work was supported by Simons Collaboration on the Global Brain grant 542997, Israel Science Foundation grant 137628, The Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 454648639 - SFB 1528, Israeli Council for Higher Education/Weizmann Data Science Research Center, Martin Kushner Schnur, and Mr. &amp; Mrs. Lawrence Feis. ES is the incumbent of the Joseph and Bessie Feinberg Chair. This research was also supported in part by grants NSF PHY-1748958 and PHY-2309135 and the Gordon and Betty Moore Foundation Grant No. 2919.02 to the Kavli Institute for Theoretical Physics (KITP).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory Communication</source><volume>1</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.003.0013</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>DG</given-names></name><name><surname>Morcos</surname><given-names>AS</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Analyzing biological and artificial neural networks: challenges with opportunities for synergy?</article-title><source>Current Opinion in Neurobiology</source><volume>55</volume><fpage>55</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.01.007</pub-id><pub-id pub-id-type="pmid">30785004</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>DH</given-names></name><name><surname>Bornschein</surname><given-names>J</given-names></name><name><surname>Mesnard</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Towards Biologically Plausible Deep Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1502.04156">https://arxiv.org/abs/1502.04156</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botella-Soler</surname><given-names>V</given-names></name><name><surname>Deny</surname><given-names>S</given-names></name><name><surname>Martius</surname><given-names>G</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Nonlinear decoding of a complex movie from the mammalian retina</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006057</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006057</pub-id><pub-id pub-id-type="pmid">29746463</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calabrese</surname><given-names>A</given-names></name><name><surname>Schumacher</surname><given-names>JW</given-names></name><name><surname>Schneider</surname><given-names>DM</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Woolley</surname><given-names>SMN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A generalized linear model for estimating spectrotemporal receptive fields from responses to natural sounds</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e16104</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0016104</pub-id><pub-id pub-id-type="pmid">21264310</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhuri</surname><given-names>R</given-names></name><name><surname>Gerçek</surname><given-names>B</given-names></name><name><surname>Pandey</surname><given-names>B</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1512</fpage><lpage>1520</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0460-x</pub-id><pub-id pub-id-type="pmid">31406365</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Drawing inspiration from biological dendrites to empower artificial neural networks</article-title><source>Current Opinion in Neurobiology</source><volume>70</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2021.04.007</pub-id><pub-id pub-id-type="pmid">34087540</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Boustani</surname><given-names>S</given-names></name><name><surname>Ip</surname><given-names>JPK</given-names></name><name><surname>Breton-Provencher</surname><given-names>V</given-names></name><name><surname>Knott</surname><given-names>GW</given-names></name><name><surname>Okuno</surname><given-names>H</given-names></name><name><surname>Bito</surname><given-names>H</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Locally coordinated synaptic plasticity of visual cortex neurons in vivo</article-title><source>Science</source><volume>360</volume><fpage>1349</fpage><lpage>1354</lpage><pub-id pub-id-type="doi">10.1126/science.aao0862</pub-id><pub-id pub-id-type="pmid">29930137</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Long-term stability of cortical population dynamics underlying consistent behavior</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>260</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0555-4</pub-id><pub-id pub-id-type="pmid">31907438</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganmor</surname><given-names>E</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sparse low-order interaction network underlies a highly correlated and learnable neural population code</article-title><source>PNAS</source><volume>108</volume><fpage>9679</fpage><lpage>9684</lpage><pub-id pub-id-type="doi">10.1073/pnas.1019641108</pub-id><pub-id pub-id-type="pmid">21602497</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganmor</surname><given-names>E</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A thesaurus for a neural population code</article-title><source>eLife</source><volume>4</volume><elocation-id>e06134</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06134</pub-id><pub-id pub-id-type="pmid">26347983</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonçalves</surname><given-names>PJ</given-names></name><name><surname>Lueckmann</surname><given-names>J-M</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Nonnenmacher</surname><given-names>M</given-names></name><name><surname>Öcal</surname><given-names>K</given-names></name><name><surname>Bassetto</surname><given-names>G</given-names></name><name><surname>Chintaluri</surname><given-names>C</given-names></name><name><surname>Podlaski</surname><given-names>WF</given-names></name><name><surname>Haddad</surname><given-names>SA</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title><source>eLife</source><volume>9</volume><elocation-id>e56261</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56261</pub-id><pub-id pub-id-type="pmid">32940606</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Deep Learning</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granot-Atedgi</surname><given-names>E</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stimulus-dependent maximum entropy models of neural population codes</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002922</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002922</pub-id><pub-id pub-id-type="pmid">23516339</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haber</surname><given-names>A</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Learning the architectural features that predict functional similarity of neural networks</article-title><source>Physical Review X</source><volume>12</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.12.021051</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haber</surname><given-names>A</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022b</year><chapter-title>The computational and learning benefits of daleian neural networks</chapter-title><person-group person-group-type="editor"><name><surname>Koyejo</surname><given-names>S</given-names></name><name><surname>Mohamed</surname><given-names>S</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Belgrave</surname><given-names>D</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Oh</surname><given-names>A</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>5194</fpage><lpage>5206</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Normalization of cell responses in cat striate cortex</article-title><source>Visual Neuroscience</source><volume>9</volume><fpage>181</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1017/s0952523800009640</pub-id><pub-id pub-id-type="pmid">1504027</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hengen</surname><given-names>KB</given-names></name><name><surname>Lambo</surname><given-names>ME</given-names></name><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Katz</surname><given-names>DB</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Firing rate homeostasis in visual cortex of freely behaving rodents</article-title><source>Neuron</source><volume>80</volume><fpage>335</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.038</pub-id><pub-id pub-id-type="pmid">24139038</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaynes</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Information theory and statistical mechanics</article-title><source>Physical Review</source><volume>106</volume><fpage>620</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1103/PhysRev.106.620</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Karpas</surname><given-names>ED</given-names></name><name><surname>Maoz</surname><given-names>O</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Strongly Correlated Spatiotemporal Encoding and Simple Decoding in the Prefrontal Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/693192</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keck</surname><given-names>T</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Jacobsen</surname><given-names>RI</given-names></name><name><surname>Eysel</surname><given-names>UT</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Synaptic scaling and homeostatic plasticity in the mouse visual cortex in vivo</article-title><source>Neuron</source><volume>80</volume><fpage>327</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.018</pub-id><pub-id pub-id-type="pmid">24139037</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keck</surname><given-names>T</given-names></name><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Feldman</surname><given-names>DE</given-names></name><name><surname>Fox</surname><given-names>K</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Haydon</surname><given-names>PG</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>HK</given-names></name><name><surname>Lisman</surname><given-names>JE</given-names></name><name><surname>Rose</surname><given-names>T</given-names></name><name><surname>Sengpiel</surname><given-names>F</given-names></name><name><surname>Stellwagen</surname><given-names>D</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>van Rossum</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integrating Hebbian and homeostatic plasticity: the current state of the field and future research directions</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160158</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0158</pub-id><pub-id pub-id-type="pmid">28093552</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamics of neural population responses in prefrontal cortex indicate changes of mind on single trials</article-title><source>Current Biology</source><volume>24</volume><fpage>1542</fpage><lpage>1547</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.049</pub-id><pub-id pub-id-type="pmid">24954050</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>SS</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Generation of stable heading representations in diverse visual scenes</article-title><source>Nature</source><volume>576</volume><fpage>126</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1767-1</pub-id><pub-id pub-id-type="pmid">31748750</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal degrees of synaptic connectivity</article-title><source>Neuron</source><volume>93</volume><fpage>1153</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.030</pub-id><pub-id pub-id-type="pmid">28215558</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maoz</surname><given-names>O</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Esteki</surname><given-names>MS</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning probabilistic neural representations with randomly connected circuits</article-title><source>PNAS</source><volume>117</volume><fpage>25066</fpage><lpage>25073</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912804117</pub-id><pub-id pub-id-type="pmid">32948691</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>El Boustani</surname><given-names>S</given-names></name><name><surname>Frégnac</surname><given-names>Y</given-names></name><name><surname>Destexhe</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Prediction of spatiotemporal patterns of neural activity from pairwise correlations</article-title><source>Physical Review Letters</source><volume>102</volume><elocation-id>138101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.102.138101</pub-id><pub-id pub-id-type="pmid">19392405</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meshulam</surname><given-names>L</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Collective behavior of place and non-place neurons in the hippocampal network</article-title><source>Neuron</source><volume>96</volume><fpage>1178</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.10.027</pub-id><pub-id pub-id-type="pmid">29154129</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meshulam</surname><given-names>L</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coarse graining, fixed points, and scaling in a large population of neurons</article-title><source>Physical Review Letters</source><volume>123</volume><elocation-id>178103</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.123.178103</pub-id><pub-id pub-id-type="pmid">31702278</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohiorhenuan</surname><given-names>IE</given-names></name><name><surname>Mechler</surname><given-names>F</given-names></name><name><surname>Purpura</surname><given-names>KP</given-names></name><name><surname>Schmid</surname><given-names>AM</given-names></name><name><surname>Hu</surname><given-names>Q</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Sparse coding and high-order correlations in fine-scale cortical networks</article-title><source>Nature</source><volume>466</volume><fpage>617</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1038/nature09178</pub-id><pub-id pub-id-type="pmid">20601940</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title><source>Vision Research</source><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00169-7</pub-id><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Collins</surname><given-names>J</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><volume>15</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id><pub-id pub-id-type="pmid">30224673</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Fellin</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cracking the neural code for sensory perception by combining statistics, intervention, and behavior</article-title><source>Neuron</source><volume>93</volume><fpage>491</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.036</pub-id><pub-id pub-id-type="pmid">28182905</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pechuk</surname><given-names>V</given-names></name><name><surname>Goldman</surname><given-names>G</given-names></name><name><surname>Salzberg</surname><given-names>Y</given-names></name><name><surname>Chaubey</surname><given-names>AH</given-names></name><name><surname>Bola</surname><given-names>RA</given-names></name><name><surname>Hoffman</surname><given-names>JR</given-names></name><name><surname>Endreson</surname><given-names>ML</given-names></name><name><surname>Miller</surname><given-names>RM</given-names></name><name><surname>Reger</surname><given-names>NJ</given-names></name><name><surname>Portman</surname><given-names>DS</given-names></name><name><surname>Ferkey</surname><given-names>DM</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Oren-Suissa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reprogramming the topology of the nociceptive circuit in <italic>C. elegans</italic> reshapes sexual behavior</article-title><source>Current Biology</source><volume>32</volume><fpage>4372</fpage><lpage>4385</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.08.038</pub-id><pub-id pub-id-type="pmid">36075218</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id><pub-id pub-id-type="pmid">18650810</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Brannon</surname><given-names>T</given-names></name><name><surname>Mel</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pyramidal neuron as two-layer neural network</article-title><source>Neuron</source><volume>37</volume><fpage>989</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00149-1</pub-id><pub-id pub-id-type="pmid">12670427</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title><source>Nature</source><volume>440</volume><fpage>1007</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1038/nature04701</pub-id><pub-id pub-id-type="pmid">16625187</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Towards the design principles of neural population codes</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>133</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.03.001</pub-id><pub-id pub-id-type="pmid">27016639</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Maximum_entropy_package</data-title><version designator="swh:1:rev:37c4411d2775cddea841d2b38c4b813738277435">swh:1:rev:37c4411d2775cddea841d2b38c4b813738277435</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:1bfa36ed070e8eb52cb4f4478e78f16ef7bb4ea5;origin=https://github.com/schneidmanlab/maximum_entropy_package;visit=swh:1:snp:08717aa5d91cd6c1838f3d82c15db922372d796b;anchor=swh:1:rev:37c4411d2775cddea841d2b38c4b813738277435">https://archive.softwareheritage.org/swh:1:dir:1bfa36ed070e8eb52cb4f4478e78f16ef7bb4ea5;origin=https://github.com/schneidmanlab/maximum_entropy_package;visit=swh:1:snp:08717aa5d91cd6c1838f3d82c15db922372d796b;anchor=swh:1:rev:37c4411d2775cddea841d2b38c4b813738277435</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Q</given-names></name><name><surname>Gupta</surname><given-names>P</given-names></name><name><surname>Boukhvalova</surname><given-names>AK</given-names></name><name><surname>Singer</surname><given-names>JH</given-names></name><name><surname>Butts</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Functional characterization of retinal ganglion cells using tailored nonlinear modeling</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>8713</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-45048-8</pub-id><pub-id pub-id-type="pmid">31213620</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Grivich</surname><given-names>MI</given-names></name><name><surname>Petrusca</surname><given-names>D</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The structure of multi-neuron firing patterns in primate retina</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>8254</fpage><lpage>8266</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1282-06.2006</pub-id><pub-id pub-id-type="pmid">16899720</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A model of neuronal responses in visual area MT</article-title><source>Vision Research</source><volume>38</volume><fpage>743</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00183-1</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>A</given-names></name><name><surname>Jackson</surname><given-names>D</given-names></name><name><surname>Hobbs</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Smith</surname><given-names>JL</given-names></name><name><surname>Patel</surname><given-names>H</given-names></name><name><surname>Prieto</surname><given-names>A</given-names></name><name><surname>Petrusca</surname><given-names>D</given-names></name><name><surname>Grivich</surname><given-names>MI</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Beggs</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>505</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3359-07.2008</pub-id><pub-id pub-id-type="pmid">18184793</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Granot-Atedgi</surname><given-names>E</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Retinal metric: a stimulus distance measure derived from population neural responses</article-title><source>Physical Review Letters</source><volume>110</volume><fpage>1079</fpage><lpage>7114</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.110.058104</pub-id><pub-id pub-id-type="pmid">23414051</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name><name><surname>Amodei</surname><given-names>D</given-names></name><name><surname>Berry II</surname><given-names>MJ</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>The simplest maximum entropy model for collective behavior in a neural network</article-title><source>Journal of Statistical Mechanics</source><volume>2013</volume><elocation-id>03011</elocation-id><pub-id pub-id-type="doi">10.1088/1742-5468/2013/03/P03011</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Amodei</surname><given-names>D</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Searching for collective behavior in a large network of sensory neurons</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003408</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003408</pub-id><pub-id pub-id-type="pmid">24391485</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Amodei</surname><given-names>D</given-names></name><name><surname>Palmer</surname><given-names>SE</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Thermodynamics and signatures of criticality in a network of neurons</article-title><source>PNAS</source><volume>112</volume><fpage>11508</fpage><lpage>11513</lpage><pub-id pub-id-type="doi">10.1073/pnas.1514188112</pub-id><pub-id pub-id-type="pmid">26330611</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Kaneko</surname><given-names>M</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Modeling the dynamic interaction of hebbian and homeostatic plasticity</article-title><source>Neuron</source><volume>84</volume><fpage>497</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.036</pub-id><pub-id pub-id-type="pmid">25374364</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truccolo</surname><given-names>W</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name><name><surname>Fellows</surname><given-names>MR</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>1074</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1152/jn.00697.2004</pub-id><pub-id pub-id-type="pmid">15356183</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Leslie</surname><given-names>KR</given-names></name><name><surname>Desai</surname><given-names>NS</given-names></name><name><surname>Rutherford</surname><given-names>LC</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title><source>Nature</source><volume>391</volume><fpage>892</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1038/36103</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The self-tuning neuron: synaptic scaling of excitatory synapses</article-title><source>Cell</source><volume>135</volume><fpage>422</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2008.10.008</pub-id><pub-id pub-id-type="pmid">18984155</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vertes</surname><given-names>E</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible and Accurate Inference and Learning for Deep Generative Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1805.11051">https://arxiv.org/abs/1805.11051</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>F</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Disentangling the functional consequences of the connectivity between optic-flow processing neurons</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>441</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nn.3044</pub-id><pub-id pub-id-type="pmid">22327473</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Whiteway</surname><given-names>MR</given-names></name><name><surname>Averbeck</surname><given-names>B</given-names></name><name><surname>Butts</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Latent Variable Approach to Decoding Neural Population Activity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.06.896423</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>YK</given-names></name><name><surname>Hengen</surname><given-names>KB</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Homeostatic mechanisms regulate distinct aspects of cortical circuit dynamics</article-title><source>PNAS</source><volume>117</volume><fpage>24514</fpage><lpage>24525</lpage><pub-id pub-id-type="doi">10.1073/pnas.1918368117</pub-id><pub-id pub-id-type="pmid">32917810</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zemel</surname><given-names>RS</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Probabilistic interpretation of population codes</article-title><source>Neural Computation</source><volume>10</volume><fpage>403</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1162/089976698300017818</pub-id><pub-id pub-id-type="pmid">9472488</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hebbian plasticity requires compensatory processes on multiple timescales</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160259</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0259</pub-id><pub-id pub-id-type="pmid">28093557</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>W</given-names></name><name><surname>Sorscher</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>A theory of weight distribution-constrained learning</chapter-title><person-group person-group-type="editor"><name><surname>Oh</surname><given-names>AH</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Belgrave</surname><given-names>D</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96566.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Salk Institute for Biological Studies</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This work is an <bold>important</bold> contribution to the development of a biologically plausible theory of statistical modeling of spiking activity. The authors <bold>convincingly</bold> implemented the statistical inference of input likelihood in a simple neural circuit, demonstrating the relationship between synaptic homeostasis, neural representations, and computational accuracy. This work will be of interest to neuroscientists, both theoretical and experimental, who are exploring how statistical computation is implemented in neural networks.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96566.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>A novel statistical model of neural population activity called the Random Projection model has been recently proposed. Not only is this model accurate, efficient, and scalable, but also is naturally implemented as a shallow neural network. This work proposes a new class of RP model called the reshaped RP model. Inheriting the virtue of the original RP model, the proposed model is more accurate in terms of data fitting and efficient in terms of lower firing rate than the original, as well as compatible with various biological constraints. In particular, the authors have demonstrated that normalizing the total synaptic input in the reshaped model has a homeostatic effect on the firing rates of the neurons, resulting in even more efficient representations with equivalent accuracy. These results suggest that synaptic normalization contributes to synaptic homeostasis as well as efficiency in neural encoding.</p><p>Strength</p><p>This paper demonstrates that the accuracy and efficiency of the random projection models can be improved by extending the model with reshaped projections. Furthermore, it broadens the applicability of the model under biological constraints of synaptic regularization. It also suggests the advantage of the sparse connectivity structure over the fully connected model for modeling spiking statistics. In summary, this work successfully integrates two different elements, statistical modeling of the spikes and synaptic homeostasis in a single biologically plausible neural network model. The authors logically demonstrate their arguments with clear visual presentations and well-structured text, facilitating an unambiguous understanding for readers.</p><p>Discussions</p><p>The authors have clearly responded to most of our questions in the revised manuscript and we are happy to recommend publishing the final version of the article as it is. Below, we would like to present some alternative interpretations of the results. These comments are not exclusive with the claims made in the articles; it is rather intended to enhance the understanding of readers by providing additional perspectives.</p><p>As summarized above, the main contribution of the work consists of two parts; (1) the reshaped RP model achieved higher performance in explaining the statistics of the spiking activity of cortical neurons with more efficient representations (=lower firing rate), (2) synaptic homeostatic normalization in the reshaped RP model yields even more efficient representations without impairing the fitting performance.</p><p>For part (1),</p><p>Suppl. Fig. 1B compares reshaped RP models with RP and RP with pruning and replacement (R&amp;P). The better performance of RP with P&amp;R might imply the advantage of pruning over gradient descent in this setting, reflecting the non-convexities of the problem. Alternatively, it might suggest the benefit of the increased number of parameters, since pruning allows the network to explore the broader parameter space during the learning process. This latter view might partially explain the superiority of the reshaped RP model over the original RP model.</p><p>It is interesting that the backprop model has higher firing rate than the reshaped model (Fig. 1D). This trend is unchanged when optimization of the neural threshold is also allowed (Supp. Fig. 2A). Since backprop model overperforms reshaped model slightly but robustly, high firing rates in the backprop model might be a genuine feature of the spike statistics.</p><p>For part (2),</p><p>We note that λ regulates the average firing rate, since maximizing the entropy &lt;-ln p(x)&gt; with a regularization term -λ &lt;Σ<sub>i</sub> f(x<sub>i</sub>)&gt; results in λ<sub>i</sub> = λ for all i in the Boltzmann distribution of eq. 2. Suppl. Fig. 2B could be understood as demonstrating this &quot;homeostatic&quot; effect of λ.</p><p>Suppl. Fig. 3 could be interpreted as demonstrating the interaction of two different homeostatic mechanisms: one at the firing-rate level (as regulated by λ) and the other at the synaptic level (as regulated by φ). It shows that the range of synaptic constraints where the fitting performance is not impaired differs by the value of λ. For example, if lambda is small (λ = 0.25), synaptic constraint can easily deteriorate the performance; on the other hand, if lambda is large (λ = 4), performance remains unchanged under strict synaptic constraint. Considering that practically we are most interested in the regime where the model performs best (λ = 2.0), an advantageous feature of the homeostatic model is that homeostatic constraint is harmless at λ=2.0 for the wide range of constraints.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96566.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mayzel</surname><given-names>Jonathan</given-names></name><role specific-use="author">Author</role><aff><institution>Weizmann Institute of Science</institution><addr-line><named-content content-type="city">Rehovot</named-content></addr-line><country>Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Schneidman</surname><given-names>Elad</given-names></name><role specific-use="author">Author</role><aff><institution>Weizmann Institute of Science</institution><addr-line><named-content content-type="city">Rehovot</named-content></addr-line><country>Israel</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Comments:</bold></p><p>(1) We find it interesting that the reshaped model showed decreased firing rates of the projection neurons. We note that maximizing the entropy &lt;-ln p(x)&gt; with a regularizing term -λ &lt;Σ<sub>i</sub> f(x<sub>i</sub>)&gt;, which reflects the mean firing rate, results in λ<sub>i</sub> = λ for all i in the Boltzmann distribution. In other words, in addition to the homeostatic effect of synaptic normalization which is shown in Figures 3B-D, setting all λ<sub>i</sub> = 1 itself might have a homeostatic effect on the firing rates. It would be better if the contribution of these two homeostatic effects be separated. One suggestion is to verify the homeostatic effect of synaptic normalization by changing the value of λ.</p></disp-quote><p>This is an interesting question and we, therefore, explored the effects of different values of λ on the performance of unconstrained reshaped RP models and their firing rates. The new supp. Figure 2B presents the results of this exploration: We found that for models with a small set of projections, a high value of λ results in better performance than models with low ones, while for models with a large set of projections we find the opposite relation. The mean firing rates of the projection neurons for models with different values of λ show a clear trend, where higher λ values results in lower mean firing rates.</p><p>Thus, these results suggest an interplay between the optimal size of the projection set and the value of λ one should pick. For the population sizes and projection sets we have used here, λ=1 is a good choice, but, for different population sizes or data sets a different value of λ might be better.</p><p>Thus, in addition to supp. Figure 2B, we therefore added the following to the main text:</p><p>“An additional set of parameters that might affect the Reshaped RP models are the coefficients λ, that weigh each of the projections. Above, we used λ=1 for all projections, here we investigated the effect of the value of λ on the performance of the Reshaped RP models (supp. Figure 2B). We find that for models with a small projection set, high λ values result in better performance than models with low values. We find an opposite relation for models with large number projection sets. (We submit that the performance decrease of Reshaped RP models with high value of λ, as the number of projections grows, is a reflection of the non-convex nature of the Reshaped RP optimization problem).</p><p>The mean firing rates of the projection neurons for models with different values of λ show a clear trend, higher λ values results in lower mean firing rates. Thus, we conclude that there is an interplay between the number of projections and the value of λ we should pick. For the sizes of projection sets we have used here, λ=1 is a good choice, but, we note that in general, one should probably seek the appropriate value of λ for different population sizes or data sets.”</p><p>In addition, we explored the effect of synaptic normalization on models with different values of λ (supp. Figure 3). We found that homeostatic Reshaped RP models are superior to the non-homeostatic Reshaped RP models: For low values of λ, the homeostatic and Reshaped RP models show similar performance in terms of log-likelihood, whereas the homeostatic models are more efficient. For high values of λ<sub>i</sub> homeostatic models are not only more efficient but also show better performance. These results indicate that the benefit of the homeostatic model is insensitive to the specific choice of λ.</p><p>In addition to supp. Figure 3, we added the following to the main text:</p><p>“Exploring the effect of synaptic normalization on models with different values of λ (supp. Figure 3), we find that homeostatic Reshaped RP models are superior to the non-homeostatic Reshaped RP models: For low values of λ, the homeostatic and Reshaped RP models show similar performance in terms of log-likelihood, whereas the homeostatic models are more efficient. Importantly, for high values of λ<sub>i</sub> homeostatic models are not only more efficient but also show better performance. We conclude that the benefit of the homeostatic model is insensitive to the specific choice of λ.”</p><disp-quote content-type="editor-comment"><p>(2) As far as we understand, θ<sub>i</sub> (thresholds of the neurons) are fixed to 1 in the article. Optimizing the neural threshold as well as synaptic weights is a natural procedure (both biologically and engineeringly), and can easily be computed by a similar expression to that of a<sub>ij</sub> (equation 3). Do the results still hold when changing θ<sub>i</sub> is allowed as well? For example,</p><p>a. If θ<sub>i</sub> becomes larger, the mean firing rates will decrease. Does the backprop model still have higher firing rates than the reshaped model when θ<sub>i</sub> are also optimized?</p><p>b. Changing θ<sub>i</sub> affects the dynamic range of the projection neurons, thus could modify the effect of synaptic constraints. In particular, does it affect the performance of the bounded model (relative to the homeostatic input models)?</p></disp-quote><p>We followed the referee’s suggestion, and extended our current analysis, and added threshold optimization to the Reshape and Backpropagation models, which is now shown in supp. Figure 2A. Comparing the performance and properties of these models to ones with fixed thresholds, we found that this addition had a small effect on the performance of the models in terms of their likelihood. (supp. Figure 2A). We further find that backpropagation models with tuned thresholds show lower firing rates compared to backpropagation models with fixed threshold, while reshaped RP models with optimized thresholds show higher firing rates compared to models with fixed threshold. These differences are, again, rather small, and both versions of the reshaped RP models show lower firing rates compared to both versions of the backpropagation models.</p><p>In addition to supp. Figure 2A, we added the following to the main text:</p><p>“The projections' threshold θ<sub>i</sub>, which is analogous to the spiking threshold of the projection neurons, strongly affects the projections' firing rates. We asked how, in addition to reshaping the coefficients of each projection, we can also change θ<sub>i</sub> to optimize the reshaped RP and backpropagation models.</p><p>We find that this addition has a small effect on the performance of the models in terms of their likelihood (supp. Figure 2A).</p><p>We also find that this has a small effect on the firing rates of the projection neurons: backpropagation models with tuned thresholds show lower firing rates compared to backpropagation models with fixed threshold, whereas reshaped RP models with optimized thresholds show higher firing rates compared to models with fixed threshold. Yet, both versions of the reshaped RP models show lower firing rates compared to both versions of the backpropagation models. Given the small effect of tuning threshold on models' performance and their internal properties, we will, henceforth, focus on Reshaped RP models with fixed thresholds.”</p><disp-quote content-type="editor-comment"><p>(3) In Figure 1, the authors claim that the reshaped RP model outperforms the RP model. This improved performance might be partly because the reshaped RP model has more parameters to be optimized than the RP model. Indeed, let the number of projections N and the in-degree of the projections K, then the RP model and the reshaped RP model have N and KN parameters, respectively. Does the reshaped model still outperform the original one when only (randomly chosen) N weights (out of a<sub>ij</sub>) are allowed to be optimized and the rest is fixed? (or, does it still outperform the original model with the same number of optimized parameters (i.e. N/K neurons)?)</p></disp-quote><p>Indeed, the number of tuned parameters in the reshaped RP model is much larger compared to the number of tuned parameters in an RP model with the same projection set size. Yet, we submit that the larger number of tuned parameters is not the reason for the improved performance of the reshaped RP model: Maoz et al [30] have already shown that by optimizing an RP model with a small projection set using the pruning and replacement of projections (P&amp;R), one can reach high accuracy with an almost order of magnitude fewer projections. Thus, we argue that the improved performance stems from the properties of the projections in the model.</p><p>Accordingly, we therefore added supp. Figure 2B that shows the performance of P&amp;R sigmoid RP model compared to RP and reshaped RP models. We added the following to the main text:</p><p>“Because reshaping may change all the existing synapses of each projection, the number of parameters is the number of projections times the projections in-degree. While this is much larger than the number of parameters that we learn for the RP model (one for each projection), we suggest that the performance of the reshaped models is not a naive result of having more parameters. In particular, we have seen that RP models that use a small set of projections can be very accurate when the projections are optimized using the pruning and replacement process [30] (see also supp. Figure 1B). Thus, it is really the nature of the projections that shapes the performance. Indeed, our results here show that a small fixed connectivity projection set with weight tuning is enough for accurate performance which is on par or better than an RP model with more projections.”</p><disp-quote content-type="editor-comment"><p>(4) In Figure 2, the authors have demonstrated that the homeostatic synaptic normalization outperforms the bounded model when the allowed synaptic cost is small. One possible hypothesis for explaining this fact is that the optimal solution lies in the region where only a small number of |a<sub>ij</sub>| is large and the rest is near 0. If it is possible to verify this idea by, for example, exhibiting the distribution of a<sub>ij</sub> after optimization, it would help the readers to better understand the mechanism behind the superiority of the homeostatic input model.</p></disp-quote><p>We modified supp. Figure 4 and made the following change in the relevant part in the main text to address the reviewer comment about the distribution of the a<sub>ij</sub> values:</p><p>“Figure 5E shows the mean rotation angle over 100 homeostatic models as a function of synaptic cost -- reflecting that the different forms of homeostatic regulation results in different reshaped projections. We show in Supp. Figure 4C the histogram of the rotation angles of several different homeostatic models, as well as the unconstrained Reshape model.</p><p>Analyzing the distribution of the synaptic weights a<sub>ij</sub> after learning leads to a similar conclusion (supp. Figure 4D): The peak of the histograms is at a<sub>ij</sub> = 0, implying that during reshaping most synapses are effectively pruned. While the distribution is broader for models with higher synaptic budget, it is asymmetric, showing local maxima at different values of a<sub>ij</sub>.</p><p>The diversity of solutions that the different model classes and parameters show imply a form of redundancy in model choice or learning procedure. This reflects a multiplicity of ways to learn or optimize such networks that biology could use to shape or tune neural population codes.“</p><disp-quote content-type="editor-comment"><p>(5) In Figures 5D and 5E, the authors present how different reshaping constraints result in different learning processes (&quot;rotation&quot;). We find these results quite intriguing, but it would help the readers understand them if there is more explanation or interpretation. For example,</p><p>a. In the &quot;Reshape - Hom. circuit 4.0&quot; plot (Fig 5D, upper-left), the rotation angle between the two models is almost always the same. This is reasonable since the Homeostatic Circuit model is the least constrained model and could be almost irrelevant to the optimization process. Is there any similar interpretation to the other 3 plots of Figure 5D?</p></disp-quote><p>We added a short discussion of this difference to the main text, but do not have a geometric or other intuitive explanation for the nature of these differences.</p><disp-quote content-type="editor-comment"><p>b. In Figure 5E, is there any intuitive explanation for why the three models take minimum rotation angle at similar global synaptic cost (~0.3)?</p></disp-quote><p>We added discussion of this issue to the main text, and the histogram of the rotation angles in Supp Figure 4c shows that they are not identical. But, we don’t have an intuitive explanation for why the mean values are so similar.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p>(1) Some claims on the effect of synaptic normalization on the reshaped model sound a little overstated since the presented evidence does not clearly show the improvement of the computational performance (in comparison to the vanilla reshaped model) in terms of maximizing the likelihood of the inputs. Here are some examples of such claims: &quot;Incorporating more biological features and utilizing synaptic normalization in the learning process, results in even more efficient and accurate models.&quot; (in Abstract), &quot;Thus, our new scalable, efficient, and highly accurate population code models are not only biologically-plausible but are actually optimized due to their biological features.&quot; (in Abstract), or &quot;in our Reshaped RP models, homeostatic plasticity optimizes the performance of network models&quot; (in Discussion).</p></disp-quote><p>We changed the wording according to the reviewers’ suggestions.</p><disp-quote content-type="editor-comment"><p>(2) In equation (1) and the following sentence, θ<sub>j</sub> (threshold) should be θ<sub>i</sub>.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p>(3) While the authors mention that &quot;reshaping with normalization or without it drives the projection neurons to converge to similar average firing rate values (Figure 3B)&quot;, they also claim that &quot;reshaping with normalization implies lower firing rates as well as... (Figure 3E)&quot;. These two claims look a little inconsistent to us. Besides, it is not very clear from Figure 3E that the normalization decreases the firing rate (it is clear from Figure 3B, though). How about just deleting &quot;lower firing rates as well as&quot;?</p></disp-quote><p>We changed the wording according to the reviewers’ suggestion.</p><disp-quote content-type="editor-comment"><p>(4) The captions of Figures 4D and 4E should be exchanged.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p>(5) Typo in In Figure 4F: &quot;normalized in-dgreree&quot;.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p>(6) In Figure 5D (upper left plot) the choice of &quot;Reshape&quot; and &quot;Bounded3.0&quot; looks a bit weird. Is this the typo of &quot;Hom. cicruit 4.0&quot;?</p></disp-quote><p>There is no typo in the figure labels. We discussed the results of figure 5D in our response to point (5) in the public comments list and addressed the upper left panel of figure 5D in the main text.</p><disp-quote content-type="editor-comment"><p>(7) In the paper, the letter θ represents (1) the threshold of the projection neurons (eq. 1), (2) the &quot;ceiling&quot; value of the bounded model, and (3) the rotation angle of projections (Figure 5). We find this notation a bit confusing and recommend using different notations for different entities.</p></disp-quote><p>Thanks for the suggestion, we changed the confusing notations: (1) The threshold of each projection neuron is still θ following the notation of the original RP model formulation [30]. (2) The notation of the “ceiling” value of the bounded model is now ω. (3) The rotation angle of the projections during reshape is now marked by α.</p></body></sub-article></article>