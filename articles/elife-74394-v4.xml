<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">74394</article-id><article-id pub-id-type="doi">10.7554/eLife.74394</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Evolutionary Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Mice and primates use distinct strategies for visual segmentation</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-257268"><name><surname>Luongo</surname><given-names>Francisco J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-257269"><name><surname>Liu</surname><given-names>Lu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0004-9272-5996</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-257270"><name><surname>Ho</surname><given-names>Chun Lum Andy</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-257271"><name><surname>Hesse</surname><given-names>Janis K</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-257272"><name><surname>Wekselblatt</surname><given-names>Joseph B</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-257273"><name><surname>Lanfranchi</surname><given-names>Frank F</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-52441"><name><surname>Huber</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-273689"><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1083-1919</contrib-id><email>dortsao@berkeley.edu</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Division of Biology and Biological Engineering, California Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>Department of Basic Neurosciences, University of Geneva</institution></institution-wrap><addr-line><named-content content-type="city">Geneva</named-content></addr-line><country>Switzerland</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Computation and Neural Systems, California Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an7q238</institution-id><institution>University of California, Berkeley</institution></institution-wrap><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Vinck</surname><given-names>Martin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vmz1g97</institution-id><institution>Ernst Strüngmann Institute (ESI) for Neuroscience in Cooperation with Max Planck Society</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>15</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e74394</elocation-id><history><date date-type="received" iso-8601-date="2021-10-02"><day>02</day><month>10</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2023-01-22"><day>22</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-07-05"><day>05</day><month>07</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.07.04.451059"/></event></pub-history><permissions><copyright-statement>© 2023, Luongo, Liu et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Luongo, Liu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-74394-v4.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-74394-figures-v4.pdf"/><abstract><p>The rodent visual system has attracted great interest in recent years due to its experimental tractability, but the fundamental mechanisms used by the mouse to represent the visual world remain unclear. In the primate, researchers have argued from both behavioral and neural evidence that a key step in visual representation is ‘figure-ground segmentation’, the delineation of figures as distinct from backgrounds. To determine if mice also show behavioral and neural signatures of figure-ground segmentation, we trained mice on a figure-ground segmentation task where figures were defined by gratings and naturalistic textures moving counterphase to the background. Unlike primates, mice were severely limited in their ability to segment figure from ground using the opponent motion cue, with segmentation behavior strongly dependent on the specific carrier pattern. Remarkably, when mice were forced to localize naturalistic patterns defined by opponent motion, they adopted a strategy of brute force memorization of texture patterns. In contrast, primates, including humans, macaques, and mouse lemurs, could readily segment figures independent of carrier pattern using the opponent motion cue. Consistent with mouse behavior, neural responses to the same stimuli recorded in mouse visual areas V1, RL, and LM also did not support texture-invariant segmentation of figures using opponent motion. Modeling revealed that the texture dependence of both the mouse’s behavior and neural responses could be explained by a feedforward neural network lacking explicit segmentation capabilities. These findings reveal a fundamental limitation in the ability of mice to segment visual objects compared to primates.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>mouse vision</kwd><kwd>segmentation</kwd><kwd>object localization</kwd><kwd>treeshrew</kwd><kwd>mouse lemur</kwd><kwd>macaque</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>DP1-NS083063</award-id><principal-award-recipient><name><surname>Tsao</surname><given-names>Doris Y</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Arnold O. Beckman postdoctoral fellowship</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Luongo</surname><given-names>Francisco J</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>310030E_190060</award-id><principal-award-recipient><name><surname>Huber</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Tsao</surname><given-names>Doris Y</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Burroughs Wellcome PDEP Award</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Luongo</surname><given-names>Francisco J</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0024/2016</award-id><principal-award-recipient><name><surname>Huber</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The behavioral ability of mice and treeshrews to detect visual figures is highly texture dependent, unlike that of primates including macaques and mouse lemurs, and neural responses to figures in mouse visual cortex are similarly texture dependent.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Primates rely primarily on vision to meaningfully interact with objects in the world. Mice, in contrast, rely far less on their visual system, though they do use visual cues for important behaviors such as hunting, evasion, and navigation (<xref ref-type="bibr" rid="bib7">Evans et al., 2018</xref>; <xref ref-type="bibr" rid="bib8">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib18">Hoy et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Leinweber et al., 2017</xref>). The field of mouse vision has attracted great excitement in recent years due to the wealth of tools available for mouse circuit dissection, with many groups adopting the mouse as a model for visual perception (<xref ref-type="bibr" rid="bib42">Pinto et al., 2013</xref>) and visually guided decision making (<xref ref-type="bibr" rid="bib1">Abbott et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Pinto et al., 2019</xref>). Yet the fundamental ethology of mouse vision remains poorly understood. What does the mouse perceive as a visual object?</p><p>While work has shown that visual responses in mouse visual cortex share low-level organizing principles with those of primate visual cortex, including temporal/spatial frequency tuning (<xref ref-type="bibr" rid="bib29">Marshel et al., 2011</xref>), orientation selectivity (<xref ref-type="bibr" rid="bib36">Niell and Stryker, 2008</xref>), and contextual surround effects (<xref ref-type="bibr" rid="bib52">Self et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Keller et al., 2020a</xref>), it remains unclear to what extent the two species share more abstract representations of visual objects and scenes.</p><p>In particular, it is unclear whether mice explicitly segment visual scenes into discrete surfaces. Segmentation refers to the identification of borders of each object in a visual scene and assignment of discrete labels to pixels corresponding to each object. In primates, segmentation is a key step in visual processing following early feature extraction (<xref ref-type="bibr" rid="bib10">Frost and Nakayama, 1983</xref>; <xref ref-type="bibr" rid="bib16">He and Nakayama, 1992</xref>; <xref ref-type="bibr" rid="bib26">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib64">Williford and von der Heydt, 2013</xref>; <xref ref-type="bibr" rid="bib68">Zhou et al., 2000</xref>). For example, in the famous ‘face-vase’ illusion, human viewers inexorably segment the scene as a face or a vase, with bistable dynamics. A large body of psychophysics suggests that the primate visual system performs segmentation by generating a <italic>surface representation</italic>, an assignment of each retinal pixel to a distinct contiguous surface situated in 3D space (<xref ref-type="bibr" rid="bib16">He and Nakayama, 1992</xref>; <xref ref-type="bibr" rid="bib56">Tsao and Tsao, 2022</xref>; <xref ref-type="fig" rid="fig1">Figure 1a</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Mechanisms for segmentation.</title><p>(<bold>a</bold>) Schematic representation of a hierarchy for visual perception. Figure-ground segmentation serves as a key intermediate step preceding object recognition. (<bold>b</bold>) Accretion and deletion signals at borders induced by object motion provide a strong cue to distinguish object versus texture edges. As objects move differently from the background, accretion and deletion of parts of the background will occur at object edges, providing local cues for object boundaries and their owners. In contrast to accretion–deletion, texture (e.g., orientation contrast) is locally ambiguous: the pocket does not constitute an object edge, even though it generates a sharp texture discontinuity. (<bold>c</bold>) Top: Figure-ground modulation provides a neural mechanism for explicit segmentation. Here, a hypothetical neuron’s firing is selectively enhanced to a stimulus when it is part of a figure (purple) compared to ground (green), even though the stimulus in the classical receptive field remains the same. A population of such neurons would be able to localize image regions corresponding to objects. Bottom: Border-ownership modulation provides an additional neural mechanism for explicit segmentation. Here, a hypothetical neuron’s response is modulated by the relative position of a figure relative to an object edge. In the example shown, the neuron prefers presence of a figure on the left (green) as opposed to figure on the right (purple). A population of such neurons would be able to effectively trace the border of an object and assign its owner.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig1-v4.tif"/></fig><p>How could the brain solve visual segmentation? The key visual cue signaling a surface border is a <italic>discontinuity</italic>, an abrupt change in features at the surface border. For example, there is often a change in luminance, orientation, or texture at a surface border. However, this need not be the case: changes in luminance, orientation, and texture can also occur within interior regions of a surface (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Conversely, object borders can exist without any change in luminance, orientation, or texture—a fact exploited by animals that use camouflage (<xref ref-type="bibr" rid="bib14">Hall et al., 2013</xref>). Thus, a key challenge of intermediate vision is to identify true object borders using ambiguous local cues. Aiding this goal, there is one cue that is unambiguous: <italic>accretion–deletion</italic>, the appearance or disappearance of pixels forming the background surface due to motion (or binocular disparity) of the foreground surface (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Gibson identified accretion–deletion as the <italic>single most important cue to surface organization</italic> because it is unambiguous, invariant to texture, and locally available (<xref ref-type="bibr" rid="bib12">Gibson, 1979</xref>). Psychophysical experiments in humans demonstrate that accretion–deletion alone is able to evoke a vivid percept of an object border (<xref ref-type="bibr" rid="bib34">Nakayama and Shimojo, 1990</xref>). Furthermore, a recent computational theory of surface representation shows how surface segmentation can be computed using local accretion–deletion cues in a simple way without requiring learning, top-down feedback, or object recognition (<xref ref-type="bibr" rid="bib56">Tsao and Tsao, 2022</xref>). Moreover, this new theory shows how such local accretion–deletion cues can be used not only to solve segmentation, but also to solve invariant tracking of objects, widely considered one of the hardest problems in vision (<xref ref-type="bibr" rid="bib44">Pitts and McCulloch, 1947</xref>; <xref ref-type="bibr" rid="bib6">DiCarlo and Cox, 2007</xref>). To summarize, while there are multiple cues to segmentation, accretion–deletion holds a special status for the following reasons: (1) it is unambiguous, occurring only at true object borders and never at internal texture borders; (2) it is cue-invariant; (3) it is especially computationally powerful, supporting not only segmentation but also invariant tracking.</p><p>A variety of neural correlates of segmentation have been found in the primate brain. Neurons in primate V1, V2, and V4 modulate their firing according to whether a stimulus is part of the foreground or background (<xref ref-type="bibr" rid="bib26">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib45">Poort et al., 2012</xref>; <xref ref-type="fig" rid="fig1">Figure 1c</xref>). Complementing figure-ground signaling, a population of neurons have been found in macaque areas V2 and V4 that explicitly signal object borders. These ‘border-ownership’ cells respond selectively to figure edges, including those defined purely by accretion–deletion, and are moreover modulated by the side of the figure relative to the edge (<xref ref-type="bibr" rid="bib68">Zhou et al., 2000</xref>; <xref ref-type="bibr" rid="bib47">Qiu and von der Heydt, 2005</xref>; <xref ref-type="fig" rid="fig1">Figure 1c</xref>).</p><p>Behaviorally, mice are capable of texture-based segmentation, in which figure and background are defined by grating patterns with different orientation and/or phase (<xref ref-type="bibr" rid="bib25">Kirchberger et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Schnabel et al., 2018a</xref>). Consistent with this behavioral capability, cells in mouse V1 show iso-orientation surround suppression (<xref ref-type="bibr" rid="bib52">Self et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Keller et al., 2020a</xref>) and have been reported to be modulated by figure versus ground (<xref ref-type="bibr" rid="bib49">Schnabel et al., 2018a</xref>; <xref ref-type="bibr" rid="bib25">Kirchberger et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Keller et al., 2020b</xref>). However, all these studies have used texture-based cues, which are fundamentally ambiguous for solving segmentation (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Thus, it remains an open question whether mice are capable of explicit object segmentation, or simply of texture segregation. In contrast, behavioral evidence unequivocally demonstrates that primates possess a mechanism for explicit, cue-invariant segmentation exploiting accretion–deletion (<xref ref-type="bibr" rid="bib34">Nakayama and Shimojo, 1990</xref>).</p><p>Here, we take advantage of the ability to record from large numbers of neurons across the mouse cortical visual hierarchy to look for behavioral and neural correlates of visual segmentation in the mouse. We discovered a surprising difference between mouse and human segmentation behavior, which led us to systematically investigate segmentation behavior in three additional species: the macaque, mouse lemur, and treeshrew. We found that the mice and treeshrews, unlike the two primate species, are behaviorally incapable of texture-invariant segmentation. In fact, mice tasked to localize objects with naturalistic textures adopted a strategy of brute force memorization—a cognitively impressive feat. Furthermore, we found no evidence for single neurons in mouse visual cortex modulated by figure/ground or border ownership in a texture-invariant manner. For patterns containing orientation or phase contrast between figure and background, we could decode figure location from population neural recordings, with best decoding in putative ventral stream area LM, followed by RL and V1, but we could not decode figure location for figures with naturalistic texture. A simple feedforward neural network could account for the observed dependence of mouse behavior and neural responses on carrier pattern. Taken together, these findings reveal a fundamental difference between primate and mouse mechanisms for object segmentation, with the mouse relying much more on texture statistics than the primate. The findings have broad implications for use of the mouse as a model for visual perception.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Mice fail to segment objects defined purely by opponent motion</title><p>We set out to clarify (1) whether mice are capable of invariantly segmenting figure from ground and (2) whether there exist texture-invariant segmentation-related signals in the mouse brain, as reported in the macaque brain (<xref ref-type="bibr" rid="bib26">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib69">Zipser et al., 1996</xref>). To address the mouse’s ability to segment objects, we designed a two-alternative forced choice task in which mice reported the side of a touch screen that contained a figure for a water reward (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). We tested mouse segmentation behavior using three classes of stimuli: (1) ‘Cross’ stimuli, in which the figure consisted of a grating, and the ground consisted of an orthogonal grating; (2) ‘Iso’ stimuli, in which the figure consisted of a grating, and the ground consisted of a grating at the same orientation, but offset in phase; (3) Naturalistic (‘Nat’) stimuli, in which both the figure and the ground consisted of 1/f noise patterns (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). In each case, figure and ground moved in counterphase providing a differential motion cue with accretion–deletion; this motion cue was essential for defining the figure in the Nat condition. The logic of including these three conditions was as follows: (1) the Cross condition has been used previously in multiple studies of figure-ground segmentation (<xref ref-type="bibr" rid="bib26">Lamme, 1995</xref>; <xref ref-type="bibr" rid="bib45">Poort et al., 2012</xref>) and extra-classical receptive field modulation <xref ref-type="bibr" rid="bib52">Self et al., 2014</xref>; (2) the Iso condition constitutes a slightly more challenging figure-ground segmentation problem due to lack of orientation contrast; nevertheless, the figure can be readily segmented in static images using the phase difference between figure and ground; (iii) the Nat condition allowed us to disambiguate true figure-ground signals from low-level orientation or phase contrast signals.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Mouse segmentation behavior: mice use orientation contrast but not opponent motion to distinguish figure from ground.</title><p>(<bold>a</bold>) Mice were trained in a touchscreen paradigm in which they were rewarded for touching the side of the screen containing a texture- and motion-defined figure. (<bold>b</bold>) Mice were tested on three classes of stimuli: ‘Cross’ where foreground and background patterns consisted of orthogonal gratings, ‘Iso’ where foreground and background patterns consisted of the same orientation gratings, and ‘Nat’ where foreground and background patterns consisted of naturalistic noise patterns with 1/f spectral content. Initially, four training stimuli were used for each condition. Figure and background oscillated back and forth, out of phase, providing a common opponent motion cue for segmentation across all conditions; the movement range of the figure and background is denoted by the red bar. (<bold>c</bold>) Mean performance curve for 12 mice in the Cross (orange), Iso (violet), and Nat (green) conditions, where the task was to report the side of the screen containing a figure; in each session, one of a bank of four possible stimuli were shown, as in (<bold>b</bold>). Shaded error bars represent standard error of the mean (SEM). (<bold>d</bold>) Performance of two macaque monkeys on the same task. Monkey behavior, unlike that of mice, showed no dependence on the carrier pattern, displaying high performance for all three conditions (Cross, Iso, and Nat). (<bold>e</bold>) Teaching a mouse the Nat condition. Mice that could not learn the Nat version of the task could be shaped to perform well on the task by a gradual training regimen over 20+ days. Using a gradual morph stimulus (see Methods), animals could be slowly transitioned from a well-trained condition (Cross) to eventually perform well on the full Nat task. Each circle represents one mouse. (<bold>f</bold>) Despite high performance on the four stimuli comprising the Nat task, performance dropped when mice were exposed to new unseen textures, suggesting that they had not learned to use opponent motion to perform the task. (<bold>g</bold>) Mice performed just as well on the Nat task even without the opponent motion cue, suggesting that they had adopted a strategy of memorizing a lookup table of textures to actions, rather than performing true segmentation in the Nat condition. (<bold>h</bold>) Left: Change in performance when the motion cue was removed on a random subset of trials. Mice experienced no drop in performance in any of the conditions when static images were displayed instead of dynamic stimuli, indicating they were not using motion information. Note that the static frame was chosen with maximal positional displacement. Right: In contrast, monkeys showed no performance drop in conditions where the figure was obvious in static frames (Cross and Iso), but showed a marked drop in performance for the Nat condition where the figure is not easily resolved without the motion cue. (<bold>i1</bold>) To confirm whether mice used an opponent motion cue in the various conditions, mice (<italic>N</italic> = 12) were trained on an initial set of 4 stimuli (Tr; 2 sides × 2 patterns/orientations, as in (<bold>b</bold>)). After performance had plateaued, they were switched to 10 novel test conditions (Te; 2 sides × 5 patterns/orientations). Animals mostly generalized for Cross and Iso conditions but failed to generalize for the Nat condition (p = 0.0011, ranksum test), suggesting they were unable to use the opponent motion in the stimulus. (<bold>i2</bold>) Same as (<bold>i1</bold>) for treeshrews. Like mice, treeshrews failed to generalize for the Nat condition (p = 0.02, ranksum test). In contrast, two primate species: mouse lemurs (<bold>i3</bold>) and macaques (<bold>i4</bold>) were able to generalize in the Nat condition, suggesting they were able to use the opponent motion cue in the task (p = 1.00 mouse lemur, p = 0.99 macaque; ranksum test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig2-v4.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Natural texture task shows advantage for learning figures with cross-oriented energy.</title><p>(<bold>a</bold>) Examples of natural textures in Cross and Iso conditions. (<bold>b</bold>) Table summarizing the training paradigm for the natural texture task. (<bold>c</bold>) Performance on baseline day, training days, and test day in Iso condition. Solid black line indicates the mean of training curves on training days (<italic>N</italic> = 10). (<bold>d</bold>) Mean performance on baseline day, training days, and test day in Iso condition. Mice showed no improvement in performance on novel textures after showing behavioral increases during the training period. Error bars represent standard error of the mean (SEM). (<bold>e</bold>) Same as (<bold>c</bold>) but for Cross condition. (<bold>f</bold>) Same as (<bold>d</bold>) but for Cross condition. Just as before, mice showed no improvement in performance on novel textures after showing behavioral increases during the training period. (<bold>g</bold>) Comparing performance between Cross and Iso conditions on training days. Each point represents the mean performance of a given texture on both Cross and Iso conditions. Most points lie above the unity line (p = 5.9 × 10<sup>−5</sup>, sign test). (<bold>h</bold>) Performance for Cross and Iso conditions on baseline and test day (Cross, p = 0.3; Iso, p = 0.97; ranksum test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig2-figsupp1-v4.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Additional behavioral performance statistics.</title><p>(<bold>a1</bold>) Schematic of training procedure indicating flow of stimuli within a training session. (<bold>a2</bold>) Schematic of training procedure used for <xref ref-type="fig" rid="fig2">Figures 2i1</xref>–<xref ref-type="fig" rid="fig3">3</xref>. Animals were first trained on a luminance square. Then they were trained on Cross, followed by Iso and then Nat. For each condition, the training period was followed by 1 day of generalization testing. (<bold>b1</bold>) Individual learning curves of 12 mice for Cross, Iso, and Nat conditions. (<bold>b2</bold>) Individual learning curves of four treeshrews for Cross, Iso, and Nat conditions. Animals could already perform Cross and Iso &gt;70% on the first training session, displaying much faster learning than mice. (<bold>b3</bold>) Individual learning curves of four mouse lemurs for Cross, Iso, and Nat conditions. (<bold>b4</bold>) Learning curves of two monkeys for Cross, Iso, and Nat conditions (monkey A: solid lines, monkey B: dashed lines). Both animals rapidly learned within a single session to perform the task (after only a brief period of training with a luminance-defined square at the beginning of the session). (<bold>c</bold>) Averaged learning curves for Nat condition (<italic>n</italic> = 12) and Nat condition with static background (<italic>n</italic> = 6). If the task is turned into a pure local motion detection task by making the background static, mice learn considerably faster, demonstrating that they are able to detect the motion in the Nat condition. Error bars indicate standard error of the mean (SEM).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig2-figsupp2-v4.tif"/></fig></fig-group><media mimetype="video" mime-subtype="mp4" xlink:href="elife-74394-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Examples of dynamic stimuli used to test mouse segmentation behavior.</title><p>Left to right: Cross, Iso, and Nat conditions.</p></caption></media><p>We first trained mice on four different patterns (two orientations/textures × two sides) for each of the three stimulus conditions (Cross, Iso, Nat, <xref ref-type="fig" rid="fig2">Figure 2b</xref>). Each session consisted of a single condition (see Methods). The learning curves for the three stimulus conditions were very different (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). Mice quickly learned the Cross task, reaching 88% performance after 7 days. They were slightly slower to learn the Iso task, reaching 77% performance after 9 days. However, they struggled to effectively learn the Nat task, reaching only around 71% performance after 13 days. We next tested two macaque monkeys on the same task. The monkeys performed at &gt;90% for all three conditions within the first session (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). Thus, there was a clear difference between the segmentation capabilities of the mouse and primate.</p><p>We next wondered whether through more gradual shaping, the mice could learn the Nat task. We trained the mice in a series of stages across 26 training sessions over which the stimulus morphed from the Cross to the Nat condition (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). For each stage, mice would reach good performance (&gt;80%), followed by a dramatic drop when a new, more difficult stage was introduced. By the end of 26 training sessions, three out of four mice successfully learned to detect the square in the full Nat condition (<xref ref-type="fig" rid="fig2">Figure 2e</xref>, ‘100%’). Thus, it appeared that through this gradual shaping, mice had acquired the ability to segment figure from ground using opponent motion.</p><p>To confirm this, we next tested the mice on seven new textures. To our surprise, the mice performed near chance on these new textures (<xref ref-type="fig" rid="fig2">Figure 2f</xref>, mean performance across three mice that had learned the task: 60%; test for significant difference between performance on new textures and performance on last day of noise shaping: p &lt; 0.01 for each mouse, Chi-square test). This lack of ability to generalize suggests that the mice had not in fact learned to segment figure from ground using opponent motion.</p><p>How then were they able to perform the Nat task on the trained patterns? We hypothesized that the animals had simply learned to memorize the mapping between the noise patterns in the Nat condition and the appropriate motor response, in effect using a lookup table from four patterns to two actions instead of relying upon visual perception of a figure. If this was the case, we reasoned that we should be able to remove motion from the stimulus and the animals should still perform well. Astonishingly, this turned out to be the case: mice displayed no change in performance upon removal of motion in the stimulus, which completely removed any way of inferring a figure (<xref ref-type="fig" rid="fig2">Figure 2g</xref>, mean performance across three mice that had learned the task: 87%; test for significant difference between performance on static textures and performance on last day of noise shaping: p &gt; 0.01 for each mouse, Chi-square test).</p><p>We next tested all three conditions (Cross/Iso/Nat) in separate cohorts of mice to further examine whether the animals were indeed discarding the opponent motion cue. We tested them on a static condition of the task after training on the motion task. As before, these mice showed no drop in behavioral performance for any of the three conditions (<xref ref-type="fig" rid="fig2">Figure 2h</xref>, Cross: p = 0.67, Iso: p = 0.02 (increasing), Nat: p = 0.26), confirming that the animals were not using the motion cue for figure detection. While this was not surprising for the Cross and Iso cases, as the single static frame still had strong edge contrast due to orientation/phase differences and thus contained a clear figure that could be detected, it was surprising for the Nat condition which had minimal cues for the figure when static. Thus, this experiment further confirmed that mice did not use the opponent motion cue to perform the segmentation task.</p><p>For comparison, we performed the same test on two monkeys. Their performance showed a very different pattern. Like mice, monkeys did not display a drop in performance in the Cross and Iso conditions. For the Nat condition, however, monkeys showed a dramatic drop in performance when motion cues were removed from the stimulus (<xref ref-type="fig" rid="fig2">Figure 2h</xref>, Fisher’s exact test, p-vals, Monkey 1: Cross: 1.0, Iso: 0.41, Nat: 9.7e−14; Monkey 2: Cross: 0.77, Iso: 0.75, Nat: 2.2e−19). This experiment reveals that monkeys and mice used fundamentally different strategies to solve the Nat condition: monkeys used the opponent motion cue, resulting in a dramatic drop in performance upon removal of motion, while mice used a learned lookup table mapping patterns to responses.</p><p>Given the inability of mice to generalize to new textures for the Nat condition (<xref ref-type="fig" rid="fig2">Figure 2f</xref>), we wondered whether the same would hold true for the Cross and Iso conditions. We next trained 8 new animals on the Cross and Iso tasks with 4 patterns (2 orientations × 2 positions), and then tested their ability to generalize to 10 new patterns from the same class (see Methods). We found that mice were able to generalize well for the Cross condition (<xref ref-type="fig" rid="fig2">Figure 2i1</xref>, left, mean performance drop = 2.47%), and moderately well for the Iso condition (mean performance drop = 6.55%). However, for the Nat condition, performance dropped almost to chance (mean performance drop = 20.61%; p = 0.0011, ranksum test), consistent with the result of our previous experiment (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). One obvious concern is that the mice simply could not see the motion cue in the Nat condition due to their low visual acuity; this concern is addressed in detail further below.</p><p>Since mice showed their best generalization performance for cross-oriented gratings, this suggested that orientation contrast is a key feature used by mice to solve the task of localizing the figure. This in turn suggested that we might improve the animal’s performance on random textures by introducing an element of orthogonal orientation. To test this, we generated two new sets of figure-ground stimuli starting from random textures: (1) ‘Iso-tex’ stimuli, in which a square was cropped from the texture and placed in either the left or right position, and (2) ‘Cross-tex’ stimuli, in which the same square was rotated 90°, increasing orientation contrast (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>); for both sets of stimuli, opponent motion between figure and ground was added. To compare the generalization ability of mice on these two classes of stimuli, we first measured baseline performance on Iso-tex and Cross-tex stimuli drawn from seven random textures. We then trained mice on Iso-tex and Cross-tex stimuli drawn from a different set of 30 textures. Finally, we re-measured performance on the original set of seven textures (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>). While there was no difference in baseline performance between the two conditions, a significant difference emerged during training (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c–f</xref>). Animals trained on the Iso-tex condition achieved a mean performance of 57% after 14 days of training (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c, d, g</xref>), whereas animals trained on the Cross-tex condition achieved 67% correct after 14 days (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1e–g</xref>)<bold>,</bold> indicating that a strong orthogonal component could aid the mice in performing the task. However, despite above chance performance on the bank of 30 random textures, just as before, mice were largely unable to utilize any information about the motion cue, as demonstrated by their drop back to initial performance for the original bank of seven textures (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1d, f, h</xref>). Overall, our behavioral results suggest that mice adopt a strategy for object localization that relies heavily on orientation contrast and phase differences between figure and ground and is blind to opponent motion cues.</p></sec><sec id="s2-2"><title>Comparing segmentation behavior in mouse, macaque, treeshrew, and mouse lemur</title><p>The striking difference between mouse and macaque segmentation behavior inspired us to run the generalization test of <xref ref-type="fig" rid="fig2">Figure 2i1</xref> on two macaque monkeys (<xref ref-type="fig" rid="fig2">Figure 2i4</xref>). The macaques showed a very different behavioral pattern compared to the mice: they were able to generalize to unseen patterns for all three conditions, indicating that they were capable of performing segmentation using the opponent motion cue, and had not simply memorized the texture pattern in the Nat condition like the mice.</p><p>This clear difference between the behavioral strategies for visual segmentation used by mice versus macaques further inspired us to perform the same pattern generalization test (i.e., train on one set of patterns/orientations, test on a different set of unseen patterns/orientations) in two additional species: (1) a second mammalian species of the order scadentia (<italic>Tupaia belangeri</italic>; treeshrew), and (2) a second primate species (<italic>Microcebus murinus</italic>; mouse lemur). The treeshrews performed similar to mice, displaying generalization for the Cross and Iso conditions but not the Nat condition (<xref ref-type="fig" rid="fig2">Figure 2i2</xref>). In contrast, and similar to macaques, the mouse lemurs were readily able to generalize for all three conditions (<xref ref-type="fig" rid="fig2">Figure 2i3</xref>), implying that they, like the macaques (<xref ref-type="fig" rid="fig2">Figure 2i4</xref>), were able to perform visual segmentation using the opponent motion cue. Training curves for all four species on this task are shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2a, b</xref>. Taken together, these results provide strong evidence that primates including mouse lemurs, macaques, and humans all use a visual segmentation strategy exploiting opponent motion cues, in contrast to mice and treeshrews, which rely on texture cues to perform visual segmentation and are incapable of using opponent motion cues.</p></sec><sec id="s2-3"><title>Controlling for visual acuity</title><p>The peak acuities of the four species studied vary over two orders of magnitude: 5 cpd (macaque fovea, <xref ref-type="bibr" rid="bib31">Merigan et al., 1991</xref>), 0.5 cpd (mouse lemur, <xref ref-type="bibr" rid="bib17">Ho et al., 2021</xref>), 0.5 cpd (treeshrew, <xref ref-type="bibr" rid="bib41">Petry et al., 1984</xref>), and 0.05 cpd (mouse, <xref ref-type="bibr" rid="bib46">Prusky et al., 2000</xref>). Thus, one obvious concern is that mice and treeshrews could not detect the square in the Nat condition due to their lower visual acuity compared to macaques. Several pieces of evidence argue against this. First, mouse and treeshrew visual acuity differ by an order of magnitude, yet neither species could perform the task. Second, even more compellingly, the treeshrew and mouse lemur have <italic>identical visual acuity</italic>, yet the mouse lemur could perform the task while the treeshrew could not. Third, mouse lemurs, treeshrews, and mice all performed the task in a freely moving paradigm that naturally generates a parametric range of spatial frequencies and figures sizes on the animal’s retina (for the mouse, the size of the behavior box was 25 cm; this generates a variation in figure size from 15° to 143° assuming the mouse position varied from 23 to 1 cm from the screen; and a tenfold variation in spatial frequency). Despite this, mice and treeshrews still could not perform the task. Importantly, 23–52% of the stimulus energy in the Nat stimulus had frequency less than 0.064 cpd, the peak contrast sensitivity of the mouse (<xref ref-type="bibr" rid="bib46">Prusky et al., 2000</xref>), when the mouse position varied from 23 to 1 cm from the screen. Fourth, the fact that the mouse could learn to distinguish the Nat patterns after extensive training (<xref ref-type="fig" rid="fig2">Figure 2c</xref>) directly demonstrates that <italic>the mouse could perceive the spatial frequencies present in the Nat stimulus</italic>. Finally, one might be concerned that the amplitude of the counterphase motion (14 pixels) was too small for the mouse to perceive. To control for this, we trained the same eight mice as in <xref ref-type="fig" rid="fig2">Figure 2i1</xref> to perform the Nat condition in a modified situation in which the background was static. The animals learned this modified Nat task much more quickly, indicating that they could see the motion cue (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2c</xref>); their deficit was specific to seeing the figure defined by opponent motion. Together, all these pieces of evidence argue strongly that the differences we observed between mouse/treeshrew on one hand, and macaque/mouse lemur on the other, cannot be attributed to differences in visual acuity.</p></sec><sec id="s2-4"><title>Absence of texture-invariant figure signals in mouse visual cortex</title><p>Given the evident inability of mice to perform texture-invariant visual segmentation, a natural question arises: what segmentation-related signals are available in mouse visual cortex to decode the location and boundary of an object? To address this, we recorded responses in mouse visual cortex to figure-ground stimuli defined by both texture and opponent motion using (1) electrophysiology with a 64-channel silicon probe, and (2) 2-photon calcium imaging. This stimulus was essentially identical to the one used to test mouse behavior, except the figure location changed from trial to trial in order to cover the cells’ receptive fields. We compared responses across three distinct mouse visual areas: primary visual cortex (V1), a putative ventral stream area (LM), and a putative dorsal stream area (RL) (<xref ref-type="bibr" rid="bib62">Wang et al., 2011</xref>).</p><p>We first localized visual areas using wide-field imaging in GCAMP6s transgenic animals and used vasculature maps to guide subsequent two photon and electrophysiology experiments (<xref ref-type="fig" rid="fig3">Figure 3a, b</xref>) (see Methods). We then measured the receptive field centers of neurons using either a sparse noise stimulus or a spatially isolated flashing Gabor of varying orientations. Imaging and electrophysiology data were generally consistent. For most analyses below (with the exception of <xref ref-type="fig" rid="fig3">Figure 3f, g</xref> and Figure 6d, f), we present electrophysiology data, as it had better temporal resolution and gave better receptive field estimates (due to absence of neuropil activity leading to blurring of receptive fields). The latter was critical as the analyses depended on accuracy of receptive field estimates for single cells.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Approach for measuring neural correlates of segmentation-related modulation in mouse visual cortex.</title><p>(<bold>a</bold>) Widefield imaging of GCaMP transgenic animals was used to localize visual areas prior to neural recording. A drifting bar stimulus was used to compute azimuth (bottom-left) and elevation (bottom-right) maps across of the visual cortex. From these maps, a field-sign map (top-left) was computed, allowing delineation of cortical visual areas (top-right). Alignment to vasculature maps guided subsequent electrophysiology or two-photon recordings to V1, LM, or RL. (<bold>b</bold>) Rodents were allowed to run freely on a spherical treadmill, with a 72-cm width (32-inch diagonal) screen centered either 18 or 27 cm away from the left eye, while undergoing either electrophysiology or two-photon imaging. (<bold>c</bold>) The stimulus consisted of a texture- and motion-defined square that was flashed randomly across a grid of 16 horizontal positions × 8 vertical positions (128 positions total). On any given trial, a neuron with a given receptive field location (schematized by the green circle) was stimulated by (1) ground, (2) figure, or (3) edge, enabling us to measure both figure-ground and border-ownership modulation. (<bold>d</bold>) Schematic response maps to the stimulus in (<bold>c</bold>). Left: A ‘figure cell’ responds only when a part of a figure is over the receptive field. Middle: A ‘border cell’ responds only when a figure border falls on the receptive field and has orientation matching that of the cell (here assumed to be vertical). Right: A simple cell with an ON subunit responds to the figure with phase dependence. (<bold>e1</bold>) Mean response at each of the 128 figure positions for an example V1 cell. Colored boxes correspond to conditions shown in (<bold>e2</bold>). (<bold>e2</bold>) Four stimulus conditions outlined in (<bold>e1</bold>), ranging from receptive field on the figure (left) to receptive field on the background (right). (<bold>e3</bold>) Raster (top) of spiking responses over 10 trials of each stimulus configuration and mean firing rate (bottom). Error bars represent standard error of the mean (SEM). (<bold>f</bold>) Example response maps from V1 using two-photon calcium imaging show reliable responses from the same neurons on successive days. Shown are six example neurons imaged across 2 days. Neurons were matched according to a procedure described in the Methods. Colormap same as in (<bold>e1</bold>). Spatial masks are from suite2P spatial filters and are meant to illustrate qualitatively similar morphology in matched neurons across days. (Correlation between days 1 and 2: first column: 0.46, 0.90, 0.83, 0.82, 0.58, 0.36; second column: 0.39, 0.96, 0.93, 0.66, 0.85, 0.47.) (<bold>g</bold>) Distribution of Pearson correlations between figure maps for all matched cell pairs (red) and a set of randomly shuffled cell pairs (black). Neurons displayed highly reliable responses to the stimulus (<italic>N</italic> = 950 cell pairs in each group, mean = 0.5579 for matched vs. mean = 0.2054 for unmatched, p = 1e−163, KS test, <italic>N</italic> = 475 cells matched, day 1: 613/day 2: 585).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig3-v4.tif"/></fig><p>To visualize a neuron’s response to figure, ground, and borders, we computed a ‘figure map’ consisting of the neuron’s mean response to a figure centered at each of 128 (16 × 8) positions across the visual field (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, <xref ref-type="video" rid="video2">Video 2</xref>). The square figure appeared for 250 ms at each position. The stimulus closely mimicked that used for behavioral tests; in particular, the square moved in counterphase to the background. This stimulus enabled us to measure responses of each neuron to figure, ground, and borders, as on any given trial a particular location contained figure, ground, or borders. For example, the figure map of an ideal figure cell would reveal a square corresponding to the figure (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, left), that of a border cell would reveal stripes corresponding to the figure borders matching the orientation of the cell (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, middle), and that of an ON-cell would reveal phase-dependent responses to the figure (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, right). As these model units illustrate, the figure map is a function of a cell’s receptive field location, low-level stimulus preferences (e.g., orientation selectivity, contrast polarity selectivity), and high-level stimulus preferences (figure/ground selectivity, border selectivity). Thus, the figure map yields a rich fingerprint of a cell’s visual selectivity.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-74394-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Examples of dynamic stimuli used to compute figure maps for cells in electrophysiology and imaging experiments.</title><p>Left to right: Cross, Iso, and Nat conditions.</p></caption></media><p><xref ref-type="fig" rid="fig3">Figure 3e1</xref> shows the figure map for one example cell from V1 recorded with electrophysiology in the Cross condition. Responses to a subset of four stimuli (<xref ref-type="fig" rid="fig3">Figure 3e2</xref>) revealed a decreasing response as the figure moved off the receptive field (<xref ref-type="fig" rid="fig3">Figure 3e1–e3</xref>). We confirmed that these figure maps were highly stable using two-photon imaging across multiple days. <xref ref-type="fig" rid="fig3">Figure 3f</xref> shows figure response maps obtained from six example cells across two different days. The mean correlation between maps from matched cell pairs across different days was very high (<italic>N</italic> = 950 cell pairs, Pearson <italic>r</italic> = 0.5579 matched vs. Pearson <italic>r</italic> = 0.2054 unmatched, KS test p = 1e−163, <xref ref-type="fig" rid="fig3">Figure 3g</xref>).</p><p>To fully characterize responses of neurons to figure, ground, and borders, we obtained figure maps using the same three conditions as those used earlier in behavior (Cross, Iso, and Nat) (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). We presented two orientations/textures for each of the three conditions. In V1, we often found cells that showed orthogonal stripes for the two different cross patterns (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), as expected for an ON- or OFF-cell (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, right). We failed to find any cells in V1, LM, or RL that showed consistent figure maps across the different conditions (<xref ref-type="fig" rid="fig4">Figure 4a–d</xref>). To quantify this across the population, we computed distributions of the mean Pearson correlation between figure maps across all possible pairs from the six conditions: the values were centered around 0 (V1: 0.032, LM: 0.034, RL: 0.015) (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Within each condition, the mean Pearson correlation between figure maps was also centered around 0 (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This shows that across the population, selectivity to figure location within individual neurons was strongly dependent on the specific texture of the figure.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Segmentation-related modulation across mouse visual cortex is pattern dependent.</title><p>(<bold>a</bold>) To search for segmentation-related neural signals, we adapted the stimulus shown in <xref ref-type="fig" rid="fig3">Figure 3c</xref> (a figure switching between 128 positions) to the three conditions that we had tested behaviorally (Cross, Iso, and Nat). As in the behavior experiments, for each condition we presented two variants (different orientations or patterns). Rows 1 and 2: Two example frames (with square at different positions) are shown for each of the six stimulus variants. Overlaid on these example stimuli are grids representing the 128 possible figure positions and a green ellipse representing the ON receptive field. Note that this receptive field is the Gaussian fit from the sparse noise experiment. Row 3: Example figure map from one cell obtained for the conditions shown above. Rows 4 and 5: Example rasters when the figure was centered on (red) or off (blue) the receptive field. Row 6: PSTHs corresponding to the rasters; shaded error bars represent standard error of the mean (SEM). (<bold>b</bold>) Figure maps for each of the six stimulus variants for four example neurons from V1 (responses measured using electrophysiology). Please note that for all of these experiments the population receptive field was centered on the grid of positions. (<bold>c</bold>) Figure maps for each of the six stimulus variants for four example neurons from LM. (<bold>d</bold>) Figure maps for each of the six stimulus variants for four example neurons from RL.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig4-v4.tif"/></fig><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Mice lack consistent segmentation signals across texture conditions.</title><p>(<bold>a</bold>) Distribution of Pearson correlations between figure maps across all <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>6</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> pairs of conditions. No neuron in any area showed high correspondence (signified by non-zero mean) across all conditions tested, indicative of a texture-invariant figure response. (<bold>b</bold>) Distribution of Pearson correlations between figure maps across the two stimulus variants for each condition (orange: Cross, violet: Iso, green: Nat) and across visual areas (left: V1, middle: LM, right: RL), V1 = 260 cells, LM = 298 cells, RL = 178 cells. Means and p value testing for difference from 0 for each condition and area: 0.03, 9.6e−4 (V1, Cross), 0.06, 1.9e−6 (LM, Cross), 0.02, 1.8 (RL, Cross), 0.03, 3.1e−5 (V1, Iso), 0.02, 2.1e−2 (LM, Iso), −0.0038, 0.60 (RL, Iso), 0.006, 0.92 (V1, Nat), 0.005, 0.44 (LM, Nat), 0.009, 0.21 (RL, Nat). (<bold>c</bold>) A figure-ground modulation (FGM) index was computed by taking the mean response on background trials (positions outlined by dashed lines) and the mean response on figure trials (positions outlined by solid line) and computing a normalized difference score. Note, the black/white colormap in this figure corresponds to a model schematic RF. (<bold>d</bold>) Distribution (shown as a kernel density estimate) of FGM indices for Cross (orange), Iso (violet), and Nat (green) conditions, pooling cells from V1, LM, and RL. (<bold>e</bold>) Fraction of cells with FGM index significantly different from zero (p &lt; 0.05) for <italic>N</italic> stimulus variants (out of the six illustrated in <xref ref-type="fig" rid="fig4">Figure 4a</xref>). Dotted gray line represents chance level false-positive rate at p &lt; 0.05 after six comparisons. For this analysis, FGM was computed similarly as (<bold>d</bold>), but responses were not averaged across orientations/patterns within each condition; thus each cell contributed 6 FGM values. (<bold>f</bold>) A border-ownership modulation index was computed by taking the mean response on left edge trials (positions outlined by dashed rectangle marked ‘L’) and the mean response on right edge trials (positions outlined by dashed rectangle marked ‘R’) and computing a normalized difference score. (<bold>g, h</bold>) Same as (<bold>d</bold>), (<bold>e</bold>), but for border-ownership modulation indices. Mean response time courses across all cells in V1, RL, and LM to figure, ground, and border in the Cross (<bold>i</bold>), Iso (<bold>j</bold>), and Nat (<bold>k</bold>) conditions (total <italic>N</italic> = 736). Time points for which the response to the figure was significantly greater than the response to ground are indicated by the horizontal line above each plot (p &lt; 0.01, <italic>t</italic>-test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig5-v4.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Mean time courses of responses across the population to figure, ground, and border in areas V1, LM, and RL.</title><p>Conventions as in <xref ref-type="fig" rid="fig4">Figure 4</xref> (V1: <italic>n</italic> = 118; LM: <italic>n</italic> = 48; RL: <italic>n</italic> = 14).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig5-figsupp1-v4.tif"/></fig></fig-group><p>We next quantified selectivity for figure ground and border ownership, two of the hallmark segmentation-related signals reported in the macaque visual system, across the V1, LM, and RL cell populations. We only analyzed neurons that had significant receptive field fits (see Methods); furthermore, we confined our analysis to neurons with receptive fields centered within four degrees of the monitor center, to ensure that there were an adequate number of figure, ground, and left/right trials from which to compute the neuron’s modulation indices. For each of the three conditions, we defined a figure-ground modulation (FGM) index as <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mi>G</mml:mi><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the mean response across the two patterns for the condition within the figure zone, i.e., the 2 × 2 (6° × 6 °) grid of locations centered on the monitor (4 locations × 10 trials = 40 figure trials) and <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the mean response across the two patterns for the condition in the background zone, that is, the leftmost and rightmost column of locations (2 × 8 locations × 10 trials = 160 background trials) (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). We were extremely conservative in our selection of figure and ground locations to avoid any mistakes in labeling due to uncertainties in receptive field location. Distributions of FGM indices were approximately centered on zero, with a slight rightward shift for Cross (Cross: 0.07, Iso: −0.014, Nat: −0.03) (<xref ref-type="fig" rid="fig5">Figure 5d</xref>). To determine significance of FGM indices, for each cell and condition, we estimated a bootstrapped p value for the corresponding FGM value using distributions of shuffled trials (see Methods); we found no neurons that showed significant figure modulation across more than three conditions (<xref ref-type="fig" rid="fig5">Figure 5e</xref>).</p><p>We quantified border-ownership selectivity in a similar way. We defined a border-ownership modulation (BOM) index as <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean response across the two patterns for the condition within the left border zone (Column 4; 8 locations × 10 trials = 80 left edge trials), and <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean response across the two patterns for the condition within the right border zone (Column 12; 8 locations × 10 trials = 80 right edge trials) (<xref ref-type="fig" rid="fig5">Figure 5f</xref>). Distributions of BOM indices were approximately centered on zero (Cross: −0.02, Iso: −0.01, Nat: −0.07) (<xref ref-type="fig" rid="fig5">Figure 5g</xref>). We found no neurons that showed significant BOM across more than three conditions (<xref ref-type="fig" rid="fig5">Figure 5h</xref>). Thus overall, we found weak signals for FGM and BOM, which moreover depended strongly on specific texture condition, across the three mouse visual areas surveyed.</p><p>Mean time courses of responses across the population to figure, ground, and border confirmed the strong texture dependence of FGM and BOM signals (<xref ref-type="fig" rid="fig5">Figure 5i–k</xref>). While there was clear enhancement in response to the figure/border versus ground for the Cross condition starting at the earliest time point of response (<xref ref-type="fig" rid="fig5">Figure 5i</xref>), differences were much smaller for Iso and Nat conditions (<xref ref-type="fig" rid="fig5">Figure 5j, k</xref>). Comparison of time courses across areas revealed a more distinct response difference between figure and ground conditions in LM compared to V1, and V1 compared to RL, with strong texture dependence of segmentation signals in all three areas (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p></sec><sec id="s2-5"><title>Neural decoding of figure position mirrors behavioral performance</title><p>The neural data so far shows a clear lack of texture-invariant segmentation signals in mouse visual areas V1, LM, and RL (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>). This is consistent with the mouse’s inability to generalize segmentation across textures for the Nat condition (<xref ref-type="fig" rid="fig2">Figure 2e, f, i1</xref>). However, the mouse was behaviorally able to generalize segmentation in the Cross and (to a lesser extent) Iso conditions (<xref ref-type="fig" rid="fig2">Figure 2i1</xref>). To what extent can the neural signals in mouse visual cortex explain this pattern of behavior?</p><p>To address this, we quantified how well we could read out the position of a figure on a given trial using a linear decoder of neural responses. Within a single condition (Cross, Iso, and Nat), decoding position would be trivial for a single stimulus pattern: a set of ON cells with localized receptive fields like the hypothetical unit in <xref ref-type="fig" rid="fig3">Figure 3d</xref> (right) would be able to solve this task, as long as cells respond reliably and differentially to stimuli with different figure locations. How well could a decoder trained on the mouse’s neural responses <italic>generalize</italic> segmentation across textures within a class? For each of the three conditions, we pooled trials for the two orientations/patterns, and then trained a least squares linear regression model using 50/50 cross-validation over trials (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). <xref ref-type="fig" rid="fig6">Figure 6b</xref> shows decoded versus actual figure position for varying numbers of cells; for convenience, we decoded azimuth position. Decoding improved monotonically with the number of cells used.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Decoding figure position from neural responses.</title><p>(<bold>a</bold>) Schematic of approach for decoding figure position from neural population responses. For each neuron, figure response maps for both types of stimuli from a given texture condition (Cross, Iso, and Nat) were pooled, and reshaped into a 1-d vector, producing a population matrix of N neurons × 128 positions; the population response matrix for the Cross condition is shown. A linear decoder for figure azimuth position was then learned with cross-validation from the population response matrix using 50% of trials for training and the remaining 50% of trials for testing. (<bold>b</bold>) A linear readout was computed for a given random sample of <italic>N</italic> neurons, shown here for 5 (top), 20 (middle), and 100 (bottom) neurons. Each dot plots the actual azimuth bin (<italic>x</italic>-axis) against the predicted bin (<italic>y</italic>-axis). Mean explained variance was then computed across 50 repeated samples and plotted as a function of number of neurons (right). (<bold>c</bold>) Variance explained by decoded azimuth position as a function of number of neurons used to train the decoder for each of the different texture conditions (electrophysiology data). The most robust position decoding was obtained for Cross (orange), followed by Iso (violet) and then Nat (green). Error bars represent standard error of the mean (SEM) (total <italic>n</italic> = 736; V1: <italic>n</italic> = 260; LM: <italic>n</italic> = 298; RL: <italic>n</italic> = 178). (<bold>d</bold>) Same plot as (<bold>c</bold>) but for deconvolved calcium imaging data (total <italic>n</italic> = 11,635; V1: <italic>n</italic> = 7490; LM: <italic>n</italic> = 2930; RL: <italic>n</italic> = 1215). (<bold>e</bold>) Same data as in (<bold>c</bold>), but broken down by both texture condition and visual region. LM (red) consistently showed better positional decoding than either V1 (blue) or RL (green). Error bars represent SEM. (<bold>f</bold>) Same as (<bold>e</bold>) but for deconvolved calcium imaging data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig6-v4.tif"/></fig><p>We quantified decoding performance as the variance in the azimuth position explained by the linear model (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). Using electrophysiology data, we found that on average neural decoding was best for the Cross condition (<italic>r</italic><sup>2</sup> = 0.89 for 200 cells), followed by Iso (<italic>r</italic><sup>2</sup> = 0.53 for 200 cells), and then Nat (<italic>r</italic><sup>2</sup> = 0.09 for 200 cells). This dependence of position decoding on texture condition (Cross &gt; Iso &gt; Nat) matched the ranking observed in the behavioral performance of animals on the generalization stage of the figure localization task (<xref ref-type="fig" rid="fig2">Figure 2i1</xref>). In particular, variance explained was close to zero for the Nat condition.</p><p>Using imaging data, we found the same qualitative pattern, though overall decoding performance was worse than that obtained from electrophysiology data for the same number of neurons (<xref ref-type="fig" rid="fig6">Figure 6d</xref>), likely due to the fact that the calcium signal is significantly noisier (<xref ref-type="bibr" rid="bib2">Berens et al., 2018</xref>).</p><p>We next examined decoding performance for each of the conditions as a function of visual area. For both the Cross and Iso conditions, decoding was best for LM followed by V1 and RL (for <italic>N</italic> = 120 cells: Cross: LM &gt; RL: p &lt; 10<sup>−4</sup>, LM &gt; V1: p &lt; 10<sup>−4</sup>, V1 &gt; RL: n.s.; Iso: LM &gt; RL: p &lt; 10<sup>−4</sup>, LM &gt; V1: p &lt; 10<sup>−4</sup>, V1 &gt; RL: p &lt; 10<sup>−4</sup>, rank sum test) (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). A similar relationship was observed with imaging data (<xref ref-type="fig" rid="fig6">Figure 6f</xref>), albeit with better decoding for RL compared to V1 for Cross.</p></sec><sec id="s2-6"><title>Mouse segmentation is well modeled by a deep network</title><p>How could neural signals in the mouse support linear decoding of figure position in the Cross and Iso conditions despite lack of explicit figure-ground and border-ownership cells? To address this, we tested different neural encoding models for how well they could explain the observed decoding performance. We first simulated a population of 25,000 simple cells with varied receptive field size, location, orientation, preferred phase, and spatial frequency (see Methods). Each unit consisted of a linear filter followed by linear rectification and additive Gaussian noise (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). We refer to this model as the ‘feedforward LN model’. We attempted to decode figure position from this model using the same procedures as we used for analyzing the neural data (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). Surprisingly, we found that we could robustly decode figure position for the Cross condition, though not for the Iso and Nat conditions (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). It is widely assumed that figure-ground segregation (i.e., detecting the location of the square in the displays in <xref ref-type="video" rid="video2">Video 2</xref>) cannot be accomplished through purely local linear filters. How could a simple feedforward LN model decode figure position when the local stimulus at the figure center is identical to that of ground after pooling across conditions? We realized that to decode figure position, one need not rely on signals from the center of the figure; instead, one can use signals at the edges, and simple cells can readily localize orientation discontinuities such as those present in the Cross condition. This underscores an important point: the Cross stimulus completely fails as a behavioral marker for a nonlinear figure-ground segmentation process (see also <xref ref-type="bibr" rid="bib61">Vinken and Op de Beeck, 2021</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Mid to late layers of a deep network recapitulate mouse neural and behavioral performance on figure position decoding across texture conditions.</title><p>(<bold>a</bold>) Schematic of the feedforward linear–nonlinear (LN) encoding model (see Methods). The stimulus was passed through a Gabor filter, followed by a rectifying nonlinearity, and then a Poisson spiking model, with noise added to responses to simulate population response variability (e.g., due to non-sensory signals such as movement or arousal). We ran the same stimuli (128 positions × 6 conditions) through the model that we used for electrophysiology and two-photon imaging (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). (<bold>b</bold>) Positional decoding performance, quantified as variance explained by decoded azimuth position, as a function of number of neurons in the feedforward LN model. Cross (orange) positional decoding was robust, while both Iso (violet) and Nat (green) were extremely poor, in contrast to electrophysiology (<xref ref-type="fig" rid="fig6">Figure 6c</xref>) and imaging (<xref ref-type="fig" rid="fig6">Figure 6d</xref>) results. Noise variance was set to twice the network-level firing rate within a condition here and in (<bold>c, e</bold>) below; for a full sweep across noise parameters, see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. Error bars represent standard error of the mean (SEM). Small random offset added for visualization purposes. (<bold>c</bold>) Adding an orientation-dependent divisive term to the LN model to mimic iso-orientation surround suppression (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>; LN + surround model) yielded more robust decoding in the Cross condition (orange), but did not improve decoding in the Iso (violet) or Nat (green) conditions. For a full sweep across noise parameters, see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. Error bars represent SEM. Small random offset added for visualization purposes. (<bold>d</bold>) Architecture of a pre-trained deep neural network (VGG16) trained on image recognition (<xref ref-type="bibr" rid="bib53">Simonyan and Zisserman, 2014</xref>). Five convolution layers are followed by three fully connected layers. (<bold>e</bold>) Positional decoding performance increases throughout the network with most robust decoding in layer 4. In mid to late layers (3–5) of the deep network, decoding performance was best for Cross (orange), followed by Iso (violet) and then Nat (green), mirroring mouse behavioral performance (<xref ref-type="fig" rid="fig2">Figure 2c, i1</xref>) and neural data (<xref ref-type="fig" rid="fig6">Figure 6c, d</xref>). For a full sweep across noise parameters, see <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>. Error bars represent SEM. (<bold>f</bold>) Schematic summary of behavioral results. Mice showed texture-dependent performance in a figure localization task, with Cross &gt; Iso &gt; Nat, despite the presence of a common motion cue for segmentation in all conditions. In contrast, primates showed no dependence on carrier texture and were able to use the differential motion cue to perform the task. (<bold>g</bold>) Schematic summary of neural and modeling results, Positional decoding from neural populations in V1, LM, and RL mirror the textural dependence of the behavior, with Cross &gt; Iso &gt; Nat. This ordering in performance was not captured by a feedforward LN model or an LN model with surround interactions. However, it emerged naturally from nonlinear interactions in mid to late layers of a deep neural network.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig7-v4.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>The effect of noise on position decoding for feedforward LN and surround models.</title><p>Top row: Position decoding performance for Cross, Iso, and Nat conditions in a population of feedforward LN neurons across noise conditions (columns). Note that positional decoding for Cross is high across noise conditions, while for Iso and Nat it remains low. The noise levels indicate the ratio between the noise variance and the network-level firing rate (see Methods). Bottom row: Same as top row, for a population of feedforward LN neurons with orientation-dependent surround interactions. Including this extra-classical receptive field modulation had no effect on the relative decoding performance for Iso versus Nat conditions. Error bars represent standard error of the mean (SEM).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig7-figsupp1-v4.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>The effect of noise on position decoding for intermediate layers of VGG16.</title><p>Position decoding performance for Cross, Iso, and Nat conditions as a function of neural network layer (rows) and increasing population response noise (columns). Note that across various noise conditions, the separation of Cross, Iso, and Nat conditions remains prominent. Error bars represent standard error of the mean (SEM).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig7-figsupp2-v4.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Modeling orientation-dependent surround interactions.</title><p>(<bold>a</bold>) Standard feedforward LN model used to model simple cell responses. (<bold>b</bold>) Model for divisive normalization that we used to model orientation-dependent surround interactions (<xref ref-type="bibr" rid="bib20">Hunter and Born, 2011</xref>). The neuron’s feedforward response was modulated by a divisive term that took into account the mean response of all neurons of a given Gabor type with RF centers within 2<italic>σ</italic> of a given receptive field’s center (<inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and those that lay &gt;2<italic>σ</italic> and &lt;5<italic>σ</italic> outside of the receptive field (<inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). From these two vectors (each 100 elements long, we computed a Pearson correlation, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>), which ranged from −1 to 1, leading to suppression when the orientation energy in <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> matched and facilitation when <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were orthogonal. (<bold>c</bold>) Schematic representation of the zones for <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , each defined relative to a cell’s receptive field center. (<bold>d</bold>) Row 1: Schematic of six stimulus conditions. Row 2: Figure maps for an example feedforward LN neuron for each of the six conditions. Row 3: Map of the modulation term value at each position; note that whether there is suppression or facilitation is a function of the stimulus condition, figure position, and receptive field position. Row 4: Figure maps for an example surround-model neuron; for the Cross condition, adding surround modulation results in general facilitation, while for Iso and Nat conditions, it results in depression or no modulation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-fig7-figsupp3-v4.tif"/></fig></fig-group><p>We next modeled orientation-dependent surround modulation, a previously reported nonlinear interaction in mouse visual cortex (<xref ref-type="bibr" rid="bib62">Wang et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Self et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Keller et al., 2020a</xref>). To simulate orientation-dependent surround modulation, we added a divisive term such that <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>∗</mml:mo><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow/></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the correlation between the mean orientation energy within a cell’s receptive field (<inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), compared to that in the surround (<inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). This surround model behaved similar to our feedforward LN model, failing to capture the texture dependence of the neural and behavioral data from the mouse (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). This is not surprising, since the nonlinear interaction in this model depends on an orientation discontinuity, which was absent from the Iso condition. These results held generally across a range of noise levels (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><p>Finally, we hypothesized that while orientation-dependent surrounds might be an insufficient nonlinearity to explain the mouse’s behavioral and neural data, a deep convolutional network (DCN) trained on object recognition might develop many nonlinearities useful for performing the figure localization task. For example, common objects in cluttered scenes can resemble either the Cross or Iso conditions. We ran our stimuli through Vgg-16 pre-trained on ImageNet to perform object classification (<xref ref-type="fig" rid="fig7">Figure 7d</xref>) and analyzed responses in convolutional layers 1–5 (<xref ref-type="fig" rid="fig7">Figure 7e</xref>; <xref ref-type="bibr" rid="bib53">Simonyan and Zisserman, 2014</xref>). We then tested decoding performance exactly as for the feedforward LN and surround models by randomly drawing subsamples of cells from a given layer. The performance of the DCN matched the mouse’s neural and behavioral data well: performance was best for Cross, followed by Iso, and then Nat, with this preference emerging in mid to late layers of the network. These results held generally across a variety of noise levels (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). Thus a DCN replicated the rank ordering of the mouse’s behavior and neural decoding performance (Cross &gt; Iso &gt; Nat). This suggests the possibility that the mouse visual system may use similar nonlinear interactions as in a feedforward deep network to accomplish object detection.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have shown that mice and primates segment visual input using fundamentally different strategies. Unlike primates, mice are unable to detect figures defined purely by opponent motion, and hence mouse segmentation behavior is strongly dependent on texture cues (<xref ref-type="fig" rid="fig7">Figure 7f</xref>). Indeed, when mice were forced to detect figures defined purely by opponent motion for a limited number of patterns, they adopted a strategy of brute force pattern memorization (<xref ref-type="fig" rid="fig2">Figure 2e–g</xref>). The strong texture dependence of mouse object detection behavior was consistent with neural signals recorded in mouse visual areas V1, RL, and LM, and could be explained by a simple feedforward deep network model lacking explicit segmentation capabilities (<xref ref-type="fig" rid="fig7">Figure 7g</xref>).</p><p>When we tested three additional species, the macaque, mouse lemur, and treeshrew, using the same paradigm, we found that only the two primate species could perform segmentation using opponent motion. It was especially surprising that the mouse lemur, a tiny (~60–80 g) prosimian primate species (<xref ref-type="bibr" rid="bib17">Ho et al., 2021</xref>), could segment purely motion-defined figures well above chance, while the treeshrew (~120–200 g), an animal with a much more highly developed visual system than the mouse (<xref ref-type="bibr" rid="bib65">Wong and Kaas, 2009</xref>; <xref ref-type="bibr" rid="bib33">Mustafar et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Van Hooser et al., 2013</xref>), could not. We emphasize again that the differences cannot be attributed to differences in visual acuity between the species for multiple reasons, including that mice had no difficulty detecting an otherwise identical moving square on a static background, and treeshrews and mouse lemurs have highly similar visual acuities (<xref ref-type="bibr" rid="bib17">Ho et al., 2021</xref>). Overall, our findings reveal a fundamental difference in the computational strategy used by mice/treeshrews versus primates for visual segmentation and suggest that surface perception from accretion–deletion may be a capability unique to primates. We believe this is highly significant because visual surface representation is a fundamental step in primate visual processing (<xref ref-type="bibr" rid="bib35">Nakayama et al., 1995</xref>; <xref ref-type="bibr" rid="bib48">Roe et al., 2012</xref>; <xref ref-type="bibr" rid="bib56">Tsao and Tsao, 2022</xref>), and accretion–deletion (<xref ref-type="fig" rid="fig1">Figure 1b, c</xref>) has been recognized since the seminal work of J.J. Gibson as <italic>the most powerful cue</italic> supporting visual surface representation (<xref ref-type="bibr" rid="bib12">Gibson, 1979</xref>). In particular, among all cues to surface organization, accretion–deletion is unique in its (1) high reliability, occurring only at true object borders and never at internal texture borders, (2) robustness to object texture, and (3) computational power, supporting not only segmentation but also invariant tracking without requiring any learning or prior experience (<xref ref-type="bibr" rid="bib56">Tsao and Tsao, 2022</xref>).</p><p>We were inspired by previous rodent behavioral studies that have sought to carefully characterize the visual capabilities of mice and rats, testing behaviors such as transformation-tolerant object recognition and natural scene discrimination (<xref ref-type="bibr" rid="bib5">De Keyser et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Vermaercke and Op de Beeck, 2012</xref>; <xref ref-type="bibr" rid="bib67">Yu et al., 2018</xref>; <xref ref-type="bibr" rid="bib71">Zoccolan, 2015</xref>; <xref ref-type="bibr" rid="bib70">Zoccolan et al., 2009</xref>; <xref ref-type="bibr" rid="bib51">Schnell et al., 2019</xref>). In particular, consistent with our finding that mice cannot detect naturalistic figures defined by opponent motion, Keyser et al. found that rats could not learn to detect a bar defined by a grid of Gabor patches moving counterphase to ones in the background (<xref ref-type="bibr" rid="bib5">De Keyser et al., 2015</xref>) (note, however, their stimulus did not contain accretion–deletion, the cue we were especially interested in for reasons explained above). Overall, our results add to a growing body of work showing that rodent and primate vision differ in essential ways beyond visual resolution.</p><p>Our finding that figure-ground signals exist in mouse visual cortex (<xref ref-type="fig" rid="fig3">Figures 3e, f</xref>, <xref ref-type="fig" rid="fig4">4</xref>, and <xref ref-type="fig" rid="fig5">5d, i</xref>), but are strongly dependent on texture (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5a, b, e, i–k</xref>), is consistent with previous studies (<xref ref-type="bibr" rid="bib49">Schnabel et al., 2018a</xref>; <xref ref-type="bibr" rid="bib25">Kirchberger et al., 2020</xref>; <xref ref-type="bibr" rid="bib50">Schnabel et al., 2018b</xref>; <xref ref-type="bibr" rid="bib23">Keller et al., 2020b</xref>). Optogenetic perturbation studies have further demonstrated that these signals are behaviorally relevant for figure detection (<xref ref-type="bibr" rid="bib25">Kirchberger et al., 2020</xref>) and require feedback (<xref ref-type="bibr" rid="bib25">Kirchberger et al., 2020</xref>; <xref ref-type="bibr" rid="bib22">Keller et al., 2020a</xref>). Thus overall, it seems clear that mouse visual cortex shows orientation-dependent surround modulation (<xref ref-type="bibr" rid="bib52">Self et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Keller et al., 2020a</xref>) which can support texture-based figure-ground segmentation behavior. However, importantly, our results show that mice lack a general, texture-invariant mechanism for surface segmentation, unlike primates. One caveat is that we did not directly record from corresponding primate visual areas using the exact same stimuli as we used for the mice in this study; however, the stimuli we used were highly similar to those reported in the macaque literature (e.g., <xref ref-type="bibr" rid="bib26">Lamme, 1995</xref>).</p><p>The inability of mice to detect figures defined purely by opponent motion was rather surprising, as cells selective for local motion (distinct from global retinal image drift due to fixational eye movements) have been reported in the retinas of both rabbits and mice (<xref ref-type="bibr" rid="bib24">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Olveczky et al., 2003</xref>). How could a signal present in the retina not be used by the animal for segmentation? First, in our experiments, figure and ground moved exactly in counterphase, a condition that the retinal object-motion cells are unable to detect (<xref ref-type="bibr" rid="bib37">Olveczky et al., 2003</xref>). Furthermore, retinal studies have generally used sinusoidal gratings, and it remains unclear how responses of retinal object-motion cells might generalize to arbitrary textures such as those used in our Nat task. Finally, we note that object localization in the Nat task required perception of <italic>differential</italic> motion cues. We confirmed that mice were readily able to detect the same moving figures against stationary backgrounds (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2c</xref>). It is possible that retinal object-motion cells are adapted for this latter condition—whose handling may be sufficient to ensure mouse survival—while different evolutionary pressures led to emergence of a more sophisticated segmentation mechanism in primates.</p><p>The distinction between mouse and primate segmentation behavior has an intriguing parallel in the difference between deep network and human object classification behavior. In recent years, deep network models of vision have started to achieve state of the art performance on object classification tasks. However, the lack of an explicit segmentation process in these networks leads to susceptibility to adversarial examples in which noise patterns that lack any surface structure are classified as objects with high confidence (<xref ref-type="bibr" rid="bib13">Goodfellow et al., 2014</xref>; <xref ref-type="bibr" rid="bib55">Szegedy et al., 2013</xref>). Furthermore, recent work has shown that deep networks, unlike humans, fail to exploit global image features such as object boundaries and shape to perform classification, instead relying much more strongly on texture features (<xref ref-type="bibr" rid="bib3">Brendel and Bethge, 2019</xref>; <xref ref-type="bibr" rid="bib11">Geirhos et al., 2018</xref>). Thus it is clear that a major difference between deep networks and the primate visual system lies in their segmentation capabilities. Our finding of strong texture dependence in mouse segmentation behavior suggests that mice may adopt a visual strategy more similar to deep networks than primates do (<xref ref-type="fig" rid="fig7">Figure 7g</xref>; see also <xref ref-type="bibr" rid="bib60">Vinken and Op de Beeck, 2020</xref>), though further detailed circuit dissection will be necessary to test this conjecture.</p><p>One may wonder whether the difference between mouse/treeshrew versus primate vision that we have uncovered is truly of ethological significance, since all of the species studied were able to segment objects from the background using texture or texture-invariant motion cues on a static background, and in the natural world, backgrounds are usually immobile. We underscore that we are not claiming that accretion–deletion is the only cue to segmentation. Rather, its chief importance is that it can enable texture-invariant segmentation and tracking <italic>without learning</italic>, thus it can provide a rich stream of training examples to self-supervise learning of other cues (<xref ref-type="bibr" rid="bib56">Tsao and Tsao, 2022</xref>; <xref ref-type="bibr" rid="bib4">Chen et al., 2022</xref>). In this respect, the discovery that non-primate brains are incapable of using accretion–deletion is highly ethologically significant, because it means a key <italic>computational mechanism</italic> for self-supervised learning of a physical world model may be unique to primates.</p><p>Objects are the fundamental building blocks of a primate’s model of the world. An object is first and foremost a <italic>moveable</italic> chunk of matter, that is, a segmentable surface. The remarkable trajectory of the human species has been variously attributed to language, tool use, upright posture, a lowered larynx, opposable thumbs, and other traits (<xref ref-type="bibr" rid="bib27">Leakey, 1996</xref>). We close with a speculation: it may not be entirely implausible that possession of machinery for cue-invariant visual object segmentation played a role in setting the human trajectory. By enabling self-supervised learning of a rich and accurate physical model of the world inside the primate brain, this perceptual machinery, seemingly less developed in all non-primate species tested so far including both mice and treeshrews, may have supplied the foundation for subsequent human capabilities requiring a hyper-accurate model of objects in the world—including tool use, causal understanding, and general intelligence.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Animal statement</title><p>The following animals were used in this study: adult mice 2–12 months old, both male and female; adult treeshrews 7–18 months old, both male and female; adult mouse lemurs 2–3.5 years, both male and female; and adult macaques 3 and 7 years old, male. All procedures on mice, macaques, and treeshrews were conducted in accordance with the ethical guidelines of the National Institutes of Health and were approved by the Institutional Animal Care and Use Committee at the California Institute of Technology.</p><p>Mouse lemur experiments were in accordance with European animal welfare regulations and were reviewed by the local ethics committee (Comite d’éthique en expérimentation animale No. 68) in Brunoy, France, by the ethics committee of the University of Geneva, Switzerland and authorized by the French ‘Ministère de l’education nationale de l’enseignement supérieur et de la recherche’.</p></sec><sec id="s4-2"><title>Transgenic animals</title><p>For imaging experiments, we used a cross between a CamKII::ttA mouse (JAX: 00310) with a tetO:GCaMP6s (JAX: 024742) to target expression to cortical excitatory neurons. For electrophysiology experiments, we used a Thy1::GCamp6s 4.12 (JAX: 025776). Behavioral experiments were carried out with a combination of Thy1 and C57BL/6 animals. We back crossed all lines to C57BL/6.</p></sec><sec id="s4-3"><title>Surgical procedures</title><p>The cranial window and headplate procedures were based on <xref ref-type="bibr" rid="bib63">Wekselblatt et al., 2016</xref> with some modifications as described below.</p></sec><sec id="s4-4"><title>Headplate surgery</title><p>For both electrophysiology and imaging experiments, a stainless steel headplate was attached to the animal’s skull in a short procedure. Animals were anesthetized using isoflurane (3% induction; 1.5–2% maintenance) in 100% O<sub>2</sub> (0.8–1.0 l/min) and positioned in a stereotax using earbars placed just below the ear canal for stability. The animals were given subcutaneous injections of the analgesic Ketoprofen (5 mg/kg) and 0.2 ml saline to prevent postoperative dehydration. Body temperature was maintained at 37.5°C by a feedback-controlled heating pad; temperature and breathing were monitored throughout surgery. Sterilized instruments and aseptic technique were used throughout. Sterile ocular lubricant (Puralube) was applied at the beginning of each surgical procedure. Scalp hair was removed using an electric shaver, and the surgical site was cleaned using a combination of dermachlor and chlorohexidine. A scalp incision was made using #3 scissors (FST) and the periosteum was pulled back using forceps. The back neck muscles were retracted to make room for a either an 8 or 10 mm circular opening headplate which was affixed to the skull using either metabond (Parker) or dental acrylic (OrthoJet). A combination of vet bond and cyanoacrylate-based glue was applied to the skull to both protect the skull surface from infection and to provide a clear surface through which to perform widefield imaging to identify cortical visual areas in electrophysiology experiments.</p></sec><sec id="s4-5"><title>Craniotomy surgery/window implantation</title><p>After allowing the animal to recover at least 1 week from the headplate surgery a craniotomy procedure was performed to either allow for acute implantation of an electrode or to install a glass coverslip for chronic imaging.</p><p>Animals were anesthetized using isoflurane (3% induction; 1.5–2% maintenance) in 100% O<sub>2</sub> (0.8–1.0 l/min) and positioned in the stereotaxic frame affixed by the headplate attached previously. The animals were given subcutaneous injections of the analgesic Ketoprofen (5 mg/kg) and 0.2 ml saline to prevent postoperative dehydration. Body temperature was maintained at 37.5°C by a feedback-controlled heating pad; temperature and breathing were monitored throughout surgery. Sterilized instruments and aseptic technique were used throughout. Sterile ocular lubricant (Puralube) was applied at the beginning of each surgical procedure.</p><p>For imaging experiments, a 4–5 mm craniotomy was cut out centered at +0.5 mm from lambda and +2.75 mm from midline on the right hemisphere. Care was taken to minimize bleeding, and any bleeds in the skull during drilling were covered with wet gelfoam (Pfizer) until they resolved. After careful removal of the bone flap, a durotomy was performed and the exposed brain was covered in a 1:1 mix of artificial dura (Dow Corning 3-4680). A sterile 4–5 mm coverslip was then pressed into the opening and sealed in place using a combination of cyanoacrylate-based glues. The remaining parts of exposed skull in the headplate well were then covered with black dental acrylic for light blocking purposes and to prevent infection.</p><p>For electrophysiology experiments, retinotopic mapping was performed prior to performing any craniotomy, resulting a vasculature and field sign map to identify vasculature landmarks corresponding to either V1, LM, or RL. Once such landmarks had been identified, a small (&lt;1 mm) craniotomy was performed on the morning of each experiment. The craniotomy was sealed with KwikSil (WPI) and animals were allowed to recover for at least 3 hr before subsequent recording experiments.</p></sec><sec id="s4-6"><title>Visual stimuli</title><p>Visual stimuli for electrophysiology/imaging were presented on a 32-inch monitor (Samsung 32 inch lcd screen; 40 × 71 cm); linearized by eye to correct for gamma (mean luminance 50 cd/m<sup>2</sup>), oriented tangentially 18 cm from the mouse’s left eye (figure size: ~30°) for all experiments except those shown in <xref ref-type="fig" rid="fig3">Figure 3f</xref> for quantifying reproducibility which were carried out at 27 cm from the eye (<xref ref-type="fig" rid="fig3">Figure 3b</xref>).</p><p>For all electrophysiology and imaging experiments, attempts were made to re-center the monitor on the general location of the receptive fields across the population to maximize the number of distinct figure and ground trials in a given experiment.</p></sec><sec id="s4-7"><title>Figure-ground/border-ownership stimulus</title><p>The stimuli used to characterize FGM and BOM of each cell consisted of a sinusoidal grating of 0.06 cpd, oriented at 45° or 135°. The figure was 27° in size. The horizontal positions surveyed spanned 45° (~3° shift per position), while the total elevation positions surveyed varied across 23° (~3° shift per position). For the Iso and Nat conditions where the texture defining the figure was identical within a given condition, the figure was generated at each position using the same texture that previously defined the background, thus creating a ‘pop out’ effect for the stimulus (see <xref ref-type="video" rid="video2">Video 2</xref>). Both figure and background moved out of phase with a sine wave of amplitude 14 pixels (3.5°) and temporal frequency of 1 cycle/s.</p><p>For the naturalistic texture used in electrophysiology/imaging experiments, we used synthetically generated naturalistic textures identical to those used in <xref ref-type="bibr" rid="bib9">Freeman et al., 2013</xref>. We did not regenerate the textures between experiments and instead chose two random textures which we presented for the two Nat conditions, similar to how two orthogonal orientations were presented for Cross and Iso conditions. These same textures were also used for Nat condition in the behavioral experiments in <xref ref-type="fig" rid="fig2">Figure 2</xref>, with the exception of the new textures in <xref ref-type="fig" rid="fig2">Figure 2f</xref>, which were taken from the naturalistic textures database. The naturalistic textures presented in the behavior in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> were also from the naturalistic textures database. For both electrophysiology and two-photon imaging experiments, the 6 conditions (Cross, Iso, Nat × 2 conditions) were pseudo-randomly interleaved, i.e., Cross/Iso/Nat were presented in a random order, and the two within-condition repetitions (e.g., two orientations or textures) were presented consecutively but also randomized.</p></sec><sec id="s4-8"><title>RF mapping</title><p>Receptive fields were mapped for neurons under two-photon using an isolated drifting Gabor patch stimulus: a patch of ~6° containing a drifting Gabor appeared in one of three different orientations (45°, 180°, and 315°) and two directions at a random position. We repeated this procedure for 9 repeats and at 16 by 9 positions and then collapsed responses across all trial types to compute a spatial PSTH. We then fit a 2D Gaussian to the response and classified neurons as having a significant receptive field fit if the goodness of fit exceeded the goodness of fit for at least 99 out of 100 bootstrapped trials where spatial location was shuffled across trials.</p><p>For electrophysiology, we used a sparse noise stimulus to map the spatial RFs in all three areas. We divided the screen into a 18 × 32 grid of squares, each 3.5° on a side (each grid square was 2.2 × 2.2 cm, 18 cm from the mouse’s left eye). In each stimulus frame, one grid square, black or white, was presented alternately. The stimulus contained 8000 frames in total and was presented at 10 Hz. RFs were computed by spike triggered average (STA). The analysis window for STA was 30–100 ms. Then we used 2D Gaussian to fit the RFs. <italic>The goodness of fit</italic> (<italic>GOF</italic>) = 1 − <italic>MSE/Var(RF</italic>), in which <italic>MSE</italic> is mean square error of the fitting =  <inline-formula><mml:math id="inf21"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> , <italic>RF</italic> is the receptive field of the cell, <italic>G</italic> is the 2D Gaussian fit of the RF, <italic>Var(RF</italic>) is the variance of the <italic>RF</italic>, <inline-formula><mml:math id="inf22"><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The cells with RF GOF above 0.1 were include in subsequent analyses.</p><p>We limited our analysis for FGM and BOM in <xref ref-type="fig" rid="fig5">Figure 5</xref> to neurons that had significant receptive field fits and used the central position of that fit as the neuron’s inferred RF center. For population neural decoding analyses, we used all neurons regardless of receptive field fits.</p></sec><sec id="s4-9"><title>Mouse behavior</title><p>We trained 16 adult mice aged 8–12 weeks (4 × C57bl/6; 12 × Thy1-Gcamp6s mice) co-housed 4 to a cage with access to a running wheel. Mice were housed under reversed light–dark cycle (dark cycle from 10 am to 9 pm), and training was performed during the dark cycle. Mice were water restricted before, during, and after training days. The weight of the animals was continuously monitored to make sure no mouse dropped below 85% of their pre-water-restriction weight.</p><p>Mice were trained for 6 days/week, with a single session per day. Each mouse performed around 200–450 trials per day depending on the behavioral task. All training was carried out a commercially available touchscreen operant chamber in a sound-attenuating box (Bussey-Saksida Touch Screen Operant Chambers/Lafayette Instrument).</p><p>Visual stimuli were presented on a touchscreen monitor (24 × 18.5 cm, <italic>W</italic> × <italic>H</italic>, 800 × 600 pixels) (Lafayette Instrument). In all experiments, size of figures is given in pixels as the animals were freely moving and thus no estimate can be made of the size of the figure in visual degrees during the behavior as it will vary depending on the animal’s position. We generated stimulus movies of size 100 pixels high × 200 pixels wide, with square figures centered at either 50 or 150 pixels and square side of 50 pixels. Both figure and background moved out of phase with a sine wave of amplitude 14 pixels and temporal frequency of 1 cycle/s.</p><p>One side of each chamber was a touch screen monitor, and on the other side was a reward tray with a water spout. Mice had to touch the left or right side of the screen based on the location of the figure. The water delivery was cued by a white light illuminating the spout. An IR sensor detected when the mouse collected the water reward and determined the start of the next trial. For incorrect responses, a white noise audio stimulus was played for 3 s, followed by a ten second timeout. After the time-out period, the stimulus of the identical trial was presented again until the mouse made the correct touch. This shaping procedure was used to prevent biases in the behavior. All trials following the first incorrect response were not included in the analysis, as other similar studies have done (<xref ref-type="bibr" rid="bib49">Schnabel et al., 2018a</xref>; <xref ref-type="bibr" rid="bib54">Stirman et al., 2016</xref>).</p><p>We first trained the animal to become familiar with the touch screen and the task rules through a luminance shaping procedure. During this procedure, the mice learned to discriminate a white square moving on a black background and touch the side of the screen that displayed the square. In this task, they also learned that a correct touch was associated with a water reward. The mice started their subsequent training steps after reaching 80% on this task.</p><sec id="s4-9-1"><title>Standard 2-pattern discrimination</title><p>For these experiments, there were 4 possible stimuli (2 orientations/natural noise × 2 positions) (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, <xref ref-type="fig" rid="fig2">Figure 2i1</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2b1</xref>). Gratings were presented at two orientations, 45° and 135° (0.25 cycles/cm).</p></sec><sec id="s4-9-2"><title>5-Pattern discrimination</title><p>Animals were tested on this after training on the 2-pattern task (<xref ref-type="fig" rid="fig2">Figure 2i1</xref>). The stimuli were the same as for the 2-pattern discrimination, except we introduced five novel orientations or naturalistic noise patterns. For gratings, the five novel orientations were 22.5°, 67.5°, 90°, 112.5°, and 157.5°. For the Nat condition, we used five new natural noise patterns that shared the same Fourier statistics.</p></sec><sec id="s4-9-3"><title>30 natural textures</title><p>After finishing training on 2- and 5-pattern discrimination tasks, mice were trained on 30 natural textures in Iso and Cross configurations (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We first tested them on seven textures (set A) to measure the baseline performance. We then trained them on 30 Iso and Cross natural textures (set B). After training, they were tested on seven textures again (set A). For experiments involving natural textures we randomly selected textures from the describable textures dataset (DTD; <ext-link ext-link-type="uri" xlink:href="https://www.robots.ox.ac.uk‌/~vgg/data/dtd/">https://www.robots.ox.ac.uk‌/~vgg/data/dtd/</ext-link>). We converted all textures to grayscale.</p></sec><sec id="s4-9-4"><title>Grating to noise shaping</title><p>We generated a gradual morph stimulus that changed oriented gratings into naturalistic noise, with the goal being to train the animals to use the motion cue (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). In total, there were 10 stages in this shaping procedure. At each stage, the stimulus represented a weighted sum of the grating and noise, and the weight was changed 10% from the previous stage for both grating (decrease) and noise (increase). For example, in stage 3, the weight assigned to grating was 70% and the weight for noise is 30%. By stage 10, the weight for grating was 0% and for noise was 100%. At each stage, figure texture was flipped vertically and presented at a random height (top, middle, and down) to discourage use of local cues in the task.</p></sec><sec id="s4-9-5"><title>Static version of tasks</title><p>We tested mice on static versions of all conditions where only a single frame of the stimulus was presented (<xref ref-type="fig" rid="fig2">Figure 2g, h</xref>). The static stimulus was the frame that had the largest phase difference between figure and background.</p></sec><sec id="s4-9-6"><title>New texture task</title><p>After mice learned the grating to noise task, they were tested on seven unseen new natural textures (<xref ref-type="fig" rid="fig2">Figure 2f</xref>).</p></sec><sec id="s4-9-7"><title>Background static natural textures</title><p>We trained the mice to detect a figure moving horizontally on a static background. This stimulus was identical to the Nat condition, the only difference was that the background was static (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2c</xref>).</p></sec></sec><sec id="s4-10"><title>Treeshrew behavior</title><p>Four adult treeshrews (three male, one female; age: 7–18 months old) bred and raised at Caltech were trained to perform the figure-ground segregation task. Animals were singly housed in a 12 hr:12 hr light:dark cycle. They were not food or water restricted, but free access to water was limited during the 4 hr prior to training each day. Training was performed during the light cycle in a custom-made behavioral arena (30 × 30 × 25 cm) containing three optical lickports (Sanworks) situated in a custom-built behavior box. Drops of 100% apple juice rewards were provided upon poking at the appropriate lickports and in some cases for trial initiation. Images were presented on a Sunfounder 10.1″ Raspberry Pi 4 (1280 × 800) screen and controlled using Bpod hardware and Python software. After an initial shaping step in which animals learned to use the lickports in a luminance detection task (2–3 days), training for Cross, Iso, and Nat conditions was performed for 4 consecutive days (five orientations or naturalistic textures), with generalization test sessions on the fifth day (two different orientations or naturalistic textures) (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2a, b2</xref>). We reversed the number of train/test patterns compared to what was used for the mice (<xref ref-type="fig" rid="fig2">Figure 2i1</xref>) because we reasoned that animals might be more likely to generalize if given more patterns for training. We had performed the mouse experiments initially, noticed the memorization approach, and were trying to avoid this behavior in treeshrews. This also means that the naturalistic train condition presented to treeshrews was harder than that for mice (five orientations for treeshrews vs. two orientations for mice in the training set).</p></sec><sec id="s4-11"><title>Mouse lemur behavior</title><p>Four adult mouse lemurs (three male, one female; age: 2–3.5 years) bred and raised in the ‘Mouse Lemur Platform’ (authorization number E-91-114-1) of the ‘Museum National d’Histoire Naturelle’ in Brunoy, France (UMR MECADEV CNRS/MNHN 7179) were trained to perform the figure-ground segregation task. Animals were co-housed 2–3 per cage in a reversed long-day (14:10 light:dark) cycle. They were food restricted, with their body weight maintained above 60 g, but had free access to water. Training was performed during the dark cycle in a custom-made behavioral arena (20 × 20 × 30 cm) containing three optical lickports (Sanworks) situated in a sound-attenuating box. Drops of liquid food rewards (standard food mixture composed of banana, cereal, milk, and egg) was provided upon poking at the appropriate lickports. Images were presented on a Dell P2414H (1920 × 1080, 60 Hz) screen and controlled using Psychopy and Matlab software. Training and testing followed the same paradigm as for treeshrews (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2a, b3</xref>).</p></sec><sec id="s4-12"><title>Macaque behavior</title><p>Two head-fixed rhesus macaque monkeys were trained to indicate whether a square was on the left or right side of a screen. Movie stimuli were shown on an LCD screen in pseudo-random succession for 2 s ON time each, without any OFF period. The stimuli were shown across the full screen (23° in height and 37° width) and contained a square of 9° length on either the left or right side. Monkeys received a juice reward for fixating within the 9° square region for at least 1 s. Eye position was monitored using an infrared eye tracking system (ISCAN). Each monkey performed only one or two sessions of 1 or 2 hr each. In the beginning of the first session, behavior was shaped by training the monkeys on the luminance squares only until they reached 90% correct performance. Prior to this, the monkeys had been trained only to fixate. Both monkeys learned this within the first session and subsequently performed the task with all other stimuli presented in pseudo-random succession (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2b4</xref>). Stimuli included static and moving luminance squares, cross-orientation and iso-orientation gratings, and natural textures.</p><p>For offline analysis, we computed the percentage of correct trials where monkeys were fixating the correct square location for at least 1 s, including only trials where monkeys were looking at the screen and not closing the eyes.</p></sec><sec id="s4-13"><title>Wide-field imaging</title><p>Prior to all electrophysiological and imaging experiments, a reference vasculature image and field-sign map was acquired under a custom-built widefield epi-fluorescence microscope. The microscope consisted of two Nikon 50 mm f1.4 lens placed front to front with a dichroic, excitation, and emission filters (Semrock) in between. Light was delivered via a blue LED light source (Luxeon Star) and images were acquired with a CMOS camera (Basler). Images were acquired at 10 Hz and were triggered on every third frame of a 30 Hz retinotopic mapping stimulus (drifting bar; trial period of 0.1 Hz) to ensure proper timing between stimulus and acquisition. Retinotopic mapping stimulus consisted of a drifting 10° bar of binarized 1/f noise (<xref ref-type="bibr" rid="bib63">Wekselblatt et al., 2016</xref>), which cycled with a period of 0.1 Hz. Elevation and azimuth maps were computed using a Fourier decomposition of the stimulus and plotting preferred phase at the stimulus frequency (<xref ref-type="bibr" rid="bib21">Kalatsky and Stryker, 2003</xref>).</p></sec><sec id="s4-14"><title>Two-photon imaging</title><p>We began imaging sessions ~2 weeks after surgery. We used a resonant, two-photon microscope (Neurolabware, Los Angeles, CA) controlled by Scanbox acquisition software (Scanbox, Los Angeles, CA). Imaging was through a ×16 water immersion lens (Nikon, 0.8 NA) at an acquisition rate of 15.6 Hz at depths ranging from 150 to 250 µM from the surface corresponding to layer 2/3. Mice were allowed to run freely on a spherical treadmill (styrofoam ball floated with air).</p><p>We ran a minimum of 7 stimulus conditions in all sessions (RF mapping +6 conditions) with a short (&lt;3 min) break between each imaging session. Most sessions lasted less than 75 min. For analysis, all of the movies from each session were aligned to a common mean image using a non-rigid registration pipeline (Suite2P). Briefly, all movies were aligned to a common reference frame by estimating a semi-rigid transformation of all frames. The mean activation image was generally used at the reference frame. After alignment, an approach to jointly estimate filters and traces corresponding to a generative model as outlined in the original suite2P paper was applied (<xref ref-type="bibr" rid="bib39">Pachitariu et al., 2017</xref>). These corresponding fluorescence traces are likely heavily contaminated with neuropil from adjoining processes and nearby pixels. Due to the retinotopic organization of the visual cortex, this sort of spatial correlation is likely to pose significant problems in trying to address minute differences in spatial coding of neurons in cortical space and visual space. To help further separate out these signals we thus went one step further and attempted deconvolution of these signals into a more nonlinear estimate of putative ‘spiking’ activity which can help disentangle the neighboring contributions.</p><p>Baseline correction was carried out via subtraction with a sliding window of baseline estimation corresponding to the 8th percentile of the raw fluorescence within a 60-s window of activity. This baseline-corrected trace was then further passed into the deconvolution algorithm in suite2P, resulting in an instantaneous and nonlinear estimate of activity that was used for all subsequent analysis.</p></sec><sec id="s4-15"><title>Matching cells across days</title><p>Cells were tracked across days by first re-targeting to the same plane by eye such that the mean fluorescence image on a given day was matched to that on the previous day, with online visual feedback provided by a custom software plugin for Scanbox. Then, a registration correction was computed from 1 day to the other such that the two planes were aligned. After extracting cell identities and cell filter maps for both days independently, a Jaccard index <italic>J</italic>(<italic>A</italic>,<italic>B</italic>)=(|<italic>A</italic>∩<italic>B</italic>|)/(|<italic>AB</italic>|) was computed for a given cell filter on day 1 with all extracted filters on day 2. A Jaccard index &gt;0.5 was considered the same cell on the next day. Note that the alignment between cells for ‘matched’ versus ‘un-matched’ was determined purely based on the morphology of the extracted filter map, after both days had been aligned to a common reference mean frame. The distributions of correlations between the spatial figure responses presented in <xref ref-type="fig" rid="fig3">Figure 3g</xref> is completely independent validation of the fact that these are the same cells, as nothing about the cell’s activity was considered in aligning cells across days. This result points to the consistency of the spatial responses in the visual cortex as a substrate for inferring figure position.</p></sec><sec id="s4-16"><title>Electrophysiology</title><p>Electrophysiology experiments were carried out with an acutely inserted 64 channel silicon probe from Sotiris Masmanidis (<xref ref-type="bibr" rid="bib66">Yang et al., 2020</xref>) attached to a 4-axis manipulator (Siskiyou), that was amplified through a 128 channel headstage (Intan). Signals were sampled at 30 kHz and filtered through a bandpass filter (300–6000 Hz), then digitized by an open-ephys acquisition box (Open-ephys) and aligned to stimulus frames through the use of a photodiode. We used Kilosort (<xref ref-type="bibr" rid="bib38">Pachitariu et al., 2016</xref>) for spike sorting of the data. The output of the automatic template-matching algorithm from Kilosort was visualized on Phy and then curated manually. Mixed single units and multi-units were included. We used two criteria to select for the cells included in our population analysis: (1) RF GOF &gt;0.1; (2) Total spikes &gt;100 for each stimuli. The probe was lowered into the brain at 5 µM/s and allowed to settle for a minimum of 5 min before experimental stimuli were presented. Animals were head-fixed for a maximum of 2 hr in any given experiment. Stimuli were presented as outlined in the Visual Stimuli section of the Methods.</p></sec><sec id="s4-17"><title>Analysis</title><p>All analyses were performed using custom scripts written in MATLAB (Mathworks) or Python using NumPy, SciPy, Pandas, seaborn, sklearn, and Matplotlib (<xref ref-type="bibr" rid="bib19">Hunter, 2007</xref>; <xref ref-type="bibr" rid="bib30">McKinney, 2010</xref>; <xref ref-type="bibr" rid="bib40">Pedregosa et al., 2011</xref>; <xref ref-type="bibr" rid="bib57">van der Walt et al., 2011</xref>; <xref ref-type="bibr" rid="bib32">Michael et al., 2018</xref>).</p><sec id="s4-17-1"><title>Trial-based response</title><p>For all trial-based analysis, we quantified the response for a given trial as the mean spike count 50–250 ms post trial onset. For imaging experiments, we use the deconvolved calcium trace where the response value was set to the mean across all frames of the trial.</p></sec><sec id="s4-17-2"><title>Positional decoding using linear regression</title><p>To quantify the amount of information present about figure position, we decoded the azimuth bin (positions 1–16) of each trial from a population of neurons using a ridge regression model and 50/50 cross-validation; results averaged across 100 iterations are reported for all data and modeling. To quantify the extent to which a single linear model could account for position across both orientations or textures, we pooled trial types within a stimulus condition (both orientations/textures for Cross/Iso/Nat).</p><p>Beta values (penalty term) for the ridge regression were computed with 50/50 cross-validation approach using the RidgeCV function from sklearn. As the beta values will be dependent on the number of regressors (neurons) in the model, this entire procedure was repeated for varying numbers of neurons to compute the decoding performance as a function of number of neurons in the decoder. All values reported for decoding correspond to the mean variance explained by the model over 100 iterations of the above procedure, and error bars correspond to 95% confidence intervals.</p></sec><sec id="s4-17-3"><title>Computing FGM and BOM</title><p>We limited analysis of FGM and BOM to electrophysiologically recorded neurons that satisfied two criteria: (1) they showed a statistically significant receptive field fit, and (2) the receptive field center was limited to a central portion of the screen (central 15° azimuth and 10° elevation). This second receptive field position criterion was to ensure a reasonable number of figure and ground or left and right trials with which to compute the FGM or BOM indices.</p><p>For each of the three conditions, we defined an FGM index as <inline-formula><mml:math id="inf23"><mml:mi>F</mml:mi><mml:mi>G</mml:mi><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></inline-formula> where <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the mean response across the two patterns for the condition within the figure zone, defined as the 2 × 2 (10° × 10°) grid of locations centered on the cell’s receptive field (<xref ref-type="fig" rid="fig5">Figure 5c</xref>) and <inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the mean response across the two patterns for the condition in the background zone, defined as all grid locations with distance greater than 1.5 * the receptive field width from the receptive field center. We computed bootstrapped p values using 500 shuffles where trial identity was randomized to establish a null distribution.</p><p>We quantified border-ownership selectivity in a similar way. We defined a BOM index as <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean response across the two patterns for the condition within the left border zone, and <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean response across the two patterns for the condition within the right border zone (<xref ref-type="fig" rid="fig5">Figure 5f</xref>). We computed the significance of the modulation using a bootstrapped distribution as above.</p></sec></sec><sec id="s4-18"><title>Modeling</title><sec id="s4-18-1"><title>Feedforward model</title><p>We modeled neuronal responses using an LN model of simple cells, with the linear filter modeled by a Gabor function and linear rectification. We simulated responses to 100 different cell types using a classic simple cell model as a linear combination of receptive field with stimulus passed through a nonlinearity.<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf29"><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is a Gabor function:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>′</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>′</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mi>λ</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:mtd><mml:mtd><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>θ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>θ</mml:mi></mml:mtd><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with parameters sampled as follows: <italic>θ</italic> = rand(0,<inline-formula><mml:math id="inf30"><mml:mi>π</mml:mi></mml:math></inline-formula>), <italic>σ</italic> = rand(2°, 7°), <inline-formula><mml:math id="inf31"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> = rand(0.05 cpd, 0.3cpd), <inline-formula><mml:math id="inf32"><mml:mi>ω</mml:mi></mml:math></inline-formula> = rand(0, <inline-formula><mml:math id="inf33"><mml:mi>π</mml:mi></mml:math></inline-formula>), <inline-formula><mml:math id="inf34"><mml:mi>γ</mml:mi></mml:math></inline-formula> = 1. <inline-formula><mml:math id="inf35"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> represents a linear rectification (max(0,x)) to ensure positive rates and * represents the convolution operator.</p></sec><sec id="s4-18-2"><title>Surround model</title><p>We added a divisive term to the neural response computed from our feedforward LN model (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>). This divisive term was not recurrent and instead can be most readily interpreted as a center-surround interaction that would arise in the feedforward inputs from thalamus to cortex.<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>∗</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the Pearson correlation between the mean response of all neuron types within the receptive field (&lt;2σ; <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and all neuron types outside the classical receptive field (&gt;2<italic>σ</italic> and &lt;5<italic>σ</italic>; <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). The term <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the extent to which orientations inside the cell’s receptive field match those outside of the receptive field for a given image. In the case of the Iso stimulus, <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, yielding suppression, while in the case of Cross stimulus, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> yielding facilitation. We set the scaling factor <inline-formula><mml:math id="inf42"><mml:mi>β</mml:mi></mml:math></inline-formula> to 0.95 to ensure that the denominator never reaches zero.</p></sec><sec id="s4-18-3"><title>Neural network model</title><p>We analyzed decoding performance of intermediate layers of a deep network (VGG-16) trained for classification on ImageNet (<xref ref-type="bibr" rid="bib53">Simonyan and Zisserman, 2014</xref>). We used the mean of the response magnitude to all 14 individual frames corresponding to a given stimulus to represent a given unit’s response to a given trial type. We extracted this response for each of the five max-pool layers in the network and computed decoding curves using the same procedures as for the neural data.</p></sec><sec id="s4-18-4"><title>Noise factor</title><p>To account for neural response variability, we added noise to all models proportional to the mean of responses for the simulated network within each condition. To achieve this, we added zero-mean, normally distributed fluctuations to each trial with variance proportional to the mean response across neurons followed by linear rectification to ensure positive outputs. Thus, the final output of a population would be equal to <inline-formula><mml:math id="inf43"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> . Sweeps across various noise factors are shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplements 1</xref> and <xref ref-type="fig" rid="fig7s2">2</xref>.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing, collected all mouse data together with L.L.He also collected treeshrew data together with JBW and FL</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Investigation, Methodology, Writing – review and editing, collected mouse lemur data under supervision of DH</p></fn><fn fn-type="con" id="con4"><p>Formal analysis, Investigation, Methodology, Writing – review and editing, collected macaque data</p></fn><fn fn-type="con" id="con5"><p>Formal analysis, Investigation, Methodology, Writing – review and editing, collected treeshrew data together with FL and FJL</p></fn><fn fn-type="con" id="con6"><p>Formal analysis, Investigation, Methodology, Writing – review and editing, collected treeshrew data together with JBW and FJL</p></fn><fn fn-type="con" id="con7"><p>Resources, Supervision, Funding acquisition, Project administration, Writing – review and editing, supervised collection of mouse lemur data</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Supervision, Funding acquisition, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing, collected all mouse data together with FJL</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The following animals were used in this study: adult mice 2–12 months old, both male and female; adult treeshrews 7–18 months old, both male and female; adult mouse lemurs 2–3.5 years, both male and female; and adult macaques 3 and 7 years old, male. All procedures on mice, macaques, and treeshrews were conducted in accordance with the ethical guidelines of the National Institutes of Health and were approved by the Institutional Animal Care and Use Committee at the California Institute of Technology. Mouse lemur experiments were in accordance with European animal welfare regulations and were reviewed by the local ethics committee (Comite d'éthique en expérimentation animale No. 68) in Brunoy, France, by the ethics committee of the University of Geneva, Switzerland and authorized by the French 'Ministére de l'education nationale de l'enseignement supérieur et de la recherche'.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-74394-transrepform1-v4.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Source data have been provided to replicate all neural and behavioral figures (2,3,4,5,6,7). These data have been uploaded to dryad: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.ngf1vhhvp">https://doi.org/10.5061/dryad.ngf1vhhvp</ext-link>. Sufficient modeling details have been provided in methods section to replicate relevant parts of figure 8.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Data from: Mice and primates use distinct strategies for visual segmentation</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.ngf1vhhvp</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIH (DP1-NS083063) and the Howard Hughes Medical Institute. We thank Audo Flores and Daniel Wagenaar for technical support, Sotiris Masmanidis for supplying the silicon recording probes, and David Fitzpatrick and Yong-Gang Yau for invaluable help setting up a tree shrew colony. FJL was supported by an Arnold O Beckman postdoctoral fellowship and a Burroughs Wellcome PDEP Award. DH and CLAH were supported by the Swiss National Science Foundation (310030E_190060) and the Human Frontiers Science Program (RGP0024/2016).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An international laboratory for systems and computational neuroscience</article-title><source>Neuron</source><volume>96</volume><fpage>1213</fpage><lpage>1218</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.013</pub-id><pub-id pub-id-type="pmid">29268092</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Deneux</surname><given-names>T</given-names></name><name><surname>Chenkov</surname><given-names>N</given-names></name><name><surname>McColgan</surname><given-names>T</given-names></name><name><surname>Speiser</surname><given-names>A</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Turaga</surname><given-names>SC</given-names></name><name><surname>Mineault</surname><given-names>P</given-names></name><name><surname>Rupprecht</surname><given-names>P</given-names></name><name><surname>Gerhard</surname><given-names>S</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Bolte</surname><given-names>B</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Ringach</surname><given-names>D</given-names></name><name><surname>Stone</surname><given-names>J</given-names></name><name><surname>Rogerson</surname><given-names>LE</given-names></name><name><surname>Sofroniew</surname><given-names>NJ</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Román Rosón</surname><given-names>M</given-names></name><name><surname>Theis</surname><given-names>L</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Community-Based benchmarking improves spike rate inference from two-photon calcium imaging data</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006157</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006157</pub-id><pub-id pub-id-type="pmid">29782491</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Approximating CNNs with Bag-of-Local-Features Models Works Surprisingly Well on ImageNet</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1904.00760">http://arxiv.org/abs/1904.00760</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1904.00760</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Venkatesh</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Bear</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unsupervised Segmentation in Real-World Images via Spelke Object Inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2205.08515">http://arxiv.org/abs/2205.08515</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.2205.08515</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Keyser</surname><given-names>R</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cue-invariant shape recognition in rats as tested with second-order contours</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.15.14</pub-id><pub-id pub-id-type="pmid">26605843</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>DA</given-names></name><name><surname>Stempel</surname><given-names>AV</given-names></name><name><surname>Vale</surname><given-names>R</given-names></name><name><surname>Ruehle</surname><given-names>S</given-names></name><name><surname>Lefler</surname><given-names>Y</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A synaptic threshold mechanism for computing escape decisions</article-title><source>Nature</source><volume>558</volume><fpage>590</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0244-6</pub-id><pub-id pub-id-type="pmid">29925954</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>A</given-names></name><name><surname>Mahringer</surname><given-names>D</given-names></name><name><surname>Oyibo</surname><given-names>HK</given-names></name><name><surname>Petersen</surname><given-names>AV</given-names></name><name><surname>Leinweber</surname><given-names>M</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1658</fpage><lpage>1664</lpage><pub-id pub-id-type="doi">10.1038/nn.4385</pub-id><pub-id pub-id-type="pmid">27618309</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Ziemba</surname><given-names>CM</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A functional and perceptual signature of the second visual area in primates</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>974</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1038/nn.3402</pub-id><pub-id pub-id-type="pmid">23685719</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frost</surname><given-names>BJ</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Single visual neurons code opposing motion independent of direction</article-title><source>Science</source><volume>220</volume><fpage>744</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1126/science.6836313</pub-id><pub-id pub-id-type="pmid">6836313</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>R</given-names></name><name><surname>Rubisch</surname><given-names>P</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>ImageNet-Trained CNNs Are Biased towards Texture; Increasing Shape Bias Improves Accuracy and Robustness</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1811.12231">http://arxiv.org/abs/1811.12231</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1811.12231</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1979">1979</year><source>The Ecological Approach to Visual Perception</source><publisher-name>Houghton Mifflin</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>IJ</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Explaining and Harnessing Adversarial Examples</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6572">http://arxiv.org/abs/1412.6572</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1412.6572</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>JR</given-names></name><name><surname>Cuthill</surname><given-names>IC</given-names></name><name><surname>Baddeley</surname><given-names>R</given-names></name><name><surname>Shohet</surname><given-names>AJ</given-names></name><name><surname>Scott-Samuel</surname><given-names>NE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Camouflage, detection and identification of moving targets</article-title><source>Proceedings. Biological Sciences</source><volume>280</volume><elocation-id>20130064</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2013.0064</pub-id><pub-id pub-id-type="pmid">23486439</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>ZJ</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Surfaces versus features in visual search</article-title><source>Nature</source><volume>359</volume><fpage>231</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1038/359231a0</pub-id><pub-id pub-id-type="pmid">1528263</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>CLA</given-names></name><name><surname>Zimmermann</surname><given-names>R</given-names></name><name><surname>Flórez Weidinger</surname><given-names>JD</given-names></name><name><surname>Prsa</surname><given-names>M</given-names></name><name><surname>Schottdorf</surname><given-names>M</given-names></name><name><surname>Merlin</surname><given-names>S</given-names></name><name><surname>Okamoto</surname><given-names>T</given-names></name><name><surname>Ikezoe</surname><given-names>K</given-names></name><name><surname>Pifferi</surname><given-names>F</given-names></name><name><surname>Aujard</surname><given-names>F</given-names></name><name><surname>Angelucci</surname><given-names>A</given-names></name><name><surname>Wolf</surname><given-names>F</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Orientation preference maps in Microcebus murinus reveal size-invariant design principles in primate visual cortex</article-title><source>Current Biology</source><volume>31</volume><fpage>733</fpage><lpage>741</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.11.027</pub-id><pub-id pub-id-type="pmid">33275889</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>JL</given-names></name><name><surname>Yavorska</surname><given-names>I</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vision drives accurate approach behavior during prey capture in laboratory mice</article-title><source>Current Biology</source><volume>26</volume><fpage>3046</fpage><lpage>3052</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.09.009</pub-id><pub-id pub-id-type="pmid">27773567</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: a 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JN</given-names></name><name><surname>Born</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Stimulus-Dependent modulation of suppressive influences in MT</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>678</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4560-10.2011</pub-id><pub-id pub-id-type="pmid">21228177</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalatsky</surname><given-names>VA</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>New paradigm for optical imaging: temporally encoded maps of intrinsic signal</article-title><source>Neuron</source><volume>38</volume><fpage>529</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00286-1</pub-id><pub-id pub-id-type="pmid">12765606</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>AJ</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Roth</surname><given-names>MM</given-names></name><name><surname>Caudill</surname><given-names>MS</given-names></name><name><surname>Ingrosso</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>A disinhibitory circuit for contextual modulation in primary visual cortex</article-title><source>Neuron</source><volume>108</volume><fpage>1181</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.11.013</pub-id><pub-id pub-id-type="pmid">33301712</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>AJ</given-names></name><name><surname>Roth</surname><given-names>MM</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Feedback generates a second receptive field in neurons of the visual cortex</article-title><source>Nature</source><volume>582</volume><fpage>545</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2319-4</pub-id><pub-id pub-id-type="pmid">32499655</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Soto</surname><given-names>F</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>An excitatory amacrine cell detects object motion and provides feature-selective input to ganglion cells in the mouse retina</article-title><source>eLife</source><volume>4</volume><elocation-id>e08025</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08025</pub-id><pub-id pub-id-type="pmid">25988808</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kirchberger</surname><given-names>L</given-names></name><name><surname>Mukherjee</surname><given-names>S</given-names></name><name><surname>Schnabel</surname><given-names>UH</given-names></name><name><surname>Barsegyan</surname><given-names>A</given-names></name><name><surname>Levelt</surname><given-names>CN</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Essential Role of Feedback Processing for Figure-Ground Perception in Mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/456459</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The neurophysiology of figure-ground segregation in primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>1605</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-02-01605.1995</pub-id><pub-id pub-id-type="pmid">7869121</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Leakey</surname><given-names>R.</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>The Origin of Humankind</source><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leinweber</surname><given-names>M</given-names></name><name><surname>Ward</surname><given-names>DR</given-names></name><name><surname>Sobczak</surname><given-names>JM</given-names></name><name><surname>Attinger</surname><given-names>A</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A sensorimotor circuit in mouse cortex for visual flow predictions</article-title><source>Neuron</source><volume>96</volume><elocation-id>1204</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.009</pub-id><pub-id pub-id-type="pmid">29216453</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization of seven mouse visual cortical areas</article-title><source>Neuron</source><volume>72</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id><pub-id pub-id-type="pmid">22196338</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Data Structures for Statistical Computing in Python</article-title><conf-name>Python in Science Conference</conf-name><conf-loc>Austin, Texas</conf-loc><pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merigan</surname><given-names>WH</given-names></name><name><surname>Katz</surname><given-names>LM</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The effects of parvocellular lateral geniculate lesions on the acuity and contrast sensitivity of macaque monkeys</article-title><source>The Journal of Neuroscience</source><volume>11</volume><fpage>994</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.11-04-00994.1991</pub-id><pub-id pub-id-type="pmid">2010820</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Michael</surname><given-names>W</given-names></name><name><surname>Botvinnik</surname><given-names>O</given-names></name><name><surname>Paul</surname><given-names>H</given-names></name><name><surname>Joel</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Mwaskom/seaborn</data-title><version designator="V.9.0">V.9.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1313201">https://doi.org/10.5281/zenodo.1313201</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mustafar</surname><given-names>F</given-names></name><name><surname>Harvey</surname><given-names>MA</given-names></name><name><surname>Khani</surname><given-names>A</given-names></name><name><surname>Arató</surname><given-names>J</given-names></name><name><surname>Rainer</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Divergent solutions to visual problem solving across mammalian species</article-title><source>ENeuro</source><volume>5</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0167-18.2018</pub-id><pub-id pub-id-type="pmid">30073190</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Da Vinci stereopsis: depth and subjective occluding contours from unpaired image points</article-title><source>Vision Research</source><volume>30</volume><fpage>1811</fpage><lpage>1825</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90161-d</pub-id><pub-id pub-id-type="pmid">2288092</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>He</surname><given-names>ZJ</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>Visual surface representation: A critical link between lower-level and higher-level vision</chapter-title><person-group person-group-type="editor"><name><surname>Kosslyn</surname><given-names>SM</given-names></name><name><surname>Osherson</surname><given-names>DN</given-names></name></person-group><source>An Invitation to Cognitive Science</source><publisher-name>MIT Press</publisher-name><fpage>1</fpage><lpage>70</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Highly selective receptive fields in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>7520</fpage><lpage>7536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0623-08.2008</pub-id><pub-id pub-id-type="pmid">18650330</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olveczky</surname><given-names>BP</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Segregation of object and background motion in the retina</article-title><source>Nature</source><volume>423</volume><fpage>401</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1038/nature01652</pub-id><pub-id pub-id-type="pmid">12754524</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Kadir</surname><given-names>SN</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast and Accurate Spike Sorting of High-Channel Count Probes with KiloSort</article-title><conf-name>Advances in Neural Information Processing Systems 29</conf-name></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Sylvia</surname><given-names>S</given-names></name><name><surname>Dalgleish</surname><given-names>H</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Suite2p: Beyond 10,000 Neurons with Standard Two-Photon Microscopy</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061507</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research: JMLR</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petry</surname><given-names>HM</given-names></name><name><surname>Fox</surname><given-names>R</given-names></name><name><surname>Casagrande</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Spatial contrast sensitivity of the tree shrew</article-title><source>Vision Research</source><volume>24</volume><fpage>1037</fpage><lpage>1042</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(84)90080-4</pub-id><pub-id pub-id-type="pmid">6506467</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>L</given-names></name><name><surname>Goard</surname><given-names>MJ</given-names></name><name><surname>Estandian</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>M</given-names></name><name><surname>Kwan</surname><given-names>AC</given-names></name><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Harrison</surname><given-names>TC</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Fast modulation of visual perception by basal forebrain cholinergic neurons</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1857</fpage><lpage>1863</lpage><pub-id pub-id-type="doi">10.1038/nn.3552</pub-id><pub-id pub-id-type="pmid">24162654</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>L.</given-names></name><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Thiberge</surname><given-names>SY</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task-Dependent changes in the large-scale dynamics and necessity of cortical regions</article-title><source>Neuron</source><volume>104</volume><fpage>810</fpage><lpage>824</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.08.025</pub-id><pub-id pub-id-type="pmid">31564591</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitts</surname><given-names>W</given-names></name><name><surname>McCulloch</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="1947">1947</year><article-title>How we know universals; the perception of auditory and visual forms</article-title><source>The Bulletin of Mathematical Biophysics</source><volume>9</volume><fpage>127</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1007/BF02478291</pub-id><pub-id pub-id-type="pmid">20262674</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Raudies</surname><given-names>F</given-names></name><name><surname>Wannig</surname><given-names>A</given-names></name><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Neumann</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of attention in figure-ground segregation in areas V1 and V4 of the visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.032</pub-id><pub-id pub-id-type="pmid">22794268</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prusky</surname><given-names>GT</given-names></name><name><surname>West</surname><given-names>PW</given-names></name><name><surname>Douglas</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Behavioral assessment of visual acuity in mice and rats</article-title><source>Vision Research</source><volume>40</volume><fpage>2201</fpage><lpage>2209</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(00)00081-x</pub-id><pub-id pub-id-type="pmid">10878281</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>FT</given-names></name><name><surname>von der Heydt</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Figure and ground in the visual cortex: V2 combines stereoscopic cues with gestalt rules</article-title><source>Neuron</source><volume>47</volume><fpage>155</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.028</pub-id><pub-id pub-id-type="pmid">15996555</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roe</surname><given-names>AW</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name><name><surname>Conway</surname><given-names>BR</given-names></name><name><surname>Fujita</surname><given-names>I</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Toward a unified theory of visual area V4</article-title><source>Neuron</source><volume>74</volume><fpage>12</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.011</pub-id><pub-id pub-id-type="pmid">22500626</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnabel</surname><given-names>UH</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>Lorteije</surname><given-names>JAM</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Figure-ground perception in the awake mouse and neuronal activity elicited by figure-ground stimuli in primary visual cortex</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>17800</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-36087-8</pub-id><pub-id pub-id-type="pmid">30542060</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schnabel</surname><given-names>UH</given-names></name><name><surname>Kirchberger</surname><given-names>L</given-names></name><name><surname>Mukherjee</surname><given-names>S</given-names></name><name><surname>Barsegyan</surname><given-names>A</given-names></name><name><surname>Lorteije</surname><given-names>JAM</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Feedforward and Feedback Processing during Figure-Ground Perception in Mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/456459v1</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnell</surname><given-names>AE</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Gijbels</surname><given-names>K</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>de Beeck</surname><given-names>HO</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Face categorization and behavioral templates in rats</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/19.14.9</pub-id><pub-id pub-id-type="pmid">31826254</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Lorteije</surname><given-names>JAM</given-names></name><name><surname>Vangeneugden</surname><given-names>J</given-names></name><name><surname>van Beest</surname><given-names>EH</given-names></name><name><surname>Grigore</surname><given-names>ME</given-names></name><name><surname>Levelt</surname><given-names>CN</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orientation-tuned surround suppression in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>9290</fpage><lpage>9304</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5051-13.2014</pub-id><pub-id pub-id-type="pmid">25009262</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stirman</surname><given-names>JN</given-names></name><name><surname>Townsend</surname><given-names>LB</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A touchscreen based global motion perception task for mice</article-title><source>Vision Research</source><volume>127</volume><fpage>74</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2016.07.006</pub-id><pub-id pub-id-type="pmid">27497283</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Zaremba</surname><given-names>W</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Bruna</surname><given-names>J</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Intriguing Properties of Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1312.6199">http://arxiv.org/abs/1312.6199</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1312.6199</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>T</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A topological solution to object segmentation and tracking</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2204248119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2204248119</pub-id><pub-id pub-id-type="pmid">36201537</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>Colbert</surname><given-names>SC</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The numpy array: a structure for efficient numerical computation</article-title><source>Computing in Science &amp; Engineering</source><volume>13</volume><fpage>22</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2011.37</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Roy</surname><given-names>A</given-names></name><name><surname>Rhodes</surname><given-names>HJ</given-names></name><name><surname>Culp</surname><given-names>JH</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Transformation of receptive field properties from lateral geniculate nucleus to superficial V1 in the tree shrew</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>11494</fpage><lpage>11505</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1464-13.2013</pub-id><pub-id pub-id-type="pmid">23843520</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A multivariate approach reveals the behavioral templates underlying visual discrimination in rats</article-title><source>Current Biology</source><volume>22</volume><fpage>50</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.11.041</pub-id><pub-id pub-id-type="pmid">22209530</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep Neural Networks Point to Mid-Level Complexity of Rodent Object Vision</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.02.08.940189</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Using deep neural networks to evaluate object vision tasks in rats</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008714</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008714</pub-id><pub-id pub-id-type="pmid">33651793</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Gao</surname><given-names>E</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Gateways of ventral and dorsal streams in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>1905</fpage><lpage>1918</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3488-10.2011</pub-id><pub-id pub-id-type="pmid">21289200</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wekselblatt</surname><given-names>JB</given-names></name><name><surname>Flister</surname><given-names>ED</given-names></name><name><surname>Piscopo</surname><given-names>DM</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Large-Scale imaging of cortical dynamics during sensory perception and behavior</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2852</fpage><lpage>2866</lpage><pub-id pub-id-type="doi">10.1152/jn.01056.2015</pub-id><pub-id pub-id-type="pmid">26912600</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williford</surname><given-names>JR</given-names></name><name><surname>von der Heydt</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Border-ownership coding</article-title><source>Scholarpedia Journal</source><volume>8</volume><elocation-id>30040</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.30040</pub-id><pub-id pub-id-type="pmid">25075249</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>P</given-names></name><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Architectonic subdivisions of neocortex in the tree shrew (tupaia belangeri)</article-title><source>Anatomical Record</source><volume>292</volume><fpage>994</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1002/ar.20916</pub-id><pub-id pub-id-type="pmid">19462403</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>L</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Villagracia</surname><given-names>J</given-names></name><name><surname>Masmanidis</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Open source silicon microprobes for high throughput neural recording</article-title><source>Journal of Neural Engineering</source><volume>17</volume><elocation-id>016036</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/ab581a</pub-id><pub-id pub-id-type="pmid">31731284</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Hira</surname><given-names>R</given-names></name><name><surname>Stirman</surname><given-names>JN</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Smith</surname><given-names>IT</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mice use robust and common strategies to discriminate natural scenes</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>1379</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-19108-w</pub-id><pub-id pub-id-type="pmid">29358739</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Friedman</surname><given-names>HS</given-names></name><name><surname>von der Heydt</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Coding of border ownership in monkey visual cortex</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>6594</fpage><lpage>6611</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-17-06594.2000</pub-id><pub-id pub-id-type="pmid">10964965</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zipser</surname><given-names>K</given-names></name><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Schiller</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Contextual modulation in primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>7376</fpage><lpage>7389</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-22-07376.1996</pub-id><pub-id pub-id-type="pmid">8929444</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Oertelt</surname><given-names>N</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A rodent model for the study of invariant visual object recognition</article-title><source>PNAS</source><volume>106</volume><fpage>8748</fpage><lpage>8753</lpage><pub-id pub-id-type="doi">10.1073/pnas.0811583106</pub-id><pub-id pub-id-type="pmid">19429704</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Invariant visual object recognition and shape processing in rats</article-title><source>Behavioural Brain Research</source><volume>285</volume><fpage>10</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2014.12.053</pub-id><pub-id pub-id-type="pmid">25561421</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74394.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Vinck</surname><given-names>Martin</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vmz1g97</institution-id><institution>Ernst Strüngmann Institute (ESI) for Neuroscience in Cooperation with Max Planck Society</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.07.04.451059" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.04.451059"/></front-stub><body><p>There is abundant evidence for differences in the organization of the visual system between primates and rodents. How do these differences yield distinct behaviors in these species? The authors show a major difference in the ability of mice and primates in detecting figures from ground based on motion and texture patterns, revealing a fundamental limitations of mice in segmenting visual scenes.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74394.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Vinck</surname><given-names>Martin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vmz1g97</institution-id><institution>Ernst Strüngmann Institute (ESI) for Neuroscience in Cooperation with Max Planck Society</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Self</surname><given-names>Matthew W</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05csn2x06</institution-id><institution>Netherlands Institute for Neuroscience</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.04.451059">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.07.04.451059v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Mice and primates use distinct strategies for visual segmentation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Matthew W. Self (Reviewer #1).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission. Overall, the reviewers were positive and we can recommend publication, provided you can fulfill a few requests for additional analyses.</p><p>Essential revisions:</p><p>1) The data would be more convincing if the authors could show that indeed activity changes of the neurons are really due to different stimuli and not due to activity changes with time or brain state. We recommend that at the very least the cell should be visually responsive throughout the 6 blocks that go into the analyses for figures 4, 5 and 6.</p><p>2) Figure 1c seems not very relevant for this paper and is hard to interpret without additional explanation.</p><p>3) In general, the reporting in the text and figures, as well as the methods is incomplete. Neither for electrophysiology, nor imaging, the number of cells going into the analyses is reported, nor are details on spike sorting (are all units single units? What are the criteria for cell inclusion?) or laminar location of their neurons for electrophysiology.</p><p>4) Similarly, reporting on the RFs (sizes, percentages of cells with significant fits, differences among areas…), as well as details on the mapping for example for the sparse noise (size of the pixels, sampled area, timing, analysis window…) are completely missing. The schematized RFs in Figure 3 c and d seem misleadingly small. I am assuming the green ellipse in 4a is an actually measured RF? This should be stated in the legend.</p><p>5) It is only mentioned for imaging that there were 7 blocks of data collection (6 conditions plus RF mapping). Were these always presented in the same order or randomized? Was it the same for imaging and ephys?</p><p>6) We recommend that Figure 3f reports the correlation values reported so the reader can judge how representative the example neurons are. The spatial masks are not described in the text, legend or methods. Are they the mapped receptive fields? Why are they on a square and not on the 16x8 grid?</p><p>7) Line 324 states that within conditions the distributions were not different from 0, but the statistics describe areas, not conditions. At least in LM it looks like the cross population could well be off 0. The difference from 0 should be tested for all 9 distributions and reported. Figures 5 d,e and g,h could be performed separately for each area, as is done in the supplemental figure for the PSTHs. But at least the fraction of cells from each area going into the batched analysis should be stated.</p><p>8) Schnabel et al. 2020 show even more robust modulation by a figure edge than figure center in mouse V1. It could be tested, if this result holds in this dataset as well.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74394.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions (for the authors):</p><p>1) The data would be more convincing if the authors could show that indeed activity changes of the neurons are really due to different stimuli and not due to activity changes with time or brain state. We recommend that at the very least the cell should be visually responsive throughout the 6 blocks that go into the analyses for figures 4, 5 and 6.</p></disp-quote><p>The three different types of stimuli (Cross/Iso/Nat) were presented <italic>randomly interleaved</italic>, and within each block the two stimulus conditions were further randomly interleaved. In a typical experiment, we would run 10 trials/block, and ~6 blocks/experiment (thus each of the three types of stimuli, Cross/Iso/Nat, were repeated twice). Therefore, it seems unlikely that differences observed between conditions arose due to brain state changes or other changes over time.</p><p>Furthermore, note that we only chose visually-responsive cells which had clear receptive fields for the analysis in Figure 5. Specifically, we state in the Methods:</p><p>“[W]e used 2D Gaussian to fit the RFs. The cells with RF’s goodness of fit (GOF) above 0.1 were included for subsequent analyses of electrophysiological data (Figures 4, 5, 6, Figure 5—figure supplement 1).”</p><p>In one experiment, we mapped the RFs before and after the Cross/Iso/Nat stimuli. As shown in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, the RFs of all the cells recorded in this session (16 cells) had similar structure before and after the Cross/Iso/Nat experiment. Again, this shows that the activity changes of neurons were unlikely to have been due to brain state changes affecting visual responsiveness.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Receptive field maps of 16 cells recorded before and after mapping responses to the Cross/Iso/Nat experiment of Figures 4-6 in one session.</title><p>Red = ON responses, Green = OFF responses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-sa2-fig1-v4.tif"/></fig><disp-quote content-type="editor-comment"><p>2) Figure 1c seems not very relevant for this paper and is hard to interpret without additional explanation.</p></disp-quote><p>We have now deleted this figure.</p><disp-quote content-type="editor-comment"><p>3) In general, the reporting in the text and figures, as well as the methods is incomplete. Neither for electrophysiology, nor imaging, the number of cells going into the analyses is reported, nor are details on spike sorting (are all units single units? What are the criteria for cell inclusion?) or laminar location of their neurons for electrophysiology.</p></disp-quote><p>Thank you for pointing this out. We have added the following details related to electrophysiology to the Methods:</p><p>“We used Kilosort (Pachitariu et al., 2016) for spike sorting of the data. The output of the automatic template-matching algorithm from Kilosort was visualized on Phy and then curated manually. Mixed single units and multi-units were included. We have two criteria for the cells included in our population analysis: (1) RF goodness of fit (GOF) &gt; 0.1; (2) Total spikes &gt; 100 for each of the six stimulus blocks.”</p><p>We did not track the laminar location of neurons for electrophysiology.</p><p>We have added the # cells used for each analysis (both electrophysiology and imaging) into the appropriate figure legends.</p><disp-quote content-type="editor-comment"><p>4) Similarly, reporting on the RFs (sizes, percentages of cells with significant fits, differences among areas…), as well as details on the mapping for example for the sparse noise (size of the pixels, sampled area, timing, analysis window…) are completely missing. The schematized RFs in Figure 3 c and d seem misleadingly small. I am assuming the green ellipse in 4a is an actually measured RF? This should be stated in the legend.</p></disp-quote><p>We computed the RF sizes based on Gaussian fit. The size for each RF was computed as the mean of the standard deviation along the x and y directions in the Gaussian fit. Only cells with GOF &gt; 0.1 were included in our population analyses (Figures 4, 5, 6, Figure 5—figure supplement 1). The percentages of cells with significant fits among the three areas was: V1: 67%; LM: 66%; RL: 51%.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Distributions of RF sizes recorded in all areas (top), and in V1, RL, and LM individually (bottom).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-sa2-fig2-v4.tif"/></fig><p>We have now added the following details on RF mapping with electrophysiology to the Methods:</p><p>“For electrophysiology, we used a sparse noise stimulus to map the spatial RFs in all three areas. We divided the screen into a 18x32 grid of squares, each 3.5° on a side (each grid square was 2.2 x 2.2 cm, 18 cm from the mouse’s left eye). In each stimulus frame, one grid square, black or white, was presented alternately. The stimulus contained 8000 frames in total and was presented at 10 Hz. RFs were computed by spike triggered average (STA). The analysis window for STA was 30-100 ms. Then we used 2D Gaussian to fit the RFs. The cells with RF goodness of fit (GOF) above 0.1 were include in subsequent analyses. We limited our analysis for figure-ground and border-ownership modulation in Figure 5 to neurons that had significant receptive field fits and used the central position of that fit as the neuron’s inferred RF center. For population neural decoding analyses, we used all neurons regardless of receptive field fits.”</p><p>In addition, we include the following text in the Methods with details on RF mapping using two photon imaging:</p><p>“<italic>RF mapping.</italic> Receptive fields were mapped for neurons under 2-photon using an isolated drifting Gabor patch stimulus: a patch of ~6° containing a drifting Gabor appeared in one of three different orientations (45°, 180°, 315°) and 2 directions at a random position. We repeated this procedure for 9 repeats and at 16 by 9 positions and then collapsed responses across all trial types to compute a spatial PSTH. We then fit a 2D Gaussian to the response and classified neurons as having a significant receptive field fit if the goodness of fit exceeded the goodness of fit for at least 99 out of 100 bootstrapped trials where spatial location was shuffled across trials.”</p><p>The green ellipse in Figure 3 is purely schematic, and it not taken from real data. In Figure 4, the green ellipses are fits from real data.</p><disp-quote content-type="editor-comment"><p>5) It is only mentioned for imaging that there were 7 blocks of data collection (6 conditions plus RF mapping). Were these always presented in the same order or randomized? Was it the same for imaging and ephys?</p></disp-quote><p>We now clarify in the methods that for both electrophysiology and two-photon imaging experiments, the 6 conditions were randomly interleaved. RF mapping was performed at the beginning of the session, and sometimes also at the end (e.g., see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>):</p><p>“For both electrophysiology and two-photon imaging experiments, the 6 conditions (Cross, Iso, Nat x 2 conditions) were pseudo-randomly interleaved, i.e., Cross/Iso/Nat were presented in a random order, and the two within-condition repetitions (e.g., two orientations or textures) were presented consecutively but also randomized.”</p><disp-quote content-type="editor-comment"><p>6) We recommend that Figure 3f reports the correlation values reported so the reader can judge how representative the example neurons are. The spatial masks are not described in the text, legend or methods. Are they the mapped receptive fields? Why are they on a square and not on the 16x8 grid?</p></disp-quote><p>We have now added the correlation values for the examples in Figure 3f to the figure legend.</p><p>The following clarification regarding the spatial mask has been added to the figure legend for Figure 3f:</p><p>“Spatial masks are from suite2P spatial filters and are meant to illustrate qualitatively similar morphology in matched neurons across days.”</p><disp-quote content-type="editor-comment"><p>7) Line 324 states that within conditions the distributions were not different from 0, but the statistics describe areas, not conditions. At least in LM it looks like the cross population could well be off 0. The difference from 0 should be tested for all 9 distributions and reported. Figures 5 d,e and g,h could be performed separately for each area, as is done in the supplemental figure for the PSTHs. But at least the fraction of cells from each area going into the batched analysis should be stated.</p></disp-quote><p>We have performed the t-test for the 9 distributions from 0, the mean and P values have now added this to the figure legend for Figure 5b.</p><p>The percentages of cells with significant fits and included in the analysis among the three areas was: V1: 67%; LM: 66%; RL: 51%.</p><disp-quote content-type="editor-comment"><p>8) Schnabel et al. 2020 show even more robust modulation by a figure edge than figure center in mouse V1. It could be tested, if this result holds in this dataset as well.</p></disp-quote><p>Consistent with Schnabel et al. 2020, we found that most cells in V1 respond more strongly to edges than figure for both cross and iso conditions.</p><fig id="sa2fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-74394-sa2-fig3-v4.tif"/></fig></body></sub-article></article>