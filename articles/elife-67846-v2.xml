<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">67846</article-id><article-id pub-id-type="doi">10.7554/eLife.67846</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Open-source, Python-based, hardware and software for controlling behavioural neuroscience experiments</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-83866"><name><surname>Akam</surname><given-names>Thomas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1810-0494</contrib-id><email>thomas.akam@psy.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-229394"><name><surname>Lustig</surname><given-names>Andy</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1358-8363</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-229395"><name><surname>Rowland</surname><given-names>James M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9140-8260</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-229396"><name><surname>Kapanaiah</surname><given-names>Sampath KT</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" id="author-229397"><name><surname>Esteve-Agraz</surname><given-names>Joan</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-229398"><name><surname>Panniello</surname><given-names>Mariangela</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-104989"><name><surname>Márquez</surname><given-names>Cristina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1948-2727</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" id="author-158691"><name><surname>Kohl</surname><given-names>Michael M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2566-5426</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-46508"><name><surname>Kätzel</surname><given-names>Dennis</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-11243"><name><surname>Costa</surname><given-names>Rui M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1707-1051</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund12"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-13785"><name><surname>Walton</surname><given-names>Mark E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0117-2894</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf3"/></contrib><aff id="aff1"><label>1</label><institution>Department of Experimental Psychology, University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Champalimaud Neuroscience Program, Champalimaud Centre for the Unknown</institution><addr-line><named-content content-type="city">Lisbon</named-content></addr-line><country>Portugal</country></aff><aff id="aff3"><label>3</label><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Department of Physiology Anatomy &amp; Genetics, University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution>Institute of Applied Physiology, Ulm University</institution><addr-line><named-content content-type="city">Ulm</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution>Instituto de Neurociencias (Universidad Miguel Hernández-Consejo Superior de Investigaciones Científicas)</institution><addr-line><named-content content-type="city">Sant Joan d’Alacant</named-content></addr-line><country>Spain</country></aff><aff id="aff7"><label>7</label><institution>Institute of Neuroscience and Psychology, University of Glasgow</institution><addr-line><named-content content-type="city">Glasgow</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff8"><label>8</label><institution>Department of Neuroscience and Neurology, Zuckerman Mind Brain Behavior Institute, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff9"><label>9</label><institution>Wellcome Centre for Integrative Neuroimaging, University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution>Rice University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>19</day><month>01</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e67846</elocation-id><history><date date-type="received" iso-8601-date="2021-02-24"><day>24</day><month>02</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-01-03"><day>03</day><month>01</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-02-23"><day>23</day><month>02</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.02.22.432227"/></event></pub-history><permissions><copyright-statement>© 2022, Akam et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Akam et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-67846-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-67846-figures-v2.pdf"/><abstract><p>Laboratory behavioural tasks are an essential research tool. As questions asked of behaviour and brain activity become more sophisticated, the ability to specify and run richly structured tasks becomes more important. An increasing focus on reproducibility also necessitates accurate communication of task logic to other researchers. To these ends, we developed pyControl, a system of open-source hardware and software for controlling behavioural experiments comprising a simple yet flexible Python-based syntax for specifying tasks as extended state machines, hardware modules for building behavioural setups, and a graphical user interface designed for efficiently running high-throughput experiments on many setups in parallel, all with extensive online documentation. These tools make it quicker, easier, and cheaper to implement rich behavioural tasks at scale. As important, pyControl facilitates communication and reproducibility of behavioural experiments through a highly readable task definition syntax and self-documenting features. Here, we outline the system’s design and rationale, present validation experiments characterising system performance, and demonstrate example applications in freely moving and head-fixed mouse behaviour.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Behaviour</kwd><kwd>open source</kwd><kwd>Software</kwd><kwd>Hardware</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>WT096193AIA</award-id><principal-award-recipient><name><surname>Akam</surname><given-names>Thomas</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>214314/Z/18/Z</award-id><principal-award-recipient><name><surname>Akam</surname><given-names>Thomas</given-names></name><name><surname>Walton</surname><given-names>Mark E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>202831/Z/16/Z</award-id><principal-award-recipient><name><surname>Walton</surname><given-names>Mark E</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004837</institution-id><institution>Ministerio de Ciencia e Innovación</institution></institution-wrap></funding-source><award-id>RTI2018-097843-B-100 and RYC-2014-16450</award-id><principal-award-recipient><name><surname>Márquez</surname><given-names>Cristina</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004837</institution-id><institution>Ministerio de Ciencia e Innovación</institution></institution-wrap></funding-source><award-id>SEV-2017-0723</award-id><principal-award-recipient><name><surname>Márquez</surname><given-names>Cristina</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Generalitat Valenciana and European Union</institution></institution-wrap></funding-source><award-id>ACIF/2019/017</award-id><principal-award-recipient><name><surname>Esteve-Agraz</surname><given-names>Joan</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>Else-Kroner-Fresenius-Foundation/German-Scholars-Organization</institution></institution-wrap></funding-source><award-id>GSO/EKFS 12</award-id><principal-award-recipient><name><surname>Kätzel</surname><given-names>Dennis</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>KA 4594/2-1</award-id><principal-award-recipient><name><surname>Kätzel</surname><given-names>Dennis</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>109908/Z/15/Z</award-id><principal-award-recipient><name><surname>Panniello</surname><given-names>Mariangela</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution>Human Frontiers Science Programme</institution></institution-wrap></funding-source><award-id>RGY0073/2015</award-id><principal-award-recipient><name><surname>Kohl</surname><given-names>Michael M</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5U19NS104649</award-id><principal-award-recipient><name><surname>Costa</surname><given-names>Rui M</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>617142</award-id><principal-award-recipient><name><surname>Costa</surname><given-names>Rui M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>pyControl is an open-source tool that makes it easy to specify complex behavioural tasks, run them at scale on low-cost hardware, and communicate task logic to other researchers.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animal behaviour is of fundamental scientific interest, both in its own right and in relation to brain function (<xref ref-type="bibr" rid="bib25">Krakauer et al., 2017</xref>). Though understanding natural behaviour is the ultimate goal, the tight control offered by laboratory tasks remains an essential tool in characterising learning mechanisms. To serve the needs of contemporary neuroscience, hardware and software for controlling behavioural experiments should be both flexible and easy to use. Additionally, an increasing focus on reproducibility (<xref ref-type="bibr" rid="bib5">Baker, 2016</xref>; <xref ref-type="bibr" rid="bib17">International Brain Laboratory et al., 2021</xref>) necessitates that behaviour control systems facilitate communication and replication of behavioural paradigms across labs.</p><p>Available commercial solutions often fall short of these desiderata. Proprietary closed-source hardware and software make it difficult to extend or adapt functionality beyond explicitly implemented use cases. Additionally, programming behavioural tasks on commercial systems can be surprisingly non-user-friendly, perhaps due to limitations of underlying legacy hardware. Commercial hardware is also typically very expensive considering the level of technology it represents, disadvantaging researchers outside well-funded institutions (<xref ref-type="bibr" rid="bib28">Marder, 2013</xref>; <xref ref-type="bibr" rid="bib27">Maia Chagas, 2018</xref>), and constraining the ability to scale behavioural assays for high throughput.</p><p>For these reasons, many groups implement their own behavioural hardware either using low-cost microcontrollers such as Arduinos or raspberry PI, or generic laboratory control software such as Labview (<xref ref-type="bibr" rid="bib13">Devarakonda et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">O’Leary et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Gurley, 2019</xref>; <xref ref-type="bibr" rid="bib7">Bhagat et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Buscher et al., 2020</xref>). Though highly flexible, building behavioural control systems from scratch has some disadvantages. It results in much duplication of effort as a lot of the required functionality is generic across experiments. Additionally, unless custom systems are well documented, it is hard for users to meaningfully share experimental protocols. This is important because scientific publications do not consistently contain sufficient information to constrain the details of the task used, yet such details are often crucial for reproducing the behaviour. Making task code public is therefore key to reproducibility, but this is only effective if it is readable and documented, as well as functional.</p><p>To address these limitations, we developed <italic>pyControl</italic>; a system of open-source hardware and software for controlling behavioural experiments. We report the design and rationale of system components, validation experiments characterising system performance, and behavioural data illustrating applications in three widely used, contrasting behavioural paradigms: the 5-choice serial reaction time task (5-CSRTT) in operant chambers, sensory discrimination in head-fixed animals, and a social decision-making task in a maze apparatus.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>System overview</title><p>pyControl consists of three components, the pyControl framework, hardware, and graphical user interface (GUI). The framework implements the syntax used to program behavioural tasks. User-created task definition files, written in Python, run directly on microcontroller hardware, supported by framework code that determines when user-defined functions are called. This takes advantage of <ext-link ext-link-type="uri" xlink:href="https://micropython.org/">MicroPython</ext-link>, a recently developed port of the popular high-level language Python to microcontrollers. The framework handles functionality that is common across tasks, such as monitoring inputs, setting and checking timers, and streaming data back to the computer. This minimises boilerplate code in task files, while ensuring that common functionality is implemented reliably and efficiently. Combined with Python’s highly readable syntax, this results in task files that are quick and straightforward to write, but also easy to read and understand (<xref ref-type="fig" rid="fig1">Figure 1</xref>), promoting replicability and communication of behavioural experiments.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Example task.</title><p>Complete task definition code (left panel) and corresponding state diagram (right panel) for a simple task that turns an LED on for 1 s when a button is pressed three times. Detailed information about the task definition syntax is provided in the <ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/programming-tasks/">Programming Tasks</ext-link> documentation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Example data file.</title><p>Text file generated by running the example task shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Lines beginning I contain information about the session including subject, task, and experiment names, start date, and time. The single line beginning S is a JSON object (also a Python dict) containing the state names and corresponding IDs used below in the data file. The single line beginning E is a JSON object containing the event names and corresponding IDs. Lines beginning D are data lines generated while the framework was running, with format D timestamp ID where timestamp is the time in milliseconds since the start of the framework run and ID is a state ID (indicating a state transition) or an event ID (indicating an event occurred). Lines beginning P are the output of print statements with format P timestamp printed output. The line beginning V indicates the value of a task variable that has been set by the user while the task was running, along with a timestamp.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig1-figsupp1-v2.tif"/></fig></fig-group><p>pyControl hardware consists of a breakout board which interfaces a pyboard microcontroller with ports and connectors, and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders, and stepper motor controllers that are connected to the breakout board to create behavioural setups. Breakout boards connect to the computer via USB. Multiple breakout boards can be connected to a single computer, each controlling a separate behavioural setup. pyControl implements a simple but robust mechanism for synchronising data with other systems such as cameras or physiology hardware. All hardware is fully open source, and assembled hardware is available at low cost from the <ext-link ext-link-type="uri" xlink:href="http://www.open-ephys.org/pycontrol">Open Ephys store</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://www.labmaker.org/collections/neuroscience">LabMaker</ext-link>.</p><p>The GUI provides a graphical interface for setting up and running experiments, visualising behaviour, and configuring setups, and is designed to facilitate high-throughput behavioural testing on many setups in parallel. To promote replicability, the GUI implements self-documenting features which ensure that all task files used to generate data are stored with the data itself, and that any changes to task parameters from default values are recorded in the data files.</p></sec><sec id="s2-2"><title>Task definition syntax</title><p>Here, we give an overview of the task definition syntax and how this contributes to the flexibility of the system. Detailed information about task programming is provided in the documentation and set of example tasks is included with the GUI, including probabilistic reversal learning and random ratio instrumental conditioning.</p><p>pyControl tasks are implemented as state machines, the basic elements of which are states and events. At any given time, the task is in one of the states, and the current state determines how the task responds to events. Events may be generated externally, for example, by the subject’s actions, or internally by timers.</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> shows the complete task definition code and the corresponding state diagram for a simple task in which pressing a button three times turns on an LED for 1 s. The code first defines the hardware that will be used, lists the task’s state and event names, specifies the initial state, and initialises task variables.</p><p>The code then specifies task behaviour by defining a <italic>state behaviour function</italic> for each state. Whenever an event occurs, the state behaviour function for the current state is called with the event name as an argument. Special events called <italic>entry</italic> and <italic>exit</italic> occur when a state is entered and exited allowing actions to be performed on state transitions. State behaviour functions typically comprise a set of <italic>if</italic> and <italic>else if</italic> statements that determine what happens when different events occur in that state. Any valid MicroPython code can be placed in a state behaviour function, the only constraint being that it must execute fast as it will block further state machine behaviour while executing. Users can define additional functions and classes in the task definition file that can be called from state behaviour functions. For example, code implementing a reversal learning task’s block structure might be separated from the state machine code in a separate function, improving readability and maintainability.</p><p>As should be clear from the above, while pyControl makes it easy to specify state machines, tasks are not strict finite state machines, in which the response to an event depends <italic>only</italic> on the current state, but rather extended state machines in which variables and arbitrary code can also determine behaviour.</p><p>We think this represents a good compromise between enforcing a specific structure on task code, which promotes readability and reliability and allows generic functionality to be efficiently implemented by the framework, while allowing users enough flexibility to compactly define a diverse range of complex tasks.</p><p>A key framework component is the ability to set timers to trigger state transitions or events. The <italic>timed_goto_state</italic> function, used in the example, triggers a transition to a specified state after a specified delay. Other functions allow timers to trigger a specified event after a specified delay, or to cancel, pause and un-pause timers that have already been set.</p><p>To make things happen in parallel with the main state set of the task, the user can define an <italic>all_states</italic> function which is called, with the event name as an argument, whenever an event occurs irrespective of the state the task is in. This can be used in combination with timers and variables to implement task behaviour that occurs independently from or interacts with the main state set. For example to make something happen after a specified duration, irrespective of the current state, the user can set a timer to trigger an event after the required duration and use the <italic>all_states</italic> function to perform the required action whenever the event occurs.</p><p>pyControl provides a set of functions for generating random variables, and maths functions are available via the MicroPython maths module. Though MicroPython implements a large subset of the core Python language (see the <ext-link ext-link-type="uri" xlink:href="https://docs.micropython.org/en/latest/library/index.html">MicroPython docs</ext-link>), it is not possible to use packages such as <italic>NumPy</italic> or <italic>SciPy</italic> as they are too large to fit on a microcontroller.</p></sec><sec id="s2-3"><title>Framework implementation</title><p>The pyControl framework consists of approximately 1000 lines of Python code. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows a simplified diagram of information flow between system components. Hardware inputs and elapsing timers place events in a queue where they await processing by the state machine. When events are processed, they are placed in a data output queue along with any state transitions and user print statements that they generate. This design allows different framework update processes to be prioritised by urgency, rather than by the order in which they become necessary, ensuring the framework responds at low latency even under heavy load (see validation experiments below). Top priority is given to processing hardware interrupts, secondary priority to passing events from the event queue to the state machine and processing their consequences, lowest priority to sending and receiving data from the computer.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Framework diagram.</title><p>Diagram showing the flow of information between different components of the framework and the graphical user interface (GUI) while a task is running. Right panel shows the priority with which processes occur in the framework update loop.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig2-v2.tif"/></fig><p>Digital inputs are detected by hardware interrupts and can be configured to generate separate framework events on rising and/or falling edges. Analog inputs can stream continuous data to the computer and trigger framework events when the signal goes above and/or below a specified threshold.</p></sec><sec id="s2-4"><title>Hardware</title><p>A typical pyControl hardware setup consists of a computer running the GUI, connected via USB to one or more breakout boards, each of which controls a single behavioural setup (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). As task code runs on the microcontroller, the computer does not need to be powerful. We typically use standard office desktops running Windows. We have not systematically tested the maximum number of setups that can be controlled from one computer but have run 24 in parallel without issue.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>pyControl hardware.</title><p>(<bold>A</bold>) Diagram of a typical pyControl hardware setup, a single computer connects to multiple breakout boards, each of which controls one behavioural setup. Each behavioural setup comprises devices connected to the breakout board RJ45 behaviour ports using standard network cables. (<bold>B</bold>) Breakout board interfacing the pyboard microcontroller with a set of behaviour ports, BNC connectors, indicator LEDs, and user buttons. See <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> for hardware configurations used in the behavioural experiments reported in this article, along with their associated hardware definition files. For more information, see the <ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/hardware/">hardware docs</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig3-v2.tif"/></fig><p>The breakout board interfaces a pyboard microcontroller (an Arm Cortex M4 running at 168 MHz with 192 KB RAM) with a set of <italic>behaviour ports</italic> used to connect devices that make up behavioural setups, and BNC connectors, indicator LEDs, and user pushbuttons (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Each behaviour port is an RJ45 connector (compatible with standard network cables) with power lines (ground, 5 V, 12 V), two digital inputs/output (DIO) lines that are directly connected to microcontroller pins, and two driver lines for switching higher current loads. The driver lines are low-side drivers (i.e. they connect the negative side of the load to ground) that can switch currents up to 150 mA at voltages up to 12 V, with clamp diodes to the 12 V rail to support inductive loads such as solenoids. Two ports have an additional driver line and two have an additional DIO. Six of the behaviour port DIO lines can alternatively be used as analog inputs and two as analog outputs. Three ports support UART and two support I2C serial communication over their DIO lines. The pinout of the behaviour port is detailed in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Behaviour port pinout.</title><p>All behaviour ports support the standard function for each pin, comprising two digital input/output (DIO) lines connected directly to microcontroller pins, two power driver lines connected to low-side MOSFET drivers for switching higher power loads, and +12 V, + 5 V and ground lines. Some behaviour ports support alternate functions on some pins. On breakout board version 1.2, ports 1 and 2 have an additional power driver line (POW C) and ports 3 and 4 have an additional DIO line (DIO C). Some DIO lines support analog input/output (ADC/DAC), serial communication (I2C, UART, or CAN), or decoding of quadrature signals from rotary encoders (ENC).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3" valign="bottom">Pinout of behaviour port RJ45 connectors</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Standard function</bold></td><td align="left" valign="bottom"><bold>Alternate function</bold></td><td align="left" valign="bottom"><bold>Pin</bold></td></tr><tr><td align="left" valign="bottom">Ground</td><td align="left" valign="bottom">None</td><td align="char" char="." valign="bottom">2</td></tr><tr><td align="char" char="." valign="bottom">+5 V</td><td align="left" valign="bottom">None</td><td align="char" char="." valign="bottom">6</td></tr><tr><td align="char" char="." valign="bottom">+12 V</td><td align="left" valign="bottom">None</td><td align="char" char="." valign="bottom">8</td></tr><tr><td align="left" valign="bottom">Digital input/output (DIO) A</td><td align="left" valign="bottom">Analog input (ADC), I2C-SCL, UART-TX, CAN-RX, ENC</td><td align="char" char="." valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Digital input/output (DIO) B</td><td align="left" valign="bottom">Analog input (ADC), I2C-SDA, UART-RX, CAN-TX, ENC</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="left" valign="bottom">Power driver (POW) A</td><td align="left" valign="bottom">None</td><td align="char" char="." valign="bottom">3</td></tr><tr><td align="left" valign="bottom">Power driver (POW) B</td><td align="left" valign="bottom">None</td><td align="char" char="." valign="bottom">7</td></tr><tr><td align="left" valign="bottom">None</td><td align="left" valign="bottom">DIO C, POW C, analog output (DAC), analog input (ADC)</td><td align="char" char="." valign="bottom">5</td></tr><tr><td align="left" colspan="3" valign="bottom"><bold>Alternate functions available on each behaviour port of breakout board version 1.2</bold></td></tr><tr><td align="left" valign="bottom"><bold>Port</bold></td><td align="left" colspan="2" valign="bottom"><bold>Alternate functions</bold></td></tr><tr><td align="char" char="." valign="bottom">1</td><td align="left" colspan="2" valign="bottom">POW C, UART 4, ENC 5, ADC (on DIO A and B)</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="left" colspan="2" valign="bottom">POW C, CAN 1</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" colspan="2" valign="bottom">DIO C, DAC 1, I2C 1, UART 1, ENC 4, ADC (on DIO C)</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" colspan="2" valign="bottom">DIO C, DAC 2, I2C 2, UART 3, ADC (on DIO C)</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" colspan="2" valign="bottom">CAN 2</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="left" colspan="2" valign="bottom">ADC (on DIO A and B)</td></tr></tbody></table></table-wrap><p>A variety of devices have been developed that connect to the ports, including nose-pokes, levers, audio boards, rotary encoders, stepper motor drivers, lickometers, and LED drivers (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). Each has its own driver file that defines a Python class for controlling the device. For detailed information about devices, see the <ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/hardware/">hardware docs</ext-link>. The hardware repository also contains open-source designs for operant boxes and sound attenuating chambers.</p><p>Though it is possible to specify the hardware that will be used directly in a task file as shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, it is typically done in a separate hardware definition file that is imported by the task. This avoids redundancy when many tasks are run on the same setup. Additionally, abstracting devices used in a task from the specific pins/ports they are connected to allows the same task to run on different setups as long as their hardware definitions instantiate the required devices. See <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> for hardware definitions and corresponding hardware diagrams for the example applications detailed below.</p><p>The design choice of running tasks on a microcontroller, and the specific set of devices developed to date, imposes some constraints on experiments supported by the hardware. The limited computational resources preclude generating complex visual stimuli, making pyControl unsuitable for most visual physiology in its current form. The devices for playing audio are aimed at general behavioural neuroscience applications and may not be suitable for some auditory neuroscience applications. One uses the pyboard’s internal DAC for stimulus generation, and hence is limited to simple sounds such as sine waves or noise. Another plays WAV files from an SD card, allowing for diverse stimuli but limited to 44 kHz sample rate.</p><p>To extend the functionality of pyControl to application not supported by the existing hardware, it is straightforward to interface setups with user-created or commercial devices. This requires creating an electrical connection between the devices and defining the inputs and outputs in the hardware definition. Triggering external hardware from pyControl, or task events from external devices, is usually achieved by connecting the device to a BNC connector on the breakout board, and using the standard pyControl digital input or output classes. More complex interactions with external devices may involve multiple inputs and outputs and/or serial communication. In this case, the electrical connection is typically made to a behaviour port as these carry multiple signal lines. A port adapter board, which breaks out an RJ45 connector to a screw terminal, simplifies connecting wires. Alternatively, if more complex custom circuitry is required, for example, to interface with a sensor, it may make sense to design a custom-printed circuit board with an RJ45 connector, similar to existing pyControl devices, as this is more scalable and robust than implementing the circuit on a breadboard. To simplify instantiating devices comprising multiple inputs and outputs, or controlling devices which require dedicated code, users can define a Python class representing the device. These are typically simple classes which instantiate the relevant pyControl input and output objects as attributes, and may have methods containing code for controlling the device, for example, to generate serial commands. More information is provided in the hardware docs, and the design files and associated code for existing pyControl devices provide a useful starting point for new designs. Alla Karpova’s lab at Janelia Research Campus has independently developed and open-sourced several pyControl-compatible devices (<ext-link ext-link-type="uri" xlink:href="https://github.com/Karpova-Lab">GitHub</ext-link>; <xref ref-type="bibr" rid="bib20">Karpova, 2021</xref>).</p><p>For neuroscience applications, straightforward and failsafe synchronisation between behavioural data and other hardware such as cameras or physiology recordings is essential. pyControl implements a simple but robust method for this. Sync pulses are sent from pyControl to the other systems, which each record the pulse times in their own reference frame. The pulse train has random inter-pulse intervals which ensures a unique match between pulse sequences recorded on each system, so it is always possible to identify which pulse corresponds to which even if pulses are missing (e.g. due to forgetting to turn a system on until after the start of a session). This also makes it unambiguous whether two files come from the same session in the event of a file name mix-up. A Python module is provided for converting times between different systems using the sync pulse times recorded by each. For more information, see the <ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/synchronisation/">synchronisation docs</ext-link>.</p></sec><sec id="s2-5"><title>Graphical user interface</title><p>The GUI provides two ways of setting up and running tasks; the <italic>Run task</italic> and <italic>Experiments</italic> tabs, as well as a <italic>Setups</italic> tab used to name and configure hardware setups.</p><p>The <italic>Run task</italic> tab allows the user to quickly upload and run a task on a single setup. It is typically used for prototyping tasks and testing hardware, but can also be used to acquire data. The values of task variables can be modified before the task is started or while the task is running. During the run, a log of events, state entries, and user print statements is displayed, and the events, states, and any analog signals are plotted live in scrolling plot panels.</p><p>The <italic>Experiments</italic> tab is used for running experiments on multiple setups in parallel and is designed to facilitate high-throughput experiments where multiple users run cohorts of animals through a set of boxes. An experiment consists of a set of subjects run in parallel on the same task. If different subjects need to be run in parallel on different tasks, this can be achieved by opening multiple instances of the GUI.</p><p>To configure an experiment, the user specifies which subjects will run on which setups, and the values of any variables that will be modified before the task starts. Variables can be set to the same value for all subjects or for individual subjects. Variables can be specified as <italic>Persistent</italic>, causing their value to be stored on the computer at the end of the session, and subsequently set to the same value the next time the experiment is run. Variables can be specified as <italic>Summary,</italic> causing their values to be displayed in a table at the end of the framework run and copied to the clipboard in a format that can be pasted directly into a spreadsheet, for example, to record the number of trials and rewards for each subject. Experiment configurations can be saved and subsequently loaded.</p><p>When an experiment is run, the experiments tab changes from the <italic>configure experiment</italic> interface to a <italic>run experiment</italic> interface. The session can be started and stopped individually for each subject or simultaneously for all subjects. While each setup is running, a log of events, state entries, and user print statements is displayed, along with the current state, most recent event, and print statement (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Variable values can be viewed and modified for individual subjects during the session. A tabbed plot window can be opened showing live scrolling plots of the events, states, and analog signals for each subject, and individual subjects’ plots can be undocked to allow behaviour of multiple subjects to be visualised simultaneously.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>pyControl graphical user interface (GUI).</title><p>The GUI’s <italic>Experiments</italic> tab is shown on the left running a multi-subject experiment, with the experiment’s plot window open on the right showing the recent states and events for one subject. For images of the other GUI functionality, see the <ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/graphical-user-interface/">GUI docs</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig4-v2.tif"/></fig><p>The GUI is implemented entirely in Python using the PyQt GUI framework and PyQtGraph plotting library. The GUI is cross-platform and has been used on Windows, Mac, and Linux, though most development and testing has been under Windows. The code is organised into modules for communication with the pyboard, different GUI components, and data visualisation.</p></sec><sec id="s2-6"><title>pyControl data</title><p>Data from pyControl sessions are saved as text files (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for an example). When a session starts, information including the subject, task and experiment names, and start data and time, are written to the data file. While the task is running, all events and state transitions are saved automatically with millisecond timestamps. The user can output additional data by using the <italic>print</italic> function in their task file. This outputs the printed line to the computer, where it is displayed in the log and saved to the data file, along with a timestamp. In decision-making tasks, we typically print one line each trial indicating the trial number, the subject’s choice, and trial outcome, along with any other relevant task variables. If an error occurs while the framework is running, a traceback reporting the error and line number in the task file where it occurred is displayed in the log and written to the data file. Continuous data from analog inputs is saved in separate binary files.</p><p>In addition to data files, task definition files used to generate data are copied to the experiment’s data folder, with a file hash appended to the file name that is also recorded in the corresponding session’s data file. This ensures that every task file version used in an experiment is automatically saved with the data, and it is always possible to uniquely identify the specific task file used for a particular session. If any variables are changed from default values in the task file, this is automatically recorded in the session’s data file. These automatic self-documenting features are designed to promote replicability of pyControl experiments. We encourage users to treat the versioned task files as part of the experiment’s data and include them in data repositories.</p><p>Modules are provided for importing data files into Python for analysis and for visualising sessions offline. Importing a data file creates a Session object with attributes containing the session’s information and data. For convenience, two representations of the state and event data are generated: (1) a dictionary whose keys are event and state names, and values are NumPy arrays with the corresponding event or state entry times, and (2) a list of events and state entries in the order they occurred, whose elements are named tuples with the event/state name and timestamp as attributes. For more information, see the <ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/pycontrol-data/">data docs</ext-link>.</p></sec><sec id="s2-7"><title>Framework performance</title><p>To validate the performance of the pyControl framework, we measured the system’s response latency and timing accuracy. Response latency was assessed using a task which set a digital output to match the state of a digital input driven by a square wave signal. We recorded the input and output signals and plot the distribution of latencies between the two signals across all rising and falling edges (<xref ref-type="fig" rid="fig5">Figure 5A and B</xref>). In a ‘low load’ condition where the pyboard was not processing other inputs, response latency was 556 ± 17 μs (mean ± SD). This latency reflects the time to detect the change in the input, trigger a state transition, and update the output during processing of the ‘entry’ event in the new state. We also measured response latency in a ‘high load’ condition where the pyboard was additionally monitoring two digital inputs each generating framework events in response to edges occurring as Poisson processes with an average rate of 200 Hz, and acquiring signal from two analog inputs at 1 kHz sample rate each. In this high load condition, the response latency was 859 ± 241 μs (mean ± SD), the longest latency recorded was 3.3 ms with 99.6% of latencies &lt; 2 ms.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Framework performance.</title><p>(<bold>A</bold>) Distribution of latencies for the pyControl framework to respond to a change in a digital input by changing the level of a digital output. (<bold>B</bold>) As (<bold>A</bold>) but under a high load condition (see main text). (<bold>C</bold>) Distribution of pulse duration errors when framework generates a 10 ms pulse. (<bold>D</bold>) As (<bold>C</bold>) but under a high load condition. (<bold>E</bold>) Effect of MicroPython garbage collection on pyControl timers. Signals are two digital outputs, one toggled on and off every 1 ms (blue), and one every 5 ms (orange), using pyControl timers. The 1 ms timer that that elapsed during garbage collection (indicated by grey shading) was processed once garbage collection had finished, causing a short delay. Garbage collection had no effect on the 5 ms timer that was running but did not elapse during garbage collection. (<bold>F</bold>) Effect of garbage collection on pyControl inputs. A signal comprising 1 ms pulses every 10 ms was received by three pyControl digital inputs. Input 1 was configured to generated framework events on rising edges (green), input 2 on falling edges (red), and input 3 on both rising (blue) and falling (orange) edges. Garbage collection (indicated by grey shading) was triggered 1 ms before an input pulse. Inputs 1 and 2 both generated their event that occurred during garbage collection with the correct timestamp. If multiple events occur on a single digital input during a single garbage collection, only the last event is generated correctly, causing the missing rising event on input 3.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig5-v2.tif"/></fig><p>To assess timing accuracy, we used a task which turned on a digital output for 10 ms when a rising edge was received on a digital input. The input was driven by a 51 Hz square wave to ensure that the timing of input edges drifted relative to the framework’s 1 ms clock ticks. We plot the distribution of errors between the measured durations of the output pulses and the 10 ms target duration (<xref ref-type="fig" rid="fig5">Figure 5C and D</xref>). In the low load condition, timing errors were approximately uniformly distributed across 1 ms (mean error –220 μs, SD 282 μs), as expected given the 1 ms resolution of the pyControl framework clock ticks. In the high load condition, timing variability was only slightly increased (mean –10 μs, SD 353 μs), with the largest recorded error 1.9 ms and 99.5% of errors &lt; 1 ms. Overall, these data show that the framework’s latency and timing accuracy are sufficient for the great majority of neuroscience applications, even when operating under loads substantially higher than experienced in typical tasks.</p><p>Users who require very tight timing/latency performance should be aware of MicroPython’s automatic garbage collection. Garbage collection is triggered when needed to free up memory and takes a couple of milliseconds. Normal code execution is paused during garbage collection, though interrupts (used to register external inputs and update the framework clock) run as normal. pyControl timers that elapse during garbage collection are processed once it has completed (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). Timers that are running but do not elapse during garbage collection are unaffected. Digital inputs that occur during garbage collection are registered with the correct timestamp (<xref ref-type="fig" rid="fig5">Figure 5F</xref>), but will only be processed once garbage collection has completed. The only situation where events may be missed due to garbage collection is if a single digital input receives multiple event-triggering edges during a single garbage collection, in which case only the last event is processed correctly (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). To avoid garbage collection affecting critical processing, the user can manually trigger garbage collection at a time when it will not cause problems (see <ext-link ext-link-type="uri" xlink:href="https://docs.micropython.org/en/latest/library/gc.html">MicroPython docs</ext-link>), for example, during the inter-trial interval (ITI). In the latency and timing accuracy validation experiments (<xref ref-type="fig" rid="fig5">Figure 5A–D</xref>), garbage collection was triggered by the task code at a point in the task where it did not affect the measurements.</p><p>A final constraint is that as each event takes time to process, there is a maximum <italic>continuous</italic> event rate above which the framework cannot process events as fast as they occur, causing the event queue to grow until available memory is exhausted. This rate will depend on the processing triggered by each event, but is approximately 960 Hz for digital inputs triggering state transitions but no additional processing. In practice, we have never encountered this when running behavioural tasks as average event rates are typically orders of magnitude lower and transiently higher rates are buffered by the queue.</p></sec><sec id="s2-8"><title>Application examples</title><p>We illustrate how pyControl is used in practice with example applications in operant box, head-fixed, and maze-based tasks. Task and hardware definition files for these experiments are provided in the article’s data repository. For additional use cases, see also <xref ref-type="bibr" rid="bib24">Korn et al., 2021</xref>; <xref ref-type="bibr" rid="bib3">Akam et al., 2021</xref>; <xref ref-type="bibr" rid="bib23">Koralek and Costa, 2020</xref>; <xref ref-type="bibr" rid="bib32">Nelson et al., 2020</xref>; <xref ref-type="bibr" rid="bib8">Blanco-Pozo et al., 2021</xref>; <xref ref-type="bibr" rid="bib40">van der Veen et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">de Barros et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">Samborska et al., 2021</xref>; <xref ref-type="bibr" rid="bib21">Kilonzo et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Strahnen et al., 2021</xref>.</p></sec><sec id="s2-9"><title>5-choice serial reaction time task (5-CSRT)</title><p>The 5-CSRT is a long-standing and widely used assay for measuring sustained visual attention and motor impulsivity in rodents (<xref ref-type="bibr" rid="bib10">Carli et al., 1983</xref>; <xref ref-type="bibr" rid="bib6">Bari et al., 2008</xref>). The subject must detect a brief flash of light presented pseudorandomly in one of five nose-poke ports and report the stimulus location by poking the port to trigger a reward delivered to a receptacle on the opposite wall.</p><p>We developed a custom operant box for the 5-CSRT (<xref ref-type="fig" rid="fig6">Figure 6A and B</xref>), discussed in detail in a separate manuscript (<xref ref-type="bibr" rid="bib19">Kapanaiah et al., 2021</xref>). The pyControl hardware comprised a breakout board connected to a 5-poke board, which integrates the IR beams and stimulus LEDs for the 5-choice ports on a single PCB, a single poke board for the reward receptacle, an audio board, and a stepper motor board to control a peristaltic pump for reward delivery (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>5-choice serial reaction time task (5-CSRTT).</title><p>(<bold>A</bold>) Trapezoidal operant box with 5-choice wall (poke-holes shown illuminated) within a sound-attenuated cubicle. (<bold>B</bold>) High-throughput training setup comprising 24 operant boxes. (<bold>C, D</bold>) Performance measures on the 5-CSRTT during protocols challenging either sustained attention – by shortening the SD or delivering a sound distraction during the waiting time (<bold>C</bold>) or motor impulsivity – by extending the inter-trial interval (ITI) to a fixed (fITI) or variable (vITI) length (<bold>D</bold>). Protocols used are indicated by x-axes. Note the rather selective decrease of attentional performance (accuracy, %omissions) or impulse control (%prematures) achieved by the respective challenges. (<bold>E</bold>) Validation of the possibility to detect cognitive enhancement in the 5-CSRTT (9s-fITI challenge) by application of atomoxetine, which increased attentional accuracy and decreased premature responding, as predicted. Asterisks in (<bold>C–E</bold>) indicate significant within-subject comparisons relative to the baseline (2 s SD, 5 s fITI; <bold>C, D</bold>) or the vehicle (<bold>E</bold>) condition (paired-samples <italic>t</italic>-test). *p&lt;0.05, *p&lt;0.01, *p&lt;0.001. Error bars display s.e.m. Note that two mice of the full cohort (N = 8) did not participate in all challenges as they required more training time to reach the baseline stage.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Hardware configuration for 5-choice serial reaction time task (5-CSRTT).</title><p>Diagram of hardware modules used to implement the 5-CSRTT. A breakout board is connected to a 5-poke board which integrates the IR beams and LEDs for the ports on the 5-choice wall onto a single PCB controlled from two behaviour ports, a stepper motor controller is used with a custom-made 3D-printed peristaltic pump for reward delivery, a single poke board is used for the reward receptacle with a 12 V LED module used for house light connected to its solenoid output connector, and an audio board for generating auditory stimuli. The hardware definition for this setup is provided in the article’s code repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/manuscript/blob/master/Five%20choice%20serial%20reaction%20time%20task/pyControl%20files/5_CSRTT_hardware_definition.py">https://github.com/pyControl/manuscript/blob/master/Five%20choice%20serial%20reaction%20time%20task/pyControl%20files/5_CSRTT_hardware_definition.py</ext-link>; <xref ref-type="bibr" rid="bib2">Akam, 2021</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig6-figsupp1-v2.tif"/></fig></fig-group><p>To validate the setup, a cohort of eight C57BL/6 mice was trained in the 5-CSRTT using a staged training procedure (see Materials and methods). The baseline protocol reached at the end of training used a stimulus duration (SD) of 2 s and a 5 s ITI from the end of reward consumption to the presentation of the next stimulus. These task parameters were then manipulated to challenge subject’s ability to either maintain sustained attention or withhold impulsive premature responses. Attention was challenged in three conditions: by decreasing the SD to either 1 s or 0.8 s, or by an auditory distraction of 70 dB white noise, played between 0.5 s and 4.5 s of the 5 s ITI. In all three attention challenges, the accuracy with which subjects selected the correct port – the primary measure of sustained attention – decreased (p&lt;0.05; paired <italic>t</italic>-tests comparing accuracy under the prior baseline protocol to accuracy under the challenge condition, <xref ref-type="fig" rid="fig6">Figure 6C</xref>). Also, as expected, omissions (i.e. failures to poke any port in the response window) increased (p&lt;0.05, <italic>t</italic>-test). In the attention challenges, the rate of premature responses – the primary measure of impulsivity – remained either unchanged (1 s SD challenge, auditory distraction; p&gt;0.1, <italic>t</italic>-test) or changed to a comparatively small extent (0.8 s SD challenge, p&lt;0.01, <italic>t</italic>-test). Similarly, when impulsivity was challenged by extending the ITI, to either a 9 s fixed ITI (fITI) or to a pseudo-randomly varied ITI length (vITI), premature responses increased strongly (p&lt;0.05, <italic>t</italic>-test), while attentional accuracy and omissions did not (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). This specificity of effects of the challenges was as good – if not better – than that achieved by us previously in a commercial set-up (Med Associates, Inc; <xref ref-type="bibr" rid="bib15">Grimm et al., 2018</xref>).</p><p>We further validated the task implementation by replicating effects of a pharmacological treatment – atomoxetine – that has been shown to reduce impulsivity in the 5-CSRTT (<xref ref-type="bibr" rid="bib31">Navarra et al., 2008</xref>; <xref ref-type="bibr" rid="bib35">Paterson et al., 2011</xref>). Using the 9 s fITI impulsivity challenge, we found that 2 mg/kg atomoxetine could reliably reduce premature responding and increase attentional accuracy (p&lt;0.05, paired <italic>t</italic>-test comparing performance under vehicle vs. atomoxetine; <xref ref-type="fig" rid="fig6">Figure 6E</xref>), consistent with its previously described effect in this rodent task (<xref ref-type="bibr" rid="bib31">Navarra et al., 2008</xref>; <xref ref-type="bibr" rid="bib35">Paterson et al., 2011</xref>; <xref ref-type="bibr" rid="bib36">Pillidge et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Fitzpatrick and Andreasen, 2019</xref>).</p></sec><sec id="s2-10"><title>Vibrissae-based object localisation task</title><p>We illustrate pyControl’s utility for head-fixed behaviours with a version of the vibrissae-based object localisation task (<xref ref-type="bibr" rid="bib33">O’Connor et al., 2010</xref>). Head-fixed mice used their vibrissae (whiskers) to discriminate the position of a pole moved into the whisker field at one of two different anterior-posterior locations (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). The anterior ‘Go’ location indicated that licking in a response window after stimulus presentation would deliver a water reward, while the posterior ‘NoGo’ location indicated that licking in the response window would trigger a timeout (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Unlike in the original task, mice were positioned on a treadmill allowing them to run. Although running was not required to perform the task, we observed 10–20 s running bouts alternated with longer stationary periods (<xref ref-type="fig" rid="fig7">Figure 7C</xref>), in line with previous reports (<xref ref-type="bibr" rid="bib4">Ayaz et al., 2019</xref>). pyControl hardware used to implement the setup comprised a breakout board, a stepper motor driver to control the anterior-posterior position of the stimulus, a lickometer, and a rotary encoder to measure running speed (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Vibrissae-based object localisation task.</title><p>(<bold>A</bold>) Diagram of the behavioural setup. Head-fixed mice were positioned on a treadmill with their running speed monitored by a rotary encoder. A pole was moved into the whisker field by a linear motor, with the anterior-posterior location controlled using a stepper motor. Water rewards were delivered via a spout positioned in front of the animal and licks to the spout were detected using an electrical lickometer. (<bold>B</bold>) Trial structure: before stimulus presentation, the stepper motor moved into the trial position (anterior or posterior). Next, the linear motor translated the stepper motor and the attached pole close to the mouse’s whisker pad, starting the stimulation period. A lick window (during Go trials) or withhold window (during NoGo trials) started after the pole was withdrawn. FA, false alarm; CR, correct rejection. (<bold>C</bold>) pyControl simultaneously recorded running speed (top trace) and licks (black dots) of the animals, as well as controlling stimulus presentation (blue and red bars for Go and NoGo stimuli) and solenoid opening (black crosses). (<bold>D</bold>) Percentage of correct trials for three mice over the training period. Mice were considered expert on the task after reaching 75% correct trials (dotted line) and maintaining such performance for three consecutive days. (<bold>E</bold>) Detected licks before, during, and after tactile stimulation, during an early session before the mouse has learned the task, sorted by trial type: hit trials (blue), correct rejection trials (green), false alarm trials (red), and miss trials (black). Each row is a trial, each dot is a detected lick. Correct trials for this session were 47.9% of total trials. (<bold>F</bold>) As (<bold>E</bold>) but for data from the same mouse after reaching the learning threshold (correct trials = 89.3% of total trials).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Hardware configuration for vibrissae-based object localisation task.</title><p>Diagram of the hardware modules used to implement the head-fixed vibrissae-based object localisation task. A breakout board is connected to a rotary encoder module, used to measure running speed, a lickometer, used to detect licks and control the reward solenoid, a stepper motor controller used to set the anterior-posterior position of the stimulus, and a controller for the linear stage used to move the stimulus in and out of the whisker field. The hardware definition for this setup is provided in the article’s code repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/manuscript/blob/master/vibrissa%20based%20object%20detection%20task/pyControl%20files/hardware_definition.py">link</ext-link>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig7-figsupp1-v2.tif"/></fig></fig-group><p>Mice were first familiarised with the experimental setup by head-fixing them on the treadmill for increasingly long periods of time (5–20 min) over 3 days. From the fourth day, mice underwent a ‘detection training’, during which the pole was only presented in the Go position, and water automatically delivered after each stimulus presentation. We then progressively introduced NoGo trials and made water delivery contingent on the detection of one or more licks in the response window. Subjects reached 75% correct performance within 5–9 days from the first training session, at which point, they were trained for at least three further days to make sure that they had reliably learned the task (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). Early in training, mice frequently licked prior to and during stimulus presentation, as well as during the response window, on both Go and NoGo trials (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). Following learning, licking prior to and during stimulus presentation was greatly reduced, and mice licked robustly during the response window on Go trials and withheld licking on NoGo trials, performing a high percentage of hit and correct rejection trials (<xref ref-type="fig" rid="fig7">Figure 7F</xref>).</p></sec><sec id="s2-11"><title>Social decision-making task</title><p>Our final application example is a maze-based social decision-making task for mice, adapted from that developed for rats by <xref ref-type="bibr" rid="bib29">Márquez et al., 2015</xref>. In this task, a ‘focal’ animal’s choices determine reward delivery for a ‘recipient’ animal, allowing preference for ‘prosocial’ vs. ‘selfish’ choices to be examined. The behavioural apparatus comprised an automated double T-maze (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). Each T-maze consisted of a central corridor with nose-poke ports on each side (choice area) and two side arms each with a food receptacle connected to a pellet dispenser at the end (<xref ref-type="fig" rid="fig8">Figure 8A and B</xref>). Access from the central choice area to the side arms was controlled by pneumatic doors.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Social decision-making task.</title><p>(<bold>A</bold>) Top view of double T-maze apparatus showing two animals interacting during social decision-making. (<bold>B</bold>) Setup diagram; in each T-maze, nose-pokes are positioned on either side of the central choice area. Sliding pneumatic doors give access to the side arms of each maze (top and bottom in diagram) where pellet dispensers deliver food rewards. Six IR beams (depicted as grey and red circles connected by a dotted red line) detect the position of the animals to safely close the doors once access to an arm is secured. (<bold>C</bold>) Focal animal individual training showing the number of trials completed per minute (left panel) and side bias (right panel) across days of training. (<bold>D</bold>) As (<bold>C</bold>) but for the recipient animal. (<bold>E</bold>) Social decision-making task. The trial starts with both animals in the central arm. The recipient animal has learnt in previous individual training to poke the port on the upper side of the diagram to give access to a food pellet in the corresponding reward area. During the social task, the recipient animal’s ports no longer control the doors but the animal can display food-seeking behaviour by repeatedly poking the previously trained port. The focal animal has previously learned in individual training to collect food from the reward areas on both sides (top and bottom of diagram) by poking the corresponding port in the central choice area to activate the doors. During social decision-making, the focal animal can either choose the ‘prosocial’ port, giving both animals access to the side (upper on diagram) of their respective mazes where both receive reward, or can choose the ‘selfish’ port, giving both animals access to the other side (lower on diagram) where only the focal animal receives reward. (<bold>F</bold>) Raster plot showing behaviour of a pair of animals over one session during early social testing. Nose-pokes are represented by vertical lines, and colour coded according to the role of each mouse and choice type (grey, recipient’s pokes, which are always directed towards the prosocial side; blue, focal’s pokes in the prosocial choice port; red, focal’s pokes in selfish port). Note that latency for focal choice varies depending on the trial, allowing the recipient to display its food-seeking behaviour or not. Circles indicate the moment where each animal visits the food-receptacle in their reward arm. Focal animals are always rewarded, and the colour of the filled circle indicates the type of trial after decision (blue, prosocial choice; red, selfish choice). Grey circles indicate time of receptacle visit for recipients, where filled circles correspond to prosocial trials, where recipient is also rewarded, and open circles to selfish trials, where no pellet is delivered.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Hardware configuration for social decision-making task.</title><p>Diagram of the hardware modules used to implement the double T-maze apparatus for the social decision-making task. A port expander is used to provide additional IO lines for IR beams, stepper motor controller boards are used to control custom made pellet dispensers, and a relay interface board is used to control the solenoids actuating the pneumatic doors. The hardware definition for this setup is provided in the article’s code repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/manuscript/blob/master/social%20decision%20making%20task/pyControl%20files/hardware_definition_double_T_maze.py">https://github.com/pyControl/manuscript/blob/master/social%20decision%20making%20task/pyControl%20files/hardware_definition_double_T_maze.py</ext-link>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67846-fig8-figsupp1-v2.tif"/></fig></fig-group><p>The task comprised two separate stages: (1) individual training, where animals learn to open doors by poking the ports in the central arms and retrieve pellets in the side arms; and (2) social testing, where the decisions of the focal animal control the doors in both mazes, and hence determine rewards for both itself and the recipient animal in the other maze.</p><p>The individual training protocols were different for the focal and recipient animals. During individual training for the focal animal, a single poke in either port in the central arm opened the corresponding door, allowing access to a side arm. Accessing either side arm was rewarded with a pellet at the food receptacle in the arm. Under this schedule, subjects increased their rate of completing trials over seven training days (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, repeated measures ANOVA F(6,42) = 12.566, p=0.000004) without developing a bias for either side of the maze (p&gt;0.27 for all animals, <italic>t</italic>-test). During individual training for the recipient animal, only one of the nose-poke ports in the central arm was active, and the number of pokes required to open the corresponding door increased over 13 days of training, with four pokes eventually required to access the side arm to obtain a pellet in the food receptacle. Under this schedule, the recipient animals developed a strong preference for the active poke over the course of training (<xref ref-type="fig" rid="fig8">Figure 8D</xref>, right panel, repeated measures ANOVA F(12,24) = 3.908, p=0.002), with approximately 95% of pokes directed to the active side by the end of training.</p><p>During social testing, the two animals were placed in the double T-maze, one in each T, separated by a transparent perforated partition that allowed the animals to interact using all sensory modalities. The doors in the recipient animal’s maze were no longer controlled by the recipient animal’s pokes, but were rather yoked to the doors of the focal animal, such that a single poke to either port in the focal animals choice area opened the doors in both mazes on the corresponding side. As in individual training, the focal animal was rewarded for accessing either side, while the recipient animal was rewarded only when it accessed one side of the maze. The choice made by the focal animal therefore determined whether the recipient animal received reward, so the focal animal could either make ‘pro-social’ choices which rewarded both it and the recipient, or ‘selfish’ choices which rewarded only the focal animal. As a proof of concept, we show nose-pokes and reward deliveries from a pair of interacting mice from one social session (<xref ref-type="fig" rid="fig8">Figure 8F</xref>). A full analysis of the social behaviour in this task will be published separately (Esteve-Agraz and Marquez, in preparation).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>pyControl is an open-source system for running behavioural experiments, whose principal strengths are (1) a flexible and intuitive Python-based syntax for programming tasks; (2) inexpensive, simple, and extensible behavioural hardware that can be purchased commercially or assembled by the user; (3) a GUI designed for efficiently running high-throughput experiments on many setups in parallel from a single computer; and (4) extensive online documentation and user support.</p><p>pyControl can contribute to behavioural neuroscience in two important ways: first, it makes it quicker, easier, and cheaper to implement a wide range of behavioural tasks and run them at scale. Second, it facilitates communication and reproducibility of behavioural experiments, both because the task definition syntax is highly readable and because self-documenting features ensure that the exact task version and parameters used to generate data are automatically stored with the data itself.</p><p>pyControl’s strengths and limitations stem from underlying design choices. We will discuss these primarily in relation to two widely used open-source systems for experiment control in neuroscience <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/bpoddocumentation/home">Bpod</ext-link> (Josh Sanders) and <ext-link ext-link-type="uri" xlink:href="https://bonsai-rx.org/">Bonsai</ext-link> (<xref ref-type="bibr" rid="bib26">Lopes et al., 2015</xref>). Bpod is a useful point of comparison as it is probably the most similar project to pyControl in terms of functionality and implementation, Bonsai because it represents a very different but powerful formalism for controlling experiments that is often complementary. Space constraints preclude detailed comparison with other projects, but see <xref ref-type="bibr" rid="bib13">Devarakonda et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">O’Leary et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Gurley, 2019</xref>; <xref ref-type="bibr" rid="bib38">Saunders and Wehr, 2019</xref>; <xref ref-type="bibr" rid="bib7">Bhagat et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Buscher et al., 2020</xref>.</p><p>Both pyControl and Bpod provide a state machine-based task definition syntax in a high-level programming language, run the state machine on a microcontroller, have commercially available open-source hardware, graphical interfaces for controlling experiments, and are reasonably mature systems with a substantial user base beyond the original developers. Despite these commonalities, there are significant differences which are useful for prospective users to understand.</p><p>The first is that in pyControl user-created task definition code runs directly on a pyboard microcontroller, supported by framework code that determines when user-defined functions are called. This contrasts with Bpod, where user code written in either MATLAB (Bpod) or Python (PyBpod) is translated into instructions passed to the microcontroller, which itself runs firmware implemented in the lower-level language C++. These two approaches offer distinct advantages and disadvantages.</p><p>Running user Python code directly on the microcontroller avoids separating the task logic into two conceptually distinct levels – flexible code written in a high-level language that runs on the computer, and the more constrained set of operations supported by the microcontroller firmware. Our understanding of how this works in Bpod is that the high-level user code implements a loop over trials where each loop defines a finite state machine for the current trial – specifying for each state which outputs are on and which events trigger transitions to which other states, then uploads this information to the microcontroller, runs the state machine until it reaches an exit condition indicating the end of the trial, and finally receives information from the microcontroller about what happened before starting the next trial’s loop. The microcontroller firmware implements some functionality beyond a strict finite state machine formalism, including timers and event counters that are not tied to a particular state, but does not support arbitrary user code or variables. We suggest readers consult the relevant documentation (<ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/programming-tasks/">pyControl</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/bpoddocumentation/user-guide/protocol-development">Bpod</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://pybpod.readthedocs.io/en/latest/getting-started/writing-protocols.html">PyBpod</ext-link>) and example tasks (<ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/code/tree/master/tasks/example">pyControl</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/sanworks/Bpod_Gen2/tree/master/Examples/Protocols">Bpod</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/pybpod/pybpod-api/tree/master/examples/state_machine_examples">PyBpod</ext-link>) to compare syntaxes directly. A second advantage of running user code directly on the microcontroller is that the user has direct access from their task code to microcontroller functionality such as serial communication. A third is that the pyControl framework (as well as the GUI) is written in Python rather than C++, facilitating code maintenance, and lowering the barrier to users extending system functionality.</p><p>The two principal disadvantages of running the task entirely on the microcontroller are (1) although modern microcontrollers are very capable, their resources are more limited than a computer – which constrains how computationally and memory-intensive task code can be and precludes using modules such as NumPy. (2) Lack of access to the computer from task code, for example, to interact with other programs or display custom plots. To address these limitations, we are currently developing an application programming interface (API) to allow pyControl tasks running on the microcontroller to interact with user code running on the computer. This will work via the user defining a Python class with methods that get called at the start and end of the run for initial setup and post-run clean-up, as well as an update method called regularly during the run with any new data received from the board as an argument.</p><p>There are also differences in hardware design. The two most significant are (1) the pyControl breakout board tries to make connectors (behaviour ports and BNC) as flexible as possible at the cost of not being specialised for particular functions. Bpod tends to use a given connector for a specific function; for example, it has separate <italic>behaviour ports</italic> and <italic>module ports</italic>, with the former designed for controlling a nose-poke, and the latter for UART serial communication with external modules. Practically, this means that pyControl exposes microcontroller pins (which often support multiple functions) directly on connectors whereas Bpod tends to incorporate intervening circuitry such as electrical isolation for BNC connectors and serial line driver ICs on module ports. (2) Bpod uses external modules, each with its own microcontroller and C++ firmware, for functions which pyControl implements using the microcontroller on the breakout board, specifically analog input and output, I2C serial communication, and acquiring signal from a rotary encoder. These design choices make pyControl hardware simpler and cheaper. Purchased commercially the Bpod state machine costs $765 compared to €250 for the pyControl breakout board, and Bpod external modules each cost hundreds of dollars. This is not to say that pyControl necessarily represents better value; a given Bpod module may offer more functionality (e.g. more channels, higher sample rates). But the two systems do represent different design approaches.</p><p>Both the pyControl and PyBpod GUIs support configuring and running experiments on multiple setups in parallel from a single computer, while the MATLAB-based Bpod GUI controls a single setup at a time. Their user interfaces are each very different; the respective user guides (<ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io/en/latest/user-guide/graphical-user-interface/">pyControl</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/bpoddocumentation/user-guide/general-concepts/bpod-console">Bpod</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://pybpod.readthedocs.io/en/latest/getting-started/basic-usage.html">PyBpod</ext-link>) give the best sense for the different approaches. We think it is a strength of the pyControl GUI, reflecting the relative simplicity of the underlying code base, that scientist users not originally involved in the development effort have made substantial contributions to its functionality (see GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/code/pulls?q=is%3Apr">pull requests</ext-link>).</p><p>Bonsai (<xref ref-type="bibr" rid="bib26">Lopes et al., 2015</xref>) represents a very different formalism for experiment control that is not based around state machines. Instead, the Bonsai user designs a <italic>dataflow</italic> by arranging and connecting nodes in a graphical interface, where nodes may represent data sources, processing steps, or outputs. Bonsai can work with a diverse range of data types including video, audio, analog, and digital signals. Multiple data streams can be processed in parallel and combined via a rich set of operators including arbitrary user code. Bonsai is very powerful, and it is likely that any task implemented in pyControl could also be implemented in Bonsai. The reverse is certainly not true as Bonsai can perform computationally demanding real-time processing on high-dimensional data such as video, which is not supported by pyControl.</p><p>Nonetheless, in applications where either system could be used, there are reasons why prospective users might consider pyControl: (1) pyControl’s task definition syntax may be more intuitive for tasks where (extended) state machines are a natural formalism. The reverse is true for tasks requiring parallel processing of multiple complex data streams. (2) pyControl is explicitly designed for efficiently running high-throughput experiments on many setups in parallel. Though it is possible to control multiple hardware setups from a single Bonsai dataflow, Bonsai does not explicitly implement the concept of a multi-setup experiment so the user must duplicate dataflow components for each setup themselves. As task parameters and data file names are specified across multiple nodes in the dataflow, configuring these for a cohort of subjects can be laborious – though it is possible to automate this by calling Bonsai’s command line interface from user-created Python scripts. (3) pyControl hardware modules can simplify the physical construction of behavioural setups. Though Bonsai itself is software, some compatible behavioural hardware has been developed by the Champalimaud Foundation Hardware Platform (<ext-link ext-link-type="uri" xlink:href="https://www.cf-hw.org/harp">https://www.cf-hw.org/harp</ext-link>), which offers tight timing synchronisation and close integration with Bonsai, though documentation is currently limited. In practice, we think the two systems are often complementary; for example, we use Bonsai in our workflow for acquiring and compressing video data from sets of pyControl operant boxes (<ext-link ext-link-type="uri" xlink:href="https://github.com/ThomasAkam/Point_Grey_Bonsai_multi_camera_acquisition">GitHub</ext-link>; <xref ref-type="bibr" rid="bib1">Akam, 2020</xref>), and we hope to integrate them more closely in future. pyControl is under active development. We are currently prototyping a home cage training system which integrates a pyControl operant box with a mouse home cage via an access control module which allows socially housed animals to individually access the operant box to train themselves with minimal user intervention. We are also developing hardware to enable much larger-scale behavioural setups, such as complex maze environments with up to 68 behaviour ports per setup. As discussed above, we are finalising an API to allow pyControl tasks to interact with user Python code running on the computer.</p><p>In summary, pyControl is a user-friendly and flexible tool addressing a commonly encountered use case in behavioural neuroscience; defining behavioural tasks as extended state machines, running them efficiently as high-throughput experiments, and communicating task logic to other researchers.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or <break/>reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, <break/>algorithm</td><td align="left" valign="bottom">pyControl</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/code">https://github.com/pyControl/code</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_021612">SCR_021612</ext-link></td><td align="left" valign="bottom">Repository containing <break/>pyControl GUI <break/>and framework <break/>code</td></tr><tr><td align="left" valign="bottom">Other</td><td align="left" valign="bottom">pyControl hardware</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/hardware">https://github.com/pyControl/hardware</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_021612">SCR_021612</ext-link></td><td align="left" valign="bottom">Repository containing <break/>pyControl <break/>hardware designs</td></tr><tr><td align="left" valign="bottom">Other</td><td align="left" valign="bottom">pyControl Docs</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://pycontrol.readthedocs.io">https://pycontrol.readthedocs.io;</ext-link>a PDF version of <break/>the docs is included <break/>in supplementary <break/>material</td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_021612">SCR_021612</ext-link></td><td align="left" valign="bottom">pyControl <break/>documentation</td></tr></tbody></table></table-wrap><p>pyControl task files used in all experiments, and data and analysis code for the performance validation experiments, are included in the article’s <ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/manuscript">data and code repository</ext-link>.</p><sec id="s4-1"><title>Framework performance validation</title><p>Framework performance was characterised using pyboards running MicroPython version 1.13 and pyControl version 1.6. Electrical signals used to characterise response latency and timing accuracy (<xref ref-type="fig" rid="fig5">Figure 5</xref>) were recorded at 50 kHz using a PicoScope 2204A USB oscilloscope.</p><p>To assess response latency (<xref ref-type="fig" rid="fig5">Figure 5A and B</xref>), a pyboard running the task file <italic>input_follower.py</italic> received a 51 Hz square wave input generated by the PicoScope’s waveform generator. The task turned an output on and off to match the state of the input signal. The latency distribution was assessed by recording 50 s of the input and output signals and evaluating the latency between the signals at each rising and falling edge.</p><p>To assess timing accuracy (<xref ref-type="fig" rid="fig5">Figure 5C and D</xref>), a pyboard running the task file <italic>triggered_pulses.py</italic> received a 51 Hz square wave input generated by the PicoScope’s waveform generator. The task triggered a 10 ms output pulse whenever a rising edge occurred in the input signal. The output signals were recorded for 50 s, and the duration of each output pulses was measured to assess the distribution of timing errors.</p><p>In both cases, the experiments were performed separately in a low load and high load condition. In the low load condition, the task was not monitoring any other inputs. In the high load condition, the task was additionally acquiring data from two analog inputs at 1 kHz sample rate each, and monitoring two digital inputs, each of which was generating framework events in response to edges occurring as a Poisson process with average rate 200 Hz. These Poisson input signals were generated by a second pyboard running the task <italic>poisson_generator.py</italic>.</p><p>To assess the effect of garbage collection on pyControl timers (<xref ref-type="fig" rid="fig5">Figure 5E</xref>), the task file <italic>gc_timer_test.py</italic> was run on a pyboard. This uses pyControl timers to toggle one digital output on and off every 1 ms and another every 5 ms. The resulting signals were recorded using the PicoScope and plotted around a garbage collection episode identified by visually inspecting the 1 ms timer signal.</p><p>To assess the effect of garbage collection on digital input processing (<xref ref-type="fig" rid="fig5">Figure 5F</xref>), a signal comprising 1 ms pulses every 10 ms was generated using the PicoScope, and connected to three digital inputs on a pyboard running the task <italic>gc_inputs_test.py</italic>. The task configures one input to generate events on rising edges, one on falling edges, and one on both rising and falling edges, and uses a pyControl timer to trigger garbage collection 1ms before a subset of the input pulses. Event times recorded by pyControl were plotted to generate the figure.</p><p>Analysis and plotting of the framework validation data was performed in Python using code included in the data repository.</p></sec><sec id="s4-2"><title>Application examples</title><p>The 5-CSRTT5.</p></sec><sec id="s4-3"><title>Animals</title><p>The 5-CSRTT experiment used a cohort of eight male C57BL/6 mice, aged 3–4 months at the beginning of training. Animals were group-housed (2–3 mice per cage) in Type II-Long individually ventilated cages (Greenline, Tecniplast, G), enriched with sawdust, sizzle-nest, and cardboard houses (Datesand, UK), and subjected to a 13  hr light/11  hr dark cycle. Mice were kept under food restriction at 85–95% of their average free-feeding weight which was measured over 3 days immediately prior to the start of food restriction at the start of the behavioural training. Water was available ad libitum.</p><p>This experiment was performed in accordance to the German Animal Rights Law (Tierschutzgesetz) 2013 and approved by the Federal Ethical Review Committee (Regierungsprädsidium Tübingen) of Baden-Württemberg.</p></sec><sec id="s4-4"><title>Behavioural hardware</title><p>The design of the operant boxes for the 5-CSRTT setups is discussed in detail in a separate manuscript (<xref ref-type="bibr" rid="bib19">Kapanaiah et al., 2021</xref>). Briefly, the box had a trapezoidal floorplan with the 5-choice wall at the wide end and reward receptacle at the narrow end of the trapezoid to minimise the floor area and hence reduce distractions. The side walls and roof were made of transparent acrylic to allow observation of the animal, the remaining walls were made from opaque PVC to minimise visual distractions (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Design files for the operant box, and peristaltic and syringe pumps for reward delivery, are at <ext-link ext-link-type="uri" xlink:href="https://github.com/KaetzelLab/Operant-Box-Design-Files">https://github.com/KaetzelLab/Operant-Box-Design-Files</ext-link>; <xref ref-type="bibr" rid="bib18">Kaetzell, 2021</xref>. Potentially distracting features (house light, cables) were located outside of the box and largely invisible from the inside. The pyControl hardware used and the associated hardware definition are shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. The operant box was enclosed by a sound attenuating chamber, custom made in 20 mm melamine-coated MDF, adapted from a design in the <ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/hardware/tree/master/Sound_attenuating_chamber_small">hardware repository</ext-link>. The pyControl breakout boards, and other PCBs that were not integrated into the box itself, were mounted on the outside of the sound attenuating chamber, and a CCTV camera was mounted on the ceiling to monitor behaviour.</p></sec><sec id="s4-5"><title>5-CSRTT training</title><p>The 5-CSRTT training protocol was similar to what we described previously (<xref ref-type="bibr" rid="bib15">Grimm et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">van der Veen et al., 2021</xref>). In brief, after initiation of food restriction, mice were accustomed to the reward (strawberry milk, Müllermilch, G) in their home cage and in the operant box (2–3 exposures each). Then, mice were trained on a simplified operant cycle in which all holes of the 5-poke wall were illuminated for an unlimited time, and the mouse could poke into any one of them to illuminate the reward receptacle on the opposite wall and dispense a 40 μl milk reward. Once mice attained at least 30 rewards each in two consecutive sessions, they were moved to the 5-CSRTT.</p><p>During 5-CSRTT training, mice transitioned through five stages of increasing difficulty, based on reaching performance criteria in each stage (<xref ref-type="table" rid="table2">Table 2</xref>). The difficulty of each stage was determined by the length of time the stimulus was presented (SD) and the length of the ITI between the end of the previous trial and the stimulus presentation on the next trial.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>5-choice serial reaction time task (5-CSRTT) training and challenge stages.</title><p>The parameters stimulus duration (SD) and inter-trial interval (ITI, waiting time before stimulus) are listed for each of the five training stages (S1–5) and the subsequent challenge protocols on which performance was tested for 1 day each (C1–5). For the training stages, performance criteria which had to be met by an animal on two consecutive days to move to the next stage are listed on the right. See Materials and methods for the definition of these performance parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="9" valign="bottom"><break/>5-CSRTT training</th></tr></thead><tbody><tr><td align="left" valign="bottom"> </td><td align="left" colspan="3" valign="bottom"><bold>Task parameters</bold></td><td align="left" colspan="5" valign="bottom"><bold>Criteria for stage transition (two consecutive days)</bold></td></tr><tr><td align="left" valign="bottom"><bold>Stage</bold></td><td align="left" colspan="2" valign="bottom"><bold>SD (s)</bold></td><td align="left" valign="bottom"><bold>ITI (s)</bold></td><td align="left" valign="bottom"><bold># correct</bold></td><td align="char" char="." valign="bottom"><bold>% correct</bold></td><td align="char" char="." colspan="2" valign="bottom"><bold>% accuracy</bold></td><td align="char" char="." valign="bottom"><bold>%omissions</bold></td></tr><tr><td align="left" valign="bottom">S1</td><td align="char" char="." colspan="2" valign="bottom">20</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">≥30</td><td align="char" char="." valign="bottom">≥40</td><td align="left" valign="bottom">-</td><td align="left" colspan="2" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">S2</td><td align="char" char="." colspan="2" valign="bottom">8</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">≥40</td><td align="char" char="." valign="bottom">≥50</td><td align="left" valign="bottom">-</td><td align="left" colspan="2" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">S3</td><td align="char" char="." colspan="2" valign="bottom">8</td><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">≥80</td><td align="char" char="." colspan="2" valign="bottom">≤50</td></tr><tr><td align="left" valign="bottom">S4</td><td align="char" char="." colspan="2" valign="bottom">4</td><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">≥80</td><td align="char" char="." colspan="2" valign="bottom">≤50</td></tr><tr><td align="left" valign="bottom">S5</td><td align="char" char="." colspan="2" valign="bottom">2</td><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">≥80</td><td align="char" char="." colspan="2" valign="bottom">≤50</td></tr><tr><td align="left" colspan="9" valign="bottom"><bold>Challenges</bold></td></tr><tr><td align="left" colspan="2" valign="bottom">C1</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">9</td><td align="left" colspan="5" valign="bottom">Impulsivity challenge</td></tr><tr><td align="left" colspan="2" valign="bottom">C2</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">5</td><td align="left" colspan="5" valign="bottom">Attention challenge 1</td></tr><tr><td align="left" colspan="2" valign="bottom">C3</td><td align="char" char="." valign="bottom">0.8</td><td align="char" char="." valign="bottom">5</td><td align="left" colspan="5" valign="bottom">Attention challenge 2</td></tr><tr><td align="left" colspan="2" valign="bottom">C4</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">5</td><td align="left" colspan="5" valign="bottom">Distraction: 1 s white noise within 0.5–4.5 s of ITI</td></tr><tr><td align="left" colspan="2" valign="bottom">C5</td><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">7, 9, 11, 13</td><td align="left" colspan="5" valign="bottom">Variable ITI: pseudo-random, equal distribution</td></tr></tbody></table></table-wrap><p>The ITI was initiated when the subject exited the reward receptacle after collection of a reward or by the end of a timeout period (see below). The ITI was followed by illumination of one hole on the 5-choice wall for the SD determined by the training stage. A poke in the correct port during the stimulus, or during a subsequent 2 s hold period, was counted as a <italic>correct response,</italic> illuminating the reward receptacle and dispensing 20 μl of milk. If the subject either poked into any hole during the ITI (<italic>premature response</italic>), poked into a non-illuminated hole during the SD or hold period (<italic>incorrect response</italic>), or failed to poke during the trial (<italic>omission</italic>), the trial was not rewarded but instead terminated with a 5 s timeout during which the house light was turned off. The relative numbers of each response type were used as performance indicators measuring premature responding [<italic>%premature</italic> = 100 * (number of premature responses)/(number of trials)], sustained attention [accuracy = 100 * (number of correct responses)/(number of correct and incorrect responses)], and lack of participation [<italic>%omissions</italic> = 100 * (number of omissions)/(number of trials)]. In all stages and tests, sessions lasted 30 min and were performed once daily at the same time of day.</p><p>Test days with behavioural challenges were interleaved with at least one training day on the baseline stage (stage 5; see <xref ref-type="table" rid="table2">Table 2</xref> for parameters of all stages). For pharmacological validation, atomoxetine (Tomoxetine hydrochloride, Tocris, UK) diluted in sterile saline (0.2 mg/ml) or saline vehicle were injected i.p. at 10 μl/g mouse injection volume 30 min before testing started. For atomoxetine vs. vehicle within-subject comparison, two tests were conducted separated by 1 week, whereby four animals received atomoxetine on the first day, while the other four received vehicle and vice versa for the second day. Effects of challenges (compared to performance on the prior day with baseline training) and atomoxetine (compared to performance under vehicle) were assessed by paired-samples <italic>t</italic>-tests. Behavioural data gathered in the 5-CSRTT was analysed with Excel and SPSS26.0 (IBM Inc, US).</p></sec><sec id="s4-6"><title>Vibrissae-based object localisation task</title><sec id="s4-6-1"><title>Animals</title><p>Subjects were three female mice expressing the calcium-sensitive protein GCaMP6s in excitatory neurons, derived by mating the floxed Ai94(TITL-GCaMP6s)-D line (Jackson Laboratories; stock number 024742) with the CamKII-tta (Jackson Laboratories; stock number 003010). Animal husbandry and experimental procedures were approved and conducted in accordance with the United Kingdom Animals (Scientific Procedures) Act 1986 under project licence P8E8BBDAD and personal licences from the Home Office.</p></sec></sec><sec id="s4-7"><title>Behavioural hardware</title><p>Mice were head-fixed on a treadmill fashioned from a 24 cm diameter Styrofoam cylinder covered with 1.5-mm-thick neoprene. An incremental optical encoder (Broadcom HEDS-5500#A02; RS Components) was used in conjunction with a pyControl rotary encoder adapter to monitor mouse running speed. The pole used for object detection was a blunt 18G needle mounted, via a 3d-printed arm, onto a stepper motor (RS PRO Hybrid 535-0467; RS Components). The stepper motor was mounted onto a motorised linear stage (DDSM100/M; Thorlabs) used to move the pole towards and away from the whisker pad (controlled by a K-Cube Brushless DC Servo Driver [KBD101; Thorlabs]). The pyControl hardware used and the associated hardware definition are shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>.</p></sec><sec id="s4-8"><title>Surgery</title><p>6- to 10-week-old mice were anaesthetised with isoflurane (0.8–1.2% in 1 l/min oxygen) and implanted with custom titanium headplates for head fixation and 4 mm diameter cranial windows for imaging as described previously (<xref ref-type="bibr" rid="bib11">Chong et al., 2019</xref>). Peri- and postoperative analgesia was used (meloxicam 5 mg/kg and buprenorphine 0.1 mg/kg), and mice were carefully monitored for 7 days post surgery.</p></sec><sec id="s4-9"><title>Behavioural training</title><p>Following recovery from surgery, mice were habituated to head fixation (<xref ref-type="bibr" rid="bib11">Chong et al., 2019</xref>) prior to training on the vibrissa-based object localisation task as detailed in the ‘Results’ section. Data were analysed using MATLAB (MathWorks).</p></sec><sec id="s4-10"><title>Social decision-making task</title><sec id="s4-10-1"><title>Animals</title><p>12 male C57BL6/J mice (Charles River, France) were used, aged 3 months at the beginning of the experiment. Animals were group-housed (four animals per cage) and maintained with ad libitum access to food and water in a 12–12 hr reversed light cycle (lights off at 8 am) at the Animal Facility of the Instituto de Neurociencias of Alicante. Short food restrictions (2 hr before the behavioural testing) were performed in the early phases of individual training to increase motivation for food-seeking behaviour, otherwise animals were tested with ad libitum chow available in their home cage. All experimental procedures were performed in compliance with institutional Spanish and European regulations, as approved by the Universidad Miguel Hernández Ethics committee.</p></sec></sec><sec id="s4-11"><title>Behavioural hardware</title><p>The social decision-making task was performed in a double maze, where two animals, the focal and the recipient, would interact and work to obtain food rewards. The outer walls of the double maze were of white laser-cut acrylic. Each double maze was divided by a transparent and perforated wall creating the individual mazes for each mouse. For each individual maze, inner walls separating central choice and side reward areas contained the mechanisms for sliding doors, 3D-printed nose-pokes, and position detectors. These inner walls were made of transparent laser-cut acrylic in order to allow visibility of the animal in the side arms of the maze. Walls of the central choice area were frosted to avoid reflections that could interfere with automated pose estimation of the interacting animals in this area.</p><p>Each double T-maze behavioural setup was positioned inside a custom-made sound isolation box, with an infrared-sensitive camera (PointGrey Flea3-U3-13S2M CS, Canada) positioned above the maze to track the animals’ location. The chamber was illuminated with dim white light (4 lux) and infrared illumination located on the ceiling of the sound attenuating chamber. The pyControl hardware configuration and associated hardware definition file are shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>. Food pellet rewards were dispensed using pellet dispensers made of 3D-printed and laser-cut parts actuated by a stepper motor (NEMA 42HB34F08AB, e-ika electrónica y robótica, Spain) controlled by a pyControl stepper driver board, placed outside the sound isolation box and delivering the pellets to the 3D-printed food receptacles through a silicon tube. Design files for the pellet dispenser and receptacles are at <ext-link ext-link-type="uri" xlink:href="https://github.com/MarquezLab/Hardware">https://github.com/MarquezLab/Hardware</ext-link>; <xref ref-type="bibr" rid="bib30">Marquez, 2021</xref>. The sliding doors that control access to the side arms were actuated by pneumatic cylinders (Cilindro ISO 6432, Vestonn Pneumatic, Spain) placed below the base of the maze, providing silent and smooth horizontal movement of the doors. These were in turn controlled via solenoid valves (8112005201, Vestonn Pneumatic) interfaced with pyControl using an optocoupled relay board (Cebek-T1, Fadisel, Spain). The speed of the opening/closing of the doors could be independently regulated by adjusting the pressure of the compressed air to the solenoid valves.</p></sec><sec id="s4-12"><title>Behavioural training</title><p>Individual training and social decision-making protocols are described in the ‘Results’ section. All behavioural experiments were performed during the first half of the dark phase of the cycle. Data were analysed with Python (Python Software Foundation, v3.6.5), and statistical analysis was performed with IBM SPSS Statistics (version 26).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>Consulting contract with Open Ephys Production Site</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf3"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Software, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Software, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Funding acquisition, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Funding acquisition, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Funding acquisition, Resources, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The 5-CSRTT experiment was performed in accordance to the German Animal Rights Law (Tierschutzgesetz) 2013 and approved by the Federal Ethical Review Committee (Regierungsprädsidium Tübingen) of Baden-Württemberg. The Vibrissae-based object localisation experiment was conducted in accordance with the United Kingdom Animals (Scientific Procedures) Act 1986 under project license P8E8BBDAD and personal licenses from the Home Office. The Social decision making experiment was performed in compliance with institutional Spanish and European regulations, as approved by the Universidad Miguel Hernández Ethics committee.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-67846-transrepform1-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>pyControl task files for all experiments, and data and analysis code for the performance validation experiments (Figure 5), are included in the manuscript's data repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/manuscript">https://github.com/pyControl/manuscript</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:781f5f12bd1e8f8214a171b74ea98408be4f0f6e;origin=https://github.com/pyControl/manuscript;visit=swh:1:snp:546190bca556c98edbd6bc85849fcf75d154f884;anchor=swh:1:rev:6be55c29ec0520a61099d25e944b30c9a3bede9b">swh:1:rev:6be55c29ec0520a61099d25e944b30c9a3bede9b</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>TA thanks current and former members of the Champalimaud hardware and software platforms; Jose Cruz, Ricardo Ribeiro, Carlos Mão de Ferro, and Matthieu Pasquet for discussions and technical assistance, and Filipe Carvalho and Lídia Fortunato of Open Ephys Production Site for hardware assembly and distribution. CM thanks Victor Rodriguez for assistance in developing the social decision-making apparatus. MP and MK thank Dr Ana Carolina Bottura de Barros and Dr Severin Limal for assistance with the Vibrissae-based object localisation task.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Point_Grey_Bonsai_multi_camera_acquisition</data-title><version designator="c83b4a5">c83b4a5</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ThomasAkam/Point_Grey_Bonsai_multi_camera_acquisition">https://github.com/ThomasAkam/Point_Grey_Bonsai_multi_camera_acquisition</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>pyControl / manuscript</data-title><version designator="6be55c2">6be55c2</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/pyControl/manuscript">https://github.com/pyControl/manuscript</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Rodrigues-Vaz</surname><given-names>I</given-names></name><name><surname>Marcelo</surname><given-names>I</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Pereira</surname><given-names>M</given-names></name><name><surname>Oliveira</surname><given-names>RF</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Anterior Cingulate Cortex Predicts Future States to Mediate Model-Based Action Selection</article-title><source>Neuron</source><volume>109</volume><fpage>149</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.10.013</pub-id><pub-id pub-id-type="pmid">33152266</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayaz</surname><given-names>A</given-names></name><name><surname>Stäuble</surname><given-names>A</given-names></name><name><surname>Hamada</surname><given-names>M</given-names></name><name><surname>Wulf</surname><given-names>MA</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Layer-specific integration of locomotion and sensory information in mouse barrel cortex</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2585</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10564-8</pub-id><pub-id pub-id-type="pmid">31197148</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>1,500 scientists lift the lid on reproducibility</article-title><source>Nature</source><volume>533</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1038/533452a</pub-id><pub-id pub-id-type="pmid">27225100</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bari</surname><given-names>A</given-names></name><name><surname>Dalley</surname><given-names>JW</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The application of the 5-choice serial reaction time task for the assessment of visual attentional processes and impulse control in rats</article-title><source>Nature Protocols</source><volume>3</volume><fpage>759</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1038/nprot.2008.41</pub-id><pub-id pub-id-type="pmid">18451784</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhagat</surname><given-names>J</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Burgess</surname><given-names>CP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rigbox: An Open-Source Toolbox for Probing Neurons and Behavior</article-title><source>ENeuro</source><volume>7</volume><elocation-id>ENEURO.0406-19.2020</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0406-19.2020</pub-id><pub-id pub-id-type="pmid">32493756</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Blanco-Pozo</surname><given-names>M</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Walton</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dopamine Reports Reward Prediction Errors, but Does Not Update Policy, during Inference-Guided Choice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.25.449995</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buscher</surname><given-names>N</given-names></name><name><surname>Ojeda</surname><given-names>A</given-names></name><name><surname>Francoeur</surname><given-names>M</given-names></name><name><surname>Hulyalkar</surname><given-names>S</given-names></name><name><surname>Claros</surname><given-names>C</given-names></name><name><surname>Tang</surname><given-names>T</given-names></name><name><surname>Terry</surname><given-names>A</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Fakhraei</surname><given-names>L</given-names></name><name><surname>Ramanathan</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Open-source raspberry Pi-based operant box for translational behavioral testing in rodents</article-title><source>Journal of Neuroscience Methods</source><volume>342</volume><elocation-id>108761</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108761</pub-id><pub-id pub-id-type="pmid">32479970</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carli</surname><given-names>M</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Evenden</surname><given-names>JL</given-names></name><name><surname>Everitt</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Effects of lesions to ascending noradrenergic neurones on performance of a 5-choice serial reaction task in rats; implications for theories of dorsal noradrenergic bundle function based on selective attention and arousal</article-title><source>Behavioural Brain Research</source><volume>9</volume><fpage>361</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(83)90138-9</pub-id><pub-id pub-id-type="pmid">6639741</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chong</surname><given-names>EZ</given-names></name><name><surname>Panniello</surname><given-names>M</given-names></name><name><surname>Barreiros</surname><given-names>I</given-names></name><name><surname>Kohl</surname><given-names>MM</given-names></name><name><surname>Booth</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Quasi-simultaneous multiplane calcium imaging of neuronal circuits</article-title><source>Biomedical Optics Express</source><volume>10</volume><fpage>267</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1364/BOE.10.000267</pub-id><pub-id pub-id-type="pmid">30775099</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>de Barros</surname><given-names>ACB</given-names></name><name><surname>Baruchin</surname><given-names>LJ</given-names></name><name><surname>Panayi</surname><given-names>MC</given-names></name><name><surname>Nyberg</surname><given-names>N</given-names></name><name><surname>Samborska</surname><given-names>V</given-names></name><name><surname>Mealing</surname><given-names>MT</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Kwag</surname><given-names>J</given-names></name><name><surname>Bannerman</surname><given-names>DM</given-names></name><name><surname>Kohl</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Retrosplenial Cortex Is Necessary for Spatial and Non-Spatial Latent Learning in Mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.21.453258</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devarakonda</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>KP</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>ROBucket: A low cost operant chamber based on the Arduino microcontroller</article-title><source>Behavior Research Methods</source><volume>48</volume><fpage>503</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.3758/s13428-015-0603-2</pub-id><pub-id pub-id-type="pmid">26019006</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzpatrick</surname><given-names>CM</given-names></name><name><surname>Andreasen</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Differential effects of ADHD medications on impulsive action in the mouse 5-choice serial reaction time task</article-title><source>European Journal of Pharmacology</source><volume>847</volume><fpage>123</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/j.ejphar.2019.01.038</pub-id><pub-id pub-id-type="pmid">30690006</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimm</surname><given-names>CM</given-names></name><name><surname>Aksamaz</surname><given-names>S</given-names></name><name><surname>Schulz</surname><given-names>S</given-names></name><name><surname>Teutsch</surname><given-names>J</given-names></name><name><surname>Sicinski</surname><given-names>P</given-names></name><name><surname>Liss</surname><given-names>B</given-names></name><name><surname>Kätzel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Schizophrenia-related cognitive dysfunction in the Cyclin-D2 knockout mouse model of ventral hippocampal hyperactivity</article-title><source>Translational Psychiatry</source><volume>8</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1038/s41398-018-0268-6</pub-id><pub-id pub-id-type="pmid">30301879</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurley</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Two open source designs for a low-cost operant chamber using Raspberry Pi</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>111</volume><fpage>508</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1002/jeab.520</pub-id><pub-id pub-id-type="pmid">31038195</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>International Brain Laboratory</collab><name><surname>Aguillon-Rodriguez</surname><given-names>V</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Bayer</surname><given-names>H</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Cazettes</surname><given-names>F</given-names></name><name><surname>Chapuis</surname><given-names>G</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Dewitt</surname><given-names>E</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Forrest</surname><given-names>H</given-names></name><name><surname>Haetzel</surname><given-names>L</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Krasniak</surname><given-names>C</given-names></name><name><surname>Laranjeira</surname><given-names>I</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Meijer</surname><given-names>G</given-names></name><name><surname>Miska</surname><given-names>NJ</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Murakami</surname><given-names>M</given-names></name><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Pan-Vazquez</surname><given-names>A</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sanders</surname><given-names>J</given-names></name><name><surname>Socha</surname><given-names>K</given-names></name><name><surname>Terry</surname><given-names>R</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Vergara</surname><given-names>H</given-names></name><name><surname>Wells</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>CJ</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Wool</surname><given-names>LE</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Standardized and reproducible measurement of decision-making in mice</article-title><source>eLife</source><volume>10</volume><elocation-id>e63711</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63711</pub-id><pub-id pub-id-type="pmid">34011433</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kaetzell</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Operant-Box-Design-Files</data-title><version designator="673cb71">673cb71</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/KaetzelLab/Operant-Box-Design-Files">https://github.com/KaetzelLab/Operant-Box-Design-Files</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kapanaiah</surname><given-names>SKT</given-names></name><name><surname>van der Veen</surname><given-names>B</given-names></name><name><surname>Strahnen</surname><given-names>D</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Kätzel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A low-cost open-source 5-choice operant box system optimized for electrophysiology and optophysiology in mice</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>22279</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-01717-1</pub-id><pub-id pub-id-type="pmid">34782697</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Karpova</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Karpova-Lab</data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Karpova-Lab">https://github.com/Karpova-Lab</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilonzo</surname><given-names>K</given-names></name><name><surname>van der Veen</surname><given-names>B</given-names></name><name><surname>Teutsch</surname><given-names>J</given-names></name><name><surname>Schulz</surname><given-names>S</given-names></name><name><surname>Kapanaiah</surname><given-names>SKT</given-names></name><name><surname>Liss</surname><given-names>B</given-names></name><name><surname>Kätzel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Delayed-matching-to-position working memory in mice relies on NMDA-receptors in prefrontal pyramidal cells</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>8788</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-88200-z</pub-id><pub-id pub-id-type="pmid">33888809</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>B</given-names></name><name><surname>Kenchappa</surname><given-names>SC</given-names></name><name><surname>Sunkara</surname><given-names>A</given-names></name><name><surname>Chang</surname><given-names>TY</given-names></name><name><surname>Thompson</surname><given-names>L</given-names></name><name><surname>Doudlah</surname><given-names>R</given-names></name><name><surname>Rosenberg</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Real-time experimental control using network-based parallel processing</article-title><source>eLife</source><volume>8</volume><elocation-id>e40231</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.40231</pub-id><pub-id pub-id-type="pmid">30730290</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Koralek</surname><given-names>AC</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sustained Dopaminergic Plateaus and Noradrenergic Depressions Mediate Dissociable Aspects of Exploitative States</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/822650</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korn</surname><given-names>C</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Jensen</surname><given-names>KHR</given-names></name><name><surname>Vagnoni</surname><given-names>C</given-names></name><name><surname>Huber</surname><given-names>A</given-names></name><name><surname>Tunbridge</surname><given-names>EM</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Distinct roles for dopamine clearance mechanisms in regulating behavioral flexibility</article-title><source>Molecular Psychiatry</source><volume>8</volume><elocation-id>01194</elocation-id><pub-id pub-id-type="doi">10.1038/s41380-021-01194-y</pub-id><pub-id pub-id-type="pmid">34193974</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience Needs Behavior: Correcting a Reductionist Bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Frazão</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><name><surname>Medina</surname><given-names>RE</given-names></name><name><surname>Calcaterra</surname><given-names>L</given-names></name><name><surname>Dreosti</surname><given-names>E</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maia Chagas</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Haves and have nots must find a better way: The case for open scientific hardware</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e3000014</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000014</pub-id><pub-id pub-id-type="pmid">30260950</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The haves and the have nots</article-title><source>eLife</source><volume>2</volume><elocation-id>e01515</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.01515</pub-id><pub-id pub-id-type="pmid">24252880</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Márquez</surname><given-names>C</given-names></name><name><surname>Rennie</surname><given-names>SM</given-names></name><name><surname>Costa</surname><given-names>DF</given-names></name><name><surname>Moita</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prosocial Choice in Rats Depends on Food-Seeking Behavior Displayed by Recipients</article-title><source>Current Biology</source><volume>25</volume><fpage>1736</fpage><lpage>1745</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.05.018</pub-id><pub-id pub-id-type="pmid">26051895</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Marquez</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>MarquezLab / Hardware</data-title><version designator="e1928bf">e1928bf</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/MarquezLab/Hardware">https://github.com/MarquezLab/Hardware</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarra</surname><given-names>R</given-names></name><name><surname>Graf</surname><given-names>R</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Logue</surname><given-names>S</given-names></name><name><surname>Comery</surname><given-names>T</given-names></name><name><surname>Hughes</surname><given-names>Z</given-names></name><name><surname>Day</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of atomoxetine and methylphenidate on attention and impulsivity in the 5-choice serial reaction time test</article-title><source>Progress in Neuro-Psychopharmacology &amp; Biological Psychiatry</source><volume>32</volume><fpage>34</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.pnpbp.2007.06.017</pub-id><pub-id pub-id-type="pmid">17714843</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>A</given-names></name><name><surname>Abdelmesih</surname><given-names>B</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Corticospinal Neurons Encode Complex Motor Signals That Are Broadcast to Dichotomous Striatal Circuits</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.31.275180</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Clack</surname><given-names>NG</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name><name><surname>Myers</surname><given-names>EW</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Vibrissa-based object localization in head-fixed mice</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>1947</fpage><lpage>1967</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3762-09.2010</pub-id><pub-id pub-id-type="pmid">20130203</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Leary</surname><given-names>JD</given-names></name><name><surname>Leary</surname><given-names>OF</given-names></name><name><surname>Cryan</surname><given-names>JF</given-names></name><name><surname>Nolan</surname><given-names>YM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A low-cost touchscreen operant chamber using a Raspberry Pi</article-title><source>Behavior Research Methods</source><volume>50</volume><fpage>2523</fpage><lpage>2530</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1030-y</pub-id><pub-id pub-id-type="pmid">29520633</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paterson</surname><given-names>NE</given-names></name><name><surname>Ricciardi</surname><given-names>J</given-names></name><name><surname>Wetzler</surname><given-names>C</given-names></name><name><surname>Hanania</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sub-optimal performance in the 5-choice serial reaction time task in rats was sensitive to methylphenidate, atomoxetine and d-amphetamine, but unaffected by the COMT inhibitor tolcapone</article-title><source>Neuroscience Research</source><volume>69</volume><fpage>41</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2010.10.001</pub-id><pub-id pub-id-type="pmid">20934466</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillidge</surname><given-names>K</given-names></name><name><surname>Porter</surname><given-names>AJ</given-names></name><name><surname>Vasili</surname><given-names>T</given-names></name><name><surname>Heal</surname><given-names>DJ</given-names></name><name><surname>Stanford</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Atomoxetine reduces hyperactive/impulsive behaviours in neurokinin-1 receptor “knockout” mice</article-title><source>Pharmacology, Biochemistry, and Behavior</source><volume>127</volume><fpage>56</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.pbb.2014.10.008</pub-id><pub-id pub-id-type="pmid">25450119</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Samborska</surname><given-names>V</given-names></name><name><surname>Butler</surname><given-names>JL</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Complementary Task Representations in Hippocampus and Prefrontal Cortex for Generalising the Structure of Problems</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.05.433967</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Saunders</surname><given-names>JL</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Autopilot: Automating Behavioral Experiments with Lots of Raspberry Pis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/807693</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Strahnen</surname><given-names>D</given-names></name><name><surname>Kapanaiah</surname><given-names>SKT</given-names></name><name><surname>Bygrave</surname><given-names>AM</given-names></name><name><surname>Liss</surname><given-names>B</given-names></name><name><surname>Bannerman</surname><given-names>DM</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Grewe</surname><given-names>BF</given-names></name><name><surname>Johnson</surname><given-names>EL</given-names></name><name><surname>Kätzel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Highly Task-Specific and Distributed Neural Connectivity in Working Memory Revealed by Single-Trial Decoding in Mice and Humans</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.04.20.440621</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Veen</surname><given-names>B</given-names></name><name><surname>Kapanaiah</surname><given-names>SKT</given-names></name><name><surname>Kilonzo</surname><given-names>K</given-names></name><name><surname>Steele-Perkins</surname><given-names>P</given-names></name><name><surname>Jendryka</surname><given-names>MM</given-names></name><name><surname>Schulz</surname><given-names>S</given-names></name><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Nicholson</surname><given-names>JR</given-names></name><name><surname>Liss</surname><given-names>B</given-names></name><name><surname>Nissen</surname><given-names>W</given-names></name><name><surname>Pekcec</surname><given-names>A</given-names></name><name><surname>Kätzel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Control of impulsivity by G<sub>i</sub>-protein signalling in layer-5 pyramidal neurons of the anterior cingulate cortex</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>662</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02188-w</pub-id><pub-id pub-id-type="pmid">34079054</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.67846.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Rice University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The importance of carefully-considered animal behavior to systems neuroscience cannot be overstated. Despite this, flexible tools for carefully monitoring and controlling behavioral apparatuses have often required significant new development by individual laboratories. The open source pyControl software and hardware toolbox is an excellent exemplar of a robust and reliable platform for experiments, with a simple interface, good performance, excellent documentation, and a growing an engaged user community. This work benchmarks and documents pyControl and hopefully will serve as a useful introduction to an even broader community.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.67846.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution>Rice University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewer</role><aff><institution>Rice University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Siegle</surname><given-names>Josh</given-names></name><role>Reviewer</role><aff><institution>Allen Institute</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for sending your article entitled &quot;pyControl: Open source, Python based, hardware and software for controlling behavioural neuroscience experiments&quot; for peer review at <italic>eLife</italic>. Your article is being evaluated by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation is being overseen by Kate Wassum as the Senior Editor.</p><p>The reviewers were unanimous in recognizing that the pyControl hardware/software suite is a useful tool. Indeed, the impressive list of current users makes that point. However, we felt that in order for the manuscript to merit publication, it needed to be more clear and transparent in presentation such that readers who are not current users would be convinced (by the material presented) that its usability – when compared with alternatives – is such that they should adopt it. By usability, we mean both (i) how easy it is to use for actual tasks and (ii) whether there are scenarios where task parameters or coding naivety make it fragile.</p><p><italic>Reviewer #1:</italic></p><p>Positives:</p><p>Most behavior systems take one of two broad approaches to the system configuration: run the task logic and data I/O through the host computer, which is connected to peripherals; or run the task logic and data I/O through an integrated external breakout board and microcontroller. Here, the authors have opted for the latter approach, arguing that it confers several advantages: 1) inexpensiveness, 2) flexibility, 3) user-friendliness, and 4) parallelism.</p><p>Certainly, microcontrollers (and their commercial or custom breakout boards) are a less expensive counterpart to desktop computers, albeit at the expense of lower processing power and memory. For experiments that require more computation, this platform may not be the best fit, which the authors acknowledge in their discussion. However, for many experiments, a microcontroller is sufficient, and thus the approach pyControl takes is reasonable.</p><p>The flexibility of the system is apparent in both the hardware and software. The custom breakout board offers a mixture of digital and analog I/O ports, as well as power supply lines for different peripheral devices. On the software side, the choice of state machine logic allows it to be broadly applicable. The ability to save both input/output streams and internal variables with timestamps is important.</p><p>Major Concern:</p><p>pyControl is a combination of hardware, API code, &quot;task code&quot;, and control-computer code. For publication, a paper should go beyond merely showing a few tasks where code was used – it needs to not only demonstrate utility, but also document the system in ways beyond a user guide. While demonstrating the efficacy and reliability of the &quot;task code&quot; is the primary goal of this paper, it fails to do so convincingly.</p><p>0) Given the advantage of microcontroller-based behavioral control, the primary question facing this paper is: &quot;Why should a lab deal with pyControl rather than rolling their own solution?&quot; It seems like the easiest way to demonstrate this would be to show, e.g., an Arduino solution and a pyControl solution side by side so that it can be seen how obviously easier it is.</p><p>1) Figure 1 is confusing to the reader where it appears:</p><p>Figure 1 shows an example of running a simple &quot;blink LED&quot; task, and while the right panel is a nice visual complement to the task code, the actual task definition code on the left is at first unclear. Where is the &quot;button_press&quot; event (or any other event) defined when it becomes input to the task functions? Where do Digital_input() and Digital_output() come from? How are &quot;entry&quot; and &quot;exit&quot; events defined? (As an aside, it is usually best to avoid &quot;import *&quot; statements, as they can at best be confusing and at worst cause namespace conflicts. In this case, it is unclear which of the other module classes/functions came from &quot;pyControl.utility&quot; vs. &quot;devices&quot;.) Perhaps it would be best to first introduce the overall organization of the hardware (Figure 2, 3), and then explain the task logic that is running on the microcontroller, which itself requires an understanding of how hardware is defined in the first place.</p><p>2) Hardware is given short shrift. The hardware definition examples in the supplementary figures seem superfluous. The configuration diagram are useful visuals for understanding how components are wired together, but the code underneath is mostly repetitive and can be left for the interested reader to find in the online documentation. Critically, what is missing is a broader explanation of how hardware devices are defined – both from an API perspective and in terms of the physical connection – that would allow the user to design their own.</p><p>3) Links to documentation do not belong in a scientific paper: Choosing a higher-level language like (micro)python makes the system user-friendly and separate from proprietary software like MATLAB. Additionally, the use of a GUI facilitates understanding of the task code and logic. The authors have clearly spent time building an extensive online documentation that is valuable for reaching a broad user base. However, the assumption of the reviewer is that our journal is more reliably archival than random sites on the internet. So anything that is relevant to understanding the paper should be included. (It may be possible to archive the code base along with the paper, which would solve this issue.)</p><p>4) There is significant concern about performance and reliability. What happens if two state transitions are triggered simultaneously? There is a description of an event queue: does the queue generation code automatically infer whether queued events are still valid – are race conditions possible? (Simple example – a triggered state transition that should occur if a timer has not elapsed, but a different set of states occurs if the timer has elapsed. What happens if the timer elapses and the trigger occur nearly simultaneously?)</p><p>5) There is concern about the choice of Micropython rather than a compiled language. In particular, it is unclear whether the performance testing adequately explored garbage collection. For example, if a naive user instantiates a timer in a state that is executed repeatedly, garbage collection will be regularly required to delete the relevant heap memory. If garbage collection is slow, this could result in unexpected loss of responsiveness or missed logging of data.</p><p>6) From the text, it appears that data are timestamped at 1 kHz, but the figures depict microsecond resolution. How fast can events occur? Most experiments will have events on the scale of second or 100s of milliseconds, but a naive user might instantiate a state tracking licks, which can occur much more rapidly when a mouse licks in a bout.</p><p><italic>Reviewer #2:</italic></p><p>The authors have presented a novel hardware/software solution called pyControl for coordinating common types of inputs and outputs in rodent behavioral experiments. It can be also be used to program tasks with other model organisms, including humans, but many of the peripheral devices (such as the nose-poke and lickometer) are specifically designed for use with rodents.</p><p>A key advantage of pyControl over other solutions in this space is the ability to program both the software and the embedded firmware in the same high-level language, Python. This allows users to have precise control of the system behavior without the need to learn a lower-level programming language, such as C++. It is also a highly cost effective solution, especially in comparison to closed-source platforms with similar functionality.</p><p>A major selling point of the pyControl ecosystem is the wide range of commercially available peripheral devices. For those considering adopting pyControl for their own work, it would be helpful if this manuscript included a more thorough characterization of the existing peripherals. Some of these devices are described in the &quot;application examples&quot; section, but there is almost no information about their specifications. It would also be useful to see an outline of the steps required to build a custom peripheral device.</p><p>To characterize the performance of pyControl, the authors have measured its response latency and timing accuracy under both &quot;low load&quot; and &quot;high load&quot; conditions. Even under conditions of high load (monitoring two highly active digital inputs and sampling two analog inputs at 1 kHz), the overwhelming majority of latencies were &lt;1 ms, which is sufficient for most applications. In cases where very precisely timed stimuli are required, pyControl can be easily configured to send triggers to external devices.</p><p>Overall, pyControl is a welcome addition to the ever-growing list of tools for coordinating behavioral tasks. Labs that currently rely on undocumented, DIY solutions may find that pyControl provides the impetus to switch to a standardized, open-source platform. And researchers that depend on closed-source tools have the opportunity to save many thousands of dollars per year by using pyControl for their future rigs.</p><p>This manuscript will be a great resource for researchers that are considering adopting pyControl. I have a few suggestions for making it even better:</p><p>I'd like to see a more candid discussion of the limitations of pyControl for various types of experiments. For example, running everything on a microcontroller precludes the display of complex visual stimuli. Therefore, pyControl would not be suitable for most visual physiology experiments. Similarly, the pyControl Audio Player peripheral can only deliver stimuli up to 48 kHz, which limits its utility for rodent auditory physiology. pyControl of course includes general-purpose output ports that can be used to trigger any auxiliary devices, so these limitations could be overcome with some customization.</p><p>The authors should include more details about the pyboard microcontroller, since this is an essential component of the platform. What are the specifications, approximate cost, etc.? Is there a plan to take advantage of the pyBoard's wireless capabilities? This could be helpful for coordinating massively parallel experiments.</p><p>The &quot;Karpova Lab&quot; is mentioned out of context. Readers will likely want more information about what this means.</p><p>Since Bonsai does not itself encompass any hardware interfaces, a more accurate comparison would be with the combination of Bonsai and Harp (https://www.cf-hw.org/harp). Unfortunately, Harp doesn't have a publication associated with it, nor much online documentation. But the main advantages are shared timestamps (within 44 µs) across all devices, and tight integration with Bonsai software. The authors should mention the availability of Harp in their discussion of Bonsai.</p><p>Another behavior control platform that should be cited is Autopilot (https://www.biorxiv.org/content/10.1101/807693v1), which looks promising for certain applications, despite its relative immaturity.</p><p>A detailed characterization of closed-source commercial hardware is beyond the scope of this manuscript, but the authors should consider presenting at least a high-level comparison of these tools (in terms of order-of-magnitude cost, available features, and customizability). The discussion seems to be targeted at users choosing between various open-source solutions, but convincing researchers to adopt pyControl in favor of closed-source hardware is a much more important goal.</p><p><italic>Reviewer #3:</italic></p><p>Akam et al. detail an open-source system called pyControl, that is intended to be used to control behavioral neuroscience experiments. The authors nicely highlight the current problem of reproducibility in behavioral neuroscience, describing the difficulty of standardizing or understanding code and behavioral tasks across labs. It is quite clear the project is setting new standards, especially in the open-source neuroscience field, by addressing many pitfalls of previous behavioral systems and platforms. Thus, the major strength of the paper is the open-source manner with which the authors detail their system. The hardware and software provided in an open manner allows for researchers to streamline their behavioral tasks in a cost-effective manner while still maintaining the flexibility needed to perform specific experiments. The authors provide rather straightforward documentation of how to program and run behavioral experiments using pyControl software and the related GUI, as well as how the hardware is generally set up. The authors then validate the use of pyControl in three different behavioral tasks that are widely used in neuroscience research.</p><p>Strengths: The pyControl system itself serves the purpose of having one streamlined behavioral setup that can provide reproducibility across research labs, the system can seemingly be used by &quot;novice&quot; researchers with a beginner level skill set in both hardware and computer programming, and comes at a cheaper price than commercial (or other open source) behavior platforms. The project's website, user forum, and github are evidence of a well-supported project that is already being used across many research labs. The documentation is written in a tutorial-style manner, providing many examples and references between hardware and software, and making it easy to follow.</p><p>The authors additionally show convincing evidence of the robustness of the pyControl system with respect to the different behavioral paradigms it is capable of supporting. Many other behavior systems are limited in what tasks they can be used with, and the evidence of a flexible steup across different experiments in the manuscript is quite convincing. An added benefit of the system is its ability to time synchronize to electrophysiological or imaging platforms for optimal data analysis. The system, as a whole, is a &quot;one-stop shop&quot; for programming, running, and visualizing experiments in real time, and then open-source nature of the system allows researchers to (mostly) pick and choose how they'd like to utilize the system. The project has already gained steam with many different labs using it currently, and has the potential for improving rigor and robustness of understanding behavioral data across research labs.</p><p>Weaknesses: While the manuscript claims that pyControl is a flexible device, it is not clear how other devices could interact with the pyControl hardware. Many labs already have behavior devices (such as lickometers or solenoids or odor delivery ports, for example), either custom-built or from commercial vendors that already work well, but the researchers may instead be looking for a way to run their tasks in a simpler or well-documented manner. It is stated that it is 'straightforward' to make a pyControl compatible device but this is unclear within the manuscript how much work this would take, or the ease of using one's own devices. While the system is intended to have everything that anyone needs, the benefit of an open-source framework is that people are allowed to flexibly choose which aspects might work optimally for their research question. It would be best to at least detail how people could adapt their own devices. In the same realm, it is a bit unclear of the extent of capabilities of the system, with respect to data processing or memory storage, running multiple animals at once, or platform dependency, for example.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Open source, Python based, hardware and software for controlling behavioural neuroscience experiments.&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Kate Wassum (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below from Reviewer 1. The reviewers agreed that it is important that the manuscript be largely self-contained.<italic>Reviewer #1:</italic></p><p>A journal paper is meant to document the past for reproduction, not advertise some potential future. The ephemeral nature of online documentation is not conducive to this. However, assuming the documentation is archived with the paper, I am satisfied that this can be addressed.</p><p>There is no reason not to adequately document the RJ-45 connector interface within the paper. It represents the current state of the hardware as opposed to some hypothetical future flexibility, requires minimal text, and represents a clear critical specification for users who wish to design their own modules. I see no reason for the authors not to do this.</p><p>The authors link to their help documentation to reply to our concern about &quot;from pyControl.utility import *&quot; and &quot;from devices import *&quot;. This documentation essentially admits that unless one has completely understood the pyControl codebase (e.g., one of the developers) to know all of the functions and objects in the namespace, one is hopeless to avoid having collisions. This seems to directly contradict the claim that they have built a broadly useful tool.</p><p><italic>Reviewer #2:</italic></p><p>The revised manuscript is much improved, and the authors have addressed all of my concerns. This manuscript, in combination with the online documentation, will be an excellent resource for learning about pyControl.</p><p><italic>Reviewer #3:</italic></p><p>The authors have adequately responded to the reviews and provided a thorough and improved manuscript. I do not believe any additional work is required to the updated manuscript. I also agree with the authors that, as discussed around lines 673 in the author comments document, it is not necessary to include the additional panels for figure 7 as it would be repetitive to what is already in the figure.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.67846.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>Positives:</p><p>Most behavior systems take one of two broad approaches to the system configuration: run the task logic and data I/O through the host computer, which is connected to peripherals; or run the task logic and data I/O through an integrated external breakout board and microcontroller. Here, the authors have opted for the latter approach, arguing that it confers several advantages: 1) inexpensiveness, 2) flexibility, 3) user-friendliness, and 4) parallelism.</p><p>Certainly, microcontrollers (and their commercial or custom breakout boards) are a less expensive counterpart to desktop computers, albeit at the expense of lower processing power and memory. For experiments that require more computation, this platform may not be the best fit, which the authors acknowledge in their discussion. However, for many experiments, a microcontroller is sufficient, and thus the approach pyControl takes is reasonable.</p><p>The flexibility of the system is apparent in both the hardware and software. The custom breakout board offers a mixture of digital and analog I/O ports, as well as power supply lines for different peripheral devices. On the software side, the choice of state machine logic allows it to be broadly applicable. The ability to save both input/output streams and internal variables with timestamps is important.</p></disp-quote><p>We thank the reviewer for their close reading of the manuscript and support for the high level design choices made in pyControl.</p><disp-quote content-type="editor-comment"><p>Major Concern:</p><p>pyControl is a combination of hardware, API code, &quot;task code&quot;, and control-computer code. For publication, a paper should go beyond merely showing a few tasks where code was used – it needs to not only demonstrate utility, but also document the system in ways beyond a user guide. While demonstrating the efficacy and reliability of the &quot;task code&quot; is the primary goal of this paper, it fails to do so convincingly.</p></disp-quote><p>We agree with the reviewer that the robustness and reliability of the system are paramount, and hope we have addressed their concerns through the additional information and validation experiments discussed below.</p><disp-quote content-type="editor-comment"><p>0) Given the advantage of microcontroller-based behavioral control, the primary question facing this paper is: &quot;Why should a lab deal with pyControl rather than rolling their own solution?&quot; It seems like the easiest way to demonstrate this would be to show, e.g., an Arduino solution and a pyControl solution side by side so that it can be seen how obviously easier it is.</p></disp-quote><p>While we understand where the reviewer is coming from, we think that this suggestion to compare an Arduino based solution and pyControl is less straightforward than it sounds. pyControl is a complete system for controlling behavioural experiments comprising a task definition syntax and associated framework code, a graphical interface for running experiments on many setups in parallel and visualising what is happening in real-time, hardware modules for building setups, plus the extensive user documentation needed to make it accessible to the broader community. It is the <italic>combination</italic> of these features which makes pyControl valuable.</p><p>It is clearly not feasible to develop an Arduino based system with equivalent functionality to address this comment. It may be that the reviewer is imagining a narrower comparison of code to implement a single basic task on an Arduino with code to implement an equivalent task in pyControl. If the editor and reviewers believe this would be valuable we are willing to do it, but we are not sure it would be very informative:</p><p>First, because getting a minimal task implementation working is really not the same thing as having a practical experiment control system. At a bare minimum, the system requires additional code on the computer to acquire the data coming of the board and save it to disk. To be practical, a system needs substantially more functionality than this, e.g. the ability to select which task to run on which setup, load the appropriate task code, configure task variables, start and stop the task, visualise what is going on during the experiment, etc. The task implementation is not independent of this functionality as it needs to interact with it. A narrow comparison of a minimal task implementation therefore gives a potentially very misleading picture of the amount of work needed to get a viable system up and running from scratch.</p><p>Second, the pyControl framework implements functionality that we are not sure would be possible to implement on an Arduino, and we certainly would not know how to, e.g. streaming data from analog inputs and rotary encoders, and triggering task events when these cross thresholds, in parallel to processing digital inputs and state machine code. Again, showing a minimal Arduino example task that does not implement this functionality could be misleading</p><disp-quote content-type="editor-comment"><p>1) Figure 1 is confusing to the reader where it appears:</p><p>Figure 1 shows an example of running a simple &quot;blink LED&quot; task, and while the right panel is a nice visual complement to the task code, the actual task definition code on the left is at first unclear. Where is the &quot;button_press&quot; event (or any other event) defined when it becomes input to the task functions? Where do Digital_input() and Digital_output() come from? How are &quot;entry&quot; and &quot;exit&quot; events defined? (As an aside, it is usually best to avoid &quot;import *&quot; statements, as they can at best be confusing and at worst cause namespace conflicts. In this case, it is unclear which of the other module classes/functions came from &quot;pyControl.utility&quot; vs. &quot;devices&quot;.) Perhaps it would be best to first introduce the overall organization of the hardware (Figure 2, 3), and then explain the task logic that is running on the microcontroller, which itself requires an understanding of how hardware is defined in the first place.</p></disp-quote><p>We think there is a tension here and elsewhere between providing detailed technical information – e.g. about every element of the example task code shown in figure 1, and maintaining the readability and length of the manuscript. Our approach to this has been a division of labour between the manuscript and online documentation, whereby the manuscript gives a high level overview of the design principles and their rationale, while detailed technical information primarily relevant to users or developers resides in the docs. The documentation currently runs to ~17000 words, and while we think this level of detail is valuable, it is clearly not possible in a manuscript. As the reviewer notes below, this necessitates that the documentation is robustly archived, as it is a key component of the overall project, and this is already implemented (see below).</p><p>We therefore think that while it is useful to clarify these points, it is probably best to do this in the documentation. We have added a link to the relevant documentation from the figure legend and have added the section &quot;Where do phControl functions come from&quot; to the programming tasks docs addressing points raised by the reviewer that were not already covered in the docs:</p><disp-quote content-type="editor-comment"><p>2) Hardware is given short shrift. The hardware definition examples in the supplementary figures seem superfluous. The configuration diagram are useful visuals for understanding how components are wired together, but the code underneath is mostly repetitive and can be left for the interested reader to find in the online documentation. Critically, what is missing is a broader explanation of how hardware devices are defined – both from an API perspective and in terms of the physical connection – that would allow the user to design their own.</p></disp-quote><p>Thanks for this suggestion – a similar point was also raised by reviewer 3. We agree that it would be useful to provide more information about how users can integrate custom external hardware with pyControl and define new devices, and think that the best way to do this is to add in a high level overview in the manuscript as well as detailed information in the documentation. We have added new material to both.</p><p>As requested we have removed the hardware definition code from the supplementary figure legends and replaced it with links to the files in the manuscript’s data repository.</p><p>New material in manuscript:</p><p>“To extend the functionality of pyControl to application not supported by the existing hardware, it is straightforward to interface setups with user created or commercial devices. This requires creating an electrical connection between the devices and defining the inputs and outputs in the hardware definition. Triggering external hardware from pyControl, or task events from external devices, is usually achieved by connecting the device to a BNC connector on the breakout board, and using the standard pyControl digital input or output classes. More complex interactions with external devices may involve multiple inputs and outputs and/or serial communication. In this case the electrical connection is typically made to a behaviour port, as these carry multiple signal lines. A port adapter board, which breaks out an RJ45 connector to a screw terminal, simplifies connecting wires. Alternatively, if more complex custom circuitry is required, e.g. to interface with a sensor, it may make sense to design a custom printed circuit board with an RJ45 connector, similar to existing pyControl devices, as this is more scalable and robust than implementing the circuit on a breadboard. To simplify instantiating devices comprising multiple inputs and outputs, or controlling devices which require dedicated code, users can define a Python class representing the device. These are typically simple classes which instantiate the relevant pyControl input and output objects as attributes, and may have methods containing code for controlling the device, e.g. to generate serial commands. More information is provided in the section “Interfacing with external hardware” in the hardware docs, and the design files and associated code for existing pyControl devices provide a useful starting point for new designs.”</p><p>New material in documentation in the section “Interfacing with external hardware”</p><disp-quote content-type="editor-comment"><p>3) Links to documentation do not belong in a scientific paper: Choosing a higher-level language like (micro)python makes the system user-friendly and separate from proprietary software like MATLAB. Additionally, the use of a GUI facilitates understanding of the task code and logic. The authors have clearly spent time building an extensive online documentation that is valuable for reaching a broad user base. However, the assumption of the reviewer is that our journal is more reliably archival than random sites on the internet. So anything that is relevant to understanding the paper should be included. (It may be possible to archive the code base along with the paper, which would solve this issue.)</p></disp-quote><p>We completely agree with the reviewer that as the docs are an important component of the overall system, it is essential that they are robustly archived. This is already implemented; the source files for the documentation are hosted in the pyControl docs repository on Github. The website where the docs are displayed (ReadTheDocs) automatically pulls the latest version from Github each time the repository is updated, ensuring that the archived and served docs are always in sync and fully version controlled. We also note that ReadTheDocs is a very well established service in its own right, which currently hosts documentation for over 240,000 open source projects, se<ext-link ext-link-type="uri" xlink:href="https://readthedocs.org/sustainability/">e</ext-link> https://readthedocs.org/sustainability/<ext-link ext-link-type="uri" xlink:href="https://readthedocs.org/sustainability/">.</ext-link></p><p>We respectfully disagree with the reviewer that links to the documentation should not be in the manuscript. We see the manuscript and documentation as providing complementary information about the system, as discussed above, and therefore think that providing tight integration between them is desirable. However, we will abide by the policy of the journal on this issue.</p><disp-quote content-type="editor-comment"><p>4) There is significant concern about performance and reliability. What happens if two state transitions are triggered simultaneously? There is a description of an event queue: does the queue generation code automatically infer whether queued events are still valid – are race conditions possible? (Simple example – a triggered state transition that should occur if a timer has not elapsed, but a different set of states occurs if the timer has elapsed. What happens if the timer elapses and the trigger occur nearly simultaneously?)</p></disp-quote><p>Performance and reliability are clearly critical. Regarding reliability, we note that pyControl has at this point been used to run many thousands of hours of behaviour across tens of labs and at least 10 different publications and preprints (referenced at line 349 in revised manuscript). The core framework code which handles functionality like processing events and elapsed timers has been largely unchanged for several years now, with recent development work largely focussed on developing and extending GUI functionality. Regarding the specific scenarios outlined above:</p><disp-quote content-type="editor-comment"><p>What happens if two state transitions are triggered simultaneously?</p></disp-quote><p>It is not possible for two state transitions to be triggered simultaneously because events are processed sequentially from the event queue, and during event processing individual lines of code in the state behaviour function are themselves processed sequentially. To further enforce predictable behaviour around state transitions, users are prevented from calling the goto_state function during processing of the entry and exit events that occur during state transitions (attempting to do so gives an informative error message), ensuring that any state entry and/or exit behaviour associated with a state transition has run to completion before another state transition can be triggered.</p><disp-quote content-type="editor-comment"><p>There is a description of an event queue: does the queue generation code automatically infer whether queued events are still valid?</p></disp-quote><p>pyControl has no concept of events being valid or invalid. If an event is in the queue it will be processed, and the consequences of this will depend on the task’s state when the event is processed.</p><disp-quote content-type="editor-comment"><p>- are race conditions possible? (Simple example – a triggered state transition that should occur if a timer has not elapsed, but a different set of states occurs if the timer has elapsed. What happens if the timer elapses and the trigger occur nearly simultaneously?)</p></disp-quote><p>If we understand correctly, the reviewer is asking whether there is any indeterminacy in the order in which an elapsing timer and a digital input will be processed if they occur near simultaneously. As already quantified in the manuscript (Figure 5C,D), there is variation in the duration of timed intervals due to the 1 ms resolution of the framework clock ticks, with standard deviation of 282us. We think in most scenarios this is likely to dominate processing order indeterminacy in this situation. Some additional variability will be introduced by the time taken to process individual events and other unitary framework operations (such as streaming a chunk of data to the computer), as if both a clock tick and external input occur during a single operation, the external input will be processed before any timers elapsing on that tick, due to the priority of framework operations shown in figure 2. Timing accuracy in the ‘high load’ condition, quantified in figure 5D, gives a reasonable worst case scenario for the combined influence of these two factors, resulting in a variation in timed intervals with standard deviation 353us. Therefore while there is some indeterminacy in execution order in this scenario it is on a timescale that is not relevant for behaviour.</p><disp-quote content-type="editor-comment"><p>5) There is concern about the choice of Micropython rather than a compiled language. In particular, it is unclear whether the performance testing adequately explored garbage collection. For example, if a naive user instantiates a timer in a state that is executed repeatedly, garbage collection will be regularly required to delete the relevant heap memory. If garbage collection is slow, this could result in unexpected loss of responsiveness or missed logging of data.</p></disp-quote><p>The use of a high level language on the microcontroller is responsible for many of the strengths of the system in terms of ease of use and flexibility. Indeed reviewer 2 stated that ‘A key advantage of pyControl over other solutions in this space is the ability to program both the software and the embedded firmware in the same high-level language, Python’<italic>.</italic></p><p>However, we agree it is important to understand issues that could be caused by garbage collection. We have performed new validation experiments and added two new panels to figure 5, and associated results text, testing the effect of garbage collection on timers and external inputs.</p><p>To address the specific concern that a user might load the system by setting timers in quick succession, causing unresponsiveness due to repeated garbage collection: We tested an extreme example of this where we used timers with 1ms duration to transition continuously between two states at a frequency of 1KHz. In one state a digital output was on and in another off, such that by monitoring the output we could observe when garbage collection occurred due to the short delay it caused in the alternation between states. Garbage collection occurred following 0.6% of state transitions, taking about 3ms each time, such that overall about 2% of the systems time was spent on garbage collection. As events that occur during garbage collection are still processed once it has completed, even this deliberately extreme scenario would have minimal effects on the systems responsiveness. New figure panels 5E,F.</p><p>New results text:</p><p>“Users who require very tight timing/latency performance should be aware of Micropython’s automatic garbage collection. Garbage collection is triggered when needed to free up memory and takes a couple of milliseconds. Normal code execution is paused during garbage collection, though interrupts (used to register external inputs and update the framework clock) run as normal. pyControl timers that elapse during garbage collection will be processed once it has completed (Figure 5E). Timers that are running but do not elapse during garbage collection are unaffected. Digital inputs that occur during garbage collection are registered with the correct timestamp (Figure 5F), but will only be processed once garbage collection has completed. The only situation where events may be missed due to garbage collection is if a single digital input receives multiple event-triggering edges during a single garbage collection, in which case only the last event is processed correctly (Figure 5F). To avoid garbage collection affecting critical event processing, the user can manually trigger garbage collection at a time when it will not cause problems (see Micropython docs<ext-link ext-link-type="uri" xlink:href="https://docs.micropython.org/en/latest/library/gc.html">)</ext-link>, for example during the inter-trial interval.”</p><disp-quote content-type="editor-comment"><p>6) From the text, it appears that data are timestamped at 1 kHz, but the figures depict microsecond resolution. How fast can events occur? Most experiments will have events on the scale of second or 100s of milliseconds, but a naive user might instantiate a state tracking licks, which can occur much more rapidly when a mouse licks in a bout.</p></disp-quote><p>The system clock tick that updates the current time (used for event timestamps) and determines when timers elapse, runs at 1Khz. However external events are detected by hardware interrupts rather than polling at the clock frequency, hence the 556 ± 17 μs latency under low load conditions. Regarding how fast events can occur; simultaneous events on different digital inputs are fine, as shown in figure 5F (reproduced above) where an input signal was connected to multiple pyControl digital inputs such that events were generated on multiple inputs by each edge of the signal. For events generated by a single digital input, pairs of edges separated by 1ms are processed correctly, unless both edges occur during a single garbage collection (see figure 5F and associated text above). Regarding the maximum continuous event rate that the system can handle, we did some additional testing of this which is reported in the revised manuscript as:</p><p>&quot;A final constraint is that as each event takes time to process, there is a maximum continuous event rate above which the framework cannot process events as fast as they occur, causing the event queue to grow until available memory is exhausted. This rate will depend on the processing triggered by each event, but is approximately 960Hz for digital inputs triggering state transitions but no additional processing. In practice we have never encountered this when running behavioural tasks as average event rates are typically orders of magnitude lower and transiently higher rates are buffered by the queue.&quot;</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…]This manuscript will be a great resource for researchers that are considering adopting pyControl. I have a few suggestions for making it even better:</p></disp-quote><p>We thank the reviewer for their positive assessment of the manuscript and system.</p><disp-quote content-type="editor-comment"><p>I'd like to see a more candid discussion of the limitations of pyControl for various types of experiments. For example, running everything on a microcontroller precludes the display of complex visual stimuli. Therefore, pyControl would not be suitable for most visual physiology experiments. Similarly, the pyControl Audio Player peripheral can only deliver stimuli up to 48 kHz, which limits its utility for rodent auditory physiology. pyControl of course includes general-purpose output ports that can be used to trigger any auxiliary devices, so these limitations could be overcome with some customization.</p></disp-quote><p>We had tried to be as straightforward as possible in the discussion about the limitations of the system as well as advantages, but we agree there are some additional points that should be raised about visual and auditory stimuli. We felt this material fitted best in the section detailing hardware, and have added a paragraph that reads (lines 195-203):</p><p>“The design choice of running tasks on a microcontroller, and the specific set of devices developed to date, impose some constraints on experiments supported by the hardware. The limited computational resources preclude generating complex visual stimuli, making pyControl unsuitable for most visual physiology in its current form. The devices for playing audio are aimed at general behavioural neuroscience applications, and may not be suitable for some auditory neuroscience applications. One uses the pyboard’s internal DAC for stimulus generation, and hence is limited to simple sounds such as sine waves or noise. Another plays WAV files from an SD card, allowing for diverse stimuli but limited to 44KHz sample rate.”</p><disp-quote content-type="editor-comment"><p>The authors should include more details about the pyboard microcontroller, since this is an essential component of the platform. What are the specifications, approximate cost, etc.?</p></disp-quote><p>We now provide the key specifications (Arm Cortex M4 running at 168MHz with 192KB RAM) at line 168. The pyboards are cheap (£28), but as the cost of the microcontroller is generally a small fraction of the overall system cost, we do not specify it to avoid giving a misleading picture.</p><disp-quote content-type="editor-comment"><p>Is there a plan to take advantage of the pyBoard's wireless capabilities? This could be helpful for coordinating massively parallel experiments.</p></disp-quote><p>The version of the pyboard that we use currently does not have built in wireless capability, though there are other Micropython boards that do. We do not have plans to implement wireless data transmission at this point as our feeling is that implementation and reliability issues are likely to outweigh advantages for most applications, particularly as the system will likely still need to be wired to provide power.</p><disp-quote content-type="editor-comment"><p>The &quot;Karpova Lab&quot; is mentioned out of context. Readers will likely want more information about what this means.</p></disp-quote><p>Fixed in revised manuscript (line 220).</p><disp-quote content-type="editor-comment"><p>Since Bonsai does not itself encompass any hardware interfaces, a more accurate comparison would be with the combination of Bonsai and Harp (https://www.cf-hw.org/harp). Unfortunately, Harp doesn't have a publication associated with it, nor much online documentation. But the main advantages are shared timestamps (within 44 µs) across all devices, and tight integration with Bonsai software. The authors should mention the availability of Harp in their discussion of Bonsai.</p></disp-quote><p>We now mention the HARP hardware in the discussion of Bonsai, saying (lines 575-578):</p><p>“Though Bonsai itself is software, some compatible behavioural hardware has been developed by the Champalimaud Foundation Hardware Platform (https://www.cf-hw.org/harp<ext-link ext-link-type="uri" xlink:href="https://www.cf-hw.org/harp">)</ext-link>, which offers tight timing synchronisation and close integration with Bonsai, but documentation is currently limited.”</p><disp-quote content-type="editor-comment"><p>Another behavior control platform that should be cited is Autopilot (https://www.biorxiv.org/content/10.1101/807693v1), which looks promising for certain applications, despite its relative immaturity.</p></disp-quote><p>Now cited at line 478.</p><disp-quote content-type="editor-comment"><p>A detailed characterization of closed-source commercial hardware is beyond the scope of this manuscript, but the authors should consider presenting at least a high-level comparison of these tools (in terms of order-of-magnitude cost, available features, and customizability). The discussion seems to be targeted at users choosing between various open-source solutions, but convincing researchers to adopt pyControl in favor of closed-source hardware is a much more important goal.</p></disp-quote><p>We do discuss the issues with commercial behavioural hardware in broad terms in the introduction (lines 44-51), and think that most of the target audience for this system, i.e. behavioural neuroscientists, are acutely aware of them. We would rather not overemphasise pyControl’s low cost relative to commercial hardware, as we think that this speaks for itself, and the flexibility, user friendliness and open nature of the system are equally important.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>Akam et al. detail an open-source system called pyControl, that is intended to be used to control behavioral neuroscience experiments. The authors nicely highlight the current problem of reproducibility in behavioral neuroscience, describing the difficulty of standardizing or understanding code and behavioral tasks across labs. It is quite clear the project is setting new standards, especially in the open-source neuroscience field, by addressing many pitfalls of previous behavioral systems and platforms. Thus, the major strength of the paper is the open-source manner with which the authors detail their system. The hardware and software provided in an open manner allows for researchers to streamline their behavioral tasks in a cost-effective manner while still maintaining the flexibility needed to perform specific experiments. The authors provide rather straightforward documentation of how to program and run behavioral experiments using pyControl software and the related GUI, as well as how the hardware is generally set up. The authors then validate the use of pyControl in three different behavioral tasks that are widely used in neuroscience research.</p><p>Strengths: The pyControl system itself serves the purpose of having one streamlined behavioral setup that can provide reproducibility across research labs, the system can seemingly be used by &quot;novice&quot; researchers with a beginner level skill set in both hardware and computer programming, and comes at a cheaper price than commercial (or other open source) behavior platforms. The project's website, user forum, and github are evidence of a well-supported project that is already being used across many research labs. The documentation is written in a tutorial-style manner, providing many examples and references between hardware and software, and making it easy to follow.</p><p>The authors additionally show convincing evidence of the robustness of the pyControl system with respect to the different behavioral paradigms it is capable of supporting. Many other behavior systems are limited in what tasks they can be used with, and the evidence of a flexible steup across different experiments in the manuscript is quite convincing. An added benefit of the system is its ability to time synchronize to electrophysiological or imaging platforms for optimal data analysis. The system, as a whole, is a &quot;one-stop shop&quot; for programming, running, and visualizing experiments in real time, and then open-source nature of the system allows researchers to (mostly) pick and choose how they'd like to utilize the system. The project has already gained steam with many different labs using it currently, and has the potential for improving rigor and robustness of understanding behavioral data across research labs.</p></disp-quote><p>We thank the reviewer for their positive comments about the manuscript and system.</p><disp-quote content-type="editor-comment"><p>Weaknesses: While the manuscript claims that pyControl is a flexible device, it is not clear how other devices could interact with the pyControl hardware. Many labs already have behavior devices (such as lickometers or solenoids or odor delivery ports, for example), either custom-built or from commercial vendors that already work well, but the researchers may instead be looking for a way to run their tasks in a simpler or well-documented manner. It is stated that it is 'straightforward' to make a pyControl compatible device but this is unclear within the manuscript how much work this would take, or the ease of using one's own devices. While the system is intended to have everything that anyone needs, the benefit of an open-source framework is that people are allowed to flexibly choose which aspects might work optimally for their research question. It would be best to at least detail how people could adapt their own devices. In the same realm, it is a bit unclear of the extent of capabilities of the system, with respect to data processing or memory storage, running multiple animals at once, or platform dependency, for example.</p></disp-quote><p>We agree that it is important to make clear how users can interface pyControl with external hardware and have added new material to both the manuscript (lines 204-224) and documentation explaining this. As this issue was also raised by reviewer 1, this material is reproduced above (lines 137-218 in this document) in response to their comment.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>A journal paper is meant to document the past for reproduction, not advertise some potential future. The ephemeral nature of online documentation is not conducive to this. However, assuming the documentation is archived with the paper, I am satisfied that this can be addressed.</p></disp-quote><p>We now include a PDF version of the documentation in supplementary material to ensure that it is archived with the paper.</p><disp-quote content-type="editor-comment"><p>There is no reason not to adequately document the RJ-45 connector interface within the paper. It represents the current state of the hardware as opposed to some hypothetical future flexibility, requires minimal text, and represents a clear critical specification for users who wish to design their own modules. I see no reason for the authors not to do this.</p></disp-quote><p>We now include a new table detailing the RJ-45 connector behaviour port interface.</p><disp-quote content-type="editor-comment"><p>The authors link to their help documentation to reply to our concern about &quot;from pyControl.utility import *&quot; and &quot;from devices import *&quot;. This documentation essentially admits that unless one has completely understood the pyControl codebase (e.g., one of the developers) to know all of the functions and objects in the namespace, one is hopeless to avoid having collisions. This seems to directly contradict the claim that they have built a broadly useful tool.</p></disp-quote><p>We thank the reviewer for their persistence on this issue, as it has led us to improve the framework code for this revision. This said, we think their above comment somewhat overstates the severity of the issue, as in the six years we have been providing technical support to pyControl users we have yet to encounter a single case in which a name collision has caused a problem. Nonetheless, we agree it would be useful to allow users to use only named imports if they prefer. This was not possible with the previous version of the codebase as some functions were patched into the task definition file after it was imported by the framework, rather than being imported into it by the user. We have now refactored the framework code such that all pyControl-specific functions, variables and classes are imported by the user into the task file. The docs now describe both the use of * imports and named imports, and we provide example task files using both approaches so users can compare the resulting code and decide for themselves which they prefer.</p><p>The docs still treat using * import for the <italic>pyControl.utility</italic> module as the default, as this results in a less verbose, and we think clearer, task definition syntax. The risk of an issue arising due to a name collision is very low: The imported functions will not overwrite user defined functions as the import is at the top of the file. If users overwrite a function in the imported module that they were not aware of, this will not cause a problem because these functions are only ever called by the user.</p><p>The modified section is in the “Imports” of the docs.</p></body></sub-article></article>