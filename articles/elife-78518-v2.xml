<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78518</article-id><article-id pub-id-type="doi">10.7554/eLife.78518</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Medicine</subject></subj-group></article-categories><title-group><article-title>Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: A scoping review of articles published in 2018-2019</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-272856"><name><surname>Cobey</surname><given-names>Kelly D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2797-1686</contrib-id><email>kcobey@ottawaheart.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273154"><name><surname>Fehlmann</surname><given-names>Christophe A</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273155"><name><surname>Christ Franco</surname><given-names>Marina</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273156"><name><surname>Ayala</surname><given-names>Ana Patricia</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-324356"><name><surname>Sikora</surname><given-names>Lindsey</given-names></name><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273158"><name><surname>Rice</surname><given-names>Danielle B</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273159"><name><surname>Xu</surname><given-names>Chenchen</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-97793"><name><surname>Ioannidis</surname><given-names>John PA</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3118-6859</contrib-id><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-203048"><name><surname>Lalu</surname><given-names>Manoj M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0322-382X</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff11">11</xref><xref ref-type="aff" rid="aff12">12</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273160"><name><surname>Ménard</surname><given-names>Alixe</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273161"><name><surname>Neitzel</surname><given-names>Andrew</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273162"><name><surname>Nguyen</surname><given-names>Bea</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273163"><name><surname>Tsertsvadze</surname><given-names>Nino</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-59120"><name><surname>Moher</surname><given-names>David</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2434-4206</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>Heart Institute, University of Ottawa</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>School of Epidemiology and Public Health, University of Ottawa</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01m1pv723</institution-id><institution>Department of Anaesthesiology, Clinical Pharmacology, Intensive Care and Emergency Medicine, Geneva University Hospitals</institution></institution-wrap><addr-line><named-content content-type="city">Geneva</named-content></addr-line><country>Switzerland</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c62dg59</institution-id><institution>Centre for Journalology, Clinical Epidemiology Program, Ottawa Hospital Research Institute</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05msy9z54</institution-id><institution>School of Dentistry, Federal University of Pelotas</institution></institution-wrap><addr-line><named-content content-type="city">Pelotas</named-content></addr-line><country>Brazil</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>Gerstein Science Information Centre, University of Toronto</institution></institution-wrap><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>Health Sciences Library, University of Ottawa</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>Department of Psychology, McGill University</institution></institution-wrap><addr-line><named-content content-type="city">Montreal</named-content></addr-line><country>Canada</country></aff><aff id="aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>Department of Medicine, University of Ottawa</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff><aff id="aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Departments of Medicine, of Epidemiology and Population Health, of Biomedical Data Science, and of Statistics, and Meta-Research Innovation Center at Stanford, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff11"><label>11</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>Department of Anesthesiology and Pain Medicine, University of Ottawa</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff><aff id="aff12"><label>12</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03c62dg59</institution-id><institution>Regenerative Medicine Program, Ottawa Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Ottawa</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Allison</surname><given-names>David B</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kg8sb98</institution-id><institution>Indiana University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Zaidi</surname><given-names>Mone</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>21</day><month>06</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e78518</elocation-id><history><date date-type="received" iso-8601-date="2022-03-10"><day>10</day><month>03</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-06-20"><day>20</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-03-09"><day>09</day><month>03</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.31219/osf.io/k6nf4"/></event></pub-history><permissions><copyright-statement>© 2023, Cobey et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Cobey et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78518-v2.pdf"/><abstract><sec id="abs1"><title>Background:</title><p>Reproducibility is a central tenant of research. We aimed to synthesize the literature on reproducibility and describe its epidemiological characteristics, including how reproducibility is defined and assessed. We also aimed to determine and compare estimates for reproducibility across different fields.</p></sec><sec id="abs2"><title>Methods:</title><p>We conducted a scoping review to identify English language replication studies published between 2018 and 2019 in economics, education, psychology, health sciences, and biomedicine. We searched Medline, Embase, PsycINFO, Cumulative Index of Nursing and Allied Health Literature – CINAHL, Education Source via EBSCOHost, ERIC, EconPapers, International Bibliography of the Social Sciences (IBSS), and EconLit. Documents retrieved were screened in duplicate against our inclusion criteria. We extracted year of publication, number of authors, country of affiliation of the corresponding author, and whether the study was funded. For the individual replication studies, we recorded whether a registered protocol for the replication study was used, whether there was contact between the reproducing team and the original authors, what study design was used, and what the primary outcome was. Finally, we recorded how reproducibilty was defined by the authors, and whether the assessed study(ies) successfully reproduced based on this definition. Extraction was done by a single reviewer and quality controlled by a second reviewer.</p></sec><sec id="abs3"><title>Results:</title><p>Our search identified 11,224 unique documents, of which 47 were included in this review. Most studies were related to either psychology (48.6%) or health sciences (23.7%). Among these 47 documents, 36 described a single reproducibility study while the remaining 11 reported at least two reproducibility studies in the same paper. Less than the half of the studies referred to a registered protocol. There was variability in the definitions of reproduciblity success. In total, across the 47 documents 177 studies were reported. Based on the definition used by the author of each study, 95 of 177 (53.7%) studies reproduced.</p></sec><sec id="abs4"><title>Conclusions:</title><p>This study gives an overview of research across five disciplines that explicitly set out to reproduce previous research. Such reproducibility studies are extremely scarce, the definition of a successfully reproduced study is ambiguous, and the reproducibility rate is overall modest.</p></sec><sec id="abs5"><title>Funding:</title><p>No external funding was received for this work</p></sec></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reproducibility</kwd><kwd>replication</kwd><kwd>meta-research</kwd><kwd>open science</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Definitions of reproducibility vary considerably across disciplines and overall rates of reproducibility are low irrespective of the definition used.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Reproducibility is a central tenant of research. Reproducing previously published studies helps us to discern discoveries from false leads. The lexicon around reproducibility studies is diverse and poorly defined (<xref ref-type="bibr" rid="bib11">Goodman et al., 2016</xref>). Here, we loosely use Nosek and Errington’s definition: ‘<italic>a study for which any outcome would be considered diagnostic evidence about a claim from prior research’</italic> (<xref ref-type="bibr" rid="bib29">Nosek and Errington, 2020a</xref>). Most scientific studies are never formally reproduced and some disciplines have lower rates of reproducibility attempts than others. For example, in education research, an analysis published in 2014 of all publications in the discipline’s top 100 journals found that only 0.13% (221 out of 164,589) of the published articles described an independent reproducibility study (<xref ref-type="bibr" rid="bib22">Makel and Plucker, 2014</xref>). There is rising concern about the reproducibility of research and increasing interest in enhancing research transparency (e.g. <xref ref-type="bibr" rid="bib3">Buck, 2015</xref>; <xref ref-type="bibr" rid="bib24">Munafò et al., 2017</xref>; <xref ref-type="bibr" rid="bib5">Collins and Tabak, 2014</xref>; <xref ref-type="bibr" rid="bib2">Begley and Ioannidis, 2015</xref>).</p><p>Knowledge about rates of reproducibility is currently dominated by a handful of well-known projects examining the reproducibility of a group of studies within a field. For example, a project estimating the reproducibility of 100 psychology studies published in three leading journals found that just 36% had statistically significant results, compared to 97% of the original studies. Just 47% of the studies had effect sizes that were within the bounds of the 95% confidence interval of the original study (<xref ref-type="bibr" rid="bib31">Open Science Collaboration, 2015</xref>). Estimates of reproducibility in economics are similarly modest or low: a large-scale study attempting to reproduce 67 papers was only able to reproduce 22 (33%) of these (<xref ref-type="bibr" rid="bib4">Chang and Li, 2015</xref>). This same study showed that when teams attempting to reproduce research involved one of the original study authors as a co-author on the project, rates of reproducibility increased. This may suggest that detailed familiarity with the original study method increases the likelihood of reproducing the research findings. A cancer biology reproducibility project launched in 2013 to independently reproduce several high-profile papers has produced rather sobering results a decade later. Most of the selected studies could not even be attempted to be reproduced (e.g. it was not clear what had been done originally) and among those that an attempt to reproduce was made, most did not seem to produce consistent results (although the exact reproducibility rate depends on the definition of reproducibility) and the effect of the reproduced studies was only 15% of the original effect (<xref ref-type="bibr" rid="bib8">Errington et al., 2021a</xref>; <xref ref-type="bibr" rid="bib18">Kane and Kimmelman, 2021</xref>; <xref ref-type="bibr" rid="bib9">Errington et al., 2021b</xref>).</p><p>In medicine, studies that do not reproduce in clinic may exaggerate patient benefits and harms (<xref ref-type="bibr" rid="bib19">Le Noury et al., 2015</xref>) especially when clinical decisions are based on a single study. Despite this and other potential consequences we know very little about what predicts research reproducibility. No data exists which provides systematic estimates for reproducibility across multiple disciplines or addresses why disciplines might vary in their rates of reproducibility. Failure to empirically examine reproducibility is regrettable: without research we can’t identify actions to take that could drive improvements in research reproducibility. This contributes to research waste (<xref ref-type="bibr" rid="bib25">Nasser et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Ioannidis et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Freedman et al., 2015</xref>).</p><p>We set out to broadly examine the reproducibility literature across five disciplines and report on characteristics including how reproducibility is defined, assessed, and document prevalence rates for reproducibility. We focused on studies that explicitly described themselves as reproducibility or replication studies addressing reproductions of previously published work.</p></sec><sec id="s2" sec-type="methods"><title>Methods</title><p>Our study was conducted using the framework proposed by <xref ref-type="bibr" rid="bib1">Arksey and O’Malley, 2005</xref> and the related update by <xref ref-type="bibr" rid="bib20">Levac et al., 2010</xref>, and follows a five stage process: (1) identifying the research question, (2) identifying relevant studies, (3) study selection, (4) charting the data, (5) collating, summarizing and reporting the results.</p><sec id="s2-1"><title>Protocol registration and Open Science statement</title><p>This protocol was shared on the Open Science Framework prior to initiating the study (<ext-link ext-link-type="uri" xlink:href="https://osf.io/59nw4/">https://osf.io/59nw4/</ext-link>). We used the PRISMA-ScR (<xref ref-type="bibr" rid="bib36">Tricco et al., 2018</xref>) checklist to guide our reporting. Study data and materials are also available on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/wn7gm/">https://osf.io/wn7gm/</ext-link>).</p></sec><sec id="s2-2"><title>Eligibility criteria</title><sec id="s2-2-1"><title>Inclusion criteria</title><p>We included all quantitative reproducibility studies within the fields of economics, education, psychology, health sciences and biomedicine that were published in 2018 or 2019. Definitions we established for each discipline can be found in Appendix 1. We included all studies that explicitly self-described as a replication or a reproducibility study in which a previously published quantitative study is referred to and conducted again. As per Nosek and Errington’s definition (<xref ref-type="bibr" rid="bib29">Nosek and Errington, 2020a</xref>), we did not require that methods be perfectly matched between the original study and the replication if the author described the study as a replication. We excluded studies where the main intention of the work was not framed as a reproducibility project.</p></sec><sec id="s2-2-2"><title>Exclusion criteria</title><p>We excluded complementary and alternative forms of medicine as defined by the National Institutes of Health’s National Center for Complementary and Integrative Health (<ext-link ext-link-type="uri" xlink:href="https://www.nccih.nih.gov/">https://www.nccih.nih.gov/</ext-link>) for feasibility based on pilot searches. We excluded literature that was not published in English for feasibility, or that described exclusively qualitative research. We excluded conference proceedings, commentaries, narrative reviews, systematic reviews (not original research), and clinical case studies. We also excluded studies that described a replication of a study but where the original study was reporting in the same publication.</p></sec></sec><sec id="s2-3"><title>Information sources and search strategy</title><p>Our search strategy was developed by trained information specialists (APA, LS), and peer reviewed using the PRESS guideline (<xref ref-type="bibr" rid="bib23">McGowan et al., 2016</xref>). We restricted our search to the years 2018 and 2019 in order to maintain feasibility of this study given our available resources for screening and data extraction. We searched the following databases: Medline via Ovid (1946–2020), Embase via Ovid (1947–2020), PsycINFO via Ovid (1806–2020), Cumulative Index of Nursing and Allied Health Literature – CINAHL (1937–2020), Education Source via EBSCOHost (1929–2020), ERIC via Ovid (1966–2020), EconPapers (inception – 2020), International Bibliography of the Social Sciences (IBSS) via ProQuest (1951–2020), and EconLit via EBSCOHost (1969–2020). We performed forward and backward citation analysis of articles included for data extraction in Scopus and Web of Science (platforms including Science Citation Index Expanded (SCI-EXPANDED) -–1900-present and Social Sciences Citation Index (SSCI) -–1900-present) to identify additional potential documents for inclusion. All searches are reported using PRISMA-S (<xref ref-type="bibr" rid="bib33">Rethlefsen et al., 2021</xref>). For full search details please see Appendix 2. A related supplementary search was developed a priori in which we searched preprint servers and conducted forward and backward citation searching (Appendix 3).</p></sec><sec id="s2-4"><title>Selection of sources of evidence and data charting process</title><p>Search results from the databases were imported into <xref ref-type="bibr" rid="bib6">Distiller SR, 2023</xref> (Evidence Partners, Ottawa, Canada) and de-duplicated. Search results from the supplementary searching were uploaded into Endnote, de-duplicated, and then uploaded into DistillerSR for screening. Team members involved in study screening (KDC, CAF, MCF, DBR, CX, AN, BN, LS, NT, APA) initially screened the titles and abstracts of 50 records and then reviewed conflicts to ensure high level of agreement among screeners (&gt;90%). After piloting was complete, all potentially relevant documents were screened in duplicate using the liberal accelerated method in which records move to full-text screening if one or more reviewers indicate unclear or yes with regards to potential inclusion and two reviewers were required to exclude a record. Then, all included documents were screened in duplicate to ensure they met all eligibility requirements. All conflicts were resolved by consensus or, when necessary, third-party arbitration (MML, DM). The study screening form can be found in Appendix 4 and 5.</p></sec><sec id="s2-5"><title>Data extraction</title><p>Two team members (MCF, CAF) extracted document characteristics. Prior to extraction a series of iterative pilot tests were done on included documents to ensure consistency between extractors. We extracted information including publication year, funding information (if funded, funder type), number of authors, ethics approval, study design, and open science practices (study registration, data sharing) from each included document. We also categorized each included documents based on its discipline area (e.g. Economics, Education, Psychology, Health Sciences, Biomedicine, or any combination of these fields) and whether a single original study was being reproduced or if the paper reported the results from reproducing more than one original study. When a single study was reproduced more than once (e.g. different labs all replicated one study) we classified this as a ‘single’ reproducibility study. We extracted what the stated primary outcome was. If there was no primary outcome stated, we recorded this, and extracted the first stated outcome described in each document. Finally, we extracted what the results of the reproducibility project as reported by the authors (replicated, not replicated, mixed finding) and categorized method by which the authors of each relevant document assess reproducibility (e.g. comparison of effect sizes, statistical significance from p-values). Where relevant we extracted p-values and related statistical information. This allowed us to test the proportion of reproducible results that were statistically significant. The study extraction form can be found in Appendix 6. In instances where documents described multiple sub-studies, we recorded this and then extracted information from all unique quantitative studies describing a reproducibility study.</p></sec><sec id="s2-6"><title>Piloting</title><p>Team members extracting data (CAF, MCF) performed a calibration pilot test prior to the onset of full-text screening and extraction. Specifically, a series of included documents were then extracted independently. The team then met to discuss differences in extraction between team members and challenges encountered before extracting from subsequent documents. This was repeated until consensus was reached. Extraction was then done by a single reviewer with a second reviewer doing quality control for all documents. Conflicts were resolved by consensus or, when necessary, third-party arbitration (KDC, DBR, MML, DM).</p></sec><sec id="s2-7"><title>Synthesis of results</title><p>SPSS 27 (Microsoft) (<xref ref-type="bibr" rid="bib15">IBM Support, 2023</xref>) was used for data analysis. The characteristics of all included documents are presented using frequencies and percentages and described narratively. We report descriptive statistics where relevant. We then report frequency characteristics of the reproducibility studies, and which reproduced based on authors description of their findings (i.e. using the varied definitions of replication that exist in the literature), per discipline. Next, we describe how factors such as team size, team composition, and discipline relate to the reproducibility study. We compared these factors based on how authors defined reproducibility as well as based on the definition that results were statistically significant (at a conventional threshold of p&lt;0.05).</p><p>Text-based responses (e.g. primary outcome) underwent content analysis and are described in thematic groups using frequencies and percentages. All content analysis was done by two independent investigators (CF, KDC) using Microsoft Excel.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><sec id="s3-1"><title>Open science</title><p>Data and materials are available on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/wn7gm/">https://osf.io/wn7gm/</ext-link>).</p></sec><sec id="s3-2"><title>Protocol amendments</title><p>In our protocol, we specified we would include all documents that explicitly self-describe themselves as a reproducibility or replication study. Over the course of the scoping review we encountered studies that described themselves as being a replication or reproducibility study, but in fact did not describe work that met this definition (e.g. a longitudinal study reporting a new cross sectional report of the data; a study with the goal of replicating a concept rather than a specific study). In these instances, we excluded the study despite the authors description that it was a reproducibility study. Studies describing themselves as being a replication but that explicitly specified that they used a novel method were also excluded, as they did not set out with the explicit goal of replicating previous research approach. We encountered a study that was included where the results were arranged by outcomes but not studies being replicated, in this instance we were unable to determine how the results corresponded to the studies the author listed they reproduced. To accommodate this, we modified our extraction form to include an item indicating that extraction of sub-study information was unclear and could not be performed. We had indicated we would record whether the research involved the study of humans or animals, and if so, how many. We did not include these items on the extraction form after piloting as we found reporting of N to be incomplete making accuracy challenging. We also do not present a re-analysis of the reproducibility studies where we recalculate the rate at which studies reproduce comparatively by discipline given the relatively small representation of disciplines outside of psychology and health science which accounted for 128 (72.3%) of the total studies.</p></sec><sec id="s3-3"><title>Selection of sources of evidence</title><p>The original search included 16,135 records. An additional 159 novel records were retrieved via grey literature searching: 7 documents were retrieved from searching citations of included documents, 49 were included from searching preprints servers, and 103 were included from citation searching. After de-duplication we screened a total of 11,224 documents, of which, 178 were sought for full-text screening. After full-text screening, 47 documents were included in the review. The remaining 131 documents were excluded because of one or more of these factors: they were not written in English (N=2), we could not obtain a full-text document via our library (N=11), the document was not published in 2018 or 2019 (N=39), the document did not describe an original quantitative research study (N=32), the study was not a quantitative reproduciblity study (N=46), or did not fit in a discipline of interest (N=1). See <xref ref-type="fig" rid="fig1">Figure 1</xref> for the study flow diagram.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Flow diagram of articles.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78518-fig1-v2.tif"/></fig></sec><sec id="s3-4"><title>Characteristics of sources of evidence</title><p>The characteristics of the included documents are summarized in <xref ref-type="table" rid="table1">Table 1</xref>. The 47 documents included described a total of 177 reproducibility studies. Thirty-six documents (76.6%) described a single reproducibility study, while 11 (23.4%) documents described multiple reproducibility efforts of distinct studies in a single paper. Twenty-eight (59.6%) documents were published in 2018 while 19 (40.4%) were published in 2019. The corresponding author on most of the documents was based at an institution in the USA 27 (57.4%). The included documents had a median of 3 authors, but papers ranged from having between 1 and 172 authors.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Characteristics of included documents.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Characteristic</th><th align="left" valign="bottom">Categories</th><th align="left" valign="bottom">All studies</th><th align="left" valign="bottom">Single replication papers(N=36)</th><th align="left" valign="bottom">Multiple replication papers(N=11)</th></tr><tr><th align="left" valign="bottom" colspan="2"/><th align="left" valign="bottom" colspan="3">N (%) (unless otherwise indicated)</th></tr></thead><tbody><tr><td align="left" valign="bottom">What discipline does the work best fit in?<xref ref-type="table-fn" rid="table1fn1">*</xref></td><td align="left" valign="bottom">Biomedicine<break/>Economics<break/>Education<break/>Health sciences<break/>Psychology<break/>Other (mixture of two or more of the abov1e)</td><td align="left" valign="bottom">6 (3.4)<break/>5 (2.8)<break/>5 (2.8)<break/>42 (23.7)<break/>86 (48.6)<break/>33 (18.6)</td><td align="left" valign="bottom">6 (16.7)<break/>5 (13.9)<break/>1 (2.8)<break/>9 (25.0)<break/>15 (41.7)<break/>-</td><td align="left" valign="bottom">-<break/>-<break/>1 (9.1)<break/>4 (36.4)<break/>4 (36.4)<break/>2 (18.2)</td></tr><tr><td align="left" valign="bottom">Year of publication</td><td align="char" char="." valign="bottom">2018<break/>2019</td><td align="left" valign="bottom">28 (59.6)<break/>19 (40.4)</td><td align="left" valign="bottom">21 (58.3)<break/>15 (41.7)</td><td align="left" valign="bottom">7 (63.6)<break/>4 (36.4)</td></tr><tr><td align="left" valign="bottom">Country of corresponding author (reported based on Top 3 overall)</td><td align="left" valign="bottom">USA<break/>The Netherlands<break/>Australia</td><td align="left" valign="bottom">27 (57.4)<break/>4 (8.5)<break/>3 (6.4)</td><td align="left" valign="bottom">19 (52.8)<break/>3 (8.3)<break/>3 (8.3)</td><td align="left" valign="bottom">8 (72.7)<break/>1 (9.1)<break/>-</td></tr><tr><td align="left" valign="bottom">Number of authors<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td><td align="left" valign="bottom">Median<break/>Range</td><td align="left" valign="bottom">3<break/>1–172</td><td align="left" valign="bottom">3<break/>1–124</td><td align="left" valign="bottom">4<break/>1–172</td></tr><tr><td align="left" valign="bottom">Funding</td><td align="left" valign="bottom">Yes<break/>No<break/>Not reported</td><td align="left" valign="bottom">32 (68.1)<break/>6 (12.8)<break/>9 (19.1)</td><td align="left" valign="bottom">23 (63.9)<break/>5 (13.9)<break/>8 (22.2)</td><td align="left" valign="bottom">9 (81.8)<break/>1 (9.1)<break/>1 (9.1)</td></tr><tr><td align="left" valign="bottom">Funding source<xref ref-type="table-fn" rid="table1fn3"><sup>‡</sup></xref></td><td align="left" valign="bottom">Government<break/>Academic<break/>Non-profit<break/>Unsure</td><td align="left" valign="bottom">19 (59.4)<break/>15 (46.9)<break/>14 (43.8)<break/>1 (3.1)</td><td align="left" valign="bottom">17 (73.9)<break/>9 (39.1)<break/>9 (39.1)<break/>-</td><td align="left" valign="bottom">2 (22.2)<break/>6 (66.7)<break/>5 (55.6)<break/>1 (11.1)</td></tr><tr><td align="left" valign="bottom">Ethics approval</td><td align="left" valign="bottom">Yes<break/>No<break/>Ethics approval not relevant</td><td align="left" valign="bottom">23 (48.9)<break/>10 (21.3)<break/>14 (29.8)</td><td align="left" valign="bottom">17 (47.2)<break/>8 (22.2)<break/>11 (30.6)</td><td align="left" valign="bottom">6 (54.5)<break/>2 (18.2)<break/>3 (27.3)</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>Data reported at the study level.</p></fn><fn id="table1fn2"><label>†</label><p>Data reports median and range.</p></fn><fn id="table1fn3"><label>‡</label><p>Data refers to funded studies only, some studies report multiple funding sources.</p></fn></table-wrap-foot></table-wrap><p>Thirty-two (68.1%) documents indicated that they received funding, 6 (12.8%) indicated the work was unfunded, and 9 (19.1%) failed to report information about funding. Among documents reporting funding, federal governments were the primary source (N=19, 59.4%). Twenty-three (48.9%) studies reported receiving ethical approval, 10 (21.3%) studies did not report ethical approval, and ethical approval was not relevant for 14 (29.8%) studies.</p></sec><sec id="s3-5"><title>Synthesis of reproducibility studies</title><p>Most replication studies captured in our sample were in the discipline of psychology (86, 48.6%), followed by health science (42, 23.7%), or were an intersection of our included disciplines (33, 18.6%). There were a relatively smaller number of studies in economics (5, 2.8%), education (5, 2.8%), and biomedicine (6, 3.4%). The most common study designs observed were observational studies (85, 48.0%) and experimental studies (52, 29.4%). The remainder of studies captured were data studies for example, re-analysis using previous data (35, 19.8%) or experimental trials (5, 2.8%).</p><p>We examined whether the authors of the reproducibility studies included in our synthesis overlapped with the research team of the original studies. To do so, we compared of author lists and examined whether the authors of the reproducibility team self-report their team overlapped or had contact with the original author(s). Sixteen (9.0%) documents had teams that overlapped with the original research team whose study was being replicated, 44 (24.9%) indicated contact with the original team but not authorship overlap, while the remaining 117 (66.1%) studies had no authorship overlap and did not report any contact with the original study authors. Other key findings include that 81 (45.8%) of the studies referred to a registered protocol, although only 41 (23.2%) indicated they used a protocol that was identical to the original study they were reproducing and 28 (23.9%) had both a registered protocol and claimed to be identical to the original study. For 112 (63.3%) of studies the authors indicated that data of the replication studies was publicly available; however, this rate was driven by a few included documents that reported multiple reproduced studies and consistently shared data. Thirty-four of 47 (72.3%) documents included indicated data was not shared. Most studies did not report a primary outcome (134, 75.7%). For studies that did not list a primary outcome, where possible, we extracted the first reported outcome. We thematically grouped the primary/first stated outcomes of the remaining documents into 12 themes, which are presented in Appendix 8. Three of these themes were related to biomedicine or health. We present the data describing the characteristics of the included documents by discipline of the document in <xref ref-type="table" rid="table2">Table 2</xref>.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Study replication methods characteristics.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Characteristic</th><th align="left" valign="bottom">Categories</th><th align="left" valign="bottom">All discipline studies(N=177)</th><th align="left" valign="bottom">BiomedicineN (%)</th><th align="left" valign="bottom">EconomicsN (%)</th><th align="left" valign="bottom">EducationN (%)</th><th align="left" valign="bottom">Health sciences<xref ref-type="table-fn" rid="table2fn1">*</xref>N (%)</th><th align="left" valign="bottom">PsychologyN (%)</th><th align="left" valign="bottom">Other (mixture of two or more of the above)N (%)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Did the replication study team specify that they contacted the original study project team?</td><td align="left" valign="bottom">Yes, the author teams overlapped<break/>Yes, there was contact<break/>No, the teams did not overlap or contact</td><td align="left" valign="bottom">16 (9.0)<break/>44 (24.9)<break/>117 (66.1)</td><td align="left" valign="bottom">2 (33.3)<break/>-<break/>4 (66.7)</td><td align="left" valign="bottom">-<break/>-<break/>5 (100)</td><td align="left" valign="bottom">-<break/>-<break/>5 (100)</td><td align="left" valign="bottom">4 (9.5)<break/>14 (33.3)<break/>24 (57.1)</td><td align="left" valign="bottom">10 (11.6)<break/>9 (10.5)<break/>67 (77.9)</td><td align="left" valign="bottom">-<break/>21 (63.6)<break/>12 (36.4)</td></tr><tr><td align="left" valign="bottom">Does the replication study refer to a protocol that was registered prior to data collection?</td><td align="left" valign="bottom">Yes<break/>No</td><td align="left" valign="bottom">81 (45.8)<break/>96 (54.2)</td><td align="left" valign="bottom">2 (33.3)<break/>4 (66.7)</td><td align="left" valign="bottom">-<break/>5 (100)</td><td align="left" valign="bottom">1 (20.0)<break/>4 (80.0)</td><td align="left" valign="bottom">18 (42.9)<break/>24 (57.1)</td><td align="left" valign="bottom">39 (45.3)<break/>47 (54.7)</td><td align="left" valign="bottom">21 (63.6)<break/>12 (36.4)</td></tr><tr><td align="left" valign="bottom">Do the authors specify that they used an identical protocol?</td><td align="left" valign="bottom">Yes<break/>No <xref ref-type="table-fn" rid="table2fn2"><sup>†</sup></xref><break/>Not reported<break/>Unsure</td><td align="left" valign="bottom">41 (23.2)<break/>70 (39.5)<break/>64 (36.2)<break/>2 (1.1)</td><td align="left" valign="bottom">2 (33.3)<break/>1 (16.7)<break/>-<break/>2 (33.3)</td><td align="left" valign="bottom">1 (20.0)<break/>3 (60.0)<break/>1 (20.0)<break/>-</td><td align="left" valign="bottom">-<break/>3 (60.0)<break/>2 (40.0)<break/>-</td><td align="left" valign="bottom">9 (21.4)<break/>15 (35.7)<break/>17 (40.5)<break/>-</td><td align="left" valign="bottom">8 (9.3)<break/>34 (39.5)<break/>44 (51.2)<break/>-</td><td align="left" valign="bottom">21 (63.6)<break/>12 (36.64)<break/>-<break/>-</td></tr><tr><td align="left" valign="bottom">Does the study indicate that data is shared publicly?</td><td align="left" valign="bottom">Yes<xref ref-type="table-fn" rid="table2fn3"><sup>‡</sup></xref><break/>No</td><td align="left" valign="bottom">112 (63.3)<break/>65 (36.7)</td><td align="left" valign="bottom">2 (33.3)<break/>4 (66.7)</td><td align="left" valign="bottom">-<break/>5 (100)</td><td align="left" valign="bottom">1 (20.0)<break/>4 (80.0)</td><td align="left" valign="bottom">18 (42.9)<break/>24 (57.1)</td><td align="left" valign="bottom">70 (81.4)<break/>16 (18.6)</td><td align="left" valign="bottom">21 (63.6)<break/>12 (36.4)</td></tr><tr><td align="left" valign="bottom">What is the study design used?</td><td align="left" valign="bottom">Data re-analysis<break/>Experimental<break/>Observational<break/>Trial</td><td align="left" valign="bottom">35 (19.8)<break/>52 (29.4)<break/>85 (48.0)<break/>5 (2.8)</td><td align="left" valign="bottom">-<break/>2 (33.3)<break/>3 (50.0)<break/>1 (16.7)</td><td align="left" valign="bottom">3 (60.0)<break/>1 (20.0)<break/>1 (20.0)<break/>-</td><td align="left" valign="bottom">1 (20.0)<break/>-<break/>3 (60.0)<break/>1 (20.0)</td><td align="left" valign="bottom">26 (61.9)<break/>10 (23.8)<break/>3 (7.1)<break/>3 (7.1)</td><td align="left" valign="bottom">5 (5.8)<break/>39 (45.3)<break/>42 (48.8)<break/>-</td><td align="left" valign="bottom">33 (100)<break/>-</td></tr><tr><td align="left" valign="bottom">Did the study specify a primary outcome?</td><td align="left" valign="bottom">Yes<break/>No</td><td align="left" valign="bottom">43 (24.3)<break/>134 (75.7)</td><td align="left" valign="bottom">-<break/>6 (100)</td><td align="left" valign="bottom">-<break/>5 (100)</td><td align="left" valign="bottom">-<break/>5 (100)</td><td align="left" valign="bottom">26 (61.9)<break/>16 (38.1)</td><td align="left" valign="bottom">13 (15.1)<break/>73 (84.9)</td><td align="left" valign="bottom">-<break/>33 (100)</td></tr></tbody></table><table-wrap-foot><fn id="table2fn1"><label>*</label><p>One study provided results by outcome not by studies being replicated, in this instance we were unable to determine how the results corresponded to the studies the author listed they replicated so these data are missing.</p></fn><fn id="table2fn2"><label>†</label><p>In these instances authors specified deviations between their protocol and that of the original research team.</p></fn><fn id="table2fn3"><label>‡</label><p>This was not verified. We simply recorded what the authors reported. It is possible that self-reported sharing and rates of actual sharing are not identical.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3-6"><title>Definition</title><p><xref ref-type="table" rid="table3">Table 3</xref> provides a summary of definitions for reproducibility. We found that studies related to psychology and health sciences tended to use a comparison of effect sizes to define reproducibility success. The number of included studies across other disciplines was low (&lt;6).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Reproducibility characteristics of studies replicated overall and across disciplines.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Characteristic</th><th align="left" valign="bottom">Categories</th><th align="left" valign="bottom">Overall</th><th align="left" valign="bottom">Biomedicine</th><th align="left" valign="bottom">Economics</th><th align="left" valign="bottom">Education</th><th align="left" valign="bottom">Health sciences</th><th align="left" valign="bottom">Psychology</th><th align="left" valign="bottom">Other</th></tr></thead><tbody><tr><td align="left" valign="bottom">How did the authors assess reproducibility?</td><td align="left" valign="bottom">Effect sizes<break/>Meta analysis of original effect size<break/>Null hypothesis testing using p-value<break/>Subjective assessment<break/>Other</td><td align="left" valign="bottom">116 (65.5)<break/>33 (18.6)<break/>17 (9.6)<break/>5 (2.8)<break/>6 (3.4)</td><td align="left" valign="bottom">1 (16.7)<break/>2 (33.3)<break/>2 (33.3)<break/>-<break/>1 (16.7)</td><td align="left" valign="bottom">1 (20.0)<break/>-<break/>2 (40.0)<break/>1 (20.0)<break/>1 (20.0)</td><td align="left" valign="bottom">1 (20.0)<break/>-<break/>-<break/>2 (40.0)<break/>2 (40.0)</td><td align="left" valign="bottom">25 (59.5)<break/>9 (21.4)<break/>5 (11.9)<break/>2 (4.8)<break/>1 (2.4)</td><td align="left" valign="bottom">76 (88.4)<break/>1 (1.2)<break/>8 (9.3)<break/>-<break/>1 (1.2)</td><td align="left" valign="bottom">12 (36.4)<break/>21 (63.6)<break/>-<break/>-<break/>-</td></tr><tr><td align="left" valign="bottom">Based on the authors definition of reproducibility, did the study replicate?</td><td align="left" valign="bottom">Yes<break/>No<break/>Mixed<break/>Unclear</td><td align="left" valign="bottom">95 (53.7)<break/>36 (20.3)<break/>8 (4.5)<break/>38 (21.5)</td><td align="left" valign="bottom">4 (66.7)<break/>1 (16.7)<break/>1 (16.7)<break/>-</td><td align="left" valign="bottom">4 (80.0)<break/>1 (20.0)<break/>-<break/>-</td><td align="left" valign="bottom">2 (40.0)<break/>2 (40.0)<break/>1 (20.0)<break/>-</td><td align="left" valign="bottom">36 (85.7)<break/>4 (9.5)<break/>1 (2.4)<break/>1 (2.4)</td><td align="left" valign="bottom">25 (29.1)<break/>19 (22.1)<break/>5 (5.8)<break/>37 (43.0)</td><td align="left" valign="bottom">24 (72.7)<break/>9 (27.3)<break/>-<break/>-</td></tr><tr><td align="left" valign="bottom">Was the p-value reported on the statistical test conducted on the primary outcome?</td><td align="left" valign="bottom">Yes<break/>No/unclear</td><td align="left" valign="bottom">116 (65.5)<break/>61 (34.5)</td><td align="left" valign="bottom">3 (50.0)<break/>3 (50.0)</td><td align="left" valign="bottom">3 (60.0)<break/>2 (40.0)</td><td align="left" valign="bottom">4 (80.0)<break/>1 (20.0)</td><td align="left" valign="bottom">33 (78.6)<break/>9 (21.4)</td><td align="left" valign="bottom">45 (52.3)<break/>41 (47.7)</td><td align="left" valign="bottom">28 (84.8)<break/>5 (15.2)</td></tr></tbody></table></table-wrap></sec><sec id="s3-7"><title>Prevalence of reproducibility</title><p>Of the 177 individual studies reproduced, based on the authors reported definition, 95 (53.7%) reproduced, 36 (20.3%) failed to reproduce, 8 (4.5%) produced mixed results. A further 38 studies (21.5%), 37 of which were from a single included document, could not be assessed due to issues with incomplete or poor-quality reporting. Rates were highest in health sciences (N=36, 85.7%), economics (N=4, 80%), inter-disciplinary studies (N=24, 72.7%). Rates of replication tended to be lower in biomedicine (N=4, 66.7%), education (N=2, 40%), and psychology (N=25, 29.1%). When we removed an included document related to psychology, which presented 37 individual studies but failed to report a reproducibility outcome clearly, rates improved to 51.0% in this discipline. When examining the 35 studies that reported data (re)-analysis projects, rates of reproducibility based on the authors definition were considerably higher (N=31, 88.6%).</p><p>Of the 177 individual studies, we were able to extract p-values from 116 (65.5%), of these, reproducibility was statistically significant at the p&lt;0.05 threshold in 82 (70.9%) studies.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>The primary objective of this study was to describe the characteristics of reproducibility studies, including how reproducibility is defined and assessed. We found 47 individual documents reporting reproducibility studies in 2018–2019 that met our inclusion criteria. Our included documents reported 177 individual reproducibility studies. Most reproducibility studies were in the disciplines of psychology and health science (&gt;72%), with 86 and 42 studies, respectively. This may suggest unique cultures around reproducibility in distinct disciplines, future research is needed to determine if such differences truly exist given the limitations of our search and approach. Some disciplines have routinely embedded replication as part of the original discovery and validation efforts, for example replications are routinely done for genomics and other -omics findings as part of the original studies rather than as separate efforts. Consistent with previous research (<xref ref-type="bibr" rid="bib21">Makel et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Sukhtankar, 2017</xref>; <xref ref-type="bibr" rid="bib14">Hubbard and Vetter, 1992</xref>), our findings suggest that overall only a very small fraction of research in any of these discipline published in a given year focuses on reproducing existing research published in previous papers. A recent evaluation of 349 randomly selected research articles from the biomedical literature published in 2015-2018 (<xref ref-type="bibr" rid="bib34">Serghiou et al., 2021</xref>) found that 33 (10%) included a reproducibility component in their research (e.g. validating previously published experiments, running a similar clinical trial in a different population, etc.). However, the vast majority of these efforts would not qualify as separate, independent replication studies in our assessment.</p><p>Most of the documents included in our study had corresponding/lead authors who were from the United States (57.4%) and most papers reported receiving funding (68.1%); papers reporting multiple studies (N=9, 81.8%) were more likely to report funding than single replication studies (N=23, 63.9%). Together this may suggest that at least some funders recognize the value of reproducibility studies, and that USA based researchers have taken a leadership role in reproducibility research. We note that just 11 of the 47 papers reported to be reproducing more than one original study, suggesting most reproducibility studies reproduce a single previous study. We note that two of the ‘single studies’ included reported being part of a larger reproducibility project (e.g. a study part of the broader cancer reproducibility project). Four of ‘single studies’ reported more than 1 reproduction of the same original article in the document (e.g. different labs reproducing the same experiment). Future qualitative research could shed light on the motivations of researchers to conduct a single versus multiple reproducibility study. This will be important to understand what, if any, supports are needed to facilitate large-scale reproducibility studies.</p><p>When we examined the 177 individual studies reproduced in the 47 documents, we found only a minority of them referred to registered protocols. In psychology, rates were highest, with 45.8% of studies referring to a registered protocol. Registered protocols are a core open science practice, they can help to enhance transparency, mitigate publication bias and selective reporting biases (<xref ref-type="bibr" rid="bib27">Nosek et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Nosek et al., 2019</xref>). Importantly, they may specify what analyses were planned a priori and which were post hoc. Registration would seem especially relevant to reproducibility projects in order to pre-specify approaches to reduce perceptions of bias. We acknowledge, however, that mandates for registration are rare and exist only in particular disciplines and for specific study designs.</p><p>There was a wide range of study designs. For example, in psychology observational (e.g. cohort study) and experimental studies dominated, while data analysis studies (e.g. re-analysis) were most prominent in health sciences and economics. Reproducing an observational or experimental study may pose a greater resource challenge as compared to reproducing a data analysis, which may explain the higher rates of reproducibility success observed among data analysis studies. When no new data are generated, it may be difficult in the current research environment, which tends to favor novelty, to publish a re-analysis of existing data that shows the exact same result (<xref ref-type="bibr" rid="bib7">Ebrahim et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Naudet et al., 2018</xref>).</p><p>Across our five disciplines of interest the norm was that author teams did not overlap or contact the team of the original study they were attempting to reproduce. This finding may not be generalizable, because by definition we did not consider documents where the original study was reproduced within the same paper, a practice that is commonplace in many disciplines, for example genomics. Nosek and Errington describe confusion and disagreement that occurred during large scale reproducibility projects they were involved in which produced results that failed to replicate the original findings, calling for original researchers and those conducting reproducibility projects to “argue about what a replication means before you do it”. <xref ref-type="bibr" rid="bib30">Nosek and Errington, 2020b</xref> Our finding that teams don’t overlap or communicate, suggests that this practice is not typically implemented, despite its potential value to improve reproducibility <xref ref-type="bibr" rid="bib4">Chang and Li, 2015</xref>. Conversely, involvement of the original authors as authors in the reproducibility efforts may increase the impact of allegiance and confirmation biases. In the published experience from the Reproducibility Project: Cancer Biology, a large share of original authors did not respond to efforts to reach them to obtain information about their study (<xref ref-type="bibr" rid="bib8">Errington et al., 2021a</xref>; <xref ref-type="bibr" rid="bib18">Kane and Kimmelman, 2021</xref>; <xref ref-type="bibr" rid="bib9">Errington et al., 2021b</xref>).</p><p>Of the 177 individual studies reproduced, based on the authors’ reported definition, 53.7% reproduced successfully. When examining definitions for reproducibility, we found that studies related to psychology and health sciences tended to use a comparison of effect sizes to define reproducibility success. The number of included studies across other disciplines was too low to yield meaningful comparison of differences in definitions across disciplines. Rates of reproducibility based on the authors definition were highest in health sciences (N=36/42, 85.7%; 24/33, 72.7%) including ‘other’ and lower in biomedicine (N=4/6, 66.7%), education (N=2/5, 40%), and psychology (N=25, 29.1%). Low rates in psychology were driven by a single document reporting 37 studies that failed to report outcomes. When this document was removed, rates improved to 51.0% in this discipline. When we applied p-values from the 116 studies where these were reported, 70.7% of studies had p-values less than the commonly used 0.05 threshold. There is an increasing literature of different definitions of reproducibility and ‘success’ rates will unavoidably depend on how replication is defined (<xref ref-type="bibr" rid="bib8">Errington et al., 2021a</xref>; <xref ref-type="bibr" rid="bib9">Errington et al., 2021b</xref>; <xref ref-type="bibr" rid="bib13">Held et al., 2022</xref>; <xref ref-type="bibr" rid="bib32">Pawel and Held, 2020</xref>).</p><p>To our knowledge, this is the first study to provide a broad comparison of the characteristics of explicit reproducibility studies across disciplines. This comparative approach may help to identify features to better support further reproducibility research projects. This study used a formal search strategy, including grey literature searching, to identify potential documents. It is possible that the terminology we used, which was broad to apply across disciplines, may not have captured all potential studies in this area. It is also possible that the databases used do not equally represent the distinct disciplines we investigated, meaning that the searches are not directly comparable cross-disciplinarily. We also were not able to locate the full-text of all included documents which may have impacted on the results. The impact of these missing texts may not have been equal across disciplines. For these reasons, generalizations about disciplines should not be made. A further limitation is that we only considered two years of research. This allowed for a contemporary view on characteristics of replication studies, but it prevented the ability to examine temporal changes. Indeed, several well-known large-scale replication studies would not have been captured in our search. Ultimately, the number of included documents in some disciplines was relatively modest, suggesting that inclusion of articles across a larger timeframe is needed to address the objective to compare more meaningfully across disciplines.</p><p>For feasibility we also only extracted information about the primary outcome listed for each paper, or if no primary outcome was specified, the first listed outcome. It is possible that rates of reproducibility differ across outcomes. Future research could consider all outcomes listed. While we conducted our screening and extraction using two reviewers, to foster quality control, the reporting of the studies captured was sometimes extremely poor. This impacted the extraction process as in some cases extraction was challenging and in others resulted in missing data. Further, our screeners and extractors were not naïve to the aims of the study, which may have created implicit bias. Future research could include training coders and extractors who are unaware of the project aims. Collectively these study design decisions and practical challenges present limitations on the overall generalizability of the findings beyond our dataset. Finally, we acknowledge that these explicit ‘reproducibility check’ documents that we targeted, are only one part of the much larger scientific literature where some reproducibility features may be embedded. Random samples of biomedical papers with empirical data published in 2015–2018 have shown that reproducibility concepts are not uncommon (<xref ref-type="bibr" rid="bib37">Wallach et al., 2018</xref>). In the psychological sciences, similarly 5% of a random sample of 188 papers with empirical data published in 2014–2017 were replications (<xref ref-type="bibr" rid="bib12">Hardwicke et al., 2020</xref>). Conversely, in the social sciences, among a random sample of 156 papers published in 2014–2017, only 2 were replication studies (1%) (<xref ref-type="bibr" rid="bib12">Hardwicke et al., 2020</xref>). Moreover, as mentioned above, some fields explicitly require replications to be included as part of the original publication, and the large and blossoming literature of tens of thousands of meta-analyses (<xref ref-type="bibr" rid="bib17">Ioannidis, 2016</xref>) suggests that for many topics there are multiple studies that address similar enough questions so that meta-analysts would combine them. Eventually, the relative advantages and disadvantages of different replication practices (e.g. reproducibility embedded in the original publication versus done explicitly in a subsequent stage versus done as part of a wider agenda that mixes replication and novel efforts) needs further empirical study in diverse scientific fields.</p><p>Our finding that, only about half of the reproducibility studies reproduced across five fields of interest is concerning, though consistent with other studies. These estimates may not necessarily represent appropriately the reproducibility rates of entire fields since the choice of what specific studies to try to replicate may include selection factors that introduce strong bias towards higher or lower replication rates. Moreover, while estimates of reproducibility vary across fields in our modest sample, so too do norms in definitions used to define reproducibility. Choice of these definitions (especially when these definitions are not clear, pre-specified and valid) mayaffect the interpretation of these results to fit various narratives of replication success or failure. This suggests the need for discipline and interdisciplinary specific exchange on how to best approach reproducibility studies. Discussion on definitions for reproducibility, but also about methodological best practices when conducting a reproducibility study (e.g. using registered reports) will help to foster integrity and quality. To ensure reliability, multiple and diverse reproducibility studies with converging evidence are needed. At present, and as illustrated by out sampling, explicit reproducibility studies done as targeted reproducibility checks are rare. To enhance research reliability, reproducibility studies need to be encouraged, incentivized, and supported.</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Supervision, Investigation, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Validation, Investigation, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Data curation, Validation, Investigation, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Data curation, Validation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Validation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Validation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Validation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Validation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Validation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con12"><p>Validation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con13"><p>Validation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con14"><p>Conceptualization, Supervision, Validation, Investigation, Methodology, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-78518-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and materials are available on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/wn7gm/">https://osf.io/wn7gm/</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Cobey</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Scoping review data on reproducibility of research in a 2018-2019 multi-discipline sample</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/WN7GM</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arksey</surname><given-names>H</given-names></name><name><surname>O’Malley</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Scoping studies: towards a methodological framework</article-title><source>International Journal of Social Research Methodology</source><volume>8</volume><fpage>19</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1080/1364557032000119616</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Begley</surname><given-names>CG</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reproducibility in science: improving the standard for basic and Preclinical research</article-title><source>Circulation Research</source><volume>116</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1161/CIRCRESAHA.114.303819</pub-id><pub-id pub-id-type="pmid">25552691</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buck</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Solving reproducibility</article-title><source>Science</source><volume>348</volume><elocation-id>1403</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac8041</pub-id><pub-id pub-id-type="pmid">26113692</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>AC</given-names></name><name><surname>Li</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say”Usually Not”</source><publisher-loc>Washington</publisher-loc><publisher-name>Board of Governors of the Federal Reserve System</publisher-name><pub-id pub-id-type="doi">10.17016/FEDS.2015.083</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>FS</given-names></name><name><surname>Tabak</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>NIH plans to enhance reproducibility</article-title><source>Nature</source><volume>505</volume><fpage>612</fpage><lpage>613</lpage><pub-id pub-id-type="doi">10.1038/505612a</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Distiller SR</collab></person-group><year iso-8601-date="2023">2023</year><article-title>DistillerSR: Literature Review Software Smarter Reviews: Trusted Evidence</article-title><ext-link ext-link-type="uri" xlink:href="https://www.evidencepartners.com/products/distillersr-systematic-review-software/">https://www.evidencepartners.com/products/distillersr-systematic-review-software/</ext-link><date-in-citation iso-8601-date="2023-06-29">June 29, 2023</date-in-citation></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebrahim</surname><given-names>S</given-names></name><name><surname>Sohani</surname><given-names>ZN</given-names></name><name><surname>Montoya</surname><given-names>L</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Thorlund</surname><given-names>K</given-names></name><name><surname>Mills</surname><given-names>EJ</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reanalyses of randomized clinical trial data</article-title><source>JAMA</source><volume>312</volume><fpage>1024</fpage><lpage>1032</lpage><pub-id pub-id-type="doi">10.1001/jama.2014.9646</pub-id><pub-id pub-id-type="pmid">25203082</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Challenges for assessing Replicability in Preclinical cancer biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e67995</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67995</pub-id><pub-id pub-id-type="pmid">34874008</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Mathur</surname><given-names>M</given-names></name><name><surname>Soderberg</surname><given-names>CK</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Investigating the Replicability of Preclinical cancer biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e71601</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71601</pub-id><pub-id pub-id-type="pmid">34874005</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>LP</given-names></name><name><surname>Cockburn</surname><given-names>IM</given-names></name><name><surname>Simcoe</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The Economics of reproducibility in Preclinical research</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002165</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002165</pub-id><pub-id pub-id-type="pmid">26057340</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>SN</given-names></name><name><surname>Fanelli</surname><given-names>D</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What does research reproducibility mean?</article-title><source>Science Translational Medicine</source><volume>8</volume><elocation-id>341ps12</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aaf5027</pub-id><pub-id pub-id-type="pmid">27252173</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardwicke</surname><given-names>TE</given-names></name><name><surname>Wallach</surname><given-names>JD</given-names></name><name><surname>Kidwell</surname><given-names>MC</given-names></name><name><surname>Bendixen</surname><given-names>T</given-names></name><name><surname>Crüwell</surname><given-names>S</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An empirical assessment of transparency and reproducibility-related research practices in the social sciences (2014–2017)</article-title><source>Royal Society Open Science</source><volume>7</volume><elocation-id>190806</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.190806</pub-id><pub-id pub-id-type="pmid">32257301</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Held</surname><given-names>L</given-names></name><name><surname>Matthews</surname><given-names>R</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Pawel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reverse-Bayes methods for evidence assessment and research synthesis</article-title><source>Research Synthesis Methods</source><volume>13</volume><fpage>295</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1002/jrsm.1538</pub-id><pub-id pub-id-type="pmid">34889058</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubbard</surname><given-names>R</given-names></name><name><surname>Vetter</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The publication incidence of Replications and critical commentary in economics</article-title><source>The American Economist</source><volume>36</volume><fpage>29</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1177/056943459203600106</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="web"><person-group person-group-type="author"><collab>IBM Support</collab></person-group><year iso-8601-date="2023">2023</year><article-title>IBM SPSS Statistics 26</article-title><ext-link ext-link-type="uri" xlink:href="https://www.ibm.com/support/pages/downloading-ibm-spss-statistics-26">https://www.ibm.com/support/pages/downloading-ibm-spss-statistics-26</ext-link><date-in-citation iso-8601-date="2023-06-29">June 29, 2023</date-in-citation></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Greenland</surname><given-names>S</given-names></name><name><surname>Hlatky</surname><given-names>MA</given-names></name><name><surname>Khoury</surname><given-names>MJ</given-names></name><name><surname>Macleod</surname><given-names>MR</given-names></name><name><surname>Moher</surname><given-names>D</given-names></name><name><surname>Schulz</surname><given-names>KF</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Increasing value and reducing waste in research design, conduct, and analysis</article-title><source>The Lancet</source><volume>383</volume><fpage>166</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62227-8</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The mass production of redundant, misleading, and Conflicted systematic reviews and meta-analyses</article-title><source>The Milbank Quarterly</source><volume>94</volume><fpage>485</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1111/1468-0009.12210</pub-id><pub-id pub-id-type="pmid">27620683</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname><given-names>PB</given-names></name><name><surname>Kimmelman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Is Preclinical research in cancer biology reproducible enough</article-title><source>eLife</source><volume>10</volume><elocation-id>e67527</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67527</pub-id><pub-id pub-id-type="pmid">34874006</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Noury</surname><given-names>J</given-names></name><name><surname>Nardo</surname><given-names>JM</given-names></name><name><surname>Healy</surname><given-names>D</given-names></name><name><surname>Jureidini</surname><given-names>J</given-names></name><name><surname>Raven</surname><given-names>M</given-names></name><name><surname>Tufanaru</surname><given-names>C</given-names></name><name><surname>Abi-Jaoude</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Restoring study 329: Eficacy and harms of paroxetine and Imipramine in treatment of major depression in adolescence</article-title><source>BMJ</source><volume>351</volume><elocation-id>h4320</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.h4320</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levac</surname><given-names>D</given-names></name><name><surname>Colquhoun</surname><given-names>H</given-names></name><name><surname>O’Brien</surname><given-names>KK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Scoping studies: advancing the methodology</article-title><source>Implementation Science</source><volume>5</volume><elocation-id>69</elocation-id><pub-id pub-id-type="doi">10.1186/1748-5908-5-69</pub-id><pub-id pub-id-type="pmid">20854677</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makel</surname><given-names>MC</given-names></name><name><surname>Plucker</surname><given-names>JA</given-names></name><name><surname>Hegarty</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Replications in psychology research: how often do they really occur</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>537</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1177/1745691612460688</pub-id><pub-id pub-id-type="pmid">26168110</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makel</surname><given-names>MC</given-names></name><name><surname>Plucker</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Facts are more important than novelty: replication in the education sciences</article-title><source>Educational Researcher</source><volume>43</volume><fpage>304</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.3102/0013189X14545513</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGowan</surname><given-names>J</given-names></name><name><surname>Sampson</surname><given-names>M</given-names></name><name><surname>Salzwedel</surname><given-names>DM</given-names></name><name><surname>Cogo</surname><given-names>E</given-names></name><name><surname>Foerster</surname><given-names>V</given-names></name><name><surname>Lefebvre</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Guideline statement PRESS peer review of electronic search strategies: 2015 guideline statement</article-title><source>Journal of Clinical Epidemiology</source><volume>75</volume><fpage>40</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.jclinepi.2016.01.021</pub-id><pub-id pub-id-type="pmid">27005575</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munafò</surname><given-names>MR</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Bishop</surname><given-names>DVM</given-names></name><name><surname>Button</surname><given-names>KS</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>du Sert</surname><given-names>NP</given-names></name><name><surname>Simonsohn</surname><given-names>U</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Ware</surname><given-names>JJ</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A manifesto for reproducible science</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0021</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-016-0021</pub-id><pub-id pub-id-type="pmid">33954258</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasser</surname><given-names>M</given-names></name><name><surname>Clarke</surname><given-names>M</given-names></name><name><surname>Chalmers</surname><given-names>I</given-names></name><name><surname>Brurberg</surname><given-names>KG</given-names></name><name><surname>Nykvist</surname><given-names>H</given-names></name><name><surname>Lund</surname><given-names>H</given-names></name><name><surname>Glasziou</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>What are Funders doing to minimise waste in research? lancet</article-title><source>Lancet</source><volume>389</volume><fpage>1006</fpage><lpage>1007</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(17)30657-8</pub-id><pub-id pub-id-type="pmid">28290987</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naudet</surname><given-names>F</given-names></name><name><surname>Sakarovitch</surname><given-names>C</given-names></name><name><surname>Janiaud</surname><given-names>P</given-names></name><name><surname>Cristea</surname><given-names>I</given-names></name><name><surname>Fanelli</surname><given-names>D</given-names></name><name><surname>Moher</surname><given-names>D</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Data sharing and Reanalysis of randomized controlled trials in leading BIOMEDICAL journals with a full data sharing policy: survey of studies published in <italic>the BMJ</italic> and <italic>PLOS Medicine</italic></article-title><source>BMJ</source><volume>360</volume><elocation-id>k400</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.k400</pub-id><pub-id pub-id-type="pmid">29440066</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Ebersole</surname><given-names>CR</given-names></name><name><surname>DeHaven</surname><given-names>AC</given-names></name><name><surname>Mellor</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Preregistration revolution</article-title><source>PNAS</source><volume>115</volume><fpage>2600</fpage><lpage>2606</lpage><pub-id pub-id-type="doi">10.1073/pnas.1708274114</pub-id><pub-id pub-id-type="pmid">29531091</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Beck</surname><given-names>ED</given-names></name><name><surname>Campbell</surname><given-names>L</given-names></name><name><surname>Flake</surname><given-names>JK</given-names></name><name><surname>Hardwicke</surname><given-names>TE</given-names></name><name><surname>Mellor</surname><given-names>DT</given-names></name><name><surname>van ’t Veer</surname><given-names>AE</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Preregistration is hard, and worthwhile</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>815</fpage><lpage>818</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.07.009</pub-id><pub-id pub-id-type="pmid">31421987</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Errington</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>What is replication</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000691</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000691</pub-id><pub-id pub-id-type="pmid">32218571</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Errington</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>The best time to argue about what a replication means? before you do it</article-title><source>Nature</source><volume>583</volume><fpage>518</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1038/d41586-020-02142-6</pub-id><pub-id pub-id-type="pmid">32694846</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>PSYCHOLOGY. Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pawel</surname><given-names>S</given-names></name><name><surname>Held</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Probabilistic forecasting of replication studies</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0231416</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0231416</pub-id><pub-id pub-id-type="pmid">32320420</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rethlefsen</surname><given-names>ML</given-names></name><name><surname>Kirtley</surname><given-names>S</given-names></name><name><surname>Waffenschmidt</surname><given-names>S</given-names></name><name><surname>Ayala</surname><given-names>AP</given-names></name><name><surname>Moher</surname><given-names>D</given-names></name><name><surname>Page</surname><given-names>MJ</given-names></name><name><surname>Koffel</surname><given-names>JB</given-names></name><collab>PRISMA-S Group</collab></person-group><year iso-8601-date="2021">2021</year><article-title>PRISMA-S: an extension to the PRISMA statement for reporting literature searches in systematic reviews</article-title><source>Systematic Reviews</source><volume>10</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.1186/s13643-020-01542-z</pub-id><pub-id pub-id-type="pmid">33499930</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serghiou</surname><given-names>S</given-names></name><name><surname>Contopoulos-Ioannidis</surname><given-names>DG</given-names></name><name><surname>Boyack</surname><given-names>KW</given-names></name><name><surname>Riedel</surname><given-names>N</given-names></name><name><surname>Wallach</surname><given-names>JD</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Assessment of transparency indicators across the BIOMEDICAL literature: how open is open</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001107</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001107</pub-id><pub-id pub-id-type="pmid">33647013</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sukhtankar</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Online Appendix for Replications in Development Economics</source><publisher-name>American Economic Association</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tricco</surname><given-names>AC</given-names></name><name><surname>Lillie</surname><given-names>E</given-names></name><name><surname>Zarin</surname><given-names>W</given-names></name><name><surname>O’Brien</surname><given-names>KK</given-names></name><name><surname>Colquhoun</surname><given-names>H</given-names></name><name><surname>Levac</surname><given-names>D</given-names></name><name><surname>Moher</surname><given-names>D</given-names></name><name><surname>Peters</surname><given-names>MDJ</given-names></name><name><surname>Horsley</surname><given-names>T</given-names></name><name><surname>Weeks</surname><given-names>L</given-names></name><name><surname>Hempel</surname><given-names>S</given-names></name><name><surname>Akl</surname><given-names>EA</given-names></name><name><surname>Chang</surname><given-names>C</given-names></name><name><surname>McGowan</surname><given-names>J</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name><name><surname>Hartling</surname><given-names>L</given-names></name><name><surname>Aldcroft</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>MG</given-names></name><name><surname>Garritty</surname><given-names>C</given-names></name><name><surname>Lewin</surname><given-names>S</given-names></name><name><surname>Godfrey</surname><given-names>CM</given-names></name><name><surname>Macdonald</surname><given-names>MT</given-names></name><name><surname>Langlois</surname><given-names>EV</given-names></name><name><surname>Soares-Weiser</surname><given-names>K</given-names></name><name><surname>Moriarty</surname><given-names>J</given-names></name><name><surname>Clifford</surname><given-names>T</given-names></name><name><surname>Tunçalp</surname><given-names>Ö</given-names></name><name><surname>Straus</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>PRISMA extension for Scoping reviews (PRISMA-SCR): checklist and explanation</article-title><source>Annals of Internal Medicine</source><volume>169</volume><fpage>467</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.7326/M18-0850</pub-id><pub-id pub-id-type="pmid">30178033</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallach</surname><given-names>JD</given-names></name><name><surname>Boyack</surname><given-names>KW</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reproducible research practices, transparency, and open access data in the biomedical literature, 2015–2017</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2006930</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2006930</pub-id><pub-id pub-id-type="pmid">30457984</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Operationalized definitions of included disciplines</title><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Discipline</th><th align="left" valign="bottom">Definition</th></tr></thead><tbody><tr><td align="left" valign="bottom">Health sciences</td><td align="left" valign="bottom">Nutritional sciences, physiotherapy, kinesiology, rehabilitation, speech language pathology, physiology, nursing, midwifery, occupational therapy, social work, medicine and all its specialties, public health, population health, global health, pathology, laboratory medicine, optometry, health services research,</td></tr><tr><td align="left" valign="bottom">Biomedicine</td><td align="left" valign="bottom">Neuroscience, pharmacology, radiation therapy, dentistry, health management, epidemiology, virology, biomedicine, clinical engineering, biomedical engineering, genetics,</td></tr><tr><td align="left" valign="bottom">Education</td><td align="left" valign="bottom">Higher education, adult education, K-12 education, medical education, health professions education</td></tr><tr><td align="left" valign="bottom">Psychology</td><td align="left" valign="bottom">All specializations in Psychology, including but not limited to: clinical psychology, group psychology, psychotherapy, counselling, industrial psychology, cognitive psychology, forensic psychology, health psychology, neuropsychology, occupational psychology, social psychology</td></tr><tr><td align="left" valign="bottom">Economics</td><td align="left" valign="bottom">Microeconomics, macroeconomics, behavioural economics, econometrics, international economics, economic development, agricultural economics, ecological economics, environmental economics, natural resource economics, economic geography, location economics, real estate economics, regional economics, rural economics, transportation economics, urban economics, capitalist systems, comparative economic systems, developmental state, economic systems, transitional economies, economic history, industrial organization</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Search Strategy</title><list list-type="order"><list-item><p>exp &quot;reproducibility of results&quot;/</p></list-item><list-item><p>((reproduc* or replicat* or reliabilit* or repeat* or repetition) adj2 (result* or research* or test*)).tw,kf.</p></list-item><list-item><p>(face adj validit*).tw,kf.</p></list-item><list-item><p>(test adj reliabilit*).tw,kf.</p></list-item><list-item><p>or/1–4</p></list-item><list-item><p>prevalence/</p></list-item><list-item><p>(prevalen* or rate or rates or recur* or reoccuren*).tw,kf.</p></list-item><list-item><p>6 or 7</p></list-item><list-item><p>5 and 8</p></list-item><list-item><p>limit 9 to (yr=&quot;2018–2019&quot; and english)</p></list-item></list></sec><sec sec-type="appendix" id="s10"><title>PRISMA-S Checklist</title><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Section/topic</th><th align="left" valign="bottom">#</th><th align="left" valign="bottom">Checklist item</th><th align="left" valign="bottom">Location(s) Reported</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="4">INFORMATION SOURCES AND METHODS</td></tr><tr><td align="left" valign="bottom">Database name</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">Name each individual database searched, stating the platform for each.</td><td align="left" valign="bottom">~line 185–190</td></tr><tr><td align="left" valign="bottom">Multi-database searching</td><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">If databases were searched simultaneously on a single platform, state the name of the platform, listing all of the databases searched.</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Study registries</td><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">List any study registries searched.</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Online resources and browsing</td><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">Describe any online or print source purposefully searched or browsed (e.g., tables of contents, print conference proceedings, web sites), and how this was done.</td><td align="left" valign="bottom">~line 195</td></tr><tr><td align="left" valign="bottom">Citation searching</td><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">Indicate whether cited references or citing references were examined, and describe any methods used for locating cited/citing references (e.g., browsing reference lists, using a citation index, setting up email alerts for references citing included studies).</td><td align="left" valign="bottom">~line 190–194</td></tr><tr><td align="left" valign="bottom">Contacts</td><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">Indicate whether additional studies or data were sought by contacting authors, experts, manufacturers, or others.</td><td align="left" valign="bottom">See appendix 3</td></tr><tr><td align="left" valign="bottom">Other methods</td><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">Describe any additional information sources or search methods used.</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom" colspan="4">SEARCH STRATEGIES</td></tr><tr><td align="left" valign="bottom">Full search strategies</td><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">Include the search strategies for each database and information source, copied and pasted exactly as run.</td><td align="left" valign="bottom">See appendix 2</td></tr><tr><td align="left" valign="bottom">Limits and restrictions</td><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">Specify that no limits were used, or describe any limits or restrictions applied to a search (e.g., date or time period, language, study design) and provide justification for their use.</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Search filters</td><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">Indicate whether published search filters were used (as originally designed or modified), and if so, cite the filter(s) used.</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Prior work</td><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom">Indicate when search strategies from other literature reviews were adapted or reused for a substantive part or all of the search, citing the previous review(s).</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Updates</td><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">Report the methods used to update the search(es) (e.g., rerunning searches, email alerts).</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Dates of searches</td><td align="char" char="." valign="bottom">13</td><td align="left" valign="bottom">For each search strategy, provide the date when the last search occurred.</td><td align="left" valign="bottom">~line 190–194</td></tr><tr><td align="left" valign="bottom" colspan="4">PEER REVIEW</td></tr><tr><td align="left" valign="bottom">Peer review</td><td align="char" char="." valign="bottom">14</td><td align="left" valign="bottom">Describe any search peer review process.</td><td align="left" valign="bottom">~line 182</td></tr><tr><td align="left" valign="bottom" colspan="4">MANAGING RECORDS</td></tr><tr><td align="left" valign="bottom">Total Records</td><td align="char" char="." valign="bottom">15</td><td align="left" valign="bottom">Document the total number of records identified from each database and other information sources.</td><td align="left" valign="bottom">Figure 1</td></tr><tr><td align="left" valign="bottom">Deduplication</td><td align="char" char="." valign="bottom">16</td><td align="left" valign="bottom">Describe the processes and any software used to deduplicate records from multiple database searches and other information sources.</td><td align="left" valign="bottom">~line 200–203</td></tr><tr><td align="left" valign="bottom" colspan="4">PRISMA-S: An Extension to the PRISMA Statement for Reporting Literature Searches in Systematic Reviews. Rethlefsen ML, Kirtley S, Waffenschmidt S, Ayala AP, Moher D, Page MJ, Koffel JB, PRISMA-S Group. Last updated February 27, 2020.</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s11"><title>Grey literature search approach</title><list list-type="order"><list-item><p>Search reference lists of articles included for data extraction.</p></list-item><list-item><p>Forward/backward citation analysis of articles included for data extraction in Scopus and Web of Science (platforms including Science Citation Index Expanded (SCI-EXPANDED) -–1900-present and Social Sciences Citation Index (SSCI) -–1900-present).</p></list-item><list-item><p>Google Scholar search as follows: “Reproducibility” limited to the years 2018–2019.</p></list-item></list><p>Search of the following preprint servers: OpenScience Framework (OSF) including OSF Preprints, bioRxiv, EdArXiv, MediArXiv, NutriXiv, PeerJ, <ext-link ext-link-type="uri" xlink:href="http://www.Preprints.org">Preprints.org</ext-link>, PsyArXiv and SocArXiv, NBER Working Papers, Munich Personal RePEc Archive</p></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s12"><title>Level 1 Screening form item and criteria</title><table-wrap id="inlinetable3" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom" colspan="2">1. ISSUE: Does the study self-report to be a replication of previous quantitative research? (yes/no/unsure)</td></tr><tr><td align="left" valign="bottom"><bold>INCLUDE</bold></td><td align="left" valign="bottom"><bold>EXCLUDE</bold></td></tr><tr><td align="left" valign="bottom">All research articles containing one or more replications of quantitative research studies.</td><td align="left" valign="bottom">All research articles not describing a replication study<break/>All research articles describing exclusively qualitative replication studies</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s13"><title>Level 2 Screening form</title><table-wrap id="inlinetable4" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom" colspan="2">1. LANGUAGE – Is this study in English? (yes/no/unsure)</td></tr><tr><td align="left" valign="bottom"><bold>INCLUDE</bold></td><td align="left" valign="bottom"><bold>EXCLUDE</bold></td></tr><tr><td align="left" valign="bottom">Studies written in English.</td><td align="left" valign="bottom">Studies written in any other language that is not English.</td></tr></tbody></table></table-wrap><table-wrap id="inlinetable5" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom" colspan="2">2. DATE– Is this study published in 2018 or 2019? Use the most recent year stated on the publication (yes/no/unsure)</td></tr><tr><td align="left" valign="bottom"><bold>INCLUDE</bold></td><td align="left" valign="bottom"><bold>EXCLUDE</bold></td></tr><tr><td align="left" valign="bottom">Studies published in 2018 or 2019.</td><td align="left" valign="bottom">Studies publisher in any other year.<break/>EPub ahead of print.</td></tr></tbody></table></table-wrap><table-wrap id="inlinetable6" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom" colspan="2">3. PUBLICATION TYPE – Is this the right publication type? (yes/no/unsure)</td></tr><tr><td align="left" valign="bottom"><bold>INCLUDE</bold></td><td align="left" valign="bottom"><bold>EXCLUDE</bold></td></tr><tr><td align="left" valign="bottom">Original research articles describing quantitative research.</td><td align="left" valign="bottom"><list list-type="bullet"><list-item><p>Narrative reviews</p></list-item><list-item><p>Scoping reviews</p></list-item><list-item><p>Systematic reviews</p></list-item><list-item><p>Realist reviews</p></list-item><list-item><p>Mapping reviews</p></list-item><list-item><p>Literature reviews</p></list-item><list-item><p>Rapid reviews</p></list-item><list-item><p>Meta-Analysis</p></list-item><list-item><p>Overview or reviews</p></list-item><list-item><p>Umbrella reviews</p></list-item><list-item><p>In short any type of literature review or synthesis should be excluded.</p></list-item><list-item><p>Conference proceedings</p></list-item><list-item><p>Book chapters</p></list-item><list-item><p>Editorials, letters to the editor, commentaries</p></list-item><list-item><p>Opinion pieces</p></list-item><list-item><p>Case reports</p></list-item><list-item><p>Case control studies</p></list-item><list-item><p>Case series</p></list-item><list-item><p>Protocols</p></list-item><list-item><p>Guidelines</p></list-item><list-item><p>Web pages</p></list-item><list-item><p>Thesis projects</p></list-item><list-item><p>Policy documents</p></list-item><list-item><p>All exclusively qualitative research studies</p></list-item></list></td></tr></tbody></table></table-wrap><table-wrap id="inlinetable7" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom" colspan="2">4. ISSUE: Does the study self-report to be a replication of previous quantitative research? (yes/no/unsure)</td></tr><tr><td align="left" valign="bottom"><bold>INCLUDE</bold></td><td align="left" valign="bottom"><bold>EXCLUDE</bold></td></tr><tr><td align="left" valign="bottom">All research articles containing one or more replications of quantitative research studies.</td><td align="left" valign="bottom">All research articles not describing a replication study<break/>All research articles describing exclusively qualitative replication studies</td></tr></tbody></table></table-wrap><table-wrap id="inlinetable8" position="anchor"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom" colspan="2">5. DISCIPLINE – Does the study focus education, economics, psychology, biomedicine or health sciences? (yes/no/unsure)</td></tr><tr><td align="left" valign="bottom"><bold>INCLUDE</bold></td><td align="left" valign="bottom"><bold>EXCLUDE</bold></td></tr><tr><td align="left" valign="bottom">All research that is related to the disciplines of education, economics, psychology, biomedicine or health sciences.<break/>(<xref ref-type="table" rid="table1">Table 1</xref> info to be provided).</td><td align="left" valign="bottom">All research that is not related to education, economics, psychology, biomedicine or health sciences<break/>Exclude research related to complementary and alternative medicine.</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s14"><title>Level 3 extraction form</title><list list-type="order"><list-item><p>Year of publication (use the most recent year stated on the document):</p></list-item><list-item><p>Name of the journal/outlet the document is published in (Do not use abbreviations):</p></list-item><list-item><p>Corresponding author e-mail (If there is more than one corresponding author indicated, extract the first listed author only. Extract the name in the format: Initial Surname, e.g., D Moher. If there is more than one e-mail listed, extract the first listed e-mail only.):</p></list-item><list-item><p>Country of corresponding author affiliation (If there is more than one corresponding author indicated, extract the first listed corresponding author only. If there is more than one affiliation listed for this individual, extract from the first affiliation only):</p></list-item><list-item><p>How many authors are named on the document?</p></list-item><list-item><p>Does the study report a funding source (yes, no)</p><list list-type="alpha-lower"><list-item><p>If yes, which type of funder. Check all that apply. (Government, Academic, Industry, Non-Profit, Other, Can’t tell)</p></list-item></list></list-item><list-item><p>Did the study report ethics approval (yes, no, ethics approval not relevant)</p></list-item><list-item><p>Did the study recruit human participants or use animal participants? (yes, no)</p><list list-type="alpha-lower"><list-item><p>If yes; Does the study involve humans or animals? (humans, animals)</p></list-item><list-item><p>If human; Please specify how many were involved? (use the total number of participants enrolled, not necessarily the number analyzed):</p></list-item><list-item><p>If animal; Please specify how many were involved? (use the total number of animals included, not necessarily the number analyzed):</p></list-item></list></list-item><list-item><p>Did the replication study team specify that they contacted the original study project team? (Yes, the author teams were reported to overlap; Yes, there was contact but author teams do not overlap; No, the replication team did not report any interaction)</p></list-item><list-item><p>Does the replication study refer to a protocol that was registered prior to data collection? (yes, no)</p></list-item><list-item><p>Does the study indicate that data is shared publicly? (yes, no)</p></list-item><list-item><p>How many quantitative replication studies are reported in the paper? (One study, more than one study).</p></list-item></list><p>Note: If more than one study is reported, we will extract information from each quantitative study using the following questions.</p><list list-type="order"><list-item><p>What is the study design used: (observational study; clinical trial; experimental; data analysis, other):</p></list-item><list-item><p>Did the study specify a primary outcome being? (Yes/No).</p></list-item><list-item><p>If a primary outcome was stated, what was it? If a primary outcome was not stated, please extract that first stated outcome described in the study results section (note these will be thematically grouped).</p></list-item><list-item><p>What discipline does this work best fit in? (Economics, Education, Psychology, Health Sciences, Biomedicine, Other)</p></list-item><list-item><p>How did the authors of the study assess reproducibility?</p><list list-type="bullet"><list-item><p><bold>Evaluating against the null hypothesis:</bold> determining whether the replication showered a statistically significant effect, in the same direction as the original study, with a <italic>P</italic>-value &lt;0.05.</p></list-item><list-item><p><bold>Effect sizes:</bold> Evaluating replication effect against original effect size to examine for differences.</p></list-item><list-item><p><bold>Meta-analysis of original effect size:</bold> Evaluates effect sizes considering variance and of 95% confidence intervals.</p></list-item><list-item><p><bold>Subjective assessment of replication:</bold> An evaluation made by the research team as to whether they were successful in replicating the study findings.</p></list-item><list-item><p><bold>Other, please specify</bold>.</p></list-item><list-item><p><bold>Unclear</bold></p></list-item></list></list-item><list-item><p>Based on the authors definition of reproducibility, did the study replicate? (yes, no, mixed)</p></list-item><list-item><p>What was the p-value reported on the statistical test conducted on the main outcome? (value:; not reported)</p></list-item><list-item><p>What was the effect size reported on the statistical test conducted on the main outcome? Size: Measure:</p></list-item></list></sec></app><app id="appendix-7"><title>Appendix 7</title><sec sec-type="appendix" id="s15"><title>List of included documents</title><table-wrap id="inlinetable9" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">ID</th><th align="left" valign="bottom">Year</th><th align="left" valign="bottom">Journal</th><th align="left" valign="bottom">Corresponding Author</th><th align="left" valign="bottom">Funding</th><th align="left" valign="bottom">Number of studies replicated</th><th align="left" valign="bottom">Discipline</th></tr></thead><tbody><tr><td align="left" valign="bottom">20131</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Advances in Methods and Practices in Psychological Science</td><td align="left" valign="bottom">RJ McCarthy</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20130</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Advances in Methods and Practices in Psychological Science</td><td align="left" valign="bottom">B Verschuere</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20128</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Association for Psychological Science</td><td align="left" valign="bottom">M O'Donnell</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20126</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Association for Psychological Science</td><td align="left" valign="bottom">CJ Soto</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20125</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Psychological Science</td><td align="left" valign="bottom">TW Watts</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20124</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">eLife</td><td align="left" valign="bottom">MS Nieuwland</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20122</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Journal of Environmental Psychology</td><td align="left" valign="bottom">S Van der Linden</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20120</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">The European Political Science Association</td><td align="left" valign="bottom">A Coppock</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">other</td></tr><tr><td align="left" valign="bottom">20119</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Finance Research Letters</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://research-repository.uwa.edu.au/en/persons/dirk-baur">dirk.baur@uwa.edu.au</ext-link></td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">economics</td></tr><tr><td align="left" valign="bottom">20094</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Journal of Economic Psychology</td><td align="left" valign="bottom">AK Shah</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">20048</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">bioRxiv</td><td align="left" valign="bottom">X Zhang</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">biomedicine</td></tr><tr><td align="left" valign="bottom">20001</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Royal society open science</td><td align="left" valign="bottom">T Schuwerk</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">10805</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Rehabilitation Counseling Bulletin</td><td align="left" valign="bottom">BN Philips</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">economics</td></tr><tr><td align="left" valign="bottom">20000</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Advances in Methods and Practices in Psychological Science</td><td align="left" valign="bottom">RA Klein</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">13598</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Molecular Neurobiology</td><td align="left" valign="bottom">A Chan</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">biomedicine</td></tr><tr><td align="left" valign="bottom">13165</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Journal of Contemporary Criminal Justice</td><td align="left" valign="bottom">JP Stamatel</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">12923</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">PLOS One</td><td align="left" valign="bottom">LM Smith</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">12287</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">J Autism Dev Disord</td><td align="left" valign="bottom">L K Fung</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">11535</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Journal of Obsessive-Compulsive and Related Disorders</td><td align="left" valign="bottom">EN Riise</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">11456</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">eLife</td><td align="left" valign="bottom">J Repass</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">biomedicine</td></tr><tr><td align="left" valign="bottom">11003</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Journal of Health Economics</td><td align="left" valign="bottom">D Powell</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">10567</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Personality and Individual Differences</td><td align="left" valign="bottom">JJ McGinley</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">10555</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">J Nerv Ment Dis</td><td align="left" valign="bottom">G Parker</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">9886</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">The BMJ</td><td align="left" valign="bottom">J P A Ioannidis</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">9505</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">BMC Geriatrics</td><td align="left" valign="bottom">S E Straus</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">9193</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Journal for Research in Mathematics Education</td><td align="left" valign="bottom">K Melhuish</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">education</td></tr><tr><td align="left" valign="bottom">9011</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Journal of Contemporary Criminal Justice</td><td align="left" valign="bottom">CD Maxwell</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">8574</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Public Opinion Quarterly</td><td align="left" valign="bottom">J A Krosnick</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">economics</td></tr><tr><td align="left" valign="bottom">6213</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Psychophysiology</td><td align="left" valign="bottom">M Arns</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">biomedicine</td></tr><tr><td align="left" valign="bottom">5825</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">BMC Geriatrics</td><td align="left" valign="bottom">J.Holroyd-Leduc</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">5539</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Empirical Economics</td><td align="left" valign="bottom">B.Hayo</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">economics</td></tr><tr><td align="left" valign="bottom">5458</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Reproducing Public Health Services and Systems Research</td><td align="left" valign="bottom">J K Harris</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">5138</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Behavior Therapy</td><td align="left" valign="bottom">E J Wolf</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">5133</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Brain, Behavior, and Immunity</td><td align="left" valign="bottom">FR Guerini</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">biomedicine</td></tr><tr><td align="left" valign="bottom">5100</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">European Neuropsychopharmacology</td><td align="left" valign="bottom">R Lanzenberger</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">biomedicine</td></tr><tr><td align="left" valign="bottom">4794</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">American Psychological Association</td><td align="left" valign="bottom">R J Giuliano</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">3962</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Journal of Applied Behaviour Analysis</td><td align="left" valign="bottom">G Rooker</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">3677</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Journal of Pediatric Psychology</td><td align="left" valign="bottom">B D Earp</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">3574</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Personnel Psychology</td><td align="left" valign="bottom">G F Dreher</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">3138</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Journal of Second Language Writing</td><td align="left" valign="bottom">C de Kleine</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">education</td></tr><tr><td align="left" valign="bottom">2310</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">PLOS ONE</td><td align="left" valign="bottom">B Chen</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">1959</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Nature Human Behaviour</td><td align="left" valign="bottom">BA Nosek</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">other</td></tr><tr><td align="left" valign="bottom">1837</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Oxford Bulletin Of Economics and Statistics</td><td align="left" valign="bottom">D Buncic</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">economics</td></tr><tr><td align="left" valign="bottom">1681</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Cortex</td><td align="left" valign="bottom">SG Brederoo</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">More than one study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">1477</td><td align="left" valign="bottom">2019</td><td align="left" valign="bottom">Frontiers in Psychology</td><td align="left" valign="bottom">M Boch</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">psychology</td></tr><tr><td align="left" valign="bottom">727</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Archives of Clinical Neuropsychology</td><td align="left" valign="bottom">P Armistead-Jehle</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr><tr><td align="left" valign="bottom">584</td><td align="left" valign="bottom">2018</td><td align="left" valign="bottom">Australian Psychologist</td><td align="left" valign="bottom">RJ Brunton</td><td align="left" valign="bottom">Not reported</td><td align="left" valign="bottom">One study</td><td align="left" valign="bottom">health sciences</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-8"><title>Appendix 8</title><sec sec-type="appendix" id="s16"><title>Thematic groups of primary outcomes of studies replicated</title><table-wrap id="inlinetable10" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">No.</th><th align="left" valign="bottom">Theme</th><th align="left" valign="bottom">N (%)</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">Clinical and biological outcomes</td><td align="char" char="." valign="bottom">19 (10.7)</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">Public health</td><td align="char" char="." valign="bottom">5 (2.8)</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">Mental health and wellbeing</td><td align="char" char="." valign="bottom">6 (3.4)</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">Criminology</td><td align="char" char="." valign="bottom">5 (2.8)</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">Economics</td><td align="char" char="." valign="bottom">8 (4.5)</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">Individual differences</td><td align="char" char="." valign="bottom">58 (32.8)</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">Visual cognition</td><td align="char" char="." valign="bottom">11 (6.2)</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">Morality</td><td align="char" char="." valign="bottom">5 (2.8)</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">Score and performance</td><td align="char" char="." valign="bottom">11 (6.2)</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">Political views</td><td align="char" char="." valign="bottom">11 (6.2)</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom">Not reported/unclear</td><td align="char" char="." valign="bottom">30 (17.0)</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">Other</td><td align="char" char="." valign="bottom">8 (4.5)</td></tr></tbody></table></table-wrap></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78518.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Allison</surname><given-names>David B</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kg8sb98</institution-id><institution>Indiana University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>It has been recognized since the beginning of science that science can always be made more rigorous. Indeed, it is part of the ethos and very nature of the scientific method and the scientific attitude, as Lee McIntyre describes in his brilliant book by that title, to be constantly striving for improvements in rigor. Yet, we know that there are breaches in rigor, reproducibility, and transparency of research conduct and reporting. Such breaches have been highlighted more intensively, or at least so it seems, for more than a decade. The field recognizes that we need to go beyond platitudinous recognition that there is always opportunity for improvement in rigor and that such improvements are vital, to identifying those key leverage points where efforts can have the most positive near-term effects. Identifying domains in which reproducibility is greater or lesser than in other domains can aid in that regard. Thus, this article represents a constructive step in identifying key opportunities for bettering our science and that is something that every scientist can stand behind.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78518.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Allison</surname><given-names>David B</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kg8sb98</institution-id><institution>Indiana University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Vorland</surname><given-names>Colby J</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kg8sb98</institution-id><institution>Indiana University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Lupia</surname><given-names>Arthur</given-names></name><role>Reviewer</role><aff><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Agley</surname><given-names>Jon</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02k40bc56</institution-id><institution>Indiana University Bloomington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your article &quot;Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: A scoping review&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Mone Zaidi as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Colby J Vorland (Reviewer #1); Arthur Lupia (Reviewer #2); Jon Agley (Reviewer #3).</p><p>I believe that the reviewers' comments are clear and should be helpful to you.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Cobey et al. conducted an exhaustive survey of a number of five disciplines to catalogue and characterize replications. The authors employed best practices in performing their review and reporting, and the results provide an important snapshot of replications. Kudos to the authors for their transparent practices – preregistering their protocol, and specifying protocol amendments.</p><p>A limitation of this work is that it only surveys 2018-2019, and many replication projects were not published then, which limits comparisons across disciplines. This is understandable given the herculean screening task for just these two years, and the authors do emphasize this limitation in their discussion. Including the years 2018-2019 in the title seems appropriate to make this clear in searches.</p><p>The manuscript is generally well written, but throughout the text the authors use the terms 'reproducibility' and 'replicability' interchangeably (sometimes within the same sentence), which is confusing. Even more so is that the authors attempt to distinguish the definitions of each in the introduction:</p><p>Line 102: [Defining 'reproducibility'] &quot;Here, we loosely use Nosek and Errington's definition: &quot;a study for which any outcome would be considered diagnostic evidence about a claim from prior research&quot;&quot;.</p><p>– However, this quote refers to the term 'replication'. Here is the full quote: &quot;Replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research.&quot; An additional quote from that paper defines reproducibility and replication like so: &quot;Credibility of scientific claims is established with evidence for their replicability using new data [1]. This is distinct from retesting a claim using the same analyses and same data (usually referred to as reproducibility or computational reproducibility) …&quot; These definitions are in alignment with the 2019 NASEM report (https://nap.nationalacademies.org/catalog/25303/reproducibility-and-replicability-in-science). As far as I can understand, the authors studied *both* concepts in this paper; most published reports were replications, but it is noted on line 330 that &quot;The remainder of studies captured were data studies e.g., re-analysis using previous data (35, 19.8%)…&quot;. If it is the case and that this means that these data studies re-analyzed the *same* dataset as the original work, the manuscript should be rewritten to distinguish replication from reproducibility throughout. The authors may wish to note the fact that different disciplines may use these terms in different ways (e.g., expand the discussion of reference #1), and that reproducibility is sometimes used to refer to transparency, and in other contexts replicability.</p><p>In the introduction, the authors note the percentages of studies that replicate for various replication projects. Yet, that is taking each project's definition of what a successful replication is. As the authors point out on line 131, some may define in different ways and thus simplified conclusions are difficult. This point could be moved earlier to emphasize that this is the case for all of these projects.</p><p>The data statement states: &quot;Data and materials are available on the Open Science Framework (https://osf.io/wn7gm/).&quot; I see the protocol there, but no extracted data; will these be added?</p><p>Could some of the results in tables 1-3 also be presented graphically?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This article offers an important discussion, and datapoint, relevant to helping scientific researchers more closely reconcile what they observe with what they claim. There is tremendous potential public and scientific value in the endeavor. The value arises from the fact that many academic reward systems are tilted towards mass production of statistically significant claims and away from truer representations of what these claims mean to readers or end users. While there is greater recognition of these problems, and many attempts to improve practices, this article raises tough questions about how well actual practice aligns with publicly stated RR goals. The authors seek a constructive way forward. They offer an empirical analysis that documents and compares practices across several disciplines. Tables 2 and 3 are particularly instructive and a model for future work of this kind.</p><p>My biggest concern with the paper is that there is a gap between the article's findings and several generalizations that are made. I find the method and empirical results interesting, but have limited confidence in how much these claims generalize – even to representative samples of articles in the fields on which the paper focuses. Edits that more closely align the text with the observations will help readers more accurately interpret the authors' findings.</p><p>For example, the authors list a series of discipline-specific article databases that they use to identify articles for comparison. If a goal is to make discipline-wide generalizations from these set of articles, it would help readers to know more about the comparability of the databases. For example, are the databases equally representative of the fields that they are characterized as covering (i.e., do some of the databases favor subfields within a discipline or lack full coverage in other ways)? As the article is written, the reader is asked to take this premise on faith. The truth-value of this premise is not apparent to me. If there is no empirical or structural basis for assuming direct comparability, the fact should be noted, cross-disciplinary comparisons or conclusions should reference the caveat, and generalizations beyond this caveat should not be made.</p><p>Another aspect of the analysis that may impede generalization is the decision to list the first reported RR outcome in studies that do not list a primary outcome. Given known incentives that lead to publication biases, and against null results, isn't it likely that first reported outcomes are more likely to be non-replications? If this is a possibility, could they compare RR success rates from first reported outcomes to last-reported outcomes? If there is no difference, the fact can be noted and a particular type of generalization becomes possible. If there is a difference, then the caveat should be added in subsequent claims.</p><p>On a related note, the authors conclude that RR frequencies and success rates are modest. But what is the relevant base rate? This standard may be easier to define for RR outcomes. On the optimal number of RR attempts, I think that there is less of a consensus. To state the question in a leading manner, if one replication/reproduction is better than none, why aren't two better than one? This question, and others like it, imply that observed rates of RR attempts that are less than 100% may not be suboptimal in a broader sense.</p><p>On a different note, I am concerned about how the coding was done. Specifically, it appears that co-authors served as &quot;screeners&quot; of how to categorize certain articles and article attributes. In other words, they made decision that influenced the data that they were analyzing. The risk is that people who know the hypotheses and desired outcomes implicitly bring that knowledge to coding decisions. An established best practice in fields where this type of coding is done is to first train independent coders who are unaware of the researchers; hypotheses and then conduct rigorous inter-coder reliability assessments to document the extent to which the combination of coders and categorical framework produce coding outcomes that parallel the underlying conceptual framework. Such practices increase the likelihood that data generating processes are independent of hypothesis evaluation processes.</p><p>For these reasons, I am very interested in the questions that the authors ask, and what they find, but I am not convinced that a number of the empirical claims pertaining to comparisons and magnitudes will generalize even to larger populations of articles in the stated disciplines.</p><p>I would like to see the Discussion section rewritten to focus on what the findings, and methodological challenges, mean for future work. In the current version, there are a number of speculative claims and generalizations that do not follow from the empirical work in this paper. These facts and some variation in the focus of the text in that section make the discussion longer than it needs to be and may have the effect of diminishing the excellent work that was done in the earlier pages.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The authors provide a fairly clear and accurate summary of the current questions around reproducibility in the scientific literature. I was impressed that they thoughtfully included a description of the areas where their utilized methodology was different than what they had proposed in their protocol and explained the reasons for those differences. That kind of documentation is valuable and fosters transparency in the research process.</p><p>The results of their analysis are provided in both narrative format and in tables and appendices. The authors effectively characterize the different, and sometimes difficult, decisions made in parsing the results of their search. While much attention has been paid to replication and reproducibility in recent years, the nature of the results reflects the reality that the replication studies themselves may have reporting issues, and it may not always be possible to ascertain how to link the results of such studies with the original work. The study results meaningfully add to the current, small body of literature examining the meta-scientific issues of reproducibility and replication of scientific findings.</p><p>The paper is cautious in its approach to interpretation, appropriately using language such as &quot;this may suggest that…&quot; to better ensure that readers do not draw inappropriately firm conclusions. This is a descriptive paper, and so readers, like the authors, would best be served by limiting strong inferences based on the findings. At the same time, the authors helpfully suggest numerous meaningful pathways forward to advance reproducibility research in multiple scientific domains. Such studies would in turn facilitate more powerful inferences.</p><p>In preparing this review, the substantive majority of recommendations that I had for the authors related to points of clarity conveyed in private comments rather than overt weaknesses or concerns.</p><p>LL50-52: The meaning of this sentence is not entirely clear (though the paragraph can still be understood when skipping the sentence). Please consider rewriting to capture your point more closely.</p><p>LL106-108: Since that analysis was published in 2014, and given the recent (albeit anecdotal) increased emphasis on reproducibility, it may be worth noting the date of the study in the text itself (or the termination date of the search used for the 0.13% statistic).</p><p>L126: In this context, &quot;some effect in inbreeding&quot; is unclear.</p><p>LL135-136: There may be value in disentangling the link between reproducibility and harms in the Introduction more explicitly. In the cited study (Le Noury et al), medications were utilized in a trial where reanalysis indicated that they were unlikely to produce a clinical benefit and each resulted in plausibly increased risk of harm. But it was not the failure to replicate, per se, that led to possible harms. Rather, the concern was clinical decision making predicated on the results of the original study, the results of which &quot;stood&quot; pending reanalysis. This may seem like a pedantic point but I think it speaks to the role not only of the original study itself, but also the degree to which decisions were made based on the findings of a single study.</p><p>L240: Can you provide a short clarification of what you mean by calibration test – such as inserting a parenthetical example &quot;…performed a calibration test (e.g., did X) prior to…&quot;</p><p>Table 1: The box indicating a cross section of number of authors and all studies appears to have inaccurate/incomplete information.</p><p>Table 1: The way the data are structured is a little unclear. For example, for &quot;All Studies&quot; the &quot;discipline&quot; row uses a denominator of total replication studies, whereas &quot;year of publication&quot; in the same column uses a denominator of the number of papers. While I see why this was done, I think additional clarity or uniformity in how the data are presented would be helpful.</p><p>LL300-301: Could you briefly indicate whether you think that excluding documents unavailable in full via your library may have affected the results (e.g., was this 11 quantitative replications, as assessed by abstracts? or was it a mixed bag?).</p><p>L384: Does the count for psychology include the three dozen or so replication studies that were unusable for this paper? If so, do the authors think that their inclusion here serves to accurately represent the replication efforts in psychology or may artificially inflate the frequency? I don't have the perspective to know which is the case, but I think the question is worth considering.</p><p>LL397-398: Would you be willing to share any information (if it exists) about whether independent and separate replications differ from those embedded within experimental protocols in meaningful ways (not just in terms of the obvious process differences)?</p><p>L417: Are you referring to registered protocols for the replication studies? Or registered protocols for the original study that can be used in completing the replication?</p><p>LL430-432: It might be helpful for readers to be briefly informed about what &quot;the current environment&quot; means in practice, as there is a lot of possible variability. For example, I might assume that there is an overemphasis on novelty at the desk editing stage, but it's not clear whether the authors were even thinking of that in particular.</p><p>LL434-448: There is a lot of interesting discussion in this paragraph that gets somewhat &quot;jumbled together&quot; in this paragraph. First, I'm not sure that it makes sense to characterize the finding as biased in L436. Rather, you found that for replication studies that were not part of the same paper, lack of overlap was common, and you do not make any assertions about studies where that was not the case (but presume the overlap might be higher – and in fact would by definition be nearly 100% since authors do not change partway through papers). Second, the larger question about what constitutes a replication is related to, but seemingly separate from, the initial discussion. Even if the authors agree on what a replication would look like for a specific study, it may not be coherent in the context of the broader field's common understanding of reproducibility.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: A scoping review of articles published in 2018-2019&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Mone Zaidi (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some modest remaining issues that need to be addressed, as outlined below:</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I would like to thank the authors for their response to my original comments. In most cases, I found that the authors' revisions were responsive to my concerns. In the instances where I did not find that to be entirely the case, I have noted it below using the new line numbers.</p><p>L126: Thank you for revising the line about inbreeding. However, I think that the replacement phrase is still a little unclear. The sentence is structured as an 'either/or' and the first part suggests familiarity with the study method may increase likelihood of reproducing the same findings. So I am not sure how &quot;researcher familiarity&quot; functions as the alternative. The sense I get is that the authors may be hedging around suggesting that in some cases, the author's approach to research (perhaps unusually rigorous, or unusually sloppy) may carry over between studies and explain some of the variability in findings. However, that is an assumption. In any case, the authors might benefit from being very direct about their theory here.</p><p>L110: The phrase &quot;…and some disciplines do worse in this regard&quot; might be interpreted to have two meanings. I interpret it to mean that some disciplines have fewer studies where reproductions have been attempted. However, &quot;do worse&quot; might also be taken to mean that some disciplines' studies are less likely to successfully have their results reproduced, which is a different concept entirely.</p><p>LL404-405: I appreciate this revision made in response to reviewer #1. I am unsure about whether an assertion of &quot;unique cultures&quot; is significantly different than the original assertion of which cultures have it as a normative value. I think the line may be fine to retain, but should be constrained with a caveat e.g., &quot;This may suggest unique cultures around reproducibility across different disciplines, but further study is needed to determine whether this is truly the case, and our study should not be taken as proof of differences between disciplines.&quot;</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78518.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: The authors appealed the original decision. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Cobey et al. conducted an exhaustive survey of a number of five disciplines to catalogue and characterize replications. The authors employed best practices in performing their review and reporting, and the results provide an important snapshot of replications. Kudos to the authors for their transparent practices – preregistering their protocol, and specifying protocol amendments.</p><p>A major limitation of this work is that it only surveys 2018-2019, and many replication projects were not published then, which limits comparisons across disciplines. This is understandable given the herculean screening task for just these two years, and the authors do emphasize this limitation in their discussion. Including the years 2018-2019 in the title seems appropriate to make this clear in searches.</p></disp-quote><p>We have updated the title to incorporate this feedback. It now reads: Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: A scoping review of articles published in 2018-2019.</p><disp-quote content-type="editor-comment"><p>The manuscript is generally well written, but throughout the text the authors use the terms 'reproducibility' and 'replicability' interchangeably (sometimes within the same sentence), which is confusing. Even more so is that the authors attempt to distinguish the definitions of each in the introduction:</p><p>Line 102: [Defining 'reproducibility'] &quot;Here, we loosely use Nosek and Errington's definition: &quot;a study for which any outcome would be considered diagnostic evidence about a claim from prior research&quot;&quot;.</p><p>– However, this quote refers to the term 'replication'. Here is the full quote: &quot;Replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research.&quot; An additional quote from that paper defines reproducibility and replication like so: &quot;Credibility of scientific claims is established with evidence for their replicability using new data [1]. This is distinct from retesting a claim using the same analyses and same data (usually referred to as reproducibility or computational reproducibility) …&quot; These definitions are in alignment with the 2019 NASEM report (https://nap.nationalacademies.org/catalog/25303/reproducibility-and-replicability-in-science). As far as I can understand, the authors studied *both* concepts in this paper; most published reports were replications, but it is noted on line 330 that &quot;The remainder of studies captured were data studies e.g., re-analysis using previous data (35, 19.8%)…&quot;. If it is the case and that this means that these data studies re-analyzed the *same* dataset as the original work, the manuscript should be rewritten to distinguish replication from reproducibility throughout. The authors may wish to note the fact that different disciplines may use these terms in different ways (e.g., expand the discussion of reference #1), and that reproducibility is sometimes used to refer to transparency, and in other contexts replicability.</p></disp-quote><p>Thanks for this thoughtful reflection. We agree the terminology could be clarified further. We have adjusted this section, it now reads:</p><p>“Reproducibility is a central tenant of research. Reproducing previously published studies to determine if results are consistent helps us to discern discoveries from false leads. The lexicon around the terms reproducibility and replication is diverse and poorly defined and may differ between disciplines <sup>1</sup><italic>.</italic> Here, we loosely use Nosek and Errington’s definition for a successful replication, namely: “a study for which any outcome would be considered diagnostic evidence about a claim from prior research”<sup>2</sup><italic>.”</italic></p><disp-quote content-type="editor-comment"><p>In the introduction, the authors note the percentages of studies that replicate for various replication projects. Yet, that is taking each project's definition of what a successful replication is. As the authors point out on line 131, some may define in different ways and thus simplified conclusions are difficult. This point could be moved earlier to emphasize that this is the case for all of these projects.</p></disp-quote><p>We have taken on this suggestion by adding the line “Comparison of rates of replication prove challenging because results will depend on the definition of replication success.” further ahead in the paper, around line 118.</p><disp-quote content-type="editor-comment"><p>The data statement states: &quot;Data and materials are available on the Open Science Framework (https://osf.io/wn7gm/).&quot; I see the protocol there, but no extracted data; will these be added?</p><p>Could some of the results in tables 1-3 also be presented graphically?</p></disp-quote><p>All study data and materials have been made available at this link now.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This article offers an important discussion, and datapoint, relevant to helping scientific researchers more closely reconcile what they observe with what they claim. There is tremendous potential public and scientific value in the endeavor. The value arises from the fact that many academic reward systems are tilted towards mass production of statistically significant claims and away from truer representations of what these claims mean to readers or end users. While there is greater recognition of these problems, and many attempts to improve practices, this article raises tough questions about how well actual practice aligns with publicly stated RR goals. The authors seek a constructive way forward. They offer an empirical analysis that documents and compares practices across several disciplines. Tables 2 and 3 are particularly instructive and a model for future work of this kind.</p><p>My biggest concern with the paper is that there is a gap between the article's findings and several generalizations that are made. I find the method and empirical results interesting, but have limited confidence in how much these claims generalize – even to representative samples of articles in the fields on which the paper focuses. Edits that more closely align the text with the observations will help readers more accurately interpret the authors' findings.</p><p>For example, the authors list a series of discipline-specific article databases that they use to identify articles for comparison. If a goal is to make discipline-wide generalizations from these set of articles, it would help readers to know more about the comparability of the databases. For example, are the databases equally representative of the fields that they are characterized as covering (i.e., do some of the databases favor subfields within a discipline or lack full coverage in other ways)? As the article is written, the reader is asked to take this premise on faith. The truth-value of this premise is not apparent to me. If there is no empirical or structural basis for assuming direct comparability, the fact should be noted, cross-disciplinary comparisons or conclusions should reference the caveat, and generalizations beyond this caveat should not be made.</p></disp-quote><p>Thanks for this comment. We have modified the paper to address this in the discussion. We have added: “It is also possible that the databases used do not equally represent the distinct disciplines we investigated, meaning that the searches are not directly comparable cross-disciplinarily”.</p><disp-quote content-type="editor-comment"><p>Another aspect of the analysis that may impede generalization is the decision to list the first reported RR outcome in studies that do not list a primary outcome. Given known incentives that lead to publication biases, and against null results, isn't it likely that first reported outcomes are more likely to be non-replications? If this is a possibility, could they compare RR success rates from first reported outcomes to last-reported outcomes? If there is no difference, the fact can be noted and a particular type of generalization becomes possible. If there is a difference, then the caveat should be added in subsequent claims.</p></disp-quote><p>We have now noted this possibility in the discussion. We have added: “For feasibility we also only extracted information about the primary outcome listed for each paper, or if no primary outcome was specified, the first listed outcome. It is possible that rates of replication differ across outcomes. Future research could consider all outcomes listed.”</p><disp-quote content-type="editor-comment"><p>On a related note, the authors conclude that RR frequencies and success rates are modest. But what is the relevant base rate? This standard may be easier to define for RR outcomes. On the optimal number of RR attempts, I think that there is less of a consensus. To state the question in a leading manner, if one replication/reproduction is better than none, why aren't two better than one? This question, and others like it, imply that observed rates of RR attempts that are less than 100% may not be suboptimal in a broader sense.</p></disp-quote><p>The paper specifies “When we examined the 177 individual studies replicated in the 47 documents, we found only a minority of them referred to registered protocols”. We feel this accurately reflects the data; the term modest is not used. We have added to the discussion to consider the points raised: “We acknowledge, however, that mandates for registration are rare and exist only in particular disciplines and for specific study designs.”</p><disp-quote content-type="editor-comment"><p>On a different note, I am concerned about how the coding was done. Specifically, it appears that co-authors served as &quot;screeners&quot; of how to categorize certain articles and article attributes. In other words, they made decision that influenced the data that they were analyzing. The risk is that people who know the hypotheses and desired outcomes implicitly bring that knowledge to coding decisions. An established best practice in fields where this type of coding is done is to first train independent coders who are unaware of the researchers; hypotheses and then conduct rigorous inter-coder reliability assessments to document the extent to which the combination of coders and categorical framework produce coding outcomes that parallel the underlying conceptual framework. Such practices increase the likelihood that data generating processes are independent of hypothesis evaluation processes.</p></disp-quote><p>Piloting was indeed undertaken to train reviewers to ensure consistency. This has been further specified by adding: “Prior to extraction a series of iterative pilot tests were done on included documents to ensure consistency between extractors.”</p><disp-quote content-type="editor-comment"><p>For these reasons, I am very interested in the questions that the authors ask, and what they find, but I am not convinced that a number of the empirical claims pertaining to comparisons and magnitudes will generalize even to larger populations of articles in the stated disciplines.</p></disp-quote><p>We have expanded the discussion about generalizability by adding: “Collectively these study design decisions and practical challenges present limitations on the overall generalizability of the findings beyond our dataset.”</p><disp-quote content-type="editor-comment"><p>I would like to see the Discussion section rewritten to focus on what the findings, and methodological challenges, mean for future work. In the current version, there are a number of speculative claims and generalizations that do not follow from the empirical work in this paper. These facts and some variation in the focus of the text in that section make the discussion longer than it needs to be and may have the effect of diminishing the excellent work that was done in the earlier pages.</p></disp-quote><p>The reviewer has provided helpful discussion above which we have addressed as per our rebuttal notes. We feel the issues regarding potential concerns about generalizability are now stressed in the discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The authors provide a fairly clear and accurate summary of the current questions around reproducibility in the scientific literature. I was impressed that they thoughtfully included a description of the areas where their utilized methodology was different than what they had proposed in their protocol and explained the reasons for those differences. That kind of documentation is valuable and fosters transparency in the research process.</p><p>The results of their analysis are provided in both narrative format and in tables and appendices. The authors effectively characterize the different, and sometimes difficult, decisions made in parsing the results of their search. While much attention has been paid to replication and reproducibility in recent years, the nature of the results reflects the reality that the replication studies themselves may have reporting issues, and it may not always be possible to ascertain how to link the results of such studies with the original work. The study results meaningfully add to the current, small body of literature examining the meta-scientific issues of reproducibility and replication of scientific findings.</p><p>The paper is cautious in its approach to interpretation, appropriately using language such as &quot;this may suggest that…&quot; to better ensure that readers do not draw inappropriately firm conclusions. This is a descriptive paper, and so readers, like the authors, would best be served by limiting strong inferences based on the findings. At the same time, the authors helpfully suggest numerous meaningful pathways forward to advance reproducibility research in multiple scientific domains. Such studies would in turn facilitate more powerful inferences.</p><p>In preparing this review, the substantive majority of recommendations that I had for the authors related to points of clarity conveyed in private comments rather than overt weaknesses or concerns.</p><p>LL50-52: The meaning of this sentence is not entirely clear (though the paragraph can still be understood when skipping the sentence). Please consider rewriting to capture your point more closely.</p></disp-quote><p>Agree. We have deleted this line.</p><disp-quote content-type="editor-comment"><p>LL106-108: Since that analysis was published in 2014, and given the recent (albeit anecdotal) increased emphasis on reproducibility, it may be worth noting the date of the study in the text itself (or the termination date of the search used for the 0.13% statistic).</p></disp-quote><p>We have specified 2014 now.</p><disp-quote content-type="editor-comment"><p>L126: In this context, &quot;some effect in inbreeding&quot; is unclear.</p></disp-quote><p>We have reworded to “some effect of researcher familiarity”.</p><disp-quote content-type="editor-comment"><p>LL135-136: There may be value in disentangling the link between reproducibility and harms in the Introduction more explicitly. In the cited study (Le Noury et al), medications were utilized in a trial where reanalysis indicated that they were unlikely to produce a clinical benefit and each resulted in plausibly increased risk of harm. But it was not the failure to replicate, per se, that led to possible harms. Rather, the concern was clinical decision making predicated on the results of the original study, the results of which &quot;stood&quot; pending reanalysis. This may seem like a pedantic point but I think it speaks to the role not only of the original study itself, but also the degree to which decisions were made based on the findings of a single study.</p></disp-quote><p>Great point to consider. We have amended this to read: “In medicine, studies that do not reproduce in clinic may exaggerate patient benefits and harms<sup>14</sup> especially when clinical decisions are based on a single study.”</p><disp-quote content-type="editor-comment"><p>L240: Can you provide a short clarification of what you mean by calibration test – such as inserting a parenthetical example &quot;…performed a calibration test (e.g., did X) prior to…&quot;</p></disp-quote><p>We have specified: “Specifically, a series of included documents were then extracted independently. The team then met to discuss differences in extraction between team members and challenges encountered before extracting from subsequent documents. This was repeated until consensus was reached.”</p><disp-quote content-type="editor-comment"><p>Table 1: The box indicating a cross section of number of authors and all studies appears to have inaccurate/incomplete information.</p></disp-quote><p>Thanks for catching this. This has been updated.</p><disp-quote content-type="editor-comment"><p>Table 1: The way the data are structured is a little unclear. For example, for &quot;All Studies&quot; the &quot;discipline&quot; row uses a denominator of total replication studies, whereas &quot;year of publication&quot; in the same column uses a denominator of the number of papers. While I see why this was done, I think additional clarity or uniformity in how the data are presented would be helpful.</p></disp-quote><p>This distinction has been reflected in the footnotes.</p><disp-quote content-type="editor-comment"><p>LL300-301: Could you briefly indicate whether you think that excluding documents unavailable in full via your library may have affected the results (e.g., was this 11 quantitative replications, as assessed by abstracts? or was it a mixed bag?).</p></disp-quote><p>We have added this to the limitation section by specifying: “We also were not able to locate the full-text of all included documents which may have impacted the results.”</p><disp-quote content-type="editor-comment"><p>L384: Does the count for psychology include the three dozen or so replication studies that were unusable for this paper? If so, do the authors think that their inclusion here serves to accurately represent the replication efforts in psychology or may artificially inflate the frequency? I don't have the perspective to know which is the case, but I think the question is worth considering.</p></disp-quote><p>It is a good point, but hard to know without accessing the full-text. In addition to the line above, we now specify: “The impact of these missing texts may not have been equal across disciplines.”</p><disp-quote content-type="editor-comment"><p>LL397-398: Would you be willing to share any information (if it exists) about whether independent and separate replications differ from those embedded within experimental protocols in meaningful ways (not just in terms of the obvious process differences)?</p></disp-quote><p>We did not examine this.</p><disp-quote content-type="editor-comment"><p>L417: Are you referring to registered protocols for the replication studies? Or registered protocols for the original study that can be used in completing the replication?</p></disp-quote><p>We refer to registered protocol of the replication studies. This has not been further clarified by modifying the line to read “we recorded whether a registered protocol for the replication study was used”.</p><disp-quote content-type="editor-comment"><p>LL430-432: It might be helpful for readers to be briefly informed about what &quot;the current environment&quot; means in practice, as there is a lot of possible variability. For example, I might assume that there is an overemphasis on novelty at the desk editing stage, but it's not clear whether the authors were even thinking of that in particular.</p></disp-quote><p>That’s correct. We have modified this line to read: “Of note, when no new data are generated, it may be difficult in the current research environment, which tends to favor novelty, to publish a re-analysis of existing data that shows the exact same result<sup>31,32</sup>.”</p><disp-quote content-type="editor-comment"><p>LL434-448: There is a lot of interesting discussion in this paragraph that gets somewhat &quot;jumbled together&quot; in this paragraph. First, I'm not sure that it makes sense to characterize the finding as biased in L436. Rather, you found that for replication studies that were not part of the same paper, lack of overlap was common, and you do not make any assertions about studies where that was not the case (but presume the overlap might be higher – and in fact would by definition be nearly 100% since authors do not change partway through papers). Second, the larger question about what constitutes a replication is related to, but seemingly separate from, the initial discussion. Even if the authors agree on what a replication would look like for a specific study, it may not be coherent in the context of the broader field's common understanding of reproducibility.</p></disp-quote><p>We have re-read this section to address the jumble and streamline.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><p>The manuscript has been improved but there are some modest remaining issues that need to be addressed, as outlined below:</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):I would like to thank the authors for their response to my original comments. In most cases, I found that the authors' revisions were responsive to my concerns. In the instances where I did not find that to be entirely the case, I have noted it below using the new line numbers.</p><p>L126: Thank you for revising the line about inbreeding. However, I think that the replacement phrase is still a little unclear. The sentence is structured as an 'either/or' and the first part suggests familiarity with the study method may increase likelihood of reproducing the same findings. So I am not sure how &quot;researcher familiarity&quot; functions as the alternative. The sense I get is that the authors may be hedging around suggesting that in some cases, the author's approach to research (perhaps unusually rigorous, or unusually sloppy) may carry over between studies and explain some of the variability in findings. However, that is an assumption. In any case, the authors might benefit from being very direct about their theory here.</p></disp-quote><p>We have amended the line to read: “This may suggest that detailed familiarity with the original study method increases the likelihood of reproducing the research findings.”</p><disp-quote content-type="editor-comment"><p>L110: The phrase &quot;…and some disciplines do worse in this regard&quot; might be interpreted to have two meanings. I interpret it to mean that some disciplines have fewer studies where reproductions have been attempted. However, &quot;do worse&quot; might also be taken to mean that some disciplines' studies are less likely to successfully have their results reproduced, which is a different concept entirely.</p></disp-quote><p>We have amended this line to read: “Most scientific studies are never formally reproduced and some disciplines have lower rates of reproducibility attempts than others.”</p><disp-quote content-type="editor-comment"><p>LL404-405: I appreciate this revision made in response to reviewer #1. I am unsure about whether an assertion of &quot;unique cultures&quot; is significantly different than the original assertion of which cultures have it as a normative value. I think the line may be fine to retain, but should be constrained with a caveat e.g., &quot;This may suggest unique cultures around reproducibility across different disciplines, but further study is needed to determine whether this is truly the case, and our study should not be taken as proof of differences between disciplines.&quot;</p></disp-quote><p>We have amended this line to read: “This may suggest unique cultures around reproducibility in distinct disciplines, future research is needed to determine if such differences truly exist given the limitations of our search and approach.”</p></body></sub-article></article>