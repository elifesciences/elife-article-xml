<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80671</article-id><article-id pub-id-type="doi">10.7554/eLife.80671</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Learning predictive cognitive maps with spiking neurons during behavior and replays</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-284008"><name><surname>Bono</surname><given-names>Jacopo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9552-3151</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-284009"><name><surname>Zannone</surname><given-names>Sara</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9526-7001</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-284010"><name><surname>Pedrosa</surname><given-names>Victor</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-17772"><name><surname>Clopath</surname><given-names>Claudia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4507-8648</contrib-id><email>c.clopath@imperial.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Department of Bioengineering, Imperial College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name><role>Reviewing Editor</role><aff><institution>Stanford School of Medicine</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>03</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e80671</elocation-id><history><date date-type="received" iso-8601-date="2022-05-30"><day>30</day><month>05</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-12"><day>12</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-08-17"><day>17</day><month>08</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.08.16.456545"/></event></pub-history><permissions><copyright-statement>© 2023, Bono, Zannone et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Bono, Zannone et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80671-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-80671-figures-v1.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.80663" id="ra1"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.80680" id="ra2"/><abstract><p>The hippocampus has been proposed to encode environments using a representation that contains predictive information about likely future states, called the successor representation. However, it is not clear how such a representation could be learned in the hippocampal circuit. Here, we propose a plasticity rule that can learn this predictive map of the environment using a spiking neural network. We connect this biologically plausible plasticity rule to reinforcement learning, mathematically and numerically showing that it implements the TD-lambda algorithm. By spanning these different levels, we show how our framework naturally encompasses behavioral activity and replays, smoothly moving from rate to temporal coding, and allows learning over behavioral timescales with a plasticity rule acting on a timescale of milliseconds. We discuss how biological parameters such as dwelling times at states, neuronal firing rates and neuromodulation relate to the delay discounting parameter of the TD algorithm, and how they influence the learned representation. We also find that, in agreement with psychological studies and contrary to reinforcement learning theory, the discount factor decreases hyperbolically with time. Finally, our framework suggests a role for replays, in both aiding learning in novel environments and finding shortcut trajectories that were not experienced during behavior, in agreement with experimental data.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>predictive</kwd><kwd>successor representation</kwd><kwd>hippocampus</kwd><kwd>modelling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>200790/Z/16/Z</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/R035806/1</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>564408</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Predictive cognitive maps can be learned during behavior and replays using in spiking neurons.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Mid twentieth century, Tolman proposed the concept of cognitive maps (<xref ref-type="bibr" rid="bib64">Tolman, 1948</xref>). These maps are abstract mental models of an environment which are helpful when learning tasks and in decision making. Since the discovery of hippocampal place cells, cells that are activated only in specific locations of an environment, it is believed that the hippocampus can provide the substrate to encode such cognitive maps (<xref ref-type="bibr" rid="bib46">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib47">O’Keefe and Nadel, 1978</xref>). More evidence of the role of the hippocampus in behavior was found in numerous experimental studies, such as the seminal water maze experiments (<xref ref-type="bibr" rid="bib44">Morris, 1981</xref>; <xref ref-type="bibr" rid="bib45">Morris et al., 1982</xref>), radial arm maze experiments (<xref ref-type="bibr" rid="bib49">Olton and Papas, 1979</xref>) as well as evidence of broader information processing beyond just cognitive maps (<xref ref-type="bibr" rid="bib67">Wood et al., 1999</xref>; <xref ref-type="bibr" rid="bib13">Eichenbaum et al., 1999</xref>; <xref ref-type="bibr" rid="bib1">Aggleton and Brown, 1999</xref>; <xref ref-type="bibr" rid="bib68">Wood et al., 2000</xref>).</p><p>While these place cells offer striking evidence in favour of cognitive maps, it is not clear what representation is actually learned by the hippocampus and how this information is exploited when solving and learning tasks. Recently, it was proposed that the hippocampus computes a cognitive map containing predictive information, called the successor representation (SR). Theoretically, this SR framework has some computational advantages, such as efficient learning, simple computation of the values of states, fast relearning when the rewards change and flexible decision making (<xref ref-type="bibr" rid="bib9">Dayan, 1993</xref>; <xref ref-type="bibr" rid="bib59">Stachenfeld et al., 2014</xref>; <xref ref-type="bibr" rid="bib60">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Russek et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Momennejad et al., 2017</xref>). Furthermore, the SR is in agreement with experimental observations. Firstly, the firing fields of hippocampal place cells are affected by the strategy used by the animal to navigate the environment (known as the <italic>policy</italic> in machine learning), as well as by changes in the environment (<xref ref-type="bibr" rid="bib40">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="bib60">Stachenfeld et al., 2017</xref>). Secondly, reward revaluation — the ability to recompute the values of the states when rewards change — would be more effective than transition revaluation (<xref ref-type="bibr" rid="bib56">Russek et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Momennejad et al., 2017</xref>).</p><p>In this work, we study how this predictive representation can be learned in the hippocampus with spike-timing dependent synaptic plasticity (STDP). Using STDP at the mechanistic level, we show that the learning is equivalent to TD(<inline-formula><mml:math id="inf1"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) on an algorithmic level. The latter is a well-studied and powerful algorithm known from reinforcement learning (<xref ref-type="bibr" rid="bib62">Sutton and Barto, 1998</xref>), which we will discuss in more detail below.</p><p>Our model can thus learn over a behavioral timescale while using STDP timescales in the millisecond range. We show mathematically that our proposed framework smoothly connects a temporally precise spiking code akin to replay activity with a rate based code akin to behavioral spiking. Subsequently, we show that the delay-discounting parameter <inline-formula><mml:math id="inf2"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> allows us to consider time as a continuous variable, therefore we don’t need to discretize time as is usual in reinforcement learning (<xref ref-type="bibr" rid="bib10">Doya, 1995</xref>; <xref ref-type="bibr" rid="bib11">Doya, 2000</xref>). Moreover, the delay-discounting in our model depends hyperbolically on time but exponentially on state transitions. We show how the <inline-formula><mml:math id="inf3"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> parameter can be modulated by neuronal firing rates and neuromodulation, allowing state-dependent discounting and in turn enabling richer information in the SR, such as the encoding of salient states, landmarks, reward locations, etc. Finally, replays have long been speculated to be involved in learning models of the environment, supported by experiments (<xref ref-type="bibr" rid="bib29">Johnson and Redish, 2007</xref>; <xref ref-type="bibr" rid="bib52">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib30">Kay et al., 2020</xref>) and models (<xref ref-type="bibr" rid="bib24">Hasselmo and Eichenbaum, 2005</xref>; <xref ref-type="bibr" rid="bib14">Erdem and Hasselmo, 2012</xref>; <xref ref-type="bibr" rid="bib33">Kubie and Fenton, 2012</xref>). Here, we investigate how replays could play an additional role in learning the SR cognitive map. Following properties of TD(<inline-formula><mml:math id="inf4"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>), we show how we can achieve both low bias and low variance by using replays, translating to both quicker initial learning and convergence to lower error. We show how we can use replays to learn <italic>offline</italic>. In this way, policies can be refined without the need for actual exploration.</p><p>Our framework allows us to make predictions about the roles of behavioral learning and replay-like activity and how they can be exploited in representation learning. Furthermore, we uncover a relation between STDP and a higher level learning algorithm. Our work therefore spans the three levels of analysis proposed by <xref ref-type="bibr" rid="bib36">Marr, 2010</xref>. On the implementational level, our model consists of a feedforward network of excitatory neurons with biologically plausible spike-timing dependent plasticity. On the algorithmic level, we show that our model learns the successor representation using the TD(<inline-formula><mml:math id="inf5"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) algorithm. On the computational theory level, our model tackles representation learning using cognitive maps.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Cognitive maps are internal models of an environment which help animals to learn, plan and make decisions during task completion. The hippocampus has long been thought to provide the substrate for learning such cognitive maps (<xref ref-type="bibr" rid="bib46">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib47">O’Keefe and Nadel, 1978</xref>; <xref ref-type="bibr" rid="bib44">Morris, 1981</xref>; <xref ref-type="bibr" rid="bib45">Morris et al., 1982</xref>; <xref ref-type="bibr" rid="bib67">Wood et al., 1999</xref>; <xref ref-type="bibr" rid="bib13">Eichenbaum et al., 1999</xref>), and recent evidence points towards a specific type of representation learned by the hippocampus, the successor representation (SR) (<xref ref-type="bibr" rid="bib60">Stachenfeld et al., 2017</xref>).</p><sec id="s2-1"><title>The successor representation</title><p>In this section, we will give an overview of the successor representation and its properties, especially geared toward neuroscientists. Readers already familiar with this representation may safely move to the next section.</p><p>To understand the concept of successor representation (SR), we can consider a spatial environment — such as a maze — while an animal explores this environment. In this setting, the SR can be understood as how likely it is for the animal to visit a future location starting from its current position. We further assume the maze to be formed out of a discrete number of states. Then, the SR can be more formally described by a matrix with dimension (<inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mrow><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">×</mml:mo><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mrow><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), where <inline-formula><mml:math id="inf7"><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mrow><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the number of states in the environment and each entry <inline-formula><mml:math id="inf8"><mml:msub><mml:mi mathsize="70%">R</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of this matrix describes the expected future occupancy of a state <inline-formula><mml:math id="inf9"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msub></mml:math></inline-formula> when the current state is <inline-formula><mml:math id="inf10"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mi mathsize="70%">i</mml:mi></mml:msub></mml:math></inline-formula>. In other words, starting from <inline-formula><mml:math id="inf11"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mi mathsize="70%">i</mml:mi></mml:msub></mml:math></inline-formula>, the more likely it is for the animal to reach the location associated with state <inline-formula><mml:math id="inf12"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msub></mml:math></inline-formula> and the nearer in the future, the higher the value of <inline-formula><mml:math id="inf13"><mml:msub><mml:mi mathsize="70%">R</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>As a first example, we consider an animal running through a linear track. We assume the animal runs at a constant speed and always travels in the same direction — left to right (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We also split the track into four sections or <italic>states</italic>, <inline-formula><mml:math id="inf14"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mn mathsize="70%">1</mml:mn></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf15"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mn mathsize="70%">4</mml:mn></mml:msub></mml:math></inline-formula>, and the SR will be represented by a matrix with dimension (<inline-formula><mml:math id="inf16"><mml:mrow><mml:mn mathsize="70%">4</mml:mn><mml:mo mathsize="70%" stretchy="false">×</mml:mo><mml:mn mathsize="70%">4</mml:mn></mml:mrow></mml:math></inline-formula>). Since the animal always runs from left to right, there is zero probability of finding the animal at position <inline-formula><mml:math id="inf17"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula> if its current position is greater than <inline-formula><mml:math id="inf18"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula>. Therefore, the lower triangle of the successor matrix is equal to zero (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Alternatively, if the animal is currently at position <inline-formula><mml:math id="inf19"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mn mathsize="70%">1</mml:mn></mml:msub></mml:math></inline-formula>, it will be subsequently found at positions <inline-formula><mml:math id="inf20"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mn mathsize="70%">2</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf21"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mn mathsize="70%">3</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf22"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mn mathsize="70%">4</mml:mn></mml:msub></mml:math></inline-formula> with probability 1. The further away from <inline-formula><mml:math id="inf23"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mn mathsize="70%">1</mml:mn></mml:msub></mml:math></inline-formula>, the longer it will take the animal to reach that other position. In terms of the successor matrix, we apply a discounting factor <inline-formula><mml:math id="inf24"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>γ</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>) for each extra ‘step’ required by the animal to reach a respective location (<xref ref-type="fig" rid="fig1">Figure 1b</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Successor representation and neuronal network.</title><p>(<bold>A</bold>) Our simple example environment consists of a linear track with 4 states (S<sub>1</sub> to S<sub>4</sub>) and the animal always moves from left to right — i.e. one epoch consists of starting in S<sub>1</sub> and ending in S<sub>4</sub>. (<bold>B</bold>) The successor matrix corresponding to the task described in panel A. (<bold>C</bold>) Our neuronal network consists of a two layers with all-to-all feedforward connections. The presynaptic layer mimics hippocampal CA3 and the postsynaptic layer mimics CA1. (<bold>D</bold>) The synaptic plasticity rule consists of a depression term and a potentiation term. The depression term is dependent on the synaptic weight and presynaptic spikes (blue). The potentiation term depends on the timing between a pre- and post-synaptic spike pair (red), following an exponentially decaying plasticity window (bottom). (<bold>E–F</bold>) Schematics illustrating some of the results of our model. (<bold>E</bold>) Our spiking model learns the top row of the successor representation (panel B) in the weights between the first CA3 place cell and the CA1 cells. (<bold>F</bold>) Our spiking model learns the third row of successor representation (panel B) in the weights between the third CA3 place cell and the CA1 cells.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Learning the SR in a two-dimensional environment.</title><p>(<bold>A</bold>) Our two-dimensional environment contains 16 states. The 11th state is assumed to be an inaccessible obstacle. A random policy is guiding the trajectories, while the starting state is always state 16 and the trajectories only end when reaching state 1. (<bold>B–C</bold>) Successor representations learned by our model mimicking active exploration (TD(0)) and replays (TD(1)) respectively. (<bold>D</bold>) Ground truth successor representation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>The equivalence with TD(<inline-formula><mml:math id="inf26"><mml:mi>λ</mml:mi></mml:math></inline-formula>) guarantees convergence even with random initial synaptic weights.</title><p>Evolution of synaptic weights from CA3 state 2 to CA1 state 3 over time. Ten simulations of the linear track task (<xref ref-type="fig" rid="fig2">Figure 2</xref>) are performed, where the CA3-CA1 synaptic weights are randomly initialized at the start. The full lines denote the synaptic weight over time for each of the ten simulations, the red dashed line is the average over those and the black dotted line is theoretical value of the respective SR. In the left panel only replays are simulated (corresponding to TD <inline-formula><mml:math id="inf27"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), in the right panel the behavioral model is simulated (corresponding to TD <inline-formula><mml:math id="inf28"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig1-figsupp2-v1.tif"/></fig></fig-group><p>Even though we introduced the linear track as an illustrative example, the SR can be learned in any environment (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for an example in an open field). Note that the representation learned by the SR is dependent not only on the structure of the environment, but also on the policy — or strategy — used by the animal to explore the environment. This is because the successor representation is not purely concerned with the physical distance between two areas in the environment, but rather it measures how long it usually takes to reach one place when starting from the other. In this first example, the animal applied a deterministic policy (always running from left to right), but the SR can also be learned for stochastic policies. Furthermore, the SR is a multi-step representation, in the sense that it stores predictive information of multiple steps ahead.</p><p>Because of this predictive information, the SR allows sample-efficient re-learning when the reward location is changed (<xref ref-type="bibr" rid="bib19">Gershman, 2018</xref>). In reinforcement learning, we tend to distinguish between model-free and model-based algorithms. The SR is believed to sit in-between these two modalities. In model-free reinforcement learning, the aim is to directly learn the value of each state in the environment. Since there is no model of the environment at all, if the location of a reward is changed, the agent will have to first unlearn the previous reward location by visiting it enough times, and only then it will be able to re-learn the new location. In model-based reinforcement learning, a precise model of the environment is learned, specifically, single-step transition probabilities between all states of the environment. Model-based learning is computationally expensive, but allows a certain flexibility. If the reward changes location it is immediate to derive the updated values of the states. As we have seen, however, the SR can re-learn a new reward location somewhat efficiently, although less so than model-based learning. The SR can also be efficiently learned using model-free methods and allows us to easily compute values for each state, which in turn can guide the policy (<xref ref-type="bibr" rid="bib9">Dayan, 1993</xref>; <xref ref-type="bibr" rid="bib56">Russek et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Momennejad et al., 2017</xref>). This position between model-based and model-free methods makes the SR framework very powerful, and its similarities with hippocampal neuronal dynamics have led to increased attention from the neuroscience community. Finally, in our examples above we considered an environment made up of a discrete number of states. This framework can be generalised to a continuous environment represented by a discrete number of place cells.</p></sec><sec id="s2-2"><title>Learning the successor representation in biologically plausible networks</title><p>We propose a model of the hippocampus that is able to learn the successor representation. We consider a feedforward network comprising of two layers. Similar to <xref ref-type="bibr" rid="bib39">McNaughton and Morris, 1987</xref>; <xref ref-type="bibr" rid="bib22">Hasselmo and Schnell, 1994</xref>; <xref ref-type="bibr" rid="bib40">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="bib23">Hasselmo et al., 2002</xref>, we assume that the presynaptic layer represents the hippocampal CA3 region and is all-to-all connected to a postsynaptic layer - representing the CA1 network (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). The synaptic connections from CA3 to CA1 are plastic such that the weight changes follow a spike-timing-dependent plasticity (STDP) rule consisting of two terms: a weight-dependent depression term for presynaptic spikes and a potentiation term for pre-post spike pairs (<xref ref-type="fig" rid="fig1">Figure 1d</xref>).</p><p>For simplicity, we assume that the animal spends a fixed time <inline-formula><mml:math id="inf29"><mml:mi mathsize="70%">T</mml:mi></mml:math></inline-formula> in each state. During this time, a constant activation current is delivered to the CA3 neuron encoding the current location and, after a delay, to the corresponding CA1 place cell (see Materials and methods). On top of these fixed and location-dependent activations, the CA3 neurons can activate neurons in CA1 through the synaptic connections. In other words, the CA3 neurons are activated according to the current location of the animal, while the CA1 neurons have a similar location-dependent activity combined with activity caused by presynaptic neurons. The constant currents delivered directly to CA3 and CA1 neurons can be thought of as location-dependent currents from entorhinal cortex. These activations subsequently trigger plasticity at the synapses, and we can show analytically that, using the spike-timing dependent plasticity rule discussed above, the SR is learned in the synaptic weights (<xref ref-type="fig" rid="fig1">Figure 1e and f</xref>, and see Appendix).</p><p>Moreover, we find that, on an algorithmic level, our weight updates are equivalent to a learning algorithm known as TD(<inline-formula><mml:math id="inf30"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>), a powerful and well-known algorithm in reinforcement learning that can be used to learn the successor representation. TD(<inline-formula><mml:math id="inf31"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) is based on a mixed methodology, which is regulated by the parameter <inline-formula><mml:math id="inf32"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>. At one extreme, when <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:math></inline-formula>, the SR is estimated by taking the average of state occupancies over past trajectories. This type of algorithm is called TD(1) or Monte Carlo (MC). At the other extreme, when <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:math></inline-formula>, the estimate of the SR is adjusted ‘online’, with every step of the trajectory, by comparing the observed position with its predicted value. This algorithm is equivalent to TD(0). For all values of <inline-formula><mml:math id="inf35"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula> in between, the algorithm employs a mixture of both methodologies. The extreme cases of TD(1) and TD(0) have different strengths and weaknesses, as we will discuss in more detail in the next sections.</p><p>In practice, we prove analytically the mathematical equivalence of the dynamics of our spiking neural network, and the TD(<inline-formula><mml:math id="inf36"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) algorithm (see Appendix). Our calculations essentially prove that, at each step, our neural network tracks the reinforcement learning algorithm, known to converge to the theoretical values of the SR. This equivalence guarantees that our neural network weights will eventually converge to the correct SR matrix. As a proof of principle, we show that it is possible to learn the SR for any initial weights (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), independently of any previous learning in the CA3 to CA1 connections.</p><p>Importantly, from our analytical derivations (see Appendix), we find that the <inline-formula><mml:math id="inf37"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula> parameter depends on the behavioral parameter T (the time an animal spends in a state). We find that, the larger the time T, the smaller the value of <inline-formula><mml:math id="inf38"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula> and vice-versa. In other words, when the animal moves through the trajectory on behavioral time-scales (large T compared to the synaptic plasticity time-scales <inline-formula><mml:math id="inf39"><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:math></inline-formula>), the network is learning the SR with TD(<inline-formula><mml:math id="inf40"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">∼</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:math></inline-formula>). For quick sequential activities (T → 0), akin to hippocampal replays, the network is learning the SR with TD(<inline-formula><mml:math id="inf41"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">∼</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:math></inline-formula>). As we will discuss below, this framework therefore combines learning based on rate coding as well as temporal coding. Furthermore, from our model follows the prediction that replays can also be used for learning purposes and that they are algorithmically equivalent to MC, whereas during behavior, the hippocampal learning algorithm is equivalent to TD(<inline-formula><mml:math id="inf42"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>). This strategy of using replays to learn is in line with recent experimental and theoretical observations (see <xref ref-type="bibr" rid="bib43">Momennejad, 2020</xref> for a review).</p><p>To validate our analytical results, we use again a linear track with a deterministic policy. Using our spiking model with either rate-code activity on behavioral time-scales (<xref ref-type="fig" rid="fig2">Figure 2a</xref> top) or temporal-code activity similar to replays (<xref ref-type="fig" rid="fig2">Figure 2b</xref> top), we show that the synaptic weights across trials match the evolution of the TD(<inline-formula><mml:math id="inf43"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) algorithm closely (<xref ref-type="fig" rid="fig2">Figure 2a and b</xref> middle). While convergence to the SR is guaranteed (<xref ref-type="fig" rid="fig2">Figure 2a and b</xref> bottom) due to the mathematical equivalence between our setup and TD(<inline-formula><mml:math id="inf44"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), the learning trajectory has more variance in the neural network case due to the noise introduced by the randomness of the spike times. This noise can be mitigated by averaging over a population of neurons. Moreover, due to the equivalence with TD(<inline-formula><mml:math id="inf45"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>), our setup is general for any type of task where discrete states are visited, in any dimension, and which may not need to be a navigation task (see e.g. <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for a 2D environment).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Comparison between TD(<inline-formula><mml:math id="inf46"><mml:mi>λ</mml:mi></mml:math></inline-formula>) and our spiking model.</title><p>(<bold>A-top</bold>) Learning during behavior corresponds to TD(<inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). States are traversed on timescales larger than the plasticity timescales and place cells use a rate-code. (A-middle) Comparison of the learning over epochs for two synaptic connections (full line denotes the mean over ten random seeds, shaded area denotes one standard deviation) with the theoretical learning curve of TD(<inline-formula><mml:math id="inf48"><mml:mi>λ</mml:mi></mml:math></inline-formula>) (dotted line). (<bold>A-bottom</bold>) Final successor matrix learned by the spiking model (left) and the theoretical TD(<inline-formula><mml:math id="inf49"><mml:mi>λ</mml:mi></mml:math></inline-formula>) algorithm (right). Star and diamond symbols denote the corresponding weights shown in the middle row. (B-top) Learning during replays corresponds to TD(<inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). States are traversed on timescales similar to the plasticity timescales and place cells use a temporally precise code. (<bold>B-middle and bottom</bold>) Analogous to panel A middle and bottom.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Comparison of the exact and approximate equations for the parameters.</title><p>The discount parameter <inline-formula><mml:math id="inf51"><mml:mi>γ</mml:mi></mml:math></inline-formula> when we vary <inline-formula><mml:math id="inf52"><mml:mi>T</mml:mi></mml:math></inline-formula> by: (<bold>a</bold>) increasing the duration of the presynaptic current, <inline-formula><mml:math id="inf53"><mml:mi>θ</mml:mi></mml:math></inline-formula> leading to a hyperbolic discount, and (<bold>b</bold>) increasing <inline-formula><mml:math id="inf54"><mml:mi>ψ</mml:mi></mml:math></inline-formula>, while keeping <inline-formula><mml:math id="inf55"><mml:mi>θ</mml:mi></mml:math></inline-formula> fixed, leading to an exponential discount. (<bold>c</bold>) The bootstrapping parameter <inline-formula><mml:math id="inf56"><mml:mi>λ</mml:mi></mml:math></inline-formula> for varying <inline-formula><mml:math id="inf57"><mml:mi>θ</mml:mi></mml:math></inline-formula>. (<bold>d</bold>) The learning rate <inline-formula><mml:math id="inf58"><mml:mi>η</mml:mi></mml:math></inline-formula> for varying <inline-formula><mml:math id="inf59"><mml:mi>θ</mml:mi></mml:math></inline-formula>. See Supplementary materials for derivation of the exact and approximate equations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig2-figsupp1-v1.tif"/></fig></fig-group><p>In summary, we showed how the network can learn the SR using a spiking neural model. We analytically showed how the learning algorithm is equivalent to TD(<inline-formula><mml:math id="inf60"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>), and confirmed this using numerical simulations. We derived a relationship between the abstract parameter <inline-formula><mml:math id="inf61"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula> and the timescale T representing the animal’s behavior — and in turn the neuronal spiking — allowing us to unify rate and temporal coding within one framework. Furthermore, we predict a role for hippocampal replays in learning the SR using an algorithm equivalent to Monte Carlo.</p></sec><sec id="s2-3"><title>Learning over behavioral time-scales using STDP</title><p>An important observation in our framework is that the SR can be learned <italic>using the same underlying STDP rule</italic> over time-scales ranging from replays up to behavior. One can now wonder how it is possible to learn relationships between events that are seconds apart during awake behavior, without any explicit error encoding signal typically used by the TD algorithm, and while the STDP rule is characterised by millisecond time-scales (<xref ref-type="fig" rid="fig3">Figure 3a</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Learning on behavioral timescales and state-dependent discounting.</title><p>(<bold>A</bold>) In our model, the network can learn relationships between neurons that are active seconds apart, while the plasticity rule acts on a millisecond timescale. (<bold>B</bold>) Due to transitions between subsequent states, each synaptic weight update depends on the weight from the subsequent CA3 neuron to the same CA1 neuron. In other words, the change of a synaptic weight depends on the weight below it in the successor matrix. The top panel visualizes how weights depend on others in our linear track example, where each lighter color depends on the darker neighbor. The bottom panel shows the learning of over 50 epochs. Notice the lighter traces converge more slowly, due to their dependence on the darker traces. (<bold>C</bold>) Place fields of the place cells in the linear track — each place cell corresponding to a column of the successor matrix. Activities of each place cell when the animal is in each of the four states (dots) are interpolated (lines). Three variations are considered: (i) the time spent in each state and the CA3 firing rates are constant (blue and panel <bold>D</bold>); (ii) the time spent in state 3 is doubled (orange and panel <bold>E</bold>); (iii) the CA3 firing rate in state 3 is doubled (green and panel <bold>F</bold>). Panels E and F lead to a modified discount parameter in state 3, affecting the receptive fields of place cells 3 and 4.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig3-v1.tif"/></fig><p>From a neuroscience perspective, this can be understood when considering the trajectory of the animal. Each time the animal moves from a position <inline-formula><mml:math id="inf62"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to a position <inline-formula><mml:math id="inf63"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msub></mml:math></inline-formula>, the CA3 cell encoding the location <inline-formula><mml:math id="inf64"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> stops firing and the CA3 cell encoding the location <inline-formula><mml:math id="inf65"><mml:msub><mml:mi mathsize="70%">S</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msub></mml:math></inline-formula> starts firing. Since in our example this transition is instantaneous, these cells are activating the same CA1 cells consecutively. Therefore, the change in the weight <inline-formula><mml:math id="inf66"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> depend on the synaptic weight of the subsequent state <inline-formula><mml:math id="inf67"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, yellow depends on orange, orange depends on red, etc). Indeed, in our example of an animal in a linear track subdivided into four locations, the weights on the diagonal, such as <inline-formula><mml:math id="inf68"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mn mathsize="70%">4</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mn mathsize="70%">4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, are the first ones to be learned, since they are learned directly. The off-diagonal weights, such as <inline-formula><mml:math id="inf69"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mn mathsize="70%">3</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mn mathsize="70%">4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf70"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mn mathsize="70%">2</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mn mathsize="70%">4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf71"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mn mathsize="70%">4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, are learned consecutively more slowly as they are dependent on the subsequent synaptic weight. Eventually, weights between neurons encoding positions that are behaviorally far apart can be learnt using a learning rule on a synaptic timescale (<xref ref-type="fig" rid="fig3">Figure 3b</xref>).</p><p>From a reinforcement learning perspective, the TD(0) algorithm relies on a property called bootstrapping. This means that the successor representation is learned by first taking an initial estimate of the SR matrix (i.e. the previously learned weights), and then gradually adjusting this estimate (i.e. the synaptic weights) by comparing it to the states in the environment that the animal actually visits. This comparison is achieved by calculating a <italic>prediction error</italic>, similar to the widely studied one for dopamine neurons (<xref ref-type="bibr" rid="bib57">Schultz et al., 1997</xref>). Since the synaptic connections carry information about the expected trajectories, in this case, the prediction error is computed between the predicted and observed trajectories (see Materials and methods).</p><p>The main point of bootstrapping, therefore, is that learning happens by adjusting our current predictions (e.g. synaptic weights) to match the observed current state. This information is available at each time step and thus allows learning over long timescales using synaptic plasticity alone. If the animal moves to a state in the environment that the current weights deem unlikely, potentiation will prevail and the weight from the previous to the current state will increase. Otherwise, the opposite will happen. It is important to notice that the prediction error in our model is not encoded by a separate mechanism in the way that dopamine is thought to do for reward prediction (<xref ref-type="bibr" rid="bib57">Schultz et al., 1997</xref>). Instead, the prediction error is represented locally, at the level of the synapse, through the depression and potentiation terms of our STDP rule, and the current weight encodes the current estimate of the SR (see Materials and methods). Notably, the prediction error is equivalent to the TD(<inline-formula><mml:math id="inf72"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) update. This mathematical equivalence ensures that the weights of our neural network track the TD(<inline-formula><mml:math id="inf73"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) update at each state, and thus stability and convergence to the theoretical values of the SR. We therefore do not need an external vector to carry prediction error signals as proposed in <xref ref-type="bibr" rid="bib17">Gardner et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Gershman, 2018</xref>. In fact, the synaptic potentiation in our model updates a row of the SR, while the synaptic depression updates a column.</p><p>On the other extreme, for very fast timescales such as replays, TD(1) is equivalent to online Monte Carlo learning (MC), which does not bootstrap at all. Instead, MC samples the whole trajectory and then simply takes the average of the discounted state occupancies to update the SR (see Materials and methods). During replays, the whole trajectory falls under the plasticity window and the network can learn without bootstrapping. For all cases in between, the network partially relies on bootstrapping and we correspondingly find a <inline-formula><mml:math id="inf74"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula> between 0 and 1.</p><p>In summary, in our framework, synaptic plasticity leads to the development of a successor representation in which synaptic weights can be directly linked to the successor matrix. In this framework, we can learn over behavioral timescales even though our plasticity rule acts on the scale of milliseconds, due to the bootstrapping property of TD algorithms.</p></sec><sec id="s2-4"><title>Different discounting for space and time</title><p>In reinforcement learning, it is usual to have delay-discounting: rewards that are further away in the future are discounted compared to rewards that are in the immediate future. Intuitively, it is indeed clear that a state leading to a quick reward can be regarded as more valuable compared to a state that only leads to an equal reward in the distant future. For tasks in a tabular setting, with a discrete state space and where actions are taken in discrete turns, such as for example chess or our simple linear track discussed in section ‘The Successor Representation’, one can simply use a multiplicative factor <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>γ</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> for each state transition. In this case the discount follows an exponential dependence, where rewards that are <inline-formula><mml:math id="inf76"><mml:mi mathsize="70%">n</mml:mi></mml:math></inline-formula> steps away are discounted by a factor of <inline-formula><mml:math id="inf77"><mml:msup><mml:mi mathsize="70%">γ</mml:mi><mml:mi mathsize="70%">n</mml:mi></mml:msup></mml:math></inline-formula>.</p><p>In order to still use the above exponential discount when time is continuous, the usual approach is to discretize time by choosing a unit of time. However, this would imply one can never remain in a state for a fraction of this unit of time, and it is not clear how this unit would be chosen. Our framework deals naturally with continuous time, through the monotonically decreasing dependence of the discount parameter <inline-formula><mml:math id="inf78"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> on the time an agent remains in a state, T. The dependence on T can be interpreted as an increased discounting the longer a state lasts.</p><p>In this way, instead of discounting by <inline-formula><mml:math id="inf79"><mml:msup><mml:mi mathsize="70%">γ</mml:mi><mml:mi mathsize="70%">n</mml:mi></mml:msup></mml:math></inline-formula> when the agent stays <inline-formula><mml:math id="inf80"><mml:mi mathsize="70%">n</mml:mi></mml:math></inline-formula> units of time in a certain state, we would discount by <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">n</mml:mi><mml:mo mathsize="70%" stretchy="false">⋅</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. More generally, for any arbitrary time T, a discount corresponding to <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will be applied. This allows the agent to act in continuous time (<xref ref-type="fig" rid="fig3">Figure 3c and e</xref>). Interestingly, the dependence of <inline-formula><mml:math id="inf83"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> on T in our model is not exponential as in the tabular case. Instead, we have a hyperbolic dependence. This hyperbolic discount is well studied in psychology and neuroeconomics and appears to agree well with experimental results (<xref ref-type="bibr" rid="bib35">Laibson, 1997</xref>; <xref ref-type="bibr" rid="bib2">Ainslie, 2012</xref>).</p><p>The difference between a hyperbolic discount and an exponential discount lays in the fact that we will attribute a different value to the same temporal delay, depending on whether it happens sooner or later. A classic example is that, when given the choice, people tend to prefer 100 dollars today instead of 101 dollars tomorrow, while they tend to prefer 101 dollars in 31 days instead of 100 dollars in 30 days. They therefore judge the 1 day of delay differently when it happens later in time. Exponential discounting, on the other hand, always attributes the same value to the same delay no matter when it occurs.</p><p>Our model therefore combines two types of discounting: exponential when we move through space — when sequentially activating different place cells — and hyperbolic when we move through time — when we prolong the activity of the same place cell.</p><p>The discount factor <inline-formula><mml:math id="inf84"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> also depends on other parameters such as firing rate and STDP amplitudes (see <xref ref-type="disp-formula" rid="equ23">Equation 22</xref> in the Appendix). This gives our model the flexibility to encode state-dependent discounting even when the trajectories and times spent in the states are the same. Such state-dependent discounting can be useful to for example encode salient locations in the environment such as landmarks or reward locations (<xref ref-type="fig" rid="fig3">Figure 3c and f</xref>).</p></sec><sec id="s2-5"><title>Bias-variance trade-off</title><p>As discussed previously (section ‘Learning the successor representation in biologically plausible networks’), the TD(<inline-formula><mml:math id="inf85"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) algorithm unifies the TD algorithm and the MC algorithm. In our framework, replay-like neuronal activations are equivalent to MC, while behavioral-like activity is equivalent to TD. In this section, we will discuss how the replays and behavior can work together when learning the cognitive map of an environment, leveraging the strengths of MC and TD.</p><p>The MC algorithm effectively works by averaging over the sampled trajectories. As such, the estimated SR matrix will be a close approximation of the theoretical value. The difference between the estimated and theoretical value is commonly referred to as bias. We can therefore say that the MC algorithm presents low bias. However, if the agent moves in the environment at random, the sampled trajectories will be quite different from each other. When taking the average, the estimated value will therefore fluctuate a lot. In this case, we say that the MC estimate has high variance as well (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Replays can be used to control the bias-variance trade-off.</title><p>(<bold>A</bold>) The agent follows a stochastic policy starting from the initial state (denoted by <italic>START</italic>). The probability to move to either neighboring state is 50%. An epoch stops when reaching a terminal state (denoted with <italic>STOP</italic>). (<bold>B</bold>) Root mean squared error (RMSE) between the learned SR estimate and the theoretical SR matrix. The full lines are mean RMSEs over 1000 random seeds. Three cases are considered: (i) learning happens exclusively due to behavioral activity (TD STDP, green); (ii) learning happens exclusively due to replay activity (MC STDP, purple); (iii) A mixture of behavioral and replay learning, where the probabilities for replays drops off exponentially with epochs (Mix STDP, pink). The <italic>mix</italic> model, with a decaying number of replays learns as quickly as MC in the first epochs and converges to a low error similar to TD, benefiting both from the low bias of MC at the start and the low variance of TD at the end. (<bold>C, D, E</bold>) Representative weight changes for each of the scenarios. Full lines show various random seeds, shaded areas denote one standard deviation over 1000 random seeds. (<bold>F</bold>) More replays are observed when an animal explores a novel environment (day 1). Panel F adapted from Figure 3A in <xref ref-type="bibr" rid="bib7">Cheng and Frank, 2008</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Combining equal amounts of replays and behavioral learning.</title><p>Unlike <xref ref-type="fig" rid="fig4">Figure 4</xref>, where the likelihood of replays is exponentially decaying over time, here we simulate equal probability for learning using replays (MC) and using behavioral experience (TD) throughout time. This mixed strategy lowers the bias and the variance to some extent, but lays between the pure MC and pure TD learning, never achieving the minimal error possible. This is in contrast with the exponential decay of replays shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, which achieves a minimal error both during early learning (similar to MC) as well as asymptotically (similar to TD).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Setting the noise for replays.</title><p>Using the settings of <xref ref-type="fig" rid="fig4">Figure 4</xref>, we calculate the variance caused by random spiking for the behavioral model first. For this purpose, we simulate the same trajectory of the agent 25 times with different random instances of spikes generated from the Poisson process. At each state transition, we calculate the standard deviation of the synaptic weights over the 25 runs. Finally, we calculate the mean of these standard deviations over all states of the trajectory. We find a value of this mean standard deviation just below 1.4 (full line). Then, we try various levels of noise for the replay model, and compute the same mean standard deviation. We find that a probability <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula> yields a similar level of variance due to random spiking as in the behavioral model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig4-figsupp2-v1.tif"/></fig></fig-group><p>Unlike MC, the TD algorithm updates its estimate of the SR by comparing the current estimate of the SR with the actual state the agent transitioned to. Because of the dependence on the current estimate, this estimate will be incrementally refined with small updates. In this way, the SR estimate will not fluctuate much, and be lower in variance. However, by this dependence on the current estimate, we introduce a bias in the algorithm, which will be especially significant when our initial estimate of the SR is bad (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>). The TD algorithm therefore presents high bias and low variance.</p><p>We now apply these concepts to learning in a novel environment. Since the MC algorithm is unbiased by the initial estimate of the SR, replays should initially speed up learning in an unfamiliar environment. Later on, when the environment becomes familiar, the SR estimate is already closer to the exact value. At this point, we prefer to have low variance and thus the TD algorithm will be preferred. We confirm this logic using our spiking neural networks, and show how we can have both quick learning and low error at convergence if we proportionally have more replays at the first trials in a novel environment (<xref ref-type="fig" rid="fig4">Figure 4a–e</xref>). In contrast, when having an equal proportion of replays throughout the whole simulation, we do not yield as quick learning as MC and as low asymptotic error as TD (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Interestingly, the pattern of proportionally more replays in novel environments versus familiar environments has also been experimentally observed (<xref ref-type="bibr" rid="bib7">Cheng and Frank, 2008</xref>; <xref ref-type="fig" rid="fig4">Figure 4f</xref>). Please note that, while we implemented an exponentially decaying probability for replays after entering a novel environment, different schemes for replay activity could be investigated. Note also that other mechanisms besides the successor representation could account for these results, including model-based reinforcement learning.</p></sec><sec id="s2-6"><title>Leveraging replays to learn novel trajectories</title><p>In the previous section, the replays re-activated the same trajectories as seen during behavior. In this section, we extend this idea and show how in our model replays can be useful during learning even when the re-activated trajectories were not directly experienced during behavior.</p><p>For this purpose, we reproduce an place-avoidance experiment from <xref ref-type="bibr" rid="bib69">Wu et al., 2017</xref>. In short, rats are allowed to freely explore a linear track on day 1. Half of the track is dark, while the other half is bright. On day 2, the animals did four trials separated by resting periods: in the first trial (pre), the animals were free to explore the track; in the second trial (shock), they started in the light zone and received two mild footshocks when entering in the shock zone; in the third and fourth trial (post and re-exposure, respectively), they were allowed to freely explore the track again, but starting from the light zone or the shock zone respectively (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). In the study, it was reported that during the post trial, animals tended to stay in the light zone and forward replays from the current position to the shock zone were observed when the animals reached the boundary between the light and the dark zone (<xref ref-type="fig" rid="fig5">Figure 5b and c</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Reproducing place-avoidence experiments with the spiking model.</title><p>(<bold>A–C</bold>) Data from <xref ref-type="bibr" rid="bib69">Wu et al., 2017</xref>. (<bold>A</bold>) Experimental protocol: the animal is first allowed to freely run in the track (Pre). In the next trial, a footshock is given in the shock zone (SZ). In subsequent trials the animal is again free to run in the track (Post, Re-exposure). Figure redrawn from <xref ref-type="bibr" rid="bib69">Wu et al., 2017</xref>. (<bold>B</bold>) In the Post trial, the animal learned to avoid the shock zone completely and the also mostly avoids the dark area of the track. Figure redrawn from <xref ref-type="bibr" rid="bib69">Wu et al., 2017</xref>. (<bold>C</bold>) Time spent per location confirms that the animal prefers the light part of the track in the Post trial. Figure redrawn from <xref ref-type="bibr" rid="bib69">Wu et al., 2017</xref>. (<bold>D</bold>) Mimicking the results from <xref ref-type="bibr" rid="bib69">Wu et al., 2017</xref>, the shock zone is indicated by the black region, the dark zone by the gray region and the light zone by the white region. Left: without replays, the agent keeps extensively exploring the dark zone even after having experienced the shock. Right: with replays, the agent largely avoids entering the dark zone after having experienced the shock (replays not shown). (<bold>E</bold>) The value of each state in the cases with and without replays. (<bold>F</bold>) Occupancy of each state in our simulations and for the various trials. Solid line and shaded areas denote the average and standard deviation over 100 simulations, respectively. Notice we do not reproduce the peak of occupancy at the middle of the track as seen in panel c, since our simplified model assumes the same amount of time is spent in each state.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Doubling the time-steps in the scenario without replays.</title><p>In <xref ref-type="fig" rid="fig5">Figure 5</xref>, the policy without replays has less SR updates. Here, we simulated this policy but doubled the time (and SR updates). Even with this modification, the agent keeps exploring the dark zone of the track, which shows that it is indeed the type of policy and not the amount of updates that leads to the different behavior. More specifically, sequential activations of states from the decision point until the shock zone, such as in replays, are different than the softmax policy during behavior. It is exactly by exploiting this different replay policy that the agent updates the SR differently and avoids the dark zone.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>The value of states can be read out by downstream neurons.</title><p>(<bold>A</bold>) A linear track with 4 states is simulated as in <xref ref-type="fig" rid="fig1">Figure 1</xref>, where we now assume the state 4 contains a reward with value 1. This reward is assumed to be encoded in the synaptic weights from the CA1 neurons to the value neurons. We simulated a population of 250 neurons encoding each state and 250 neurons encoding the value. (<bold>B</bold>) Firing rates of the value neurons when the agent is occupying each state of the linear track. Only the firing rates during the first 80% of the dwelling time are shown, since no external input is applied during that time. Bars denote the mean firing rate of the 250 value neurons, error bars denote one standard deviation. Red dots denote the ground truth value for each state. (<bold>C</bold>) When the firing rates of the value neurons are computed over the full dwelling time in each state, the estimate of the value changes qualitatively due to the external input to CA1 in the final fraction of the dwelling time. Bars denote the mean firing rate of the 250 value neurons, error bars denote one standard deviation. Red dots denote the ground truth value for each state. Note that the ranking of the states by value is not affected. Practically, using the parameters in our simulation, one could either read out the correct estimate of the value during the first 80% of the dwelling time in a state, or learn a correction to this perturbation in the weights to the value neuron, or simply use a policy that is based on the ranking of the values instead of the actual firing rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Dependency of <inline-formula><mml:math id="inf87"><mml:mi>γ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf88"><mml:mi>λ</mml:mi></mml:math></inline-formula> and the place-tuned input to CA1 on <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>θ</mml:mi><mml:mo mathvariant="normal">/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, for various values of T and depression amplitude <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</title><p>(<bold>A</bold>) The <inline-formula><mml:math id="inf91"><mml:mi>γ</mml:mi></mml:math></inline-formula> variable increases with the ratio <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and decreases with T. (<bold>B</bold>) The <inline-formula><mml:math id="inf93"><mml:mi>λ</mml:mi></mml:math></inline-formula> variable decreases with the ratio <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and decreases with T. (<bold>C</bold>) The <inline-formula><mml:math id="inf95"><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> input to CA1 increases with the ratio.<inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> Therefore, the longer this input lasts (i.e. the lower <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>), the smaller it is. The distortion of the diagonal elements of the SR thus remains similar across various.<inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> For high values of T, the input remains more or less constant at low values. (<bold>D</bold>) The <inline-formula><mml:math id="inf99"><mml:mi>γ</mml:mi></mml:math></inline-formula> variable decreases with the depression amplitude <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and decreases with T. For fixed <inline-formula><mml:math id="inf101"><mml:mi>γ</mml:mi></mml:math></inline-formula>, one can choose any <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> by tuning this amplitude. (<bold>E</bold>) The <inline-formula><mml:math id="inf103"><mml:mi>λ</mml:mi></mml:math></inline-formula> variable increases with the depression amplitude <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. (<bold>F</bold>) The <inline-formula><mml:math id="inf105"><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> input to CA1 increases with the depression amplitude <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. For fixed <inline-formula><mml:math id="inf107"><mml:mi>γ</mml:mi></mml:math></inline-formula>, this implies that stronger <inline-formula><mml:math id="inf108"><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> inputs are coupled with higher <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and therefore shorter duration. The distortion of the diagonal elements of the SR thus remains similar across various <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig5-figsupp3-v1.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Readout of the state value for various parameters.</title><p>A population of readout neurons is simulated as in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>. Each row corresponds to a different choice of parameters <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf112"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, chosen such that the <inline-formula><mml:math id="inf113"><mml:mi>γ</mml:mi></mml:math></inline-formula> variable has a value close to 0.815. The rate of the readout neurons is computed when the agent occupies the various states. Rates are either normalized by the final state (left column) or the initial state (middle column). The right column denotes a computation of the firing rate of the readout neurons during the initial part of the dwelling time, <inline-formula><mml:math id="inf114"><mml:mi>θ</mml:mi></mml:math></inline-formula>. As in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, bars denote the mean firing rate of the 250 value neurons, error bars denote one standard deviation. Red dots denote the ground truth value for each state. We notice that, for various values of <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, the deviation from the ground truth remains similar. Furthermore, since the deviation is only affecting the diagonal elements of the SR with a fixed amount determined by <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, some compensatory mechanism could be easily incorporated in the network.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80671-fig5-figsupp4-v1.tif"/></fig></fig-group><p>We simulated a simplified version of this task. Our simulated agent moves through the linear track following a softmax policy, and all states have equal value during the first phase (pre) (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, blue trajectories). Then, the agent is allowed to move through the linear track until it reaches the shock zone and experiences a negative reward. Finally, the third phase is similar as the first phase and the animal is free to explore the track. Two versions of this third phase were simulated. In one version, there are no replays (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, orange trajectories in left panel), while in the second version a forward replay until the shock zone is simulated every time the agent enters the middle state (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, orange trajectories in right panel, replays not shown). The replays affect the learning of the successor representation and the negative reward information is propagated towards the decision point in the middle of the track. The states in the dark zone therefore have lower value compared to the case without replays (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). In turn, this different value affects the policy of the agent which now tends to avoid the dark zone all together, while the agent without replays still occupies many states of the dark zone as much as states in the light zone (<xref ref-type="fig" rid="fig5">Figure 5f</xref>). Moreover, even when doubling the amount of SR updates in the scenario without replays, the behavior of the agent remains unaltered (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). This shows that it is not the amount of updates, but the type of policy that is important when updating the SR, and how using a different policy in the replay activity can significantly alter behavior.</p><p>Our setup for this simulation is simplified, and does not aim to reproduce the complex decision making of the rats. Observe for example the peak of occupancy of the middle state by the animals (<xref ref-type="fig" rid="fig5">Figure 5c</xref>), which is not captured by our model because we assume the agent to spend the same amount of time in each state. Nonetheless, it is interesting to see how replaying trajectories that were not directly experienced before, in combination with a model allowing replays to affect the learning of a cognitive map, can substantially influence the final policy of an agent and the overall performance. This mental imagination of trajectories could be exploited to refine our cognitive maps, avoiding unfavourable locations or finding shortcuts to rewards. It is important to note here that, while we are suggesting a potential role for the SR in solving this task, the data itself would also be compatible with a model-based strategy. In fact, experimental evidence suggests that humans may use a mixed strategy involving both model-based reinforcement learning and the successor representation (<xref ref-type="bibr" rid="bib42">Momennejad et al., 2017</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this article, we investigated how a spiking neural network model of the hippocampus can learn the successor representation. Interestingly, we show that the updates in synaptic weights resulting from our biologically plausible STDP rule are equivalent to TD(<inline-formula><mml:math id="inf118"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) updates, a well-known and powerful reinforcement learning algorithm.</p><sec id="s3-1"><title>Reinforcement learning</title><p>Our network learns the SR in the CA3-CA1 weights. Since we have modeled neurons to integrate the synaptic EPSPs and generate spikes using an inhomogeneous Poisson process based on the depolarization, the firing rate is proportional to the total synaptic weights. Therefore, the successor representation can be read out simply by a downstream neuron. Moreover, since the value of a state is defined by the inner product between the successor matrix and the reward vector, it is sufficient for the synaptic weights to the downstream neuron to learn the reward vector, and the downstream neuron will then encode the state value in its firing rate (see <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). While the neuron model used is simple, it will be interesting for future work to study analogous models with non-linear neurons.</p><p>It is worth noting that, during learning, both pre-synaptic and post-synaptic layers receive external inputs representing the current state (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref> and <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> in Materials and methods). This may induce a distortion in the read out of the diagonal elements of the SR matrix (see <xref ref-type="disp-formula" rid="equ13 equ15">Equations 13 and 15</xref>, and <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). At a first glance, this may indicate that learning and reading out are antagonistic. However, there are multiple ways we could resolve this apparent conflict: (i) Since the external current in CA1 is present for only a fraction of the time T in each state, the readout might happen during the period of CA3 activation exclusively; (ii) The readout may be over the whole time T but becomes more noisy towards the end. Even in the case where the readout is noisy, the distortion would be limited to the diagonal elements of the matrix; (iii) Learning and readout may be separate mechanisms, where the CA3 driving current is present during readout only. This could be for instance signaled by neuromodulation (e.g. noradrenaline and acetylcholine are active during learning but not exploration <xref ref-type="bibr" rid="bib41">Micheau and Marighetto, 2011</xref>; <xref ref-type="bibr" rid="bib25">Hasselmo and Sarter, 2011</xref>; <xref ref-type="bibr" rid="bib54">Robbins, 1997</xref>; <xref ref-type="bibr" rid="bib63">Teles-Grilo Ruivo and Mellor, 2013</xref>; <xref ref-type="bibr" rid="bib51">Palacios-Filardo et al., 2021</xref>), or it could be that readout happens during replays; (iv) The weights to or activation functions of the readout neuron may learn to compensate for the distorted signal in CA1.</p><p>Furthermore, we can notice that the external inputs encoding the current state activate CA3 first, and CA1 later. The delay between these activations <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">/</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref> and <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> in Materials and methods) is an arbitrary parameter that can be adjusted. Varying this delay will change the reinforcement learning representation, especially parameters <inline-formula><mml:math id="inf120"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula>, but also the strength of the input current (see <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). However, this will not impact the distortion of the diagonal elements of the SR matrix, which remains similar across various delay values <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">/</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>).</p></sec><sec id="s3-2"><title>Biological plausibility</title><p>Uncovering a connection between STDP and TD(<inline-formula><mml:math id="inf123"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) shows how, using minimal assumptions, a theoretically grounded learning algorithm can emerge from a biological implementation of plasticity. Similar learning rules have indeed been observed in the hippocampus (<xref ref-type="bibr" rid="bib58">Shouval et al., 2002</xref> and proposed on theoretical grounds <xref ref-type="bibr" rid="bib40">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="bib66">Waddington et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">van Rossum et al., 2012</xref>).</p><p>The TD algorithm is most commonly known in neuroscience for describing how reward prediction can be computed in the brain. More specifically, it is widely believed that dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) encode the prediction error between the observed and expected reward (<xref ref-type="bibr" rid="bib57">Schultz et al., 1997</xref>), dopamine thus acts as a global signal that can be broadcasted to other areas of the brain like the striatum to compute the expected reward. In our model, the TD algorithm estimates the SR (i.e. expected future occupancy), rather than the value. However, since the prediction error for the SR is different for every synaptic connection (i.e. each pair of states), it is not clear how it could be carried by a global signal analogous to dopamine. The SR would need multiple signals, or a matrix transformation of the global signal. Furthermore, we would need to postulate that such error – or errors – are computed elsewhere in the brain. Instead, in our model, the prediction error simply emerges from the synaptic plasticity rule itself. Furthermore, thanks to the presynaptic depression, our STDP rule alone allows us to compute negative prediction errors, which still poses an open challenge for computation with dopamine because of the low baseline dopaminergic firing rate (<xref ref-type="bibr" rid="bib20">Glimcher, 2011</xref>; <xref ref-type="bibr" rid="bib8">Daw et al., 2002</xref>; <xref ref-type="bibr" rid="bib37">Matsumoto and Hikosaka, 2007</xref>).</p><p>Our framework smoothly connects a temporally precise spiking code with a fully rate-based code, and anything in between. As we have proven mathematically, this translates in moving smoothly from Monte Carlo to Temporal Difference by means of TD(<inline-formula><mml:math id="inf124"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>). Fast spiking sequences (temporal code) can be used for consolidation of previous experiences using Monte Carlo learning, while the behavioral timescale activity (rate code) results in TD updates, allowing learning on the timescale of seconds even with plasticity timeconstants on the order of milliseconds. This type of Hebbian learning over behavioral timescale exploits the bootstrapping property of TD, and is different than the one-shot behavioral plasticity described in <xref ref-type="bibr" rid="bib4">Bittner et al., 2017</xref>. However, these two mechanisms could be complementary, where the latter could play a more significant role in the formation of new place fields, while the former would be more relevant to shape the existing place fields to contain predictive information. Learning on behavioral timescales using STDP was also investigated in <xref ref-type="bibr" rid="bib12">Drew and Abbott, 2006</xref>. The main difference between <xref ref-type="bibr" rid="bib12">Drew and Abbott, 2006</xref> and our work, is that the former relies on overlapping neural activity between the pre- and post-synaptic neurons from the start, while in our case no such overlap is required. In other words, our setup allows us to learn connections between a presynaptic neuron and a postsynaptic neuron whose activities are separated by behavioral timescales initially. For this to be possible, there are two requirements: (1) the task needs to be repeated many times and (2) a chain of neurons are consecutively activated between the aforementioned presynaptic and postsynaptic neuron. Due to this chain of neurons, over time the activity of the postsynaptic neuron will start earlier, eventually overlapping with the presynaptic neuron.</p><p>In our work, we did not include theta modulation, but phase precession and theta sequences could be yet another type of activity within the TD lambda framework. A recent work (<xref ref-type="bibr" rid="bib18">George et al., 2023</xref>) incorporated the theta sweeps into behavioral activity, showing it approximately learns the SR. Moreover, theta sequences allow for fast learning, playing a similar role as replays (or any other fast temporal-code sequences) in our work. By simulating the temporally compressed and precise theta sequences, their model also reconciles the learning over behavioral timescales with STDP. In contrast, our framework reconciles both timescales relying purely on rate-coding during behavior. Finally, their method allows to learn the SR within continuous space. It would be interesting to investigate whether these methods co-exist in the hippocampus and other brain areas. Furthermore, (<xref ref-type="bibr" rid="bib15">Fang et al., 2023</xref>) et al. recently showed how the SR can be learned using recurrent neural networks with biologically plausible plasticity.</p><p>There are three different neural activities in our proposed framework: the presynaptic layer (CA3), the postsynaptic layer (CA1), and the external inputs. These external inputs could for example be location-dependent currents from the entorhinal cortex, with timings guided by the theta oscillations. The dependence of CA1 place fields on CA3 and entorhinal input is in line with lesion studies (see e.g. <xref ref-type="bibr" rid="bib6">Brun et al., 2008</xref>; <xref ref-type="bibr" rid="bib21">Hales et al., 2014</xref>; <xref ref-type="bibr" rid="bib50">O’Reilly et al., 2014</xref>). It would be interesting for future studies to further dissect the role various areas play in learning cognitive maps.</p><p>Notably, even though we have focused on the hippocampus in our work, the SR does not require predictive information to come from higher-level feedback inputs. This framework could therefore be useful even in sensory areas: certain stimuli are usually followed by other stimuli, essentially creating a sequence of states whose temporal structure can be encoded in the network using our framework. Interestingly, replays have been observed in other brain areas besides the hippocampus (<xref ref-type="bibr" rid="bib34">Kurth-Nelson et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Staresina et al., 2013</xref>). Furthermore, temporal difference learning in itself has been proposed in the past as a way to implement prospective coding (<xref ref-type="bibr" rid="bib5">Brea et al., 2016</xref>).</p></sec><sec id="s3-3"><title>Replays</title><p>We have also proposed a role for replays in learning the SR, in line with experimental findings and RL theories (<xref ref-type="bibr" rid="bib56">Russek et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Momennejad et al., 2017</xref>). In general, replays are thought to serve different functions, spanning from consolidation to planning (<xref ref-type="bibr" rid="bib55">Roscow et al., 2021</xref>). Here, we have shown that when the replayed trajectories are similar to the ones observed during behavior, they play the role of speeding up and consolidating learning by regulating the bias-variance trade-off, which is especially useful in novel environments. On the other hand, if the replayed trajectories differ from the ones experienced during wakefulness, replays can play a role in reshaping the representation of space, which would suggest their involvement in planning. Experimentally, it has been observed that replays often start and end from relevant locations in the environment, like reward sites, decision points, obstacles or the current position of the animal (<xref ref-type="bibr" rid="bib48">Ólafsdóttir et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib27">Jackson et al., 2006</xref>; <xref ref-type="bibr" rid="bib38">Mattar and Daw, 2017</xref>). Since these are salient locations, it is in line with our proposition that replays can be used to maintain a convenient representation of the environment. It is worth noticing that replays can serve a variety of functions, and our framework merely proposes additional beneficial properties without claiming to explain all observed replays. For example, in addition to forward replays, also reverse replays are ubiquitous in hippocampus (<xref ref-type="bibr" rid="bib53">Pfeiffer, 2020</xref>). The reverse replays are not included in our framework, and it is not clear yet whether they play different roles, with some evidence suggesting that reverse replays are more closely tied to the reward encoding (<xref ref-type="bibr" rid="bib3">Ambrose et al., 2016</xref>). Moreover, while indirect evidence supports the idea that replays can play a role during learning (<xref ref-type="bibr" rid="bib26">Igata et al., 2021</xref>), it is not yet clear how synaptic plasticity is manifested during replays (<xref ref-type="bibr" rid="bib16">Fuchsberger and Paulsen, 2022</xref>).</p></sec><sec id="s3-4"><title>Learning flexibility</title><p>Multiple ideas from reinforcement learning, such as TD(<inline-formula><mml:math id="inf125"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>), state-dependent discounting and the successor representation, emerge quite naturally from our simple biologically plausible setting. We propose in our work that time and space can be discounted differently. Moreover, the flexibility to change the discounting factor by modulating firing rates and plasticity parameters — which is ubiquitous in neural circuits — suggests that these mechanisms could be used to encode a variety of information in a cognitive map. Moreover, the specific dependence of the discount factor on the biological parameters leads to experimentally testable predictions. Indeed, our framework predicts well-defined changes in place fields after modulations of firing rates, speed of the agent or neuromodulation of the plasticity parameters (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Importantly, the discount parameter also depends on the time spent in each state. This eliminates the need for time discretization, which does not reflect the continuous nature of the response of time cells (<xref ref-type="bibr" rid="bib32">Kraus et al., 2013</xref>).</p></sec><sec id="s3-5"><title>Limitations of the reinforcement learning framework</title><p>We have already outlined some of the benefits of using reinforcement learning for modeling behavior, including providing clear computational and algorithmic frameworks. However, there are several intrinsic limitations to this framework. For example, RL agents that only use spatial data do not provide complete descriptions of behavior, which likely arises from integrating information across multiple sensory inputs. Whereas an animal would be able to smell and see a reward from a certain distance, an agent exploring the environment would only be able to discover it when randomly visiting the exact reward location. Furthermore, the framework rests on fairly strict mathematical assumptions: typically the state space needs to be markovian, time and space need to be discretized (which we manage to evade in this particular framework) and the discounting needs to follow an exponential decay. These assumptions are simplistic and it is not clear how often they are actually met. Reinforcement Learning is also a sample-intensive technique, whereas we know that some animals, including humans, are capable of much faster or even one-shot learning.</p><p>Even though we have provided a neural implementation of the SR, and of the value function as its read-out (see <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), the whole action selection process is still computed only at the algorithmic level. It may be interesting to extend the neural implementation to the policy selection mechanism in the future.</p><p>Taken together, our work joins — in a single framework — a variety of concepts from the neuronal level over cognitive theories to reinforcement learning.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>The successor representation</title><p>In a tabular environment, we define the value of a state <inline-formula><mml:math id="inf126"><mml:mi mathsize="70%">s</mml:mi></mml:math></inline-formula> as being the expected cumulative reward that an agent will receive following a certain policy starting in <inline-formula><mml:math id="inf127"><mml:mi mathsize="70%">s</mml:mi></mml:math></inline-formula>. The future rewards are multiplied by a factor <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf129"><mml:mi mathsize="70%">n</mml:mi></mml:math></inline-formula> is the number of steps until reaching the reward location and <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>γ</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is the delay discount factor. It is usual to use <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, which ensures that earlier rewards are given more importance compared to later rewards. Formally, the value of a state <inline-formula><mml:math id="inf132"><mml:mi mathsize="70%">s</mml:mi></mml:math></inline-formula> under a certain policy <inline-formula><mml:math id="inf133"><mml:mi mathsize="70%">π</mml:mi></mml:math></inline-formula> is defined as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf134"><mml:mi mathsize="70%">a</mml:mi></mml:math></inline-formula> denotes the action, <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi mathsize="70%">R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">s</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the reward function and <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi mathsize="70%">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">s</mml:mi><mml:mo mathsize="70%" stretchy="false">′</mml:mo></mml:msup><mml:mo lspace="2.5pt" mathsize="70%" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi mathsize="70%">s</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">a</mml:mi></mml:mrow></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the transition function, i.e. the probability that taking an action <inline-formula><mml:math id="inf137"><mml:mi mathsize="70%">a</mml:mi></mml:math></inline-formula> in state <inline-formula><mml:math id="inf138"><mml:mi mathsize="70%">s</mml:mi></mml:math></inline-formula> will result in a transition to state <inline-formula><mml:math id="inf139"><mml:msup><mml:mi mathsize="70%">s</mml:mi><mml:mo mathsize="70%" stretchy="false">′</mml:mo></mml:msup></mml:math></inline-formula>. Following (<xref ref-type="bibr" rid="bib9">Dayan, 1993</xref>), we can decompose the value function into the inner product of reward function and successor matrix<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This representation is known as the successor representation (SR), where each element <inline-formula><mml:math id="inf140"><mml:msub><mml:mi mathsize="70%">M</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the expected future occupancy of state <inline-formula><mml:math id="inf141"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> when in state <inline-formula><mml:math id="inf142"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula>. By decomposing the value into the SR and the reward function (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>), relearning the state values <inline-formula><mml:math id="inf143"><mml:mi mathsize="70%">V</mml:mi></mml:math></inline-formula> after changing the reward function is fast, similar to model-based learning. At the same time, the SR can be learned in a model-free manner, using for example temporal difference (TD) learning (<xref ref-type="bibr" rid="bib56">Russek et al., 2017</xref>).</p></sec><sec id="s4-2"><title>Derivation of the TD(<inline-formula><mml:math id="inf144"><mml:mi>λ</mml:mi></mml:math></inline-formula>) update for the SR</title><p>The TD(<inline-formula><mml:math id="inf145"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) update for the SR is then implemented according to (see e.g. <xref ref-type="bibr" rid="bib62">Sutton and Barto, 1998</xref>)<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathsize="70%">δ</mml:mi><mml:mn mathsize="70%">0</mml:mn><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi></mml:mrow></mml:msubsup><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">δ</mml:mi><mml:mn mathsize="70%">1</mml:mn><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">δ</mml:mi><mml:mn mathsize="70%">2</mml:mn><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%" mathvariant="normal">…</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Using <inline-formula><mml:math id="inf146"><mml:msubsup><mml:mi mathsize="70%">δ</mml:mi><mml:mi mathsize="70%">i</mml:mi><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> for the TD error at step <inline-formula><mml:math id="inf147"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mi mathsize="70%">x</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the Kronecker delta,<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathsize="70%">δ</mml:mi><mml:mi mathsize="70%">n</mml:mi><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi></mml:mrow></mml:msubsup><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>corresponds to the TD error for element <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the successor representation after the transition from state <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula> to state <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:math></inline-formula>. Combining <xref ref-type="disp-formula" rid="equ5 equ6">Equations 5 and 6</xref>, we find<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="70%" minsize="70%">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo lspace="22.5pt" mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">2</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="70%" minsize="70%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo lspace="22.5pt" mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">2</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">3</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">2</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="70%" minsize="70%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo lspace="22.5pt" mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%" mathvariant="normal">…</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo lspace="22.5pt" mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">γ</mml:mi><mml:mn mathsize="70%">2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">2</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">2</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%" mathvariant="normal">…</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" mathsize="70%" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="70%">n</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow><mml:mi mathsize="70%">N</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mi mathsize="70%">n</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mi mathsize="70%">n</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="70%" minsize="70%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>and<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo stretchy="false">←</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo stretchy="false">←</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-3"><title>Neural network model</title><sec id="s4-3-1"><title>Plasticity rule</title><p>The synaptic plasticity rule (<xref ref-type="fig" rid="fig1">Figure 1d</xref>) consists of a weight-dependent depression for presynaptic spikes and a spike-timing dependent potentiation, given by<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mtext>STDP</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mtext>STDP</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>LTD</mml:mtext></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf152"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the synaptic connection from presynaptic neuron <inline-formula><mml:math id="inf153"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> to postsynaptic neuron <inline-formula><mml:math id="inf154"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">r</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow><mml:mi mathsize="70%">j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the plasticity trace, a low-pass filter of the presynaptic spike train with time constant <inline-formula><mml:math id="inf156"><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf157"><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf158"><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mi mathsize="70%">i</mml:mi></mml:msup></mml:math></inline-formula> are the spike times of the postsynaptic and presynaptic neuron respectively, <inline-formula><mml:math id="inf159"><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">LTD</mml:mtext></mml:msub></mml:math></inline-formula> are the amplitudes of potentiation and depression respectively, <inline-formula><mml:math id="inf161"><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mtext mathsize="70%">STDP</mml:mtext></mml:msub></mml:math></inline-formula> is the learning rate for STDP and the <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi mathsize="70%">δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mo mathsize="70%" stretchy="false">⋅</mml:mo><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Dirac delta function.</p></sec><sec id="s4-3-2"><title>Place cell activation</title><p>We assume that each state in the environment is represented by a population of place cells in the network. In our model, this is achieved by delivering place-tuned currents to the neurons. Whenever a state <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:math></inline-formula> is entered, the presynaptic neurons encoding state <inline-formula><mml:math id="inf164"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> start firing at a constant rate <inline-formula><mml:math id="inf165"><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for a time <inline-formula><mml:math id="inf166"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula>, following a Poisson process with parameter <inline-formula><mml:math id="inf167"><mml:mrow><mml:msubsup><mml:mi mathsize="70%">ρ</mml:mi><mml:mi mathsize="70%">h</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The other presynaptic neurons are assumed to be silent:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if </mml:mtext><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where the Kronecker delta function is defined as <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mi mathsize="70%">h</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi mathsize="70%">h</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:math></inline-formula> and zero otherwise. Here we use the index <inline-formula><mml:math id="inf170"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> to denote any neuron belonging to the population of neurons encoding state <inline-formula><mml:math id="inf171"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>. After a short delay, at time <inline-formula><mml:math id="inf172"><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup></mml:math></inline-formula>, a similar current <inline-formula><mml:math id="inf173"><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is delivered to the postsynaptic neuron encoding state <inline-formula><mml:math id="inf174"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>, for a duration of time <inline-formula><mml:math id="inf175"><mml:mi mathsize="70%">ω</mml:mi></mml:math></inline-formula>.<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if </mml:mtext><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Besides the place-tuned input current, CA1 neurons receive inputs from the presynaptic layer (CA3). The postsynaptic potential <inline-formula><mml:math id="inf176"><mml:msubsup><mml:mi mathsize="70%">ρ</mml:mi><mml:mi mathsize="70%">i</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> when the agent is in state j is thus given by<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with the first sum running over all <inline-formula><mml:math id="inf177"><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> presynaptic neurons encoding state <inline-formula><mml:math id="inf178"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>, and the second sum over all presynaptic firing times <inline-formula><mml:math id="inf179"><mml:msubsup><mml:mi mathsize="70%">t</mml:mi><mml:mi mathsize="70%">k</mml:mi><mml:mi mathsize="70%">f</mml:mi></mml:msubsup></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf180"><mml:mi mathsize="70%">k</mml:mi></mml:math></inline-formula> happened before <inline-formula><mml:math id="inf181"><mml:mi mathsize="70%">t</mml:mi></mml:math></inline-formula>. The excitatory postsynaptic current <inline-formula><mml:math id="inf182"><mml:mi mathsize="70%">κ</mml:mi></mml:math></inline-formula> is modeled as an exponential decay described as <inline-formula><mml:math id="inf183"><mml:mrow><mml:mrow><mml:mi mathsize="70%">κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">x</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">x</mml:mi><mml:mo mathsize="70%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi mathsize="70%">x</mml:mi><mml:mo mathsize="70%" stretchy="false">≥</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:math></inline-formula> and zero otherwise. Each CA1 neuron <inline-formula><mml:math id="inf185"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula> fires following an inhomogeneous Poisson process with rate <inline-formula><mml:math id="inf186"><mml:mrow><mml:msubsup><mml:mi mathsize="70%">ρ</mml:mi><mml:mi mathsize="70%">i</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Note that, in most simulations we will use a single neuron in the population <inline-formula><mml:math id="inf187"><mml:mrow><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">p</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:math></inline-formula>. In addition, we normally set <inline-formula><mml:math id="inf188"><mml:mrow><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">θ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf189"><mml:mrow><mml:mi mathsize="70%">ω</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">θ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. However, we will keep these as explicit parameters for theoretical purposes.</p></sec><sec id="s4-3-3"><title>Equivalence with TD(<inline-formula><mml:math id="inf190"><mml:mi>λ</mml:mi></mml:math></inline-formula>)</title><sec id="s4-3-3-1"><title>Total plasticity update</title><p>Since we have the mathematical equation for the plasticity rule, and CA3 and CA1 neurons follow an inhomogeneous Poisson process with time-dependent firing rate, we can calculate analytically the average total weight change for the synapse <inline-formula><mml:math id="inf191"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, given a certain trajectory (details in the Appendix). Please notice that our calculation is based on <xref ref-type="bibr" rid="bib31">Kempter et al., 1999</xref>, which takes into account the fact that our plasticity rule is sensitive to spike timing and involves a spike-spike correlation term. We find that:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mstyle mathsize="1.44em"><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf192"><mml:mi mathsize="70%">N</mml:mi></mml:math></inline-formula> is the number of states until the end of the trajectory and<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mi>θ</mml:mi><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mi>θ</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">B</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:msup><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">ω</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">B</mml:mi><mml:mo mathsize="70%" stretchy="false">′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">C</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">p</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:msup><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec id="s4-3-3-2"><title>Comparison with TD(<inline-formula><mml:math id="inf193"><mml:mi>λ</mml:mi></mml:math></inline-formula>)</title><p>Comparing the total weight change due to STDP (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>) to the TD(<inline-formula><mml:math id="inf194"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) update (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), we can see that the two equations are very similar in form:<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mstyle mathsize="1.44em"><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mstyle><mml:mo stretchy="false">]</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo>−</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mi>λ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We impose <inline-formula><mml:math id="inf195"><mml:mrow><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="70%">M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and find:<disp-formula id="equ18"><label>(17)</label><mml:math id="m18"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">A</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">η</mml:mi></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ19"><label>(18)</label><mml:math id="m19"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathsize="70%">B</mml:mi><mml:mi mathsize="70%">A</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" mathvariant="italic" separator="true" stretchy="false"> </mml:mo><mml:mo mathsize="70%" stretchy="false">→</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="70%" mathvariant="italic" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msup><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathsize="70%">A</mml:mi><mml:msup><mml:mi mathsize="70%">B</mml:mi><mml:mo mathsize="70%" stretchy="false">′</mml:mo></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo mathsize="70%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">γ</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ21"><label>(20)</label><mml:math id="m21"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi mathsize="70%">C</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo mathsize="70%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mi mathsize="70%">A</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">λ</mml:mi></mml:mrow><mml:mi mathsize="70%">γ</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi mathsize="70%">A</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">B</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:msup><mml:mi mathsize="70%">B</mml:mi><mml:mo mathsize="70%" stretchy="false">′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf197"><mml:mi mathsize="70%">C</mml:mi></mml:math></inline-formula> are defined as in <xref ref-type="disp-formula" rid="equ14 equ15 equ16">Equations 14, 15, and 16</xref>.</p><p>Hence, our plasticity rule is learning the Successor Representation through a TD(<inline-formula><mml:math id="inf198"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) model with parameters:<disp-formula id="equ22"><label>(21)</label><mml:math id="m22"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">η</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">A</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ23"><label>(22)</label><mml:math id="m23"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi mathsize="70%">A</mml:mi><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">C</mml:mi></mml:mrow><mml:mi mathsize="70%">A</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">T</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ24"><label>(23)</label><mml:math id="m24"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">A</mml:mi><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">C</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>To ensure the learning rate <inline-formula><mml:math id="inf199"><mml:mi mathsize="70%">η</mml:mi></mml:math></inline-formula> is positive, one condition resulting from <xref ref-type="disp-formula" rid="equ22">Equation 21</xref> is<disp-formula id="equ25"><label>(24)</label><mml:math id="m25"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec></sec></sec><sec id="s4-4"><title>Learning during normal behavior (<inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mo>&gt;&gt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>)</title><p>During normal behavior, we assume the place-tuned currents are on larger timescales than the plasticity constants: <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>&gt;&gt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. We can see from <xref ref-type="disp-formula" rid="equ14 equ16">Equations 14 and 16</xref> that the factor <inline-formula><mml:math id="inf202"><mml:mi mathsize="70%">A</mml:mi></mml:math></inline-formula> grows linearly with <inline-formula><mml:math id="inf203"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula> while <inline-formula><mml:math id="inf204"><mml:mi mathsize="70%">C</mml:mi></mml:math></inline-formula> grows exponentially with <inline-formula><mml:math id="inf205"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula>. From <xref ref-type="disp-formula" rid="equ24">Equation 23</xref>, we then have<disp-formula id="equ26"><label>(25)</label><mml:math id="m26"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">→</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>(See also <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec><sec id="s4-5"><title>Learning during replays (<inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mo>&lt;&lt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>)</title><sec id="s4-5-1"><title>Assumptions</title><p>For the replay model we assume the place-tuned currents are impulses, which make the neurons emit exactly one spike at a given time. Specifically, we can make the duration of the place-tuned currents go to 0,<disp-formula id="equ27"><label>(26)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">ω</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">→</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>while the intensity of the currents goes to infinity. For simplicity, we will take:<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>θ</mml:mi></mml:mfrac><mml:mo stretchy="false">→</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ω</mml:mi></mml:mfrac><mml:mo stretchy="false">→</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>ω</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Furthermore, we assume that the contribution of the postsynaptic currents due to the single presynaptic spikes is negligible in terms of driving plasticity, allowing us to set<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub><mml:mo mathsize="70%" stretchy="false">→</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5-2"><title>Calculations of TD parameters</title><p>Given the assumptions above, we can see from <xref ref-type="disp-formula" rid="equ14 equ16">Equations 14 and 16</xref> that:<disp-formula id="equ30"><mml:math id="m30"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>For <xref ref-type="disp-formula" rid="equ15">Equation 15</xref>, we can use the Taylor expansion for <inline-formula><mml:math id="inf207"><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mfrac><mml:mi mathsize="70%">x</mml:mi><mml:mi mathsize="70%">τ</mml:mi></mml:mfrac></mml:msup></mml:math></inline-formula> around <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi mathsize="70%">x</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:math></inline-formula>, such that: <inline-formula><mml:math id="inf209"><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mfrac><mml:mi mathsize="70%">x</mml:mi><mml:mi mathsize="70%">τ</mml:mi></mml:mfrac></mml:msup><mml:mo mathsize="70%" stretchy="false">≈</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi mathsize="70%">x</mml:mi><mml:mi mathsize="70%">τ</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> :<disp-formula id="equ31"><mml:math id="m31"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>B</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mfrac><mml:mi>ω</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Using <xref ref-type="disp-formula" rid="equ22 equ23 equ24 equ1">Equations 21, 22, 23 and 18</xref>, we can calculate the parameters and constraints for the TD model:<disp-formula id="equ32"><label>(27)</label><mml:math id="m32"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>A</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>−</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mo>−</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mi>A</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>As expected, the bootstrapping parameter <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:math></inline-formula> (see also <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec></sec><sec id="s4-6"><title>Alternative derivation of replay model</title><sec id="s4-6-1"><title>Place cell activation during replays</title><p>We model a replay event as a precise temporal sequence of spikes. Since every neuron represents a state in the environment, a replay sequence reproduces a trajectory of states. We assume that, when the agent is in state <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:math></inline-formula>, the neurons representing state <inline-formula><mml:math id="inf212"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> fire <inline-formula><mml:math id="inf213"><mml:msub><mml:mi mathsize="70%">n</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> spikes at some point in the time interval <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">∈</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:mn mathsize="70%">0</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%">σ</mml:mi><mml:mo maxsize="70%" minsize="70%">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the exact firing times are uniformly sampled. After a short delay, the CA1 neurons representing state <inline-formula><mml:math id="inf215"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> fire <inline-formula><mml:math id="inf216"><mml:msub><mml:mi mathsize="70%">n</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> spikes at a time uniformly sampled from the interval <inline-formula><mml:math id="inf217"><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">σ</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">]</mml:mo></mml:mrow></mml:math></inline-formula>. The time between two consecutive state visits is <inline-formula><mml:math id="inf218"><mml:mi mathsize="70%">T</mml:mi></mml:math></inline-formula>. The exact number of spikes in each replay event is random but small. Specifically, it is sampled from the set <inline-formula><mml:math id="inf219"><mml:mrow><mml:mo maxsize="70%" minsize="70%">{</mml:mo><mml:mn mathsize="70%">0</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mn mathsize="70%">2</mml:mn><mml:mo maxsize="70%" minsize="70%">}</mml:mo></mml:mrow></mml:math></inline-formula> according to the probability vector<disp-formula id="equ33"><label>(28)</label><mml:math id="m33"><mml:mrow><mml:mrow><mml:mtext mathvariant="bold">p</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It is worth noting here that other implementations are possible but that we assume the average number of spikes in each state is 1, and that the average time between a presynaptic and a postsynaptic spike is <inline-formula><mml:math id="inf220"><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup></mml:math></inline-formula>. The model could be further generalized for a higher number of average spikes per state.</p></sec><sec id="s4-6-2"><title>Plasticity update</title><p>We can consider again our learning rule, composed of a positive pre-post potentiation window and presynaptic weight-dependent depression (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>). Let’s consider the synapse <inline-formula><mml:math id="inf221"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we can see that on average the total amount of depression will be determined by the number of times the state <inline-formula><mml:math id="inf222"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> is visited in the trajectory replayed:<disp-formula id="equ34"><mml:math id="m34"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">pre</mml:mtext></mml:msub><mml:mo mathsize="70%" stretchy="false">⋅</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf223"><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msub></mml:math></inline-formula> is the number of times the state <inline-formula><mml:math id="inf224"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> is visited. The amount of potentiation will be determined, instead, by the time difference between the postsynaptic and presynaptic firing times, which encode the distance between state <inline-formula><mml:math id="inf225"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> and state <inline-formula><mml:math id="inf226"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula>:<disp-formula id="equ35"><mml:math id="m35"><mml:mrow><mml:mrow><mml:mtext mathsize="70%">LTP</mml:mtext><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" mathsize="70%" movablelimits="false" stretchy="false" symmetric="true">∑</mml:mo><mml:mi mathsize="70%">k</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathsize="70%">k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup></mml:mrow><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">n</mml:mi><mml:mi mathsize="70%">k</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf227"><mml:msubsup><mml:mi mathsize="70%">n</mml:mi><mml:mi mathsize="70%">k</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> represents the number of times the agent visited state <inline-formula><mml:math id="inf228"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula> k steps after <inline-formula><mml:math id="inf229"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>. Combining the equations above we find that:<disp-formula id="equ36"><label>(29)</label><mml:math id="m36"><mml:mrow><mml:mtable columnalign="right right" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>If we assume that the this value has converged to its stationary state, <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>;<disp-formula id="equ37"><label>(30)</label><mml:math id="m37"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-6-3"><title>Comparison with online Monte Carlo learning</title><p>Given the stable weight <inline-formula><mml:math id="inf231"><mml:msup><mml:mi mathsize="70%">w</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ37">Equation 30</xref>, we can impose that:<disp-formula id="equ38"><label>(31)</label><mml:math id="m38"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">pre</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" separator="true" stretchy="false">  </mml:mo><mml:mtext mathsize="70%"> and </mml:mtext></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ39"><label>(32)</label><mml:math id="m39"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">T</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">γ</mml:mi></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>we find that the stable weight is:<disp-formula id="equ40"><label>(33)</label><mml:math id="m40"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>which is the definition of the Successor Representation matrix (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). Indeed, <inline-formula><mml:math id="inf232"><mml:msubsup><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">⋆</mml:mo></mml:msubsup></mml:math></inline-formula> is computing the sample mean of the discounted distance between states <inline-formula><mml:math id="inf233"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf234"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>, which is equivalent to performing an every-state Monte Carlo or TD(<inline-formula><mml:math id="inf235"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>=1) update. Notably, from <xref ref-type="disp-formula" rid="equ36">Equation 29</xref>, we have that the learning rate for the Monte Carlo update is given by:<disp-formula id="equ41"><label>(34)</label><mml:math id="m41"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">η</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">pre</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec></sec><sec id="s4-7"><title>Simulation details for <xref ref-type="fig" rid="fig2">Figure 2</xref></title><p>A linear track with four states is simulated. The policy of the agent in this simulation is to traverse the track from left to right, with one epoch consisting of starting in state 1 and ending in state 4. One simulation consists of 50 epochs, and we re-run the whole simulation ten times with different random seeds. Over these ten seeds, mean and standard deviation of the synaptic weights are recorded after every epoch.</p><p>Our neural network consists of two layers, each with a single neuron per state (as in <xref ref-type="fig" rid="fig1">Figure 1</xref>). Synaptic connections are made from each presynaptic neuron to all postsynaptic neurons, resulting in a 4-by-4 matrix which is initialized as the identity matrix. The plasticity rule and neuronal activations follow <xref ref-type="disp-formula" rid="equ9 equ10 equ11 equ12">Equations 9–12</xref>.</p><p>The STDP parameters are listed in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameters used for the spiking network.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf236"><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf237"><mml:msub><mml:mi mathsize="70%">ρ</mml:mi><mml:mtext mathsize="70%">pre</mml:mtext></mml:msub></mml:math></inline-formula></td><td align="left" valign="middle">0.1ms<sup>-1</sup></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf238"><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">m</mml:mtext></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">2ms</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">stepsize</td><td align="left" valign="bottom">0.01ms</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf242"><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mtext mathsize="70%">stdp</mml:mtext></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0.003</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf243"><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mtext mathsize="70%">LTP</mml:mtext></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">60ms</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle">1 ms<sup>-1</sup></td></tr></tbody></table></table-wrap><p>To obey <xref ref-type="disp-formula" rid="equ25">Equation 24</xref>, we set <inline-formula><mml:math id="inf245"><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mtext mathsize="70%">pre</mml:mtext></mml:msub></mml:math></inline-formula> equal to the right hand side augmented with 5.</p><p>For the behavioral case, we choose T=100ms, <inline-formula><mml:math id="inf246"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula>=80ms, <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi mathsize="70%">ω</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mi mathsize="70%">θ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which correspond to TD(<inline-formula><mml:math id="inf248"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) parameters <inline-formula><mml:math id="inf249"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0.21</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0.89</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf251"><mml:mrow><mml:mi mathsize="70%">η</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0.12</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>In the replay case, we have a sequence of single spike per neuron (see <xref ref-type="fig" rid="fig2">Figure 2b</xref> and section ‘Alternative derivation of replay model’). Following <xref ref-type="disp-formula" rid="equ32">Equation 27</xref>, we choose <inline-formula><mml:math id="inf252"><mml:mrow><mml:mi mathsize="70%">T</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="70%">log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathsize="70%">γ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo mathsize="70%" stretchy="false">≈</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> 7ms, where <inline-formula><mml:math id="inf253"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf254"><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the same as in <xref ref-type="table" rid="table1">Table 1</xref>. We set <inline-formula><mml:math id="inf255"><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">2</mml:mn></mml:mrow></mml:math></inline-formula> ms and <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi mathsize="70%">σ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0.5</mml:mn></mml:mrow></mml:math></inline-formula> ms. By setting the <inline-formula><mml:math id="inf257"><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">p</mml:mi></mml:mrow></mml:msub><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi mathsize="70%">exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">/</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, the corresponding TD(<inline-formula><mml:math id="inf258"><mml:mi mathsize="70%">λ</mml:mi></mml:math></inline-formula>) parameters are <inline-formula><mml:math id="inf259"><mml:mrow><mml:mi mathsize="70%">λ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0.89</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf261"><mml:mrow><mml:mi mathsize="70%">η</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0.12</mml:mn></mml:mrow></mml:math></inline-formula> just as in the behavioral case.</p><p>More details on the place cell activation during replays in our model can be found in section ‘Alternative derivation of replay model’. Using exactly one single spike per neuron with the above parameters would allow us to follow the TD(1) learning trajectories without any noise. For more biological realism, we choose <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi mathsize="70%">p</mml:mi><mml:mn mathsize="70%">1</mml:mn></mml:msub><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mn mathsize="70%">0.15</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ33">Equation 28</xref>, in order to achieve an equal amount of noise due to the random spiking as in the case of behavioral activity (see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p></sec><sec id="s4-8"><title>Simulation details for <xref ref-type="fig" rid="fig3">Figure 3</xref></title><p>Using the same neural network and plasticity parameters as the behavioral learning in <xref ref-type="fig" rid="fig2">Figure 2</xref> (see previous section), we simulate the linear track in the following two situations:</p><list list-type="bullet"><list-item><p>The third state has T=200ms instead of 100ms. All other parameters remain the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Results plotted in <xref ref-type="fig" rid="fig3">Figure 3E</xref>.</p></list-item><list-item><p>The third state has <inline-formula><mml:math id="inf263"><mml:mrow><mml:msub><mml:mi mathsize="70%">ρ</mml:mi><mml:mtext mathsize="70%">pre</mml:mtext></mml:msub><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mn mathsize="70%">0.2</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mtext mathsize="70%">ms</mml:mtext><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf264"><mml:mrow><mml:mpadded width="+5pt"><mml:mn mathsize="70%">0.1</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mtext mathsize="70%">ms</mml:mtext><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. All other parameters remain the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Results plotted in <xref ref-type="fig" rid="fig3">Figure 3F</xref>.</p></list-item></list></sec><sec id="s4-9"><title>Simulation details for <xref ref-type="fig" rid="fig4">Figure 4</xref></title><p>A linear track with three states is simulated, and the agent has 50% probability to move left or right in each state (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>). One epoch lasts until the agent reaches one of the <italic>STOP</italic> locations.</p><p>We then use the same neural network and plasticity parameters as used for <xref ref-type="fig" rid="fig2">Figure 2</xref>. We simulate three scenarios:</p><list list-type="bullet"><list-item><p>Only replay-based learning during all epochs (no behavioral learning). This scenario corresponds to <italic>MC STDP</italic> in <xref ref-type="fig" rid="fig4">Figure 4B</xref> and to <xref ref-type="fig" rid="fig4">Figure 4C</xref>.</p></list-item><list-item><p>Mixed learning using both behavior and replays. The probability for an epoch to be a replay is decaying over time following <inline-formula><mml:math id="inf265"><mml:mrow><mml:mi mathsize="70%">exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo mathsize="70%" stretchy="false">/</mml:mo><mml:mn mathsize="70%">6</mml:mn></mml:mrow></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf266"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula> the epoch number. This scenario corresponds to <italic>Mix STDP</italic> in <xref ref-type="fig" rid="fig4">Figure 4B</xref> and to <xref ref-type="fig" rid="fig4">Figure 4E</xref>.</p></list-item><list-item><p>Only behavioral learning during all epochs (no replays). This scenario corresponds to <italic>TD STDP</italic> in <xref ref-type="fig" rid="fig4">Figure 4B</xref> and to <xref ref-type="fig" rid="fig4">Figure 4D</xref>.</p></list-item></list></sec><sec id="s4-10"><title>Simulation details for <xref ref-type="fig" rid="fig5">Figure 5</xref></title><p>A linear track with 21 states is simulated. The SR is initialized as the identity matrix, and the reward vector (containing the reward at each state) is also initialized as the zero vector. We simulate the learning of the SR during behavior using the theoretical TD(0) updates and during replays using the theoretical TD(1) updates. The value of each state is then calculated as the matrix-vector product between the SR and the reward vector, resulting in an initial value of zero for each state.</p><p>The policy of the agent is a softmax policy (i.e. the probability to move to neighboring states is equal to the softmax of the values of those neighboring states). The first time the agent reaches the leftmost state of the track (state 1), the negative reward of –2 is revealed, mimicking the shock in the actual experiments, and the reward vector is updated accordingly for this state.</p><p>We now simulate two scenarios: in the first scenario, the agent always follows the softmax policy and no replays are triggered (see <xref ref-type="fig" rid="fig5">Figure 5D</xref>, left panel). In the second scenario, every time the agent enters the dark zone from the light zone (i.e. transitions from state 12 to state 11 in our simulation), a replay is triggered from that state until the leftmost state (state 1) (see <xref ref-type="fig" rid="fig5">Figure 5D</xref>, right panel). Both scenarios are simulated for 2000 state transitions. We then run these two scenarios 100 times and calculate mean and standard deviation of state occupancies (<xref ref-type="fig" rid="fig5">Figure 5F</xref>).</p><p>Finally, since the second scenario has more SR updates than the first scenario, we also simulate the first scenario for 4000 state transitions (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) and show how the observed behavior of <xref ref-type="fig" rid="fig5">Figure 5</xref> is unaffected by this.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con3"><p>Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80671-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modelling code is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/jacopobono/learning_cognitive_maps_code">https://github.com/jacopobono/learning_cognitive_maps_code</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a5166815043fc36f4e804ba90d29164f34c7006b;origin=https://github.com/jacopobono/learning_cognitive_maps_code;visit=swh:1:snp:3e11e29283bbaf71786af24d2d77c12596b6e584;anchor=swh:1:rev:d86b262545547353c7050bbc2d476c2f4a297989">swh:1:rev:d86b262545547353c7050bbc2d476c2f4a297989</ext-link>; <xref ref-type="bibr" rid="bib28">Jacopo, 2023</xref>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aggleton</surname><given-names>JP</given-names></name><name><surname>Brown</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Episodic memory, amnesia, and the hippocampal-anterior thalamic axis</article-title><source>The Behavioral and Brain Sciences</source><volume>22</volume><fpage>425</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">11301518</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ainslie</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Pure hyperbolic discount curves predict “ eyes open ” self-control</article-title><source>Theory and Decision</source><volume>73</volume><fpage>3</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1007/s11238-011-9272-5</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambrose</surname><given-names>RE</given-names></name><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reverse replay of hippocampal place cells is uniquely modulated by changing reward</article-title><source>Neuron</source><volume>91</volume><fpage>1124</fpage><lpage>1136</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.047</pub-id><pub-id pub-id-type="pmid">27568518</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname><given-names>KC</given-names></name><name><surname>Milstein</surname><given-names>AD</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioral time scale synaptic plasticity underlies CA1 place fields</article-title><source>Science</source><volume>357</volume><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id><pub-id pub-id-type="pmid">28883072</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brea</surname><given-names>J</given-names></name><name><surname>Gaál</surname><given-names>AT</given-names></name><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prospective coding by spiking neurons</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005003</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005003</pub-id><pub-id pub-id-type="pmid">27341100</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brun</surname><given-names>VH</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Wu</surname><given-names>HQ</given-names></name><name><surname>Schwarcz</surname><given-names>R</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Impaired spatial representation in CA1 after lesion of direct input from entorhinal cortex</article-title><source>Neuron</source><volume>57</volume><fpage>290</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.11.034</pub-id><pub-id pub-id-type="pmid">18215625</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>S</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>New experiences enhance coordinated neural activity in the hippocampus</article-title><source>Neuron</source><volume>57</volume><fpage>303</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.11.035</pub-id><pub-id pub-id-type="pmid">18215626</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Kakade</surname><given-names>S</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Opponent interactions between serotonin and dopamine</article-title><source>Neural Networks</source><volume>15</volume><fpage>603</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(02)00052-7</pub-id><pub-id pub-id-type="pmid">12371515</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Improving generalization for temporal difference learning: the successor representation</article-title><source>Neural Computation</source><volume>5</volume><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1162/neco.1993.5.4.613</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>Temporal difference learning in continuous time and space</chapter-title><person-group person-group-type="editor"><name><surname>Touretzky</surname><given-names>D</given-names></name><name><surname>Mozer</surname><given-names>M</given-names></name><name><surname>Hasselmo</surname><given-names>M</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-loc>Massachusetts, United States</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Reinforcement learning in continuous time and space</article-title><source>Neural Computation</source><volume>12</volume><fpage>219</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1162/089976600300015961</pub-id><pub-id pub-id-type="pmid">10636940</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drew</surname><given-names>PJ</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Extending the effects of spike-timing-dependent plasticity to behavioral timescales</article-title><source>PNAS</source><volume>103</volume><fpage>8876</fpage><lpage>8881</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600676103</pub-id><pub-id pub-id-type="pmid">16731625</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name><name><surname>Dudchenko</surname><given-names>P</given-names></name><name><surname>Wood</surname><given-names>E</given-names></name><name><surname>Shapiro</surname><given-names>M</given-names></name><name><surname>Tanila</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The hippocampus, memory, and place cells: is it spatial memory or a memory space?</article-title><source>Neuron</source><volume>23</volume><fpage>209</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80773-4</pub-id><pub-id pub-id-type="pmid">10399928</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erdem</surname><given-names>UM</given-names></name><name><surname>Hasselmo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A goal-directed spatial navigation model using forward trajectory planning based on grid cells</article-title><source>The European Journal of Neuroscience</source><volume>35</volume><fpage>916</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.08015.x</pub-id><pub-id pub-id-type="pmid">22393918</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>C</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Mackevicius</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural learning rules for generating flexible predictions and computing the successor representation</article-title><source>eLife</source><volume>12</volume><elocation-id>e80680</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80680</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuchsberger</surname><given-names>T</given-names></name><name><surname>Paulsen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Modulation of hippocampal plasticity in learning and memory</article-title><source>Current Opinion in Neurobiology</source><volume>75</volume><elocation-id>102558</elocation-id><pub-id pub-id-type="doi">10.1016/j.conb.2022.102558</pub-id><pub-id pub-id-type="pmid">35660989</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>MPH</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rethinking dopamine as generalized prediction error</article-title><source>Proceedings of the Royal Society B</source><volume>285</volume><elocation-id>20181645</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2018.1645</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>George</surname><given-names>TM</given-names></name><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>Stachenfeld</surname><given-names>K</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Rapid learning of predictive maps with STDP and theta phase precession</article-title><source>eLife</source><volume>12</volume><elocation-id>e80663</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80663</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The successor representation: its computational logic and neural substrates</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7193</fpage><lpage>7200</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0151-18.2018</pub-id><pub-id pub-id-type="pmid">30006364</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis</article-title><source>PNAS</source><volume>108</volume><fpage>15647</fpage><lpage>15654</lpage><pub-id pub-id-type="doi">10.1073/pnas.1014269108</pub-id><pub-id pub-id-type="pmid">21389268</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hales</surname><given-names>JB</given-names></name><name><surname>Schlesiger</surname><given-names>MI</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Clark</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Medial entorhinal cortex lesions only partially disrupt hippocampal place cells and hippocampus-dependent place memory</article-title><source>Cell Reports</source><volume>9</volume><fpage>893</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2014.10.009</pub-id><pub-id pub-id-type="pmid">25437546</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Schnell</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Laminar selectivity of the cholinergic suppression of synaptic transmission in rat hippocampal region CA1: computational modeling and brain slice physiology</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>3898</fpage><lpage>3914</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-06-03898.1994</pub-id><pub-id pub-id-type="pmid">8207494</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Bodelón</surname><given-names>C</given-names></name><name><surname>Wyble</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A proposed function for hippocampal theta rhythm: separate phases of encoding and retrieval enhance reversal of prior learning</article-title><source>Neural Computation</source><volume>14</volume><fpage>793</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1162/089976602317318965</pub-id><pub-id pub-id-type="pmid">11936962</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Hippocampal mechanisms for the context-dependent retrieval of episodes</article-title><source>Neural Networks</source><volume>18</volume><fpage>1172</fpage><lpage>1190</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2005.08.007</pub-id><pub-id pub-id-type="pmid">16263240</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Sarter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Modes and models of forebrain cholinergic neuromodulation of cognition</article-title><source>Neuropsychopharmacology</source><volume>36</volume><fpage>52</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1038/npp.2010.104</pub-id><pub-id pub-id-type="pmid">20668433</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Igata</surname><given-names>H</given-names></name><name><surname>Ikegaya</surname><given-names>Y</given-names></name><name><surname>Sasaki</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Prioritized experience replays on a hippocampal predictive map for learning</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2011266118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2011266118</pub-id><pub-id pub-id-type="pmid">33443144</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>JC</given-names></name><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Hippocampal sharp waves and reactivation during awake states depend on repeated sequential experience</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>12415</fpage><lpage>12426</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4118-06.2006</pub-id><pub-id pub-id-type="pmid">17135403</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jacopo</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Learning_cognitive_maps_code</data-title><version designator="swh:1:rev:d86b262545547353c7050bbc2d476c2f4a297989">swh:1:rev:d86b262545547353c7050bbc2d476c2f4a297989</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a5166815043fc36f4e804ba90d29164f34c7006b;origin=https://github.com/jacopobono/learning_cognitive_maps_code;visit=swh:1:snp:3e11e29283bbaf71786af24d2d77c12596b6e584;anchor=swh:1:rev:d86b262545547353c7050bbc2d476c2f4a297989">https://archive.softwareheritage.org/swh:1:dir:a5166815043fc36f4e804ba90d29164f34c7006b;origin=https://github.com/jacopobono/learning_cognitive_maps_code;visit=swh:1:snp:3e11e29283bbaf71786af24d2d77c12596b6e584;anchor=swh:1:rev:d86b262545547353c7050bbc2d476c2f4a297989</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>12176</fpage><lpage>12189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3761-07.2007</pub-id><pub-id pub-id-type="pmid">17989284</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Chung</surname><given-names>JE</given-names></name><name><surname>Sosa</surname><given-names>M</given-names></name><name><surname>Schor</surname><given-names>JS</given-names></name><name><surname>Karlsson</surname><given-names>MP</given-names></name><name><surname>Larkin</surname><given-names>MC</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Constant sub-second cycling between representations of possible futures in the hippocampus</article-title><source>Cell</source><volume>180</volume><fpage>552</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.01.014</pub-id><pub-id pub-id-type="pmid">32004462</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kempter</surname><given-names>R</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>van Hemmen</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hebbian learning and spiking neurons</article-title><source>Physical Review E</source><volume>59</volume><fpage>4498</fpage><lpage>4514</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.59.4498</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kraus</surname><given-names>BJ</given-names></name><name><surname>Robinson</surname><given-names>RJ</given-names></name><name><surname>White</surname><given-names>JA</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal “time cells”: time versus path integration</article-title><source>Neuron</source><volume>78</volume><fpage>1090</fpage><lpage>1101</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.015</pub-id><pub-id pub-id-type="pmid">23707613</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubie</surname><given-names>JL</given-names></name><name><surname>Fenton</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Linear look-ahead in conjunctive cells: an entorhinal mechanism for vector-based navigation</article-title><source>Frontiers in Neural Circuits</source><volume>6</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2012.00020</pub-id><pub-id pub-id-type="pmid">22557948</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Economides</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast sequences of non-spatial state representations in humans</article-title><source>Neuron</source><volume>91</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.028</pub-id><pub-id pub-id-type="pmid">27321922</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laibson</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Golden eggs and hyperbolic discounting</article-title><source>The Quarterly Journal of Economics</source><volume>112</volume><fpage>443</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1162/003355397555253</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Vision</source><publisher-name>The MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9780262514620.001.0001</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname><given-names>M</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Lateral habenula as a source of negative reward signals in dopamine neurons</article-title><source>Nature</source><volume>447</volume><fpage>1111</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1038/nature05860</pub-id><pub-id pub-id-type="pmid">17522629</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>A Rational Model of Prioritized Experience Replay</source><publisher-loc>Ann Arbor, USA</publisher-loc><publisher-name>The University of Michigan</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Morris</surname><given-names>RGM</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hippocampal synaptic enhancement and information storage within a distributed memory system</article-title><source>Trends in Neurosciences</source><volume>10</volume><fpage>408</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(87)90011-7</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>MR</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Experience-dependent asymmetric shape of hippocampal receptive fields</article-title><source>Neuron</source><volume>25</volume><fpage>707</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81072-7</pub-id><pub-id pub-id-type="pmid">10774737</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Micheau</surname><given-names>J</given-names></name><name><surname>Marighetto</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Acetylcholine and memory: a long, complex and chaotic but still living relationship</article-title><source>Behavioural Brain Research</source><volume>221</volume><fpage>424</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2010.11.052</pub-id><pub-id pub-id-type="pmid">21130809</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The successor representation in human reinforcement learning</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0180-8</pub-id><pub-id pub-id-type="pmid">31024137</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning structures: predictive representations, replay, and generalization</article-title><source>Current Opinion in Behavioral Sciences</source><volume>32</volume><fpage>155</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.02.017</pub-id><pub-id pub-id-type="pmid">35419465</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>RGM</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Spatial localization does not require the presence of local cues</article-title><source>Learning and Motivation</source><volume>12</volume><fpage>239</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1016/0023-9690(81)90020-5</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>RG</given-names></name><name><surname>Garrud</surname><given-names>P</given-names></name><name><surname>Rawlins</surname><given-names>JN</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Place navigation impaired in rats with hippocampal lesions</article-title><source>Nature</source><volume>297</volume><fpage>681</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/297681a0</pub-id><pub-id pub-id-type="pmid">7088155</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>The Hippocampus as a Cognitive Map</source><publisher-loc>Oxford, United Kingdom</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.013</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal place cells construct reward related sequences through unexplored space</article-title><source>eLife</source><volume>4</volume><elocation-id>e06063</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06063</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olton</surname><given-names>DS</given-names></name><name><surname>Papas</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Spatial memory and hippocampal function</article-title><source>Neuropsychologia</source><volume>17</volume><fpage>669</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(79)90042-3</pub-id><pub-id pub-id-type="pmid">522981</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>KC</given-names></name><name><surname>Alarcon</surname><given-names>JM</given-names></name><name><surname>Ferbinteanu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Relative contributions of CA3 and medial entorhinal cortex to memory in rats</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>8</volume><elocation-id>292</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00292</pub-id><pub-id pub-id-type="pmid">25221487</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palacios-Filardo</surname><given-names>J</given-names></name><name><surname>Udakis</surname><given-names>M</given-names></name><name><surname>Brown</surname><given-names>GA</given-names></name><name><surname>Tehan</surname><given-names>BG</given-names></name><name><surname>Congreve</surname><given-names>MS</given-names></name><name><surname>Nathan</surname><given-names>PJ</given-names></name><name><surname>Brown</surname><given-names>AJH</given-names></name><name><surname>Mellor</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Acetylcholine prioritises direct synaptic inputs from entorhinal cortex to CA1 by differential modulation of feedforward inhibitory circuits</article-title><source>Nature Communications</source><volume>12</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1038/s41467-021-25280-5</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nature12112</pub-id><pub-id pub-id-type="pmid">23594744</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The content of hippocampal “ replay. ”</article-title><source>Hippocampus</source><volume>30</volume><fpage>6</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1002/hipo.22824</pub-id><pub-id pub-id-type="pmid">29266510</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Arousal systems and attentional processes</article-title><source>Biological Psychology</source><volume>45</volume><fpage>57</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/s0301-0511(96)05222-2</pub-id><pub-id pub-id-type="pmid">9083644</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roscow</surname><given-names>EL</given-names></name><name><surname>Chua</surname><given-names>R</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Jones</surname><given-names>MW</given-names></name><name><surname>Lepora</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning offline: memory replay in biological and artificial reinforcement learning</article-title><source>Trends in Neurosciences</source><volume>44</volume><fpage>808</fpage><lpage>821</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2021.07.007</pub-id><pub-id pub-id-type="pmid">34481635</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005768</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005768</pub-id><pub-id pub-id-type="pmid">28945743</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shouval</surname><given-names>HZ</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name><name><surname>Cooper</surname><given-names>LN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A unified model of NMDA receptor-dependent bidirectional synaptic plasticity</article-title><source>PNAS</source><volume>99</volume><fpage>10831</fpage><lpage>10836</lpage><pub-id pub-id-type="doi">10.1073/pnas.152343099</pub-id><pub-id pub-id-type="pmid">12136127</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Design Principles of the Hippocampal Cognitive Map</article-title><conf-name>NeurIPS Proceedings</conf-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hippocampus as a predictive MAP</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staresina</surname><given-names>BP</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Awake reactivation predicts memory in humans</article-title><source>PNAS</source><volume>110</volume><fpage>21159</fpage><lpage>21164</lpage><pub-id pub-id-type="doi">10.1073/pnas.1311989110</pub-id><pub-id pub-id-type="pmid">24324174</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Reinforcement learning: an introduction</article-title><source>IEEE Transactions on Neural Networks</source><volume>9</volume><fpage>1054</fpage><lpage>1056</lpage><pub-id pub-id-type="doi">10.1109/TNN.1998.712192</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teles-Grilo Ruivo</surname><given-names>LM</given-names></name><name><surname>Mellor</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cholinergic modulation of hippocampal network function</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>5</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.3389/fnsyn.2013.00002</pub-id><pub-id pub-id-type="pmid">23908628</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Rossum</surname><given-names>MCW</given-names></name><name><surname>Shippi</surname><given-names>M</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Soft-bound synaptic plasticity increases storage capacity</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002836</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002836</pub-id><pub-id pub-id-type="pmid">23284281</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waddington</surname><given-names>A</given-names></name><name><surname>Appleby</surname><given-names>PA</given-names></name><name><surname>De Kamps</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Triphasic spike-timing-dependent plasticity organizes networks to produce robust sequences of neural activity</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00088</pub-id><pub-id pub-id-type="pmid">23162457</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>ER</given-names></name><name><surname>Dudchenko</surname><given-names>PA</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The global record of memory in hippocampal neuronal activity</article-title><source>Nature</source><volume>397</volume><fpage>613</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1038/17605</pub-id><pub-id pub-id-type="pmid">10050854</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>ER</given-names></name><name><surname>Dudchenko</surname><given-names>PA</given-names></name><name><surname>Robitsek</surname><given-names>RJ</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Hippocampal neurons encode information about different types of memory episodes occurring in the same location</article-title><source>Neuron</source><volume>27</volume><fpage>623</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)00071-4</pub-id><pub-id pub-id-type="pmid">11055443</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>CT</given-names></name><name><surname>Haggerty</surname><given-names>D</given-names></name><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Ji</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hippocampal awake replay in fear memory retrieval</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>571</fpage><lpage>580</lpage><pub-id pub-id-type="doi">10.1038/nn.4507</pub-id><pub-id pub-id-type="pmid">28218916</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Analytical derivations for the total weight change in the behavioural model</title><sec sec-type="appendix" id="s8-1"><title>Presynaptic rate during state <inline-formula><mml:math id="inf267"><mml:mi>j</mml:mi></mml:math></inline-formula></title><p>Whenever a state <inline-formula><mml:math id="inf268"><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:math></inline-formula> is entered, the presynaptic neurons encoding state <inline-formula><mml:math id="inf269"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> start firing at a constant rate <inline-formula><mml:math id="inf270"><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for a time <inline-formula><mml:math id="inf271"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula>, following a Poisson process with parameter <inline-formula><mml:math id="inf272"><mml:mrow><mml:msubsup><mml:mi mathsize="70%">ρ</mml:mi><mml:mi mathsize="70%">j</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ42"><label>(35)</label><mml:math id="m42"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if </mml:mtext><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The other presynaptic neurons are silent.</p></sec><sec sec-type="appendix" id="s8-2"><title>Postsynaptic rate during state <inline-formula><mml:math id="inf273"><mml:mi>j</mml:mi></mml:math></inline-formula></title><p>The average postsynaptic rate can be calculated as follows. The probability of a presynaptic spike between <inline-formula><mml:math id="inf274"><mml:mi mathsize="70%">t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf275"><mml:mrow><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is equal to <inline-formula><mml:math id="inf276"><mml:mrow><mml:msub><mml:mi mathsize="70%">ρ</mml:mi><mml:mi mathsize="70%">j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:math></inline-formula>. The size of the presynaptic population encoding state <inline-formula><mml:math id="inf277"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> is equal to <inline-formula><mml:math id="inf278"><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mtext mathsize="70%">pop</mml:mtext></mml:msub></mml:math></inline-formula> and each excitatory postsynaptic potential (EPSP) is modeled by an immediate jump with amplitude <inline-formula><mml:math id="inf279"><mml:mrow><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, followed by exponential decay of EPSP with time constant <inline-formula><mml:math id="inf280"><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>Following <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> in the main paper, reproduced below,<disp-formula id="equ43"><mml:math id="m43"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>we find that the average postsynaptic potential at time <inline-formula><mml:math id="inf281"><mml:mi mathsize="70%">t</mml:mi></mml:math></inline-formula> is given by (assuming t=0 when entering the state <inline-formula><mml:math id="inf282"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>):<disp-formula id="equ44"><label>(36)</label><mml:math id="m44"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>We assume that <inline-formula><mml:math id="inf283"><mml:mrow><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> changes slowly compared to the timescale <inline-formula><mml:math id="inf284"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula> allowing us to consider the weight constant during that time. We can then approximate the average postsynaptic rate as:<disp-formula id="equ45"><label>(37)</label><mml:math id="m45"><mml:mrow><mml:mtable columnalign="right right" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ω</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>If <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, both the first and the second term will contribute to the postsynaptic rate in the time between <inline-formula><mml:math id="inf286"><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">⋆</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf287"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s8-3"><title>LTP trace during state <inline-formula><mml:math id="inf288"><mml:mi>j</mml:mi></mml:math></inline-formula></title><p>Given <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> in the main paper, reproduced below,<disp-formula id="equ46"><mml:math id="m46"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>and combined with <xref ref-type="disp-formula" rid="equ42">Equation 35</xref>, we can calculate the evolution of the LTP trace for neuron <inline-formula><mml:math id="inf289"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> during state <inline-formula><mml:math id="inf290"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>:<disp-formula id="equ47"><label>(38)</label><mml:math id="m47"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mi>θ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mi>θ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, the presynaptic neuron <inline-formula><mml:math id="inf292"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> is active and therefore the trace builds up with the presynaptic spikes, for <inline-formula><mml:math id="inf293"><mml:mrow><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">≥</mml:mo><mml:mi mathsize="70%">θ</mml:mi></mml:mrow></mml:math></inline-formula>, the trace decays exponentially with time constant <inline-formula><mml:math id="inf294"><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s8-4"><title>Total amount of LTP during state <inline-formula><mml:math id="inf295"><mml:mi>j</mml:mi></mml:math></inline-formula></title><p>Following (<xref ref-type="bibr" rid="bib31">Kempter et al., 1999</xref>), first we calculate the amount of LTP without taking into account spike-to-spike correlation:</p><p>The probability for a postsynaptic spike between <inline-formula><mml:math id="inf296"><mml:mi mathsize="70%">t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="inf298"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathsize="70%">ρ</mml:mi><mml:mo mathsize="70%" stretchy="false">¯</mml:mo></mml:mover><mml:mi mathsize="70%">i</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo maxsize="70%" minsize="70%" rspace="7.5pt">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:math></inline-formula>. The amount of LTP due to a single spike at time <inline-formula><mml:math id="inf299"><mml:mi mathsize="70%">t</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf300"><mml:mrow><mml:mpadded width="+5pt"><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">r</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow><mml:mi mathsize="70%">j</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mi mathsize="70%">t</mml:mi><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Hence, combining <xref ref-type="disp-formula" rid="equ45 equ47">Equations 37 and 38</xref>, the total amount of LTP during a state (i.e. between time 0 and <inline-formula><mml:math id="inf301"><mml:mi mathsize="70%">T</mml:mi></mml:math></inline-formula>) becomes:<disp-formula id="equ48"><label>(39)</label><mml:math id="m48"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>non-causal</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ49"><label>(40)</label><mml:math id="m49"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ω</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Following (<xref ref-type="bibr" rid="bib31">Kempter et al., 1999</xref>), the amount of LTP due to the causal part (each presynaptic spike temporarily increase the probability of a postsynaptic spike) is given by:<disp-formula id="equ50"><label>(41)</label><mml:math id="m50"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">P</mml:mi><mml:mtext mathsize="70%">causal</mml:mtext></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">θ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Combining equations for the non-causal 40 and causal 41 parts, we get the <italic>total</italic> amount of LTP during a state (assuming <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;&lt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ51"><label>(42)</label><mml:math id="m51"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ω</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mi>θ</mml:mi><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s8-5"><title>Total amount of LTD during state <inline-formula><mml:math id="inf303"><mml:mi>j</mml:mi></mml:math></inline-formula></title><p>There is a weight-dependent depression for each presynaptic spike, hence the amount of LTD during a state is given by:<disp-formula id="equ52"><label>(43)</label><mml:math id="m52"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">θ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s8-6"><title>Total plasticity during state <inline-formula><mml:math id="inf304"><mml:mi>j</mml:mi></mml:math></inline-formula></title><p>Combining <xref ref-type="disp-formula" rid="equ51 equ52">Equations 42 and 43</xref>, we can calculate the total amount of plasticity during the time the agent spends in the current state <inline-formula><mml:math id="inf305"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>:<disp-formula id="equ53"><label>(44)</label><mml:math id="m53"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msup><mml:mi mathsize="70%" mathvariant="normal">Δ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:mi mathsize="70%">A</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">B</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>with<disp-formula id="equ54"><label>(45)</label><mml:math id="m54"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>A</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mi>θ</mml:mi><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mi>θ</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ55"><label>(46)</label><mml:math id="m55"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">B</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mpadded width="+5pt"><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:msup><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">ω</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo maxsize="70%" minsize="70%">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s8-7"><title>Plasticity due to states transitioning</title><p>Once the agent leaves state <inline-formula><mml:math id="inf306"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula>, the decaying LTP trace can still cause potentiation due to the activity in the following states, <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf308"><mml:mrow><mml:mi mathsize="70%">n</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mn mathsize="70%">2</mml:mn><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mi mathsize="70%" mathvariant="normal">…</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> . Given that the agent spends a time <inline-formula><mml:math id="inf309"><mml:mi mathsize="70%">T</mml:mi></mml:math></inline-formula> in each state, we find that the agent visits state <inline-formula><mml:math id="inf310"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula> during time <inline-formula><mml:math id="inf311"><mml:mrow><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">∈</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">[</mml:mo><mml:mrow><mml:mi mathsize="70%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="70%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We will now calculate the contribution to plasticity due to these state transitions.</p></sec><sec sec-type="appendix" id="s8-8"><title>Postsynaptic rate during the new state <inline-formula><mml:math id="inf312"><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula></title><p>During state <inline-formula><mml:math id="inf313"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula>, the activity of the postsynaptic neurons is driven by the presynaptic neurons coding for <inline-formula><mml:math id="inf314"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula>, and the bias current. We can thus generalize <xref ref-type="disp-formula" rid="equ45">Equation 37</xref> and find that the average postsynaptic rate <inline-formula><mml:math id="inf315"><mml:msubsup><mml:mover accent="true"><mml:mi mathsize="70%">ρ</mml:mi><mml:mo mathsize="70%" stretchy="false">¯</mml:mo></mml:mover><mml:mi mathsize="70%">i</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> during state <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula> is:<disp-formula id="equ56"><label>(47)</label><mml:math id="m56"><mml:mrow><mml:mtable columnalign="right right" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> if </mml:mtext><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ω</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext> otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s8-9"><title>LTP trace from state <inline-formula><mml:math id="inf317"><mml:mi>j</mml:mi></mml:math></inline-formula>, during the new state <inline-formula><mml:math id="inf318"><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula></title><p>Following <xref ref-type="disp-formula" rid="equ47">Equation 38</xref>, we find that the amplitude of the LTP trace from state <inline-formula><mml:math id="inf319"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> during state <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula> is:<disp-formula id="equ57"><label>(48)</label><mml:math id="m57"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s8-10"><title>LTP due to state transitioning</title><p>We can then calculate the amount of LTP between the presynaptic neuron <inline-formula><mml:math id="inf322"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> and the postsynaptic neuron <inline-formula><mml:math id="inf323"><mml:mi mathsize="70%">i</mml:mi></mml:math></inline-formula>, when the agent is in state <inline-formula><mml:math id="inf324"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula>. We refer to <xref ref-type="disp-formula" rid="equ48">Equation 39</xref> and find:<disp-formula id="equ58"><mml:math id="m58"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>switch</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ω</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The amount of plasticity in state <inline-formula><mml:math id="inf325"><mml:mrow><mml:mi mathsize="70%">j</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:math></inline-formula> when starting from state <inline-formula><mml:math id="inf326"><mml:mi mathsize="70%">j</mml:mi></mml:math></inline-formula> is thus:<disp-formula id="equ59"><label>(49)</label><mml:math id="m59"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msup><mml:mi mathsize="70%" mathvariant="normal">Δ</mml:mi><mml:mi mathsize="70%">n</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="70%">C</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">T</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mi mathsize="70%">n</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mrow><mml:mi mathsize="70%">B</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">T</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mi mathsize="70%">n</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">δ</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where<disp-formula id="equ60"><label>(50)</label><mml:math id="m60"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">C</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">N</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">p</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mi mathsize="70%">m</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:msup><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ61"><label>(51)</label><mml:math id="m61"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathsize="70%">B</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:msub><mml:mi mathsize="70%">η</mml:mi><mml:mrow><mml:mi mathsize="70%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="70%">A</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">p</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">e</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow><mml:mn mathsize="70%">2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mfrac><mml:mi mathsize="70%">θ</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:msup><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mn mathsize="70%">1</mml:mn></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:msup><mml:mi mathsize="70%">t</mml:mi><mml:mo mathsize="70%" stretchy="false">*</mml:mo></mml:msup><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">ω</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">ρ</mml:mi><mml:mrow><mml:mi mathsize="70%">b</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>It is worth noting that the parameter B derived here is the same as <xref ref-type="disp-formula" rid="equ55">Equation 46</xref>.</p></sec><sec sec-type="appendix" id="s8-11"><title>Summary: total STDP update</title><p>If we combine together <xref ref-type="disp-formula" rid="equ53 equ59">Equations 44 and 49</xref>, we have that the total weight change for the synapse <inline-formula><mml:math id="inf327"><mml:msub><mml:mi mathsize="70%">w</mml:mi><mml:mrow><mml:mi mathsize="70%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is given by:<disp-formula id="equ62"><label>(52)</label><mml:math id="m62"><mml:mrow><mml:mtable columnalign="left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mstyle mathsize="1.44em"><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf328"><mml:mi mathsize="70%">N</mml:mi></mml:math></inline-formula> is the number of states until the end of the trajectory and A, B, C are as defined in <xref ref-type="disp-formula" rid="equ54 equ55 equ60">Equations 45, 46 and 50</xref> respectively.</p></sec></sec><sec sec-type="appendix" id="s9"><title>Analytical calculations for hyperbolic discounting</title><p>From <xref ref-type="disp-formula" rid="equ23">Equation 22</xref> in the main paper, we have that, in the behavioural model <inline-formula><mml:math id="inf329"><mml:mrow><mml:mi mathsize="70%">γ</mml:mi><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="70%" minsize="70%">(</mml:mo><mml:mrow><mml:mn mathsize="70%">1</mml:mn><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi mathsize="70%">C</mml:mi><mml:mi mathsize="70%">A</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo maxsize="70%" minsize="70%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">T</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, we will derive an approximation to this value.</p><p>If we assume that <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mo>&gt;&gt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can approximate <inline-formula><mml:math id="inf331"><mml:mi mathsize="70%">A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf332"><mml:mi mathsize="70%">C</mml:mi></mml:math></inline-formula> as:<disp-formula id="equ63"><label>(53)</label><mml:math id="m63"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mi>θ</mml:mi><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mi>θ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ64"><label>(54)</label><mml:math id="m64"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mtext>with</mml:mtext></mml:mtd><mml:mtd><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>pop</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ65"><label>(55)</label><mml:math id="m65"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>If we define <inline-formula><mml:math id="inf333"><mml:mi mathsize="70%">ψ</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="inf334"><mml:mrow><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">+</mml:mo><mml:mi mathsize="70%">ψ</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow></mml:math></inline-formula>, we can rewrite and approximate the discount parameter as:<disp-formula id="equ66"><label>(56)</label><mml:math id="m66"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>γ</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>ψ</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ψ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ψ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mfrac><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ψ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ66">Equation 56</xref>, we can see that the discount <inline-formula><mml:math id="inf335"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> follows a hyperbolic function if we increase the duration of the presynaptic current <inline-formula><mml:math id="inf336"><mml:mi mathsize="70%">θ</mml:mi></mml:math></inline-formula>. If, instead, we vary <inline-formula><mml:math id="inf337"><mml:mi mathsize="70%">ψ</mml:mi></mml:math></inline-formula>, the discount becomes exponential (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a and b</xref>).</p><p>Notice that this analysis extends to the replay model. Following what was done after <xref ref-type="disp-formula" rid="equ27">Equation 26</xref>, we can connect the behavioural model with the replay model by making <inline-formula><mml:math id="inf338"><mml:mrow><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">→</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:math></inline-formula>, which implies <inline-formula><mml:math id="inf339"><mml:mrow><mml:mi mathsize="70%">ψ</mml:mi><mml:mo mathsize="70%" stretchy="false">→</mml:mo><mml:mi mathsize="70%">T</mml:mi></mml:mrow></mml:math></inline-formula>. From <xref ref-type="disp-formula" rid="equ66">Equation 56</xref> we find that:<disp-formula id="equ67"><mml:math id="m67"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo mathsize="70%" movablelimits="false" stretchy="false">lim</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="70%">θ</mml:mi><mml:mo mathsize="70%" stretchy="false">,</mml:mo><mml:msub><mml:mi mathsize="70%">ϵ</mml:mi><mml:mn mathsize="70%">0</mml:mn></mml:msub></mml:mrow><mml:mo mathsize="70%" stretchy="false">→</mml:mo><mml:mn mathsize="70%">0</mml:mn></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mi mathsize="70%">γ</mml:mi></mml:mrow><mml:mo mathsize="70%" stretchy="false">=</mml:mo><mml:msup><mml:mi mathsize="70%">e</mml:mi><mml:mrow><mml:mo mathsize="70%" stretchy="false">-</mml:mo><mml:mfrac><mml:mi mathsize="70%">T</mml:mi><mml:msub><mml:mi mathsize="70%">τ</mml:mi><mml:mrow><mml:mi mathsize="70%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="70%">P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo mathsize="70%" stretchy="false">,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which is exactly the definition of <inline-formula><mml:math id="inf340"><mml:mi mathsize="70%">γ</mml:mi></mml:math></inline-formula> in the replay model (<xref ref-type="disp-formula" rid="equ32">Equations 27</xref> in Materials and methods). For replays, the discount is therefore strictly exponential.</p><p>Furthermore, using the same calculations and <xref ref-type="disp-formula" rid="equ22 equ1">Equations 21 and 19</xref> in the main paper, we can find approximated values for the other parameters too (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c and d</xref>).<disp-formula id="equ68"><mml:math id="m68"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>γ</mml:mi></mml:mfrac><mml:mo>≈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mfrac><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>ψ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mfrac><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80671.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Stanford School of Medicine</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.08.16.456545" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.08.16.456545"/></front-stub><body><p>This is an important article that leverages a spiking network model of the hippocampal circuit to show how spike-time-dependent plasticity can implement predictive reinforcement learning and form a predictive map of the environment. The authors provide a convincing and solid framework for understanding the prediction based learning rules that may be employed by the hippocampus to optimize an animal's behavior. This paper will be of interest to theoretical and experimental neuroscientists working on learning and memory as it provides new ways to connect computational models to experimental data that has yet to be fully explored from a reinforcement learning perspective.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80671.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name><role>Reviewing Editor</role><aff><institution>Stanford School of Medicine</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Hasselmo</surname><given-names>Michael E</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Boston University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.08.16.456545">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.08.16.456545v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Learning predictive cognitive maps with spiking neurons during behaviour and replays&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Laura Colgin as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Michael E. Hasselmo (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Based on the comments from all reviewers, I'd recommend focusing your revisions on the primary topic of improving the link between their model and experimental data. Specifically, this would include: Consideration (or modeling) of how the model would extend to 2D, discussion on how the neural activity would be used to perform computations, the limitations of RL as it relates to interpreting experimental data, and to more appropriately frame the work in the context of experimental studies (all reviewers had detailed suggestions for how to do this with text changes). I've provided highlights from the three reviewers below that apply to these concerns but note that reviewer 2 also provided a number of specific suggestions related to text/reference changes. All reviewer comments are also included at the bottom of this message.</p><p>Reviewer 1:</p><p>– The successor representation is learned at the level of synaptic weights between the two layers. It is not clear how it is read out into neural activity and exploited to perform actual computations, as both layers are assumed to be strongly driven by external inputs. This is a major limitation of this work.</p><p>– One of the results is that STDP at the timescale of milliseconds can lead to learning over behavioral timescales of seconds. This result seems related to Drew and Abbott PNAS 2006. In that work, the mapping between learning on micro and macro timescales in fact relied on precise tuning of plasticity parameters. It is not clear to which extent similar limitations apply here, and what is the precise relation with Drew and Abbott.</p><p>– Most of the results are presented at a formal, descriptive level relating plasticity to reinforcement learning algorithms. The provided examples are quite limited and focus on a simplified setting, a linear track. It would be important to see that the results extend to two-dimensional environments, and to show how the successor representation is actually used (see first comment).</p><p>– The main text does not explain clearly how replays are implemented.</p><p>Reviewer 2:</p><p>I think the authors of this article need to be clear about the shortcomings of RL. They should devote some space in the discussion to noting neuroscience data that has not been addressed yet. They could note that most components of their RL framework are still implemented as algorithms rather than neural models. They could note that most RL models usually don't have neurons of any kind in them and that their own model only uses neurons to represent state and successor representations, without representing actions or action selection processes. They could note that the agents in most RL models commonly learn about barriers by needing to bang into the barrier in every location, rather than learning to look at it from a distance. The ultimate goal of research such as this should be to link cellular level neurophysiological data to experimental data on behavior. To the extent possible, they should focus on how they link neurophysiological data at the cellular level to spatial behavior and the unit responses of place cells in behaving animals, rather than basing the validity of their work on the assumption that the successor representation is correct.</p><p>Reviewer 3:</p><p>1. Could the authors elaborate more on the connection between the biological replays that are observed in a different context in the brain and the replays implemented in their model? Within the modeling context, when are replays induced upon learning in a novel environment, and what is the influence of replays when/if they are generated upon revisiting the previously seen/navigated environment?</p><p>2. The model is composed of CA1 and CA3, what are the roles of the other hippocampal subregions in learning predictive maps? From the reported results, it looks like it may be possible that prediction-based learning can be successfully achieved simply via the CA1-CA3 circuit. Are there studies (e.g., lesioned) that show this causal relationship to behavior? Along this line, what are the potential limitations of the proposed framework in understanding the circuit computation adopted by the hippocampus?</p><p>3. Do the authors believe that the plasticity rules/computational principles observed within the 2-layer model are specific to the CA1-CA3 circuit? Can these rules be potentially employed elsewhere within the medial temporal lobe or sensory areas? What are the model parameters used that could suggest that the observed results are specific to hippocampus-based predictive learning?</p><p>4. The analytical illustration linking the proposed model with reinforcement learning is well executed. However, in practice, the actual implementation of reinforcement learning within the model is unclear. Given the sample task provided where animals are navigating a simple environment, how can one make use of value-based learning to enhance behavior? Explicit discussion on the extent to which reinforcement learning is related to the actual computation potentially needed to navigate sensory environments (both learned and novel) would be really helpful in understanding the link between the model to reinforcement learning.</p><p>5. Subplots both within and across figures seem to be of very different text formatting and sizing (such as panel F in Figure 4 and Figure 5). Please reformat them accordingly.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Important: Note that the page numbers refer to the page in the PDF, which is their own page number-1 (due to <italic>eLife</italic> adding a header page).</p><p>Page 3 – &quot;smoothly… and anything in between&quot; – this is overstated and should be removed.</p><p>Page 3 – &quot;don't need to discretize time…&quot;. Here and elsewhere there should be citations to the work of Doya, NeurIPS 1995, Neural Comp 2000 on the modeling of continuous time in RL.</p><p>Page 3 – &quot;using replays&quot; – It is very narrowminded to assume that all replay do is set up successor representations. They could also be involved in model-based planning of behavior as suggested in the work of Johnson and Redish, 2012; Pfeiffer and Foster, 2018; Kay et al. 2021 and modeled in Hasselmo and Eichenbaum, 2005; Erdem and Hasselmo, 2012 and Fenton and Kubie, 2012.</p><p>Page 3 – They assume that STDP can occur during replay, but evidence for STDP during replay is unclear. McNaughton's lab showed that LTP is less likely to be induced during the modulatory states during which sharp-wave ripple replay events occur. They should look for citations that have actually shown LTP induction during the replay state.</p><p>Page 4 – Marr's three levels – They should remove this discussion about Marr's three levels as I think the implementation level is relatively sparse and the behavioral level is also relatively sparse.</p><p>Page 4 – &quot;The hippocampus has long been thought&quot; – It's astounding that the introduction only cites two experimental papers (O'Keefe and Dostrovsky, 1971; Mehta et al. 2000) and then the start of the Results section makes a statement like this and only cites Stachenfeld et al. 2017 as if it were an experimental paper. There are numerous original research papers that should be cited for the role of hippocampus in behavior. They should cite at least five or six so that the reader doesn't get the impression all o this work started with the holy paper by Stachenfeld et al. 2017. For example, they could again cite O'Keefe and Nadel, 1978 for the very comprehensive review of the literature up to that time, plus the seminal work of Morris et al. 1982 in Morris water maze and Olton, 1979 in 8-arm radial maze and perhaps some of the work by Aggleton and by Eichenbaum on spatial alternation.</p><p>Page 5 – The description of successor representations is very one dimensional. They should mention how it can be expanded to two dimensions.</p><p>Page 5 – &quot;Usually attributed to model based…&quot;. They cannot just talk about SR being model free. Since this section was supposed to be for neuroscientists, they need to clearly explain the distinction between model free and model-based RL, and describe why successor representations are not just model-based RL, but instead provide a look-up table of predictive state that does NOT involve model-based planning of behavior. The blog of Vitay gives a much better overview that compares model-free, model-based and successor representations:</p><p>https://julien-vitay.net/post/successor_representations/ – This needs more than just a citation – there should be a clear description of model-based and model-free RL in contrast to SR, and Vitay is an example of that.</p><p>Page 5 – Related to this issue – they need to repeatedly address the fact that Successor representations are just an hypothesis contrast with model-based behavior, and repeatedly throughout the paper discuss that model-based behavior could still be the correct accounting for all of the data that they address.</p><p>Page 5 – &quot;similar to (Mehta et al. 2000)&quot; – Learning in the CA3-CA1 network has been modeled like this in many previous models that should be cited here including McNaughton and Morris, 1987; Hasselmo and Schnell, 1994; Treves and Rolls, 1994; Mehta et al. 2000; Hasselmo, Bodelon and Wyble, 2002.</p><p>Page 6 – Figure 1d looks like the net outcome of the learning rule in this example is long-term depression. Is that intended? Given the time interval between pre and post, it looks like it ought to be potentiation in the example.</p><p>Page 7 – They should address the problem of previously existing weights in the CA3 to CA1 connections. For example, what if there are pre-existing weights that are strong enough to cause post-synaptic spiking in CA1 independent of any entorhinal input? How do they avoid the strengthening of such connections? (i.e. the problem of prior weights driving undesired learning on CA3-CA1 synapses is addressed in Hasselmo and Schnell, 1994; Hasselmo, Bodelon and Wyble, 2002, which should be cited).</p><p>Page 7 – &quot;Elegantly combines rate and temporal&quot; – This is overstated. The possible temporal codes in the hippocampus include many possible representations beyond just one step prediction. They need to specify that this combines one type of possible temporal code. I also recommend removing the term &quot;elegant&quot; from the paper. Let someone else call your work elegant.</p><p>Page 7 – &quot;replays for learning&quot; – as noted above in experiments LTP has not been shown to be induced during the time periods of replay – sharp-wave ripple replay events seem to be associated with lower cholinergic tone (Buzsaki et al. 1983; VandeCasteele et al. 2014) whereas LTP is stronger when Ach levels are higher (Patil et al. J. Neurophysiol. 1998). This is not an all-or-none difference, but it should be addressed.</p><p>Page 7 – &quot;equivalent to TD…&quot;. Should this say &quot;equivalent to TD(0)&quot;?</p><p>Page 8 – &quot;Bootstrapping means that a function is updated using current estimates of the same function…&quot;. This is a confusing and vague description of bootstrapping. They should try to give a clearer definition for neuroscientists (or reduce their reference to this).</p><p>Page 9 – Figure 2 – Do TD λ and TD zero really give equivalent weight outputs?</p><p>Page 8 – &quot;that are behaviorally far apart&quot; – I don't understand how this occurs.</p><p>page 10 – &quot;dependency of synaptic weights on each other as discussed above.&quot; This was not made sufficiently clear either here or above.</p><p>Page 10 – &quot;dependency of synaptic weights on each other&quot; This also suggests a problem of stability if the weights can start to drive their own learning and cause instability – how is this prevented?</p><p>Page 10 – &quot;average of the discounted state occupancies&quot; – this would be uniform without discounting but what is the biological mechanism for the discounting that is used here?</p><p>Page 10 – &quot;due to the bootstrapping&quot; – again this is unclear – can be improved by giving a better definition of bootstrapping and possibly by referring to specific equation numbers.</p><p>Page 11 – &quot;exponential dependence&quot; what is the neural mechanism for this?</p><p>Page 11 – &quot;Ainsley&quot; is not a real citation in the bibliography. Should fix and also provide a clearer definition (or equation) for hyperbolic.</p><p>Page 11 – &quot;elegantly combines two types of discounting&quot; – how is useful? Also, let other people call your work elegant.</p><p>Page 11 – how does discounting depend on both firing rate and STDP -- should provide some explanation or at least refer to where this is shown in the equations.</p><p>page 13 – &quot;Cheng and Frank&quot; – this is a good citation, but they could add more here on timing of replay events.</p><p>Page 15 – This whole section on the shock experiment starts with the assumption of a successor representation. As noted above, they need to explicitly discuss the important alternate hypothesis that the neural activity reflects model-based planning that guides the behavior in the task (and could perhaps better account for the peak of occupancy at the border of light and dark).</p><p>Page 16 – &quot;mental imagination&quot; – rather than using it for modifying SR, why couldn't mental imagination just be used for model-based behavior?</p><p>Page 17 – &quot;spiking&quot; – again, if they are going to refer to their model as a &quot;spiking&quot; model, they need to add some plots showing spiking activity.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I found the proposed modeling framework to be very exciting and of potential interest to not only computational neuroscientists but also to readers who are interested in neural mechanisms underlying learning in general. The manuscript is well-written and includes a detailed description and rationale of the model setups as well as the findings and their relevance to biological findings. That said, I have a few comments that I hope the authors could help address:</p><p>1. Could the authors elaborate more on the connection between the biological replays that are observed in a different context in the brain and the replays implemented in their model? Within the modeling context, when are replays induced upon learning in a novel environment, and what is the influence of replays when/if they are generated upon revisiting the previously seen/navigated environment?</p><p>2. The model is composed of CA1 and CA3, what are the roles of the other hippocampal subregions in learning predictive maps? From the reported results, it looks like it may be possible that prediction-based learning can be successfully achieved simply via the CA1-CA3 circuit. Are there studies (e.g., lesioned) that show this causal relationship to behavior? Along this line, what are the potential limitations of the proposed framework in understanding the circuit computation adopted by the hippocampus?</p><p>3. Do the authors believe that the plasticity rules/computational principles observed within the 2-layer model are specific to the CA1-CA3 circuit? Can these rules be potentially employed elsewhere within the medial temporal lobe or sensory areas? What are the model parameters used that could suggest that the observed results are specific to hippocampus-based predictive learning?</p><p>4. The analytical illustration linking the proposed model with reinforcement learning is well executed. However, in practice, the actual implementation of reinforcement learning within the model is unclear. Given the sample task provided where animals are navigating a simple environment, how can one make use of value-based learning to enhance behavior? Explicit discussion on the extent to which reinforcement learning is related to the actual computation potentially needed to navigate sensory environments (both learned and novel) would be really helpful in understanding the link between the model to reinforcement learning.</p><p>5. Subplots both within and across figures seem to be of very different text formatting and sizing (such as panel F in Figure 4 and Figure 5). Please reformat them accordingly.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Learning predictive cognitive maps with spiking neurons during behaviour and replays&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Laura Colgin (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Reviewer 1 makes two text suggestions that I believe would clarify the findings. There remains some lack of clarification around (1) how the second layer of the model mixes the successor representation with a representation of the current state itself and (2) justification for the difference in duration of the external inputs to the two layers. Reviewer 1 also suggests an additional figure, but I leave the decision to add (or not) this to the authors. Details regarding the requested clarifications are below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The revised article has only partly resolved my confusion.</p><p>My main issue was the following: in the proposed feed-forward model, the synaptic weights between the two layers learn the entries of the successor matrix. If external inputs were fed only to the first layer, the second layer would directly read out the successor representation (this is suggested in Figure 1 E-F, but not explicitly mentioned in the text as far as I can tell). Instead, in the model, both layers are driven by external inputs representing the current state. This is crucial for learning, but it implies that the activity of the second layer mixes the successor representation with a representation of the current state itself. Learning and readout, therefore, seem antagonistic. It would be worth explaining this fact in the main text.</p><p>In their reply, the authors clarify that the external inputs drive the activity of the second layer only for a limited time (20%). As far as I can tell, in the text, this is mentioned explicitly only in the legend of Figure 5-S2. That seems to imply that there is a large difference in the duration of the external inputs to the two layers. How can that be justified?</p><p>More importantly, it seems that varying the value of the delay should lead to a tradeoff between the accuracy of learning and the accuracy of the subsequent readout in the second layer. Is that the case? It would be useful to have a figure where the delay is varied.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80671.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer 1:</p><p>– The successor representation is learned at the level of synaptic weights between the two layers. It is not clear how it is read out into neural activity and exploited to perform actual computations, as both layers are assumed to be strongly driven by external inputs. This is a major limitation of this work.</p></disp-quote><p>We thank the reviewer for this important remark. Since we modelled our neurons to integrate the synaptic EPSPs and generated spikes using an inhomogeneous Poisson process based on the depolarization, the firing rate is proportional to the total synaptic weights. Therefore, the successor representation can be read out simply by a downstream neuron. Moreover, since the value of a state is defined by the inner product between the successor matrix and the reward vector, it is sufficient for the synaptic weights to the downstream neuron to learn the reward vector, and the downstream neuron will then encode the state value in its firing rate. We performed additional simulations and have added a supplementary figure (Figure 5—figure supplement S2) showing this setup.</p><p>We also want to note that the external inputs driving the activity affect the firing rate of the value neuron only during a limited amount of time (20% of the time in each state in our simulation of Figure 5—figure supplement S2). Moreover, this effect changes the estimate of the value quantitatively, but not qualitatively, i.e. the ranking of the states by value is not affected.</p><p>Practically, using the parameters in our simulation, one could either read out the correct estimate of the value during the first 80% of the time in a state, learn a correction to this perturbation in the weights to the value neuron, or simply use a policy that is based on the ranking of the values instead of the actual firing rate.</p><p>Besides the supplementary figure (Figure 5—figure supplement S2), we added the following paragraph in the Discussion section:</p><p>“Since we modelled our neurons to integrate the synaptic EPSPs and generate spikes using an inhomogeneous Poisson process based on the depolarization, the firing rate is proportional to the total synaptic weights. Therefore, the successor representation can be read out simply by a downstream neuron. Moreover, since the value of a state is defined by the inner product between the successor matrix and the reward vector, it is sufficient for the synaptic weights to the downstream neuron to learn the reward vector, and the downstream neuron will then encode the state value in its firing rate (see Figure 5—figure supplement S2). While the neuron model used is simple, it will be interesting future work to study analogous models with non-linear neurons.”</p><disp-quote content-type="editor-comment"><p>– One of the results is that STDP at the timescale of milliseconds can lead to learning over behavioral timescales of seconds. This result seems related to Drew and Abbott PNAS 2006. In that work, the mapping between learning on micro and macro timescales in fact relied on precise tuning of plasticity parameters. It is not clear to which extent similar limitations apply here, and what is the precise relation with Drew and Abbott.</p></disp-quote><p>We thank the reviewer for pointing us to the interesting work by Drew and Abbott. We added the following paragraph in the Discussion section:</p><p>“Learning on behavioural timescales using STDP was also investigated in Drew 2006. The main difference between Drew 2006 and our work, is that the former relies on overlapping neural activity between the pre- and post-synaptic neurons from the start, while in our case no such overlap is required. In other words, our setup allows us to learn connections between a presynaptic neuron and a postsynaptic neuron whose activities are separated by behavioural timescales initially. For this to be possible, there are two requirements: (1) the task needs to be repeated many times and (2) a chain of neurons are consecutively activated between the aforementioned presynaptic and postsynaptic neuron. Due to this chain of neurons, over time the activity of the postsynaptic neuron will start earlier, eventually overlapping with the presynaptic neuron.”</p><disp-quote content-type="editor-comment"><p>– Most of the results are presented at a formal, descriptive level relating plasticity to reinforcement learning algorithms. The provided examples are quite limited and focus on a simplified setting, a linear track. It would be important to see that the results extend to two-dimensional environments, and to show how the successor representation is actually used (see first comment).</p></disp-quote><p>We thank the reviewer for the feedback, and have now included two new supplementary figures. Figure 5—figure supplement S2 shows the usefulness of our setup when learning the state values, as discussed above. Figure 1—figure supplement S1shows a simulation in a 2D environment. In fact, due to the exact link with TD(λ), our setup is general for any type of task, in any dimension, where states are visited and which may not need to be a navigation task.</p><p>We have added the following sentences in the second to last paragraph of section 2.2:</p><p>Moreover, due to the equivalence with TD(λ), our setup is general for any type of task where discrete states are visited, in any dimension, and which may not need to be a navigation task (see e.g. Figure 2d for a 2D environment).</p><disp-quote content-type="editor-comment"><p>– The main text does not explain clearly how replays are implemented.</p></disp-quote><p>We thank the reviewer for pointing out this issue. We have now updated the methods section 4.3.6 to be more clear about the implementation of the replays. We have also updated the description of replay generation in the methods of Figure 2 as follows:</p><p>“More details on the place cell activation during replays in our model can be found in section 4.3.6. Using exactly one single spike per neuron with the above parameters would allow us to follow the TD(1) learning trajectories without any noise. For more biological realism, we choose p1=0.15 in equation 28, in order to achieve an equal amount of noise due to the random spiking as in the case of behavioural activity (see Supplementary Figure S2).”</p><disp-quote content-type="editor-comment"><p>Reviewer 2:</p><p>I think the authors of this article need to be clear about the shortcomings of RL. They should devote some space in the discussion to noting neuroscience data that has not been addressed yet. They could note that most components of their RL framework are still implemented as algorithms rather than neural models. They could note that most RL models usually don't have neurons of any kind in them and that their own model only uses neurons to represent state and successor representations, without representing actions or action selection processes. They could note that the agents in most RL models commonly learn about barriers by needing to bang into the barrier in every location, rather than learning to look at it from a distance. The ultimate goal of research such as this should be to link cellular level neurophysiological data to experimental data on behavior. To the extent possible, they should focus on how they link neurophysiological data at the cellular level to spatial behavior and the unit responses of place cells in behaving animals, rather than basing the validity of their work on the assumption that the successor representation is correct.</p></disp-quote><p>We thank the reviewer for the important feedback. We have addressed the reviewer 2's concerns in the &quot;Public Evaluation&quot; section, which includes their detailed comments. In short, here, we made substantial changes to refer more thoroughly to the experimental literature, discuss the limitations of RL in general and the limitations of our proposed framework, discuss the link between neuronal data and behaviour, and finally, we made sure not to overstate the validity of the successor representation.</p><disp-quote content-type="editor-comment"><p>Reviewer 3:</p><p>1. Could the authors elaborate more on the connection between the biological replays that are observed in a different context in the brain and the replays implemented in their model? Within the modeling context, when are replays induced upon learning in a novel environment, and what is the influence of replays when/if they are generated upon revisiting the previously seen/navigated environment?</p></disp-quote><p>We thank the reviewer for the interesting questions. Within our modelling context, we speculate that replays may contribute to the learning of the SR. To understand why this may be beneficial, we show that in our framework, learning is faster in a novel environment when the proportion of replays is larger. Replays, because they rely on the MC algorithm, are great for learning quickly as they are fast to override obsolete weights. Specifically for Figure 4, we implemented a probability for replays that decays exponentially with the number of epochs in a novel environment, but other schemes that introduce more replays in novel environments than in familiar ones should lead to similar conclusions. We also show that, when replays are generated in familiar environments, they still contribute to learning the same SR but introduce more variance. We also argue that replays can be used to imagine novel trajectories (similar to ideas in model-based planning) and thus update the SR without actually walking the trajectory (Figure 5). In summary, we believe that replays can functionally serve a variety of purposes, and our framework merely proposes additional beneficial properties without claiming to explain all observed replays. For example, our framework does not encompass reverse replays. We have updated the ‘Replays’ paragraph of the Discussion section to reflect this.</p><p>“We have also proposed a role for replays in learning the SR, in line with experimental findings and RL theories Russek 2017 Momennejad 2017. In general, replays are thought to serve different functions, spanning from consolidation to planning Roscow 2021. Here, we have shown that when the replayed trajectories are similar to the ones observed during behaviour, they play the role of speeding up and consolidating learning by regulating the biasvariance trade-off, which is especially useful in novel environments. On the other hand, if the replayed trajectories differ from the ones experienced during wakefulness, replays can play a role in reshaping the representation of space, which would suggest their involvement in planning. Experimentally it has been observed that replays often start and end from relevant locations in the environment, like reward sites, decision points, obstacles or the current position of the animal FreyjaOlafsdottir 2015, Pfeiffer 2013, Jackson 2006,Mattar 2017. Since these are salient locations, it is in line with our proposition that replays can be used to maintain a convenient representation of the environment. It is worth noticing that replays can serve a variety of functions, and our framework merely proposes additional beneficial properties without claiming to explain all observed replays. For example, next to forward replays, also reverse replays are ubiquitous in hippocampus Pfeiffer 2020. The reverse replays are not</p><p>included in our framework, and it is not clear yet whether they play different roles, with some evidence suggesting that reverse replays are more closely tied to the reward encoding Ambrose 2016.”</p><disp-quote content-type="editor-comment"><p>2. The model is composed of CA1 and CA3, what are the roles of the other hippocampal subregions in learning predictive maps? From the reported results, it looks like it may be possible that prediction-based learning can be successfully achieved simply via the CA1-CA3 circuit. Are there studies (e.g., lesioned) that show this causal relationship to behavior? Along this line, what are the potential limitations of the proposed framework in understanding the circuit computation adopted by the hippocampus?</p></disp-quote><p>We thank the reviewer for the feedback. We have added the following paragraph in the discussion to address this point:</p><p>“There are three different neural activities in our proposed framework: the presynaptic layer (CA3), the postsynaptic layer (CA1), and the external inputs. These external inputs could for example be location-dependent currents from the entorhinal cortex, with timings guided by the theta oscillations. The dependence of CA1 place fields on CA3 and entorhinal input is in line with lesion studies (see e.g. Brun2008, Hales2014, Oreilly2014). It would be interesting for future studies to further dissect the role various areas play in learning cognitive maps.”</p><disp-quote content-type="editor-comment"><p>3. Do the authors believe that the plasticity rules/computational principles observed within the 2-layer model are specific to the CA1-CA3 circuit? Can these rules be potentially employed elsewhere within the medial temporal lobe or sensory areas? What are the model parameters used that could suggest that the observed results are specific to hippocampus-based predictive learning?</p></disp-quote><p>We thank the reviewer for this important question. We refer to the following paragraph in the discussion regarding this topic:</p><p>“Notably, even though we have focused on the hippocampus in our work, the SR does not require predictive information to come from higher-level feedback inputs. This framework could therefore be useful even in sensory areas: certain stimuli are usually followed by other stimuli, essentially creating a sequence of states whose temporal structure can be encoded in the network using our framework. Interestingly, replays have been observed in other brain areas besides the hippocampus Kurth-Nelson2016, Staresina2013. Furthermore, temporal difference learning in itself has been proposed in the past as a way to implement prospective coding Brea2016”</p><disp-quote content-type="editor-comment"><p>4. The analytical illustration linking the proposed model with reinforcement learning is well executed. However, in practice, the actual implementation of reinforcement learning within the model is unclear. Given the sample task provided where animals are navigating a simple environment, how can one make use of value-based learning to enhance behavior? Explicit discussion on the extent to which reinforcement learning is related to the actual computation potentially needed to navigate sensory environments (both learned and novel) would be really helpful in understanding the link between the model to reinforcement learning.</p></disp-quote><p>We thank the reviewer for this important remark. We have added Figure 5—figure supplement S2 and updated the discussion to address this point. Since we modelled our neurons to integrate the synaptic EPSPs and generated spikes using an inhomogeneous Poisson process based on the depolarization, the firing rate is proportional to the total synaptic weights. Therefore, the successor representation can be read out simply by a downstream neuron. Moreover, since the value of a state is defined by the inner product between the successor matrix and the reward vector, it is sufficient for the synaptic weights to the downstream neuron to learn the reward vector, and the downstream neuron will then encode the state value in its firing rate. We performed additional simulations and have added a supplementary figure (Figure 5—figure supplement S2) showing this setup.</p><p>Besides the supplementary figure (Figure 5—figure supplement S2), we added the following paragraph in the Discussion section:</p><p>“Since we modelled our neurons to integrate the synaptic EPSPs and generate spikes using an inhomogeneous Poisson process based on the depolarization, the firing rate is proportional to the total synaptic weights. Therefore, the successor representation can be read out simply by a downstream neuron. Moreover, since the value of a state is defined by the inner product between the successor matrix and the reward vector, it is sufficient for the synaptic weights to the downstream neuron to learn the reward vector, and the downstream neuron will then encode the state value in its firing rate (see Figure 5—figure supplement S2). While the neuron model used is simple, it will be interesting for future work to study analogous models with nonlinear neurons.”</p><disp-quote content-type="editor-comment"><p>5. Subplots both within and across figures seem to be of very different text formatting and sizing (such as panel F in Figure 4 and Figure 5). Please reformat them accordingly.</p></disp-quote><p>Thank you, we have reformatted the figures.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Important: Note that the page numbers refer to the page in the PDF, which is their own page number-1 (due to eLife adding a header page).</p><p>Page 3 – &quot;smoothly… and anything in between&quot; – this is overstated and should be removed.</p></disp-quote><p>We thank the reviewer for the feedback. We have adapted the sentence as follows:</p><p>“We show mathematically that our proposed framework smoothly connects a temporally precise spiking code akin to replay activity with a rate based code akin to behavioural spiking.”</p><p>While we agree that our model implements only one type of temporal code, it is important to stress that there is a smooth transition between this temporal code and a pure rate encoding of the state. The larger the time T in each state, the less precise the spikes become organised in time. In all cases, the learning dynamics have the same fixed point, namely, they converge to the successor representation. We believe that this fact is non-trivial. Furthermore, we show how this smooth transition changes how the fixed point (SR) is reached. On one extreme (T=0), algorithmically it uses TD(1), on the other extreme (T=infinity) TD(0) and all intermediate cases implement a value of λ between 0 and 1.</p><disp-quote content-type="editor-comment"><p>Page 3 – &quot;don't need to discretize time…&quot;. Here and elsewhere there should be citations to the work of Doya, NeurIPS 1995, Neural Comp 2000 on the modeling of continuous time in RL.</p></disp-quote><p>Thank you, we have added those citations</p><disp-quote content-type="editor-comment"><p>Page 3 – &quot;using replays&quot; – It is very narrowminded to assume that all replay do is set up successor representations. They could also be involved in model-based planning of behavior as suggested in the work of Johnson and Redish, 2012; Pfeiffer and Foster, 2018; Kay et al. 2021 and modeled in Hasselmo and Eichenbaum, 2005; Erdem and Hasselmo, 2012 and Fenton and Kubie, 2012.</p></disp-quote><p>We thank the reviewer for the feedback, but assure that we did not mean to assume that is all replays do. In fact, we are well aware of other potential benefits of replays in memory consolidation, model-based planning etc. We merely propose to add another potential benefit of replays to the existing hypotheses, namely that they can be used to learn and in doing so they can reduce bias and learn offline. We have adapted the ‘Replays’ paragraph in the discussion to reflect this and have also adapted the highlighted text to make this clearer:</p><p>“Finally, replays have long been speculated to be involved in learning models of the environment (Eichenbaum, 2005; Hasselmo and Erdem and Hasselmo, 2012 and Fenton and Kubie, 2012; Johnson and Redish, 2012; Pfeiffer and Foster, 2018; Kay et al. 2021;). Here, we investigate how replays could play an additional role in learning the SR cognitive map”</p><disp-quote content-type="editor-comment"><p>Page 3 – They assume that STDP can occur during replay, but evidence for STDP during replay is unclear. McNaughton's lab showed that LTP is less likely to be induced during the modulatory states during which sharp-wave ripple replay events occur. They should look for citations that have actually shown LTP induction during the replay state.</p></disp-quote><p>Thank you for this comment. We have added the following sentence in the &quot;Replay&quot; paragraph of the discussion.</p><p>“Moreover, while indirect evidence supports the idea that replays can play a role during learning Igata2020, it is not yet clear how synaptic plasticity is manifested during replays Fuchsberger2022.”</p><disp-quote content-type="editor-comment"><p>Page 4 – Marr's three levels – They should remove this discussion about Marr's three levels as I think the implementation level is relatively sparse and the behavioral level is also relatively sparse.</p></disp-quote><p>We believe that this link between computation, algorithm and implementation is actually a strength of the proposed work. Typically, only one or at most two levels are discussed, without a more holistic view. We believe that our work (and e.g. George et al.) goes one step beyond by making the link between the implementation (spiking neurons with STDP), algorithm (TD learning) and computational theory (SR / predictive cognitive maps) explicit. While of course, these modelling studies are abstractions of reality, we believe this is not trivial and would like to maintain this paragraph to stimulate future research to bridge these levels as well.</p><disp-quote content-type="editor-comment"><p>Page 4 – &quot;The hippocampus has long been thought&quot; – It's astounding that the introduction only cites two experimental papers (O'Keefe and Dostrovsky, 1971; Mehta et al. 2000) and then the start of the Results section makes a statement like this and only cites Stachenfeld et al. 2017 as if it were an experimental paper. There are numerous original research papers that should be cited for the role of hippocampus in behavior. They should cite at least five or six so that the reader doesn't get the impression all o this work started with the holy paper by Stachenfeld et al. 2017. For example, they could again cite O'Keefe and Nadel, 1978 for the very comprehensive review of the literature up to that time, plus the seminal work of Morris et al. 1982 in Morris water maze and Olton, 1979 in 8-arm radial maze and perhaps some of the work by Aggleton and by Eichenbaum on spatial alternation.</p></disp-quote><p>We agree and thank the reviewer for the suggestion. We have now expanded the citations of relevant experimental work.</p><disp-quote content-type="editor-comment"><p>Page 5 – The description of successor representations is very one dimensional. They should mention how it can be expanded to two dimensions.</p></disp-quote><p>To address this, we perform a new simulation in a 2D environment (Supplementary Figure 2) as well as a discussion on the generality of the approach to any dimension in section 2.1</p><p>“Even though we introduced the linear track as an illustrative example, the SR can be learned in any environment (see Figure 1—figure supplement S1 for an example in an open field)”.</p><disp-quote content-type="editor-comment"><p>Page 5 – &quot;Usually attributed to model based…&quot;. They cannot just talk about SR being model free. Since this section was supposed to be for neuroscientists, they need to clearly explain the distinction between model free and model-based RL, and describe why successor representations are not just model-based RL, but instead provide a look-up table of predictive state that does NOT involve model-based planning of behavior. The blog of Vitay gives a much better overview that compares model-free, model-based and successor representations:</p><p>https://julien-vitay.net/post/successor_representations/ – This needs more than just a citation – there should be a clear description of model-based and model-free RL in contrast to SR, and Vitay is an example of that.</p></disp-quote><p>We have now extended this paragraph with a more thorough explanation of model-free and model-based RL, including an example of what happens when a reward location changes in all three cases.</p><p>Because of this predictive information, the SR allows sample-efficient re-learning when the reward location is changed Gershman2018. In reinforcement learning, we tend to distinguish between model-free and model-based algorithms. The SR is believed to sit inbetween these two modalities. In model-free reinforcement learning, the aim is to directly learn the value of each state in the environment. Since there is no model of the environment at all, if the location of a reward is changed, the agent will have to first unlearn the previous reward location by visiting it enough times, and then is able to re-learn the new location. In modelbased reinforcement learning, a precise model of the environment is learned, specifically, single-step transition probabilities between all states of the environment. Model-based learning is computationally expensive, but allows a certain flexibility. If the reward changes location it is immediate to derive the updated values of the states. As we have seen, however, the SR can re-learn a new reward location somewhat efficiently, although less so than model-based learning. The SR can also be efficiently learned using model-free methods and allows us to easily compute values for each state, which in turn can guide the policy Dayan1993, Russek2017, Momennejad2017. This position between model-based and model-free methods makes the SR framework very powerful, and its similarities with hippocampal neuronal dynamics have led to increased attention from the neuroscience community. Finally, in our examples above, we considered an environment made up of a discrete number of states. This framework can be generalised to a continuous environment represented by a discrete number of place cells.</p><disp-quote content-type="editor-comment"><p>Page 5 – Related to this issue – they need to repeatedly address the fact that Successor representations are just an hypothesis contrast with model-based behavior, and repeatedly throughout the paper discuss that model-based behavior could still be the correct accounting for all of the data that they address.</p></disp-quote><p>We thank the reviewer for pointing out that the successor representation is only one of the possible hypotheses.</p><p>We have proceeded to address this for the Frank and Cheng data (page 14, ‘Please note, however, that other mechanisms besides the Successor Representation could account for these results, including model-based reinforcement learning.), and in the final section of the shock experiment (page 16, ‘It is important to note here that, while we are suggesting a potential role for the SR in solving this task, the data itself would also be compatible with a model-based strategy. In fact, experimental evidence suggests that humans may use a mixed strategy involving both model-based reinforcement learning and the successor representation [Momennejad et al., 2017].’).</p><disp-quote content-type="editor-comment"><p>Page 5 – &quot;similar to (Mehta et al. 2000)&quot; – Learning in the CA3-CA1 network has been modeled like this in many previous models that should be cited here including McNaughton and Morris, 1987; Hasselmo and Schnell, 1994; Treves and Rolls, 1994; Mehta et al. 2000; Hasselmo, Bodelon and Wyble, 2002.</p></disp-quote><p>Thank you, we have added the citations</p><disp-quote content-type="editor-comment"><p>Page 6 – Figure 1d looks like the net outcome of the learning rule in this example is long-term depression. Is that intended? Given the time interval between pre and post, it looks like it ought to be potentiation in the example.</p></disp-quote><p>This depends on the pre-post spike timing, and in this example the postsynaptic spike is too far from the presynaptic spike, leading to depression. When moving the postsynaptic spike closer to the presynaptic spike, potentiation would occur. The plasticity rule qualitatively results in three regions depending on the spike timing: depression (post-pre), potentiation (pre-post with small interval), and depression (pre-post with large interval). Qualitatively it is in line with e.g. Shouval et al. 2002</p><disp-quote content-type="editor-comment"><p>Page 7 – They should address the problem of previously existing weights in the CA3 to CA1 connections. For example, what if there are pre-existing weights that are strong enough to cause post-synaptic spiking in CA1 independent of any entorhinal input? How do they avoid the strengthening of such connections? (i.e. the problem of prior weights driving undesired learning on CA3-CA1 synapses is addressed in Hasselmo and Schnell, 1994; Hasselmo, Bodelon and Wyble, 2002, which should be cited).</p></disp-quote><p>This is an important point, and since our model guarantees a stable fixed point of the learning dynamics, the initial conditions are not influencing the final convergence. To show this explicitly, we have performed a new simulation and added Figure 1—figure supplement S2 to illustrate this.</p><p>This is reflected in the manuscript:</p><p>“As a proof of principle, we show that it is possible to learn the SR for any initial weights (Figure 1—figure supplement S2), independently of any previous learning in the CA3 to CA1 connections.”</p><disp-quote content-type="editor-comment"><p>Page 7 – &quot;Elegantly combines rate and temporal&quot; – This is overstated. The possible temporal codes in the hippocampus include many possible representations beyond just one step prediction. They need to specify that this combines one type of possible temporal code. I also recommend removing the term &quot;elegant&quot; from the paper. Let someone else call your work elegant.</p></disp-quote><p>We thank the reviewer for the feedback. We have modified the manuscript to reflect the comments:</p><p>“As we will discuss below, this framework, therefore, combines learning based on rate coding as well as temporal coding.”</p><disp-quote content-type="editor-comment"><p>Page 7 – &quot;replays for learning&quot; – as noted above in experiments LTP has not been shown to be induced during the time periods of replay – sharp-wave ripple replay events seem to be associated with lower cholinergic tone (Buzsaki et al. 1983; VandeCasteele et al. 2014) whereas LTP is stronger when Ach levels are higher (Patil et al. J. Neurophysiol. 1998). This is not an all-or-none difference, but it should be addressed.</p></disp-quote><p>Thank you for this comment. We have added the following sentence in the &quot;Replay&quot; paragraph of the discussion.</p><p>“Moreover, while indirect evidence supports the idea that replays can play a role during learning Igata2020, it is not yet clear how synaptic plasticity is manifested during replays Fuchsberger2022.”</p><disp-quote content-type="editor-comment"><p>Page 7 – &quot;equivalent to TD…&quot;. Should this say &quot;equivalent to TD(0)&quot;?</p></disp-quote><p>We changed the phrasing in the manuscript to: “equivalent to TD(λ)”.</p><disp-quote content-type="editor-comment"><p>Page 8 – &quot;Bootstrapping means that a function is updated using current estimates of the same function…&quot;. This is a confusing and vague description of bootstrapping. They should try to give a clearer definition for neuroscientists (or reduce their reference to this).</p></disp-quote><p>We have now changed the explanation in the manuscript in the corresponding section.</p><p>From a reinforcement learning perspective, the TD(0) algorithm relies on a property called bootstrapping. This means that the successor representation is learned by first taking an initial estimate of the SR matrix (i.e. the previously learned weights), and then gradually adjusting this estimate (i.e. the synaptic weights) by comparing it to the states in the environment the animal actually visits. This comparison is achieved by calculating a prediction error, similar to the widely studied one for dopamine neurons Schultz1997. Since the synaptic connections carry information about the expected trajectories, in this case, the prediction error is computed between the predicted and observed trajectories (see Methods).</p><p>The main point of bootstrapping, therefore, is that learning happens by adjusting our current predictions (e.g. synaptic weights) to match the observed current state. This information is available at each time step and thus it allows learning over long timescales using synaptic plasticity alone. If the animal moves to a state in the environment that the current weights deem unlikely, potentiation will prevail and the weight from the previous to the current state will increase. Otherwise, the opposite will happen. It is important to notice that the prediction error in our model is not encoded by a separate mechanism in the way that dopamine is thought to do for reward predictionSchultz1997. Instead, the prediction error is represented locally, at the level of the synapse, through the depression and potentiation terms of our STDP rule, and the current weight encodes the current estimate of the SR (see Methods). Notably, the prediction error updates result in a total update equivalent to the TD(\λ) update. This mathematical equivalence ensures that the weights of our neural network track the TD(\λ) update at each state, and thus stability and convergence to the theoretical values of the SR. We therefore do not need an external vector to carry prediction error signals as proposed in Gardner2018, Gershman2018. In fact, the synaptic potentiation in our model updates a row of the SR, while the synaptic depression updates a column.</p><disp-quote content-type="editor-comment"><p>Page 9 – Figure 2 – Do TD λ and TD zero really give equivalent weight outputs?</p></disp-quote><p>The theoretical value of the SR is the same, independently of the algorithm used to learn it. TD λ, for any value of λ, converges to this theoretical value. No tuning is needed for this, it is mathematically guaranteed that the algorithm converges. Thus, also for λ=0 the results are exact.</p><disp-quote content-type="editor-comment"><p>Page 8 – &quot;that are behaviorally far apart&quot; – I don't understand how this occurs.</p></disp-quote><p>See explanation above.</p><disp-quote content-type="editor-comment"><p>page 10 – &quot;dependency of synaptic weights on each other as discussed above.&quot; This was not made sufficiently clear either here or above.</p></disp-quote><p>See explanation above.</p><disp-quote content-type="editor-comment"><p>Page 10 – &quot;dependency of synaptic weights on each other&quot; This also suggests a problem of stability if the weights can start to drive their own learning and cause instability – how is this prevented?</p></disp-quote><p>See explanation above. The dynamics have a fixed point of convergence (the SR). No matter what the initial weights are, it is guaranteed to converge to the SR. We illustrated that in the new Supplementary Figure 7.</p><disp-quote content-type="editor-comment"><p>Page 10 – &quot;average of the discounted state occupancies&quot; – this would be uniform without discounting but what is the biological mechanism for the discounting that is used here?</p></disp-quote><p>This is an important point. No extra mechanism is needed to induce the discount in our framework. Instead, the decaying LTP window is sufficient to drive the discounting of states further away.</p><disp-quote content-type="editor-comment"><p>Page 10 – &quot;due to the bootstrapping&quot; – again this is unclear – can be improved by giving a better definition of bootstrapping and possibly by referring to specific equation numbers.</p></disp-quote><p>We have now changed the explanation in the manuscript in the corresponding section, see comment above.</p><disp-quote content-type="editor-comment"><p>Page 11 – &quot;exponential dependence&quot; what is the neural mechanism for this?</p></disp-quote><p>In our case, it is the LTP window which exponentially decays.</p><disp-quote content-type="editor-comment"><p>Page 11 – &quot;Ainsley&quot; is not a real citation in the bibliography. Should fix and also provide a clearer definition (or equation) for hyperbolic.</p><p>Page 11 – &quot;elegantly combines two types of discounting&quot; – how is useful? Also, let other people call your work elegant.</p></disp-quote><p>Very true! We have now rephrased our statement. Hyperbolic discounting has been observed in neuroeconomics experiments, whereas reinforcement learning algorithms normally rely on exponential discounting for its favourable mathematical properties. This has been however considered confusing when applying reinforcement learning to behaviour. Our model uses exponential discounting at a neural level and in space, but can account for the findings with hyperbolic discounting.</p><disp-quote content-type="editor-comment"><p>Page 11 – how does discounting depend on both firing rate and STDP -- should provide some explanation or at least refer to where this is shown in the equations.</p></disp-quote><p>We have now added a reference to the equation.</p><disp-quote content-type="editor-comment"><p>page 13 – &quot;Cheng and Frank&quot; – this is a good citation, but they could add more here on timing of replay events.</p></disp-quote><p>We added the following sentence regarding the replay timing:</p><p>“Please note that, while we implemented an exponentially decaying probability for replays after entering a novel environment, different schemes for replay activity could be investigated.”</p><disp-quote content-type="editor-comment"><p>Page 15 – This whole section on the shock experiment starts with the assumption of a successor representation. As noted above, they need to explicitly discuss the important alternate hypothesis that the neural activity reflects model-based planning that guides the behavior in the task (and could perhaps better account for the peak of occupancy at the border of light and dark).</p></disp-quote><p>We have changed the text to reflect this suggestion.</p><disp-quote content-type="editor-comment"><p>Page 16 – &quot;mental imagination&quot; – rather than using it for modifying SR, why couldn't mental imagination just be used for model-based behavior?</p></disp-quote><p>We believe it is indeed possible that mental imagination can be used for model-based behaviour. However, it is possible that humans use a mixed strategy involving both modelbased learning and the successor representation, as suggested by the findings in Momennejad et al. 2017.</p><disp-quote content-type="editor-comment"><p>Page 17 – &quot;spiking&quot; – again, if they are going to refer to their model as a &quot;spiking&quot; model, they need to add some plots showing spiking activity.</p></disp-quote><p>One example of the spiking activity of our model can be found in Figure 2 panel A.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p></disp-quote><p>We have responded to the reviewer 3's concerns at the beginning of the rebuttal, in the &quot;Essential Revision&quot; section.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The revised article has only partly resolved my confusion.</p><p>My main issue was the following: in the proposed feed-forward model, the synaptic weights between the two layers learn the entries of the successor matrix. If external inputs were fed only to the first layer, the second layer would directly read out the successor representation (this is suggested in Figure 1 E-F, but not explicitly mentioned in the text as far as I can tell). Instead, in the model, both layers are driven by external inputs representing the current state. This is crucial for learning, but it implies that the activity of the second layer mixes the successor representation with a representation of the current state itself. Learning and readout, therefore, seem antagonistic. It would be worth explaining this fact in the main text.</p></disp-quote><p>We thank the reviewer for the question, and we now added an extra explanation in the main text. In brief, the CA1 layer indeed mixes the SR with a representation of the current state. However, we argue that learning and readout are not necessarily antagonistic. We suggest here a few possibilities to resolve this conflict:</p><p>1. Since the external current in CA1 is present for only a fraction of the time T in each state, the readout might happen during the period of CA3 activation exclusively.</p><p>2. The readout may be over the whole time T but becomes noisier towards the end. It is worth noting that, even in the case where the readout is noisy, the distortion would be limited to the diagonal elements of the matrix (see equation 13 and 15 in the Methods section, and Figure 5 – supplement 2, panels B and C).</p><p>3. Learning and readout may be separate mechanisms, where during readout only the CA3 driving current is present. This could be for instance signaled by neuromodulation (e.g. acetylcholine has been associated to attention and arousal during spatial learning [1,2,3,4,5]), or it could be that readout happens during replays with a population of neurons.</p><p>4. The weights to or activation functions of the readout neuron may learn to compensate for the distorted signal in CA1.</p><p>We have now included this explanation in the main text in the Discussion, in the Reinforcement Learning subchapter.</p><disp-quote content-type="editor-comment"><p>In their reply, the authors clarify that the external inputs drive the activity of the second layer only for a limited time (20%). As far as I can tell, in the text, this is mentioned explicitly only in the legend of Figure 5-S2. That seems to imply that there is a large difference in the duration of the external inputs to the two layers. How can that be justified?</p><p>More importantly, it seems that varying the value of the delay should lead to a tradeoff between the accuracy of learning and the accuracy of the subsequent readout in the second layer. Is that the case? It would be useful to have a figure where the delay is varied.</p></disp-quote><p>As suggested, we added two figures (Figure 5—figure supplement 3, Figure 5—figure supplement 4) where we varied this delay. In our model, the external inputs are first active in CA3 and afterwards in CA1 (equation 10 and 11). We model these inputs as step currents. We call the overall time in each state T, and say that the external current for CA3 has duration θ, while the external current for CA1 has duration ω omega. If we assume that there is no pause in between these two stimuli, we have ω = T – θ. We can therefore define the delay between the two stimuli as: θ/T.</p><p>As the reviewer points out, the delay value θ/T that we chose for Figure 5 – Supplement 2 was 0.8. However, this was an arbitrary choice and other values are possible. We explore here what happens when we vary the delay parameter θ/T:</p><p>– Impact on the reinforcement learning representation</p><p>The delay parameter θ/T influences the reinforcement learning representation, as it can be seen from equations 22 and 23. When the delay is longer, the value for γ increases and λ decreases (Figure 5 —figure supplement 3, panels b-c). However, it is important to notice that γ and λ are determined by other biological values as well (see equations 22 and 23), such as the depression amplitude (Figure 5 —figure supplement 3, panels e-f) or the decay of the potentiation learning window. The value we choose for the delay is therefore flexible: even if we want to learn the SR with certain fixed γ or λ, we can always vary other parameters in the spiking model.</p><p>– Impact on learning and readout</p><p>The strength of the place-tuned input to CA1 also depends on the choice of the delay (see equation 18). Typically, the CA1 place-tuned input strength increases with the delay, except for long values of T or low depression amplitudes, where the input remains fairly constant (Figure 5 —figure supplement 3, panels a and d respectively). Therefore, the longer the CA1 place-tuned input lasts, the weaker it has to be to ensure the proper learning. Intuitively, this means that we can compensate for a shorter duration of the external current by increasing its strength. The distortion of the readout thus remains more or less constant independently of our choice of delay (Figure 5 —figure supplement 4). We therefore do not find a trade-off between accuracy of learning and accuracy of readout, and the distortion in the value could be corrected for example by one of the mechanisms proposed above.</p><p>We addressed the above points in the Discussion, in the Reinforcement Learning subchapter.</p><p>References:</p><p>Micheau, J. and Marighetto, A. Acetylcholine and memory: a long, complex and chaotic but still living relationship. Behav. Brain Res. 221, 424–429 (2011)</p><p>Hasselmo, M. E. and Sarter, M. Modes and models of forebrain cholinergic neuromodulation of cognition. Neuropsychopharmacology 36, 52–73 (2011).</p><p>Robbins, T. W. Arousal systems and attentional processes. Biol. Psychol. 45, 57–71 (1997).</p><p>Teles-Grilo Ruivo, L. M. and Mellor, J. R. Cholinergic modulation of hippocampal network function. Front. Synaptic Neurosci. 5, 2 (2013).</p><p>Palacios-Filardo, Jon, et al. &quot;Acetylcholine prioritises direct synaptic inputs from entorhinal cortex to CA1 by differential modulation of feedforward inhibitory circuits.&quot; Nature communications 12.1 (2021): 1-16.</p></body></sub-article></article>