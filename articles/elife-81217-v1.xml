<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">81217</article-id><article-id pub-id-type="doi">10.7554/eLife.81217</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A generalizable brain extraction net (BEN) for multimodal MRI data from rodents, nonhuman primates, and humans</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-283667"><name><surname>Yu</surname><given-names>Ziqi</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8201-5481</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-286706"><name><surname>Han</surname><given-names>Xiaoyang</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3007-6079</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-298144"><name><surname>Xu</surname><given-names>Wenjing</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-168618"><name><surname>Zhang</surname><given-names>Jie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-95047"><name><surname>Marr</surname><given-names>Carsten</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2154-4552</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-286704"><name><surname>Shen</surname><given-names>Dinggang</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-286705"><name><surname>Peng</surname><given-names>Tingying</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-248169"><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8965-1077</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-105630"><name><surname>Feng</surname><given-names>Jianfeng</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5987-2258</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Institute of Science and Technology for Brain-Inspired Intelligence</institution>, <institution>Fudan University</institution>, <addr-line><named-content content-type="city">Shanghai</named-content></addr-line>, <country>China</country></aff><aff id="aff2"><institution content-type="dept">Institute of AI for Health</institution>, <institution>Helmholtz Zentrum München</institution>, <addr-line><named-content content-type="city">Neuherberg</named-content></addr-line>, <country>Germany</country></aff><aff id="aff3"><institution content-type="dept">School of Biomedical Engineering</institution>, <institution>ShanghaiTech University</institution>, <addr-line><named-content content-type="city">Shanghai</named-content></addr-line>, <country>China</country></aff><aff id="aff4"><institution content-type="dept">Helmholtz AI</institution>, <institution>Helmholtz Zentrum München</institution>, <addr-line><named-content content-type="city">Neuherberg</named-content></addr-line>, <country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-106148"><name><surname>Jbabdi</surname><given-names>Saad</given-names></name><role>Reviewing editor</role><aff><institution>University of Oxford</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>tingying.peng@helmholtz-muenchen.de</email> (TP);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>xiaoyong_zhang@fudan.edu.cn</email> (XZ);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>22</day><month>12</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e81217</elocation-id><history><date date-type="received"><day>20</day><month>06</month><year>2022</year></date><date date-type="accepted"><day>21</day><month>12</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Yu et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Yu et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-81217-v1.pdf"/><abstract><p>Accurate brain tissue extraction on magnetic resonance imaging (MRI) data is crucial for analyzing brain structure and function. While several conventional tools have been optimized to handle human brain data, there have been no generalizable methods to extract brain tissues for multimodal MRI data from rodents, nonhuman primates, and humans. Therefore, developing a flexible and generalizable method for extracting whole brain tissue across species would allow researchers to analyze and compare experiment results more efficiently. Here, we propose a domain-adaptive and semi-supervised deep neural network, named the Brain Extraction Net (BEN), to extract brain tissues across species, MRI modalities, and MR scanners. We have evaluated BEN on 18 independent datasets, including 783 rodent MRI scans, 246 nonhuman primate MRI scans, and 4,601 human MRI scans, covering five species, four modalities, and six MR scanners with various magnetic field strengths. Compared to conventional toolboxes, the superiority of BEN is illustrated by its robustness, accuracy, and generalizability. Our proposed method not only provides a generalized solution for extracting brain tissue across species but also significantly improves the accuracy of atlas registration, thereby benefiting the downstream processing tasks. As a novel fully automated deep-learning method, BEN is designed as an open-source software to enable high-throughput processing of neuroimaging data across species in preclinical and clinical applications.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Rat</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>81873893,82171903,92043301</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003347</institution-id><institution>Fudan University</institution></institution-wrap></funding-source><award-id>the Office of Global Partnerships (Key Projects Development Fund)</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>Shanghai Municipal Science and Technology Major Project</institution></institution-wrap></funding-source><award-id>No.2018SHZDZX01</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf2"><p>Dinggang Shen, is affiliated with Shanghai United Imaging Intelligence Co., Ltd. He has financial interests to declare..</p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: Partial rodent MRI data collection were approved by the Animal Care and Use Committee of Fudan University, China. The rest rodent data (Rat-T2WI-9.4T and Rat-EPI-9.4T datasets) are publicly available (CARMI: https://openneuro.org/datasets/ds002870/versions/1.0.0). Marmoset MRI data collection were approved by the Animal Care and Use Committee of the Institute of Neuroscience, Chinese Academy of Sciences, China. Macaque MRI data are publicly available from the nonhuman PRIMatE Data Exchange (PRIME-DE) (https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html).</p></fn><fn fn-type="other"><p>Human subjects: The Zhangjiang International Brain Biobank (ZIB) protocols were approved by the Ethics Committee of Fudan University (AF/SC-03/20200722) and written informed consents were obtained from all volunteers. UK Biobank (UKB) and Adolescent Brain Cognitive Development (ABCD) are publicly available.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All data (MRI data, source codes, pretrained weights and replicate demo notebooks for Figure 1-7) are included in the manuscript or available at https://github.com/yu02019/BEN.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Ziqi Yu</collab><collab>Wenjing Xu</collab><collab>Xiao-Yong Zhang</collab></person-group><year iso-8601-date="2022">2022</year><source>A longitudinal MRI dataset of young adult C57BL6J mouse brain</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6844489">https://doi.org/10.5281/zenodo.6844489</ext-link><comment>Zenodo, doi:10.5281/zenodo.6844489</comment></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Li-Ming Hsu</collab><collab>Woomi Ban</collab><collab>Tzu-Hao Chao</collab><collab>Sheng Song</collab><collab>Domenic Hayden Cerri</collab><collab>Lindsay Walton</collab><collab>Margaret Broadwater</collab><collab>Sung-Ho Lee</collab><collab>Yenyu Ian Shih</collab></person-group><year iso-8601-date="2020">2020</year><source>CAMRI Rat Brain MRI Data</source><ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002870/versions/1.0.0">https://openneuro.org/datasets/ds002870/versions/1.0.0</ext-link><comment>OpenNeuro, doi:10.18112/openneuro.ds002870.v1.0.1</comment></element-citation><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Li-Ming Hsu</collab><collab>Woomi Ban</collab><collab>Tzu-Hao Chao</collab><collab>Sheng Song</collab><collab>Domenic Hayden Cerri</collab><collab>Lindsay Walton</collab><collab>Margaret Broadwater</collab><collab>Sung-Ho Lee</collab><collab>Yenyu Ian Shih</collab></person-group><year iso-8601-date="2020">2020</year><source>CAMRI Mouse Brain MRI Data</source><ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002868/versions/1.0.0">https://openneuro.org/datasets/ds002868/versions/1.0.0</ext-link><comment>OpenNeuro, doi:10.18112/openneuro.ds002868.v1.0.1</comment></element-citation><element-citation id="dataset4" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Michael P. Milham</collab><collab>Lei Ai</collab><collab>Bonhwang Koo</collab><collab>...</collab><collab>Yong-di Zhou</collab><collab>Daniel S. Margulies</collab><collab>Charles E. Schroeder</collab></person-group><year iso-8601-date="2018">2018</year><source>An Open Resource for Non-human Primate Imaging</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2018.08.039">https://doi.org/10.1016/j.neuron.2018.08.039</ext-link><comment>Neuron, doi:10.1016/j.neuron.2018.08.039</comment></element-citation><element-citation id="dataset5" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Casey B J</collab><collab>Cannonier T</collab><collab>Conley M I</collab><collab>et al.</collab></person-group><year iso-8601-date="2018">2018</year><source>The Adolescent Brain Cognitive Development (ABCD) study: Imaging acquisition across 21 sites</source><ext-link ext-link-type="uri" xlink:href="https://abcdstudy.org/scientists/data-sharing/">https://abcdstudy.org/scientists/data-sharing/</ext-link><comment>Developmental Cognitive Neuroscience, doi:10.1016/j.dcn.2018.03.001</comment></element-citation><element-citation id="dataset6" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Miller K L</collab><collab>Alfaro-Almagro F</collab><collab>Bangerter N K</collab><collab>et al.</collab></person-group><year iso-8601-date="2018">2018</year><source>Multimodal population brain imaging in the UK Biobank prospective epidemiological study</source><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.4393">https://www.nature.com/articles/nn.4393</ext-link><comment>Nature Neuroscience, doi:10.1038/nn.4393</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-81217-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>