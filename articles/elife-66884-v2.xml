<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">66884</article-id><article-id pub-id-type="doi">10.7554/eLife.66884</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Activity in perirhinal and entorhinal cortex predicts perceived visual similarities among category exemplars with highest precision</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-223669"><name><surname>Ferko</surname><given-names>Kayla M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4362-7295</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-225290"><name><surname>Blumenthal</surname><given-names>Anna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8768-0189</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-96981"><name><surname>Martin</surname><given-names>Chris B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7014-4371</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-137887"><name><surname>Proklova</surname><given-names>Daria</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-257320"><name><surname>Minos</surname><given-names>Alexander N</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-149405"><name><surname>Saksida</surname><given-names>Lisa M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-225291"><name><surname>Bussey</surname><given-names>Timothy J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-160397"><name><surname>Khan</surname><given-names>Ali R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0760-8647</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-20234"><name><surname>Köhler</surname><given-names>Stefan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1905-6453</contrib-id><email>stefank@uwo.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Brain and Mind Institute, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Robarts Research Institute Schulich School of Medicine and Dentistry, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04sjchr03</institution-id><institution>Cervo Brain Research Center, University of Laval</institution></institution-wrap><addr-line><named-content content-type="city">Quebec</named-content></addr-line><country>Canada</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05g3dte14</institution-id><institution>Department of Psychology, Florida State University</institution></institution-wrap><addr-line><named-content content-type="city">Tallahassee</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Department of Physiology and Pharmacology, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>School of Biomedical Engineering, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Department of Medical Biophysics, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Department of Psychology, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>University Medical Center Hamburg-Eppendorf</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>21</day><month>03</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e66884</elocation-id><history><date date-type="received" iso-8601-date="2021-01-25"><day>25</day><month>01</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-03-17"><day>17</day><month>03</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-01-21"><day>21</day><month>01</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.01.21.427602"/></event></pub-history><permissions><copyright-statement>© 2022, Ferko et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Ferko et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-66884-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-66884-figures-v2.pdf"/><abstract><p>Vision neuroscience has made great strides in understanding the hierarchical organization of object representations along the ventral visual stream (VVS). How VVS representations capture fine-grained visual similarities between objects that observers subjectively perceive has received limited examination so far. In the current study, we addressed this question by focussing on perceived visual similarities among subordinate exemplars of real-world categories. We hypothesized that these perceived similarities are reflected with highest fidelity in neural activity patterns downstream from inferotemporal regions, namely in perirhinal (PrC) and anterolateral entorhinal cortex (alErC) in the medial temporal lobe. To address this issue with functional magnetic resonance imaging (fMRI), we administered a modified 1-back task that required discrimination between category exemplars as well as categorization. Further, we obtained observer-specific ratings of perceived visual similarities, which predicted behavioural discrimination performance during scanning. As anticipated, we found that activity patterns in PrC and alErC predicted the structure of perceived visual similarity relationships among category exemplars, including its observer-specific component, with higher precision than any other VVS region. Our findings provide new evidence that subjective aspects of object perception that rely on fine-grained visual differentiation are reflected with highest fidelity in the medial temporal lobe.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>medial temporal lobe</kwd><kwd>ventral visual pathway</kwd><kwd>visual discrimination</kwd><kwd>fMRI</kwd><kwd>object recognition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000024</institution-id><institution>Canadian Institutes of Health Research</institution></institution-wrap></funding-source><award-id>366062</award-id><principal-award-recipient><name><surname>Khan</surname><given-names>Ali R</given-names></name><name><surname>Köhler</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Ferko</surname><given-names>Kayla M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000236</institution-id><institution>Ontario Trillium Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Blumenthal</surname><given-names>Anna</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010785</institution-id><institution>Canada First Research Excellence Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Khan</surname><given-names>Ali R</given-names></name><name><surname>Köhler</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Brain Canada Platform Support Grant</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Khan</surname><given-names>Ali R</given-names></name><name><surname>Köhler</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The present findings reveal that subjectively perceived similarities between objects, including those unique to individual observers, are reflected with highest fidelity at the apex of the ventral visual pathway in the medial temporal lobe.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The ability to perceive similarities and differences between objects plays an integral role in cognition and behaviour. Perceived similarities are important, for example, for categorizing a fruit at the grocery store as an apple rather than a pear. The appreciation of more fine-grained similarities between exemplars of a category also shapes behaviour, such as when comparing different apples in order to select one for purchase. Indeed, experimental work in psychology has confirmed that the similarity of objects influences performance in numerous behavioural contexts, including but not limited to categorization, object discrimination, recognition memory, and prediction (see <xref ref-type="bibr" rid="bib43">Medin et al., 1993</xref>; <xref ref-type="bibr" rid="bib24">Goldstone and Son, 2012</xref>; <xref ref-type="bibr" rid="bib31">Hebart et al., 2020</xref>, for review). Yet, despite the well-established links to behaviour, how the brain represents these similarities between objects is only beginning to be understood. A central question that has received limited investigation so far is how fine-grained visual similarities that observers subjectively perceive among subordinate category exemplars map onto neural object representations. Answering this question can provide insight as to what brain regions provide the ‘read-out’ for such subjective reports. Moreover, this endeavor holds promise for understanding how differences in the way in which observers perceive their visual environment are reflected in variations in functional brain organization (<xref ref-type="bibr" rid="bib13">Charest and Kriegeskorte, 2015</xref>).</p><p>Functional neuroimaging, combined with pattern analysis techniques, provides a powerful tool for examining the mapping between similarity relationships in visual perception of objects and similarities in corresponding neural representations (<xref ref-type="bibr" rid="bib37">Kriegeskorte and Kievit, 2013</xref>). A significant body of research addressing this issue has focused on the characterization of category structure and other coarse object distinctions, such as animacy. Findings from this research indicate that activity patterns in a large expanse of the ventral visual stream (VVS), often referred to as inferotemporal (IT) cortex or ventral temporal cortex, capture much of this structure in the environment (<xref ref-type="bibr" rid="bib35">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Mur et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Cichy et al., 2019</xref>). For example, numerous studies have revealed a similarity-based clustering of response patterns for objects in IT that is tied to category membership (e.g., <xref ref-type="bibr" rid="bib35">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib55">Proklova et al., 2016</xref>; see <xref ref-type="bibr" rid="bib28">Grill-Spector and Weiner, 2014</xref> for review). In these studies, and in most related work, the primary focus has been on similarity in relation to object distinctions that are defined in objective terms, and on the characterization of neural representations that is shared by observers. As such, they do not address whether activity patterns in IT also capture similarity relationships among objects that characterize subjective aspects of visual perception that may vary across individuals’ reports. When neural activity that corresponds to subjectively perceived visual similarities has been examined, extant research has mostly focussed on specific object features, such as shape or size (<xref ref-type="bibr" rid="bib52">Op de Beeck et al., 2008</xref>; <xref ref-type="bibr" rid="bib30">Haushofer et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Schwarzkopf et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Moutsiana et al., 2016</xref>) rather than on similarities between complex real-world objects that differ from each other on multiple dimensions. A notable exception to this research trend is an fMRI study that focussed on perceived similarities among select real-world objects that are personally meaningful (e.g., images of observers’ own car, their own bicycle; <xref ref-type="bibr" rid="bib12">Charest et al., 2014</xref>), which demonstrated links between observer-specific perceived similarity and the similarity structure embedded in activity patterns in IT.</p><p>In order to reveal the mapping between fine-grained perceived visual similarities among category exemplars and similarities in neural activity, it may not be sufficient to focus on activity in IT but critical to consider VVS regions situated downstream on the medial surface of the temporal lobe. Perirhinal cortex (PrC), and the primary region to which it projects, that is, lateral entorhinal cortex, are of particular interest when such perceived similarity relationships concern entire objects. The representational–hierarchical model of object processing in the VVS asserts that structures in the medial temporal lobe constitute the apex of its processing hierarchy (<xref ref-type="bibr" rid="bib50">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib10">Bussey and Saksida, 2007</xref>; <xref ref-type="bibr" rid="bib19">Cowell et al., 2010</xref>; <xref ref-type="bibr" rid="bib33">Kent et al., 2016</xref>). It proposes that there is a progressive integration of object features along the VVS that allows for increasing differentiation in the representation of objects from posterior to more anterior regions in the service of perception as well as other cognitive domains (e.g., recognition memory). Visual objects are thought to be represented in PrC in their most highly integrated form based on complex feature conjunctions (<xref ref-type="bibr" rid="bib50">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib8">Buckley and Gaffan, 2006</xref>; <xref ref-type="bibr" rid="bib10">Bussey and Saksida, 2007</xref>; <xref ref-type="bibr" rid="bib26">Graham et al., 2010</xref>; <xref ref-type="bibr" rid="bib33">Kent et al., 2016</xref>), which support perceptual discrimination of even highly similar exemplars from the same object category (e.g., <xref ref-type="bibr" rid="bib54">O’Neil et al., 2013</xref>; <xref ref-type="bibr" rid="bib53">O’Neil et al., 2009</xref>). Lateral entorhinal cortex (or its human homologue anterolateral entorhinal cortex [alErC; <xref ref-type="bibr" rid="bib40">Maass et al., 2015</xref>; <xref ref-type="bibr" rid="bib65">Yeung et al., 2017</xref>]) has been suggested to extend this role in visual object discrimination through integration with additional spatial features (<xref ref-type="bibr" rid="bib18">Connor and Knierim, 2017</xref>; <xref ref-type="bibr" rid="bib65">Yeung et al., 2017</xref>). Taken together, these properties make PrC and alErC ideally suited for providing the read-out for subjective reports of perceived visual similarity among the subordinate exemplars of object categories. Although more posterior VVS regions may also predict perceived similarity, the representational–hierarchical model asserts that they do not provide the same level of differentiation as regions at the apex in the medial temporal lobe. Moreover, they may also not capture those aspects of perceived similarity structure that are observer specific.</p><p>There is some evidence from human lesion studies suggesting that the integrity of regions in the medial temporal lobe is critical for perceiving the similarity among complex objects when it is high and objects cannot be easily discriminated from each other. This evidence comes from experiments that used variants of the oddity-discrimination task. In this task, participants view multiple objects (a minimum of three) on a computer screen and are asked to report the item that is least similar to the others. A finding documented in multiple reports (<xref ref-type="bibr" rid="bib7">Buckley et al., 2001</xref>; <xref ref-type="bibr" rid="bib2">Barense et al., 2007</xref>; <xref ref-type="bibr" rid="bib3">Bartko et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Inhoff et al., 2019</xref>; cf., <xref ref-type="bibr" rid="bib62">Stark and Squire, 2000</xref>; <xref ref-type="bibr" rid="bib39">Levy et al., 2005</xref>; <xref ref-type="bibr" rid="bib29">Hales and Clark, 2015</xref>) is that patients with medial temporal lobe lesions that include PrC and alErC show deficits in oddity-discrimination tasks when complex objects with substantial feature overlap are compared, but not when oddity judgements require comparison of simple visual stimuli that can be distinguished based on a single feature such as colour or luminance. While the results of this lesion research have critically informed theoretical arguments that PrC plays a role in perceptual discrimination of objects (see <xref ref-type="bibr" rid="bib5">Bonnen et al., 2021</xref>, for a recent computationally focused review), it is important to note that they do not provide a characterization of similarity structure of neural representations in PrC and alErC, nor a characterization of the transformation of representations from more posterior VVS regions to these regions in the medial temporal lobe. Moreover, lesion findings do not speak to whether neural representations in the medial temporal lobe capture the perceived similarity structure that is unique to individual observers.</p><p>In the current fMRI study, we tested the idea that the visual similarity structure among exemplars of real-world categories that observers subjectively perceive is predicted with higher precision by the similarity structure of neural activity in PrC and alErC than in the more posterior VVS regions traditionally considered in neuroscience investigations of visual object perception, including IT. During scanning, we administered a novel experimental task that required visual discrimination between consecutively presented exemplars from real-world categories as well as categorization (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). In addition, we obtained ratings of perceived visual similarities among these exemplars from each observer offline (see <xref ref-type="fig" rid="fig2">Figure 2</xref>), as well as estimates of similarity derived from an influential computational model that describes objects at their intermediate visual feature level (HMAX, <xref ref-type="bibr" rid="bib56">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib11">Cadieu et al., 2007</xref>; <xref ref-type="bibr" rid="bib60">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>). At the behavioural level, we found that discrimination performance was highly sensitive to fine-grained perceived similarity among exemplars, including those aspects that were observer specific. Representational similarity analyses (RSAs) of ultra-high-resolution fMRI data revealed, in line with our hypotheses, that activation patterns in PrC and alErC predict this fine-grained structure within categories with higher precision than any other VVS region, and that they predict even those aspects of similarity structure that are unique to individual observers.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Perceived visual similarity ratings obtained offline with inverse multidimensional scaling (iMDS).</title><p>(<bold>A</bold>) Task required placement of all exemplars from each category in circular arena, with distances reflecting perceived visual similarity. Arrows indicate the six pairwise distances used to compute representational dissimilarity matrix (RDM). (<bold>B</bold>) Behaviour-based RDM computed using dissimilarity (1 − Pearson’s <italic>r</italic>) and conversion to percentiles for individual observers. Only values below diagonal were included. Six distances (between four exemplars) per category were rank ordered and grouped into three levels of similarity (low, middle, and high; for more detail, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). (<bold>C</bold>) Intersubject correlations for perceived similarity ratings across all exemplars and categories. Correlations were computed between each participant’s RDM with the mean RDM (excluding the participant). Red horizontal line marks mean intersubject correlation, with variability in perceived visual similarity structure across observers reflected in the range displayed.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Stimuli employed and behavioural data obtained for specific object categories.</title><p>(<bold>A</bold>) Forty images of object stimuli sampled from <xref ref-type="bibr" rid="bib44">Migo et al., 2013</xref> and employed in current study. Images of objects are from 10 categories with 4 exemplars in each category. Feature dimensions on which exemplars differ include external shape envelope, texture, and internal spatial configuration of parts. (<bold>B</bold>) Behavioural results from inverse multidimensional scaling (iMDS) task obtained offline from all 24 participants in the fMRI study (mean in thick red line). Perceived similarity ratings (expressed as dissimilarity percentile) were obtained separately for each category. They are displayed for all pairwise comparisons (with numbers indicating specific exemplars as shown in A) in arbitrary order along <italic>x</italic>-axis. Ratings reveal some variability across participants in all categories, which is also reflected in intersubject correlations shown in <xref ref-type="fig" rid="fig2">Figure 2c</xref> in Main text. (<bold>C</bold>) Ratings from (<bold>B</bold>) reordered according to participant’s own rank ordering of perceived similarity. We grouped the two lowest, two middle, and two highest similarities in observer-specific manner. Shown dissimilarity percentiles reflect average values for two pairs of ratings in each grouped ranking. Notably, the range of dissimilarity percentile values was comparable across the three levels (p &gt; 0.5). Displayed rank-ordered data were employed to assess sensitivity of behaviour and fMRI responses to participants’ own perceived similarity structure among category exemplars.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>In a follow-up study, a separate group of 30 participants completed 2 sessions of the inverse multidimensional scaling (iMDS) task for the 10 object categories separated by 7 ± 1 days.</title><p>(<bold>A</bold>) Intrasubject correlations for perceived similarity ratings across all exemplars and categories. Correlations were computed between each participants’ perceived similarity representational dissimilarity matrices (RDMs) from Session 1 and from Session 2. The mean within-subject correlation across the two sessions was 0.84, indicating high stability of participant’s perceived similarity ratings over 1 week. (<bold>B</bold>) Intersubject correlations for perceived similarity ratings across all exemplars and categories. Correlations were computed between each participant’s RDM Session 1 with the mean RDM (excluding the participant) in Session 2. Mean intersubject correlation was 0.68. Critically, a paired <italic>t</italic>-test (intrasubject &gt; intersubject correlation) confirmed that intrasubject correlations were significantly higher than intersubject correlations (p &lt; 0.0001). This pattern of results indicates that the perceived similarity structure that is unique to the individual observer is a stable characteristic.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig1-figsupp2-v2.tif"/></fig></fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>fMRI task: Category-Exemplar 1-Back Task.</title><p>(<bold>A</bold>) Images of objects depicting one of the 4 exemplars from 10 different categories were presented. Participants indicated repetitions on catch trials with two different button presses depending on whether the image was an exact repeat of the one previously presented (same exemplar, same category) or a repeat at the category level (different exemplar, same category). The majority of trials (75%) reflected no repetitions on either level and required no response. Only trials that required no response (noncatch trials) were included in the fMRI analyses. (<bold>B</bold>) Percentage of errors that reflect incorrect same-exemplar responses on same-category trials as a function of perceived similarity (mean slope indicated with thick red line; for accuracy on all other trial types see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Own values reflect behavioural performance as a function of participants’ own visual similarity ratings; Other values reflect performance based on other participants’ ratings (for a total of 22 iterations, which were then averaged). Error rate increased with increasing similarity as reflected in slopes (in black/grey ***p &lt; 0.0001). Error rate was more sensitive to participants’ own ratings as reflected in significantly higher slopes for the Own versus Other ratings (in green ***p &lt; 0.0001). (<bold>C</bold>) Response times on correct same-category trials as a function of perceived similarity (mean slope indicated with thick red line). Own and Other values calculated as in (<bold>B</bold>). Response times increased with increasing similarity and were more sensitive to participants’ own than other participants’ ratings. Results in (<bold>B, C</bold>) show that behavioural performance on 1-back task is most sensitive to perceived similarity as reflected in participants’ own ratings.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig2-v2.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Perceived visual similarity structure among exemplars varies across observers</title><p>We used inverse multidimensional scaling (iMDS, <xref ref-type="bibr" rid="bib36">Kriegeskorte and Mur, 2012</xref>) to create participant-specific models of perceived visual similarity for 4 exemplars from 10 different categories (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Specifically, participants were instructed to arrange images of objects in a circular arena by placing those they perceived to be more visually similar closer together, and those they perceived to be less visually similar farther apart. Participants completed these arrangements offline, that is, outside of the scanner, in two phases, with the first phase involving sorting of the full set of 40 objects in a single arrangement (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1–</xref> <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The second phase required sorting of exemplars within categories in 10 separate arrangements (one per category; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Given our interest in representations that capture fine-grained object similarities within categories, our fMRI analyses relied on the similarity structures computed based on sorting in this second phase. The pairwise distances between all exemplars within each category were used to create a behaviour-based (i.e., subjective-report) representational dissimilarity matrix (RDM; <xref ref-type="fig" rid="fig1">Figure 1B</xref>), which included a split of the range of similarities into three levels for sensitivity analyses in behaviour and neural activation patterns (see Methods for further detail). Examination of intersubject correlations of each participant’s RDM and the mean of all other participants’ RDMs (excluding their own) revealed a mean value of <italic>r</italic> = 0.69 (<italic>t</italic>-test <italic>r</italic> &gt; 0: p &lt; 0.001). When we calculated an RDM for ratings averaged across participants, and compared it with an RDM derived from a computational model developed to capture objects at their intermediate visual feature level (HMAX, <xref ref-type="bibr" rid="bib56">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib11">Cadieu et al., 2007</xref>; <xref ref-type="bibr" rid="bib60">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>), we also found a significant correlation (<italic>r</italic> = 0.25, p &lt; 0.001), suggesting that average ratings capture a shared component in the subjective ratings that relate to objective image characteristics. Follow-up analyses that focussed on this relationship in different subranges of similarity among exemplars (low, medium, and high; see <xref ref-type="fig" rid="fig1">Figure 1B</xref>) revealed, however, that the estimates derived from the HMAX model were only significantly correlated with average ratings at low and medium levels (low <italic>r</italic> = 0.45; p &lt; 0.001; medium <italic>r</italic> = 0.24; p &lt; 0.001; high <italic>r</italic> = 0.05; p &gt; 0.05). This pattern suggests that at their finest grain, judgements of similarity within categories rely on integrated object representations that (1) are not captured by intermediate feature-level descriptions, and (2) are observer specific. Indeed, the range of intersubject correlations in reported similarity (calculated across exemplars and categories) that was present in our sample of participants provides direct evidence for variability across observers (<italic>r</italic> = 0.54–0.81; see <xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for data on individual categories). There are also hints that this variability is most pronounced at the finest grain of judgements, as reflected in reduced intersubject correlations for high similarity exemplars (high <italic>r</italic> = 0.64; medium <italic>r</italic> = 0.65, low <italic>r</italic> = 0.72; high &lt; low; <italic>t</italic>(22) = 3.41; p &lt; 0.001). Critically, in a separate behavioural experiment conducted in another sample of participants with multiple assessments, we found that individual differences in perceived visual similarity structure among exemplars are temporally stable observer characteristics (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). We leveraged this variability across observers in subsequent analyses for assessing the precision of mapping in our fMRI results, examining whether the structure of neural representations in PrC and alErC also predict the perceived similarity structure that is unique to individual observers.</p></sec><sec id="s2-2"><title>Behavioural discrimination performance during scanning is sensitive to observers’ perceived visual similarity structure</title><p>Participants underwent ultra-high-resolution fMRI scanning while completing a novel Category-Exemplar 1-Back Task designed to tax fine-grained visual object discrimination (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>). This task required responding to two different types of repetition, namely repetition of same exemplars or of different exemplars from the same category, across consecutive trials. Participants were asked to provide a button-press response when they noticed repetitions, with different buttons for each type of repetition. On all other trials, participants were not required to provide a response. Importantly, this task was designed to ensure that participants engaged in categorization, while also discriminating between exemplars within categories. Performance on the Category-Exemplar 1-Back Task was sensitive to perceived visual similarity between exemplars as reflected in observers’ offline ratings and formalized in the behavioural RDMs with 3 different levels of similarity (<xref ref-type="fig" rid="fig2">Figure 2B,C</xref>; see also <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Specifically, response errors on same category trials increased with increasing visual similarity (significant linear slope; <italic>t</italic>(22) = 18.35, p &lt; 0.0001). Moreover, response times for correct responses on same category trials were positively correlated with perceived visual similarity level (<italic>t</italic>(22) = 13.47, p &lt; 0.0001). Critically, task performance was also sensitive to the unique perceived similarity structure within categories expressed by observers. When we compared the influence of participant’s own similarity ratings with that of others on behaviour (<xref ref-type="fig" rid="fig2">Figure 2B,C</xref>), we found a significantly larger positive slope in error rate (<italic>t</italic>(22) = 8.30; p &lt; 0.0001) and in response times (<italic>t</italic>(22) = 9.68; p &lt; 0.0001) for participants’ own ratings. This pattern of behaviour suggests that perceived visual similarity between exemplars influenced participants’ discrimination performance during scanning, and, that it did so in an observer-specific manner.</p></sec><sec id="s2-3"><title>Patterns in multiple VVS regions predict perceived visual similarity structure among exemplars</title><p>To investigate whether the similarities between activation patterns in PrC and downstream alErC predict perceived similarities between exemplars in observers’ reports, we employed anatomically defined regions of interest (ROIs) and created participant-specific models of neural similarity among category exemplars on the no-response trials. In order to examine the anatomical specificity of our findings, we also created such models for ROIs in other VVS and medial temporal lobe regions. Specifically, these ROIs included early visual cortex (EVC), lateral occipital cortex (LOC), posteromedial entorhinal cortex (pmErC), parahippocampal cortex (PhC), and temporal pole (TP) for comparison (see Figure 5 for visualization; note that LOC and PhC have typically been included in ROI definitions of IT in prior work; e.g., <xref ref-type="bibr" rid="bib12">Charest et al., 2014</xref>; for direct comparison of results in IT and LOC, see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Pairwise dissimilarities of neural patterns were employed to compute the brain-based RDMs (<xref ref-type="fig" rid="fig3">Figure 3A,B</xref>); Pearson’s correlations were calculated so as to examine whether these RDM’s predicted participants’ own behaviour-based RDMs that were derived from their offline reports of perceived similarity (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Our analyses revealed that neural activation patterns in PrC and alErC did indeed correlate with participants’ perceived visual similarity RDMs (Bonferroni-corrected p &lt; 0.01). Patterns in other regions of the VVS (EVC, p &lt; 0.003; LOC, p &lt; 0.002) were also significantly correlated with these behaviour-based RDMs. Critically, patterns in regions previously implicated in scene processing, specifically PhC and pmErC (<xref ref-type="bibr" rid="bib58">Schultz et al., 2015</xref>; <xref ref-type="bibr" rid="bib40">Maass et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Schröder et al., 2015</xref>), did not predict the perceived similarity structure for objects (all p &gt; 0.5). Having established that activity patterns in multiple VVS regions predict perceived visual similarities between exemplars, we followed up on this finding by asking whether PrC and alErC capture these similarities with higher fidelity than earlier regions (EVC and LOC). Towards this end, we next examined whether these regions predict similarity structure even when exemplars only differ from each other in subtle ways.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Brain-based representational dissimilarity matrices (RDMs) and their relationship to perceived visual similarity.</title><p>(<bold>A</bold>) In each region of interest (ROI), mean multivoxel activation patterns were calculated for every exemplar using the no-response trials in the Category-Exemplar 1-Back Task. Pairwise pattern dissimilarities were computed as 1 − Pearson’s <italic>r</italic>. (<bold>B</bold>) Pairwise pattern dissimilarity percentiles were used to create participant-specific brain-based RDMs. (<bold>C</bold>) Brain-based RDMs were correlated with participants’ own behaviour-based similarity RDMs derived from their offline reports (black double arrows = within subject <italic>r</italic>). (<bold>D</bold>) Activation patterns in EVC, LOC, PrC, and alErC show significant correlations with participants’ own perceived similarity ratings of objects (brain-based RDM × behaviour-based perceived similarity RDM within subjects; *p &lt; 0.05, **p &lt; 0.01 Bonferroni-corrected based on regions; error bars represent SEM). EVC = early visual cortex; LOC = lateral occipital complex; PrC = perirhinal cortex; alErC = anterolateral entorhinal cortex; pmErC = posteromedial entorhinal cortex; PhC = parahippocampal cortex; TP = temporal pole; see <xref ref-type="fig" rid="fig5">Figure 5A</xref> for visualization.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Temporal signal-to-noise ratio in perirhinal cortex (PrC; <italic>M</italic> = 12.25, SD = 2.68) and anterolateral entorhinal cortex (alErC; <italic>M</italic> = 10.80, SD = 3.05) in each of the 25 participants (average denoted by bolded red dot).</title><p>Green arrows indicate two participants with tSNR in alErC below 2 SD of the mean. These participants were excluded from fMRI analyses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig3-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-4"><title>PrC and alErC are the only regions whose patterns predict perceived visual similarity structure among exemplars when similarity is high</title><p>In this set of analyses, we examined the mapping between perceived similarity structure and neural activity patterns at a more fine-grained level within categories. Given prior evidence that PrC allows for the disambiguation of highly similar objects (<xref ref-type="bibr" rid="bib50">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib8">Buckley and Gaffan, 2006</xref>; <xref ref-type="bibr" rid="bib10">Bussey and Saksida, 2007</xref>; <xref ref-type="bibr" rid="bib26">Graham et al., 2010</xref>; <xref ref-type="bibr" rid="bib33">Kent et al., 2016</xref>), we anticipated that patterns in PrC, and possibly downstream alErC, would represent even the most subtle visual differences that observers perceive among exemplars, whereas earlier VVS regions would not. To address this issue with RSA in our stimulus set, participants’ behaviour RDMs, which were based on six pairwise distances between exemplars in each category, were divided into the three levels of similarity (low, medium, and high; see <xref ref-type="fig" rid="fig1">Figure 1B</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for range comparison). Recall that our behavioural analyses revealed, as previously described, that discrimination performance on the Category-Exemplar 1-Back Task was highly sensitive to these different levels of similarity. <xref ref-type="fig" rid="fig4">Figure 4</xref> displays the results of our level-specific fMRI analyses, which were conducted for those regions whose activity patterns showed significant correlations with participants’ full perceived visual similarity space between exemplars (as shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Correlations between perceived similarity structure and activity patterns in PrC and alErC were significant at all three levels of similarity (Bonferroni-corrected p &lt; 0.01; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). In contrast, correlations for activity patterns in posterior VVS regions were significant only at the lowest level of perceived similarity (Bonferroni-corrected p &lt; 0.01). This pattern of results cannot be attributed to differences in stability of activity patterns across levels of similarity for different regions; supplementary analyses revealed that stability was significant for all regions and did not interact with level of similarity (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Direct comparison between regions also revealed that correlations in PrC and alErC were higher than in LOC and EVC at medium and high levels of perceived similarity (Bonferroni-corrected p &lt; 0.05; <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Relationship between brain representational dissimilarity matrices (RDMs) and reports at different levels of perceived visual similarity for region of interests (ROIs) showing significant effects in <xref ref-type="fig" rid="fig3">Figure 3D</xref>.</title><p>(<bold>A</bold>) Correlation of brain-based RDM and participants’ own behaviour-based RDM at low, medium, and high levels of similarity. Only activation patterns perirhinal cortex (PrC) and anterolateral entorhinal cortex (alErC) show significant correlation with ratings at middle and high levels of perceived similarity (**p &lt; 0.01, *p &lt; 0.05, Bonferroni-corrected for regions and levels). Correlations in PrC and alErC were significantly larger than those in early visual cortex (EVC) and lateral occipital cortex (LOC) at the medium and high levels of perceived similarity (horizontal lines indicate p &lt; 0.05). (<bold>B</bold>) Box and whisker plots for classification accuracy of neural activation patterns at each level of perceived similarity in different ROIs. We adopted a common classification approach using linear support vector classifier and leave-one-run-out cross-validation (<xref ref-type="bibr" rid="bib46">Misaki et al., 2010</xref>). Results from one tailed <italic>t</italic>-tests to probe whether classifier performance was above chance indicate that patterns in LOC are distinguishable only at lowest level of perceived similarity within categories. PrC and alErC are only regions in which patterns are distinguishable at medium and high levels of perceived similarity (*p &lt; 0.05, Bonferroni-corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Relationship between brain-based representational dissimilarity matrices (RDMs) in even and odd runs, within and between participants at different levels of perceived similarity.</title><p>Correlation of brain-based RDMs for even and odd runs at low, medium, and high levels of similarity. Activation patterns in all region of interests (ROIs) show significant correlations at all levels of perceived similarity (***p &lt; 0.001, Bonferroni-corrected for regions and levels).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig4-figsupp1-v2.tif"/></fig></fig-group><p>The described results suggest that object representations in posterior VVS regions may not have sufficient fidelity to allow for differentiation of exemplars that were perceived to be highly similar by observers, and that were most difficult to discriminate on our Category-Exemplar 1-Back Task. We followed up on this idea with complementary classification analyses of our fMRI data using a linear support vector machine (<xref ref-type="bibr" rid="bib48">Mur et al., 2009</xref>). These analyses were conducted so as to examine in which VVS regions activity patterns associated with exemplars of high perceived similarity would be sufficiently separable so as to allow for classification as distinct items. They confirmed that activity patterns associated with specific exemplars can indeed be successfully classified in PrC and alErC at higher levels of perceived similarity than in posterior VVS regions (<xref ref-type="fig" rid="fig4">Figure 4B</xref> for further detail).</p><p>The analyses presented so far only focused on specific regions of interest. To answer the question of whether PrC and alErC are the only regions whose activity patterns predict perceived visual similarity among exemplars at its highest level, we also conducted whole-volume searchlight-based RSA. As expected based on our ROI analyses, patterns in PrC and alErC, as well as in earlier VVS regions, showed a predictive relationship in these searchlight analyses when the full range of perceived visual similarity values among exemplars was considered; no regions in the scanned brain volume outside of the VVS exhibited this predictive relationship (threshold-free cluster enhancement [TFCE] corrected p &lt; 0.05; <xref ref-type="fig" rid="fig5">Figure 5C,D</xref>). Critically, our searchlight analyses revealed that PrC and alErC were indeed the only regions in the entire scanned brain volume whose patterns correlated with observer’s reports when similarity between exemplars was perceived to be high, and objects were most difficult to discriminate on the Category-Exemplar 1-Back Task (TFCE-corrected p &lt; 0.05; <xref ref-type="fig" rid="fig5">Figure 5E,F</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Visualization of region of interests (ROIs) and results from whole-volume searchlight analyses.</title><p>(<bold>A, B</bold>) Visual depiction of ROIs. Early visual cortex (EVC = green), lateral occipital complex (LOC = cyan), perirhinal cortex (PrC = pink), parahippocampal cortex (PhC = orange), anterolateral entorhinal cortex (alErC = blue), posteromedial entorhinal cortex (pmErC = yellow), temporal pole (TP = purple). (<bold>C, D</bold>) Cortical regions revealed with searchlight analysis in which brain-based representational dissimilarity matrices (RDMs) were significantly correlated with behaviour-based perceived similarity RDMs across full range. Significant correlations were observed in PrC, alErC, and more posterior ventral visual stream (VVS) regions. (<bold>E, F</bold>) Cortical regions revealed with searchlight analysis in which brain-based RDMs were significantly correlated with behavioural RDMs at highest level of perceived similarity. Significant correlations were observed only in PrC and alErC. Maps are displayed with corrected statistical threshold of p &lt; 0.05 at cluster level (using threshold-free cluster enhancement).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig5-v2.tif"/></fig></sec><sec id="s2-5"><title>Patterns in posterior VVS regions predict similarity structure that is shared by observers and tied to object characteristics at the intermediate feature level</title><p>Because our behavioural results point to shared as well as observer-specific components in the perceived similarity structure that participants reported, we conducted several additional sets of analyses, aiming to address which VVS regions would show activity patterns that might predict these different components. In this context, we considered that the perceived similarity structure among exemplars that is shared across observers for low and medium levels is tied to objective image characteristics at the intermediate feature level, as reflected in the significant correlation between averaged similarity ratings and estimates derived from the HMAX model in our data (see Behavioural results). Given the assertion of the representational hierarchical model that objects are represented in PrC and alErC based on complex feature conjunctions (rather than intermediate features), we predicted that the shared component of perceived similarity estimates of the HMAX model would predict activity patterns only in VVS regions posterior to PrC and alErC. Indeed, we found a significant predictive relationship of activity patterns in regions EVC and LOC (Bonferroni-corrected p &lt; 0.01) but not in PrC and alErC (all Bonferroni-corrected p &gt; 0.05; see <xref ref-type="fig" rid="fig6">Figure 6A,B</xref>) for average perceived similarity. This significant relationship in posterior VVS regions was also only present at low (for EVC and LOC) and medium levels of similarity (for LOC; see <xref ref-type="fig" rid="fig6">Figure 6B</xref>). Analyses that directly employed the estimates of similarity between object images obtained from the computational HMAX model (<xref ref-type="bibr" rid="bib56">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib11">Cadieu et al., 2007</xref>; <xref ref-type="bibr" rid="bib60">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>) revealed strikingly similar findings (see <xref ref-type="fig" rid="fig6">Figure 6B vs D</xref>). Activity patterns in EVC and LOC showed a significant correlation with the HMAX model at low and medium but not at the highest level of similarity; Bonferroni-corrected p &lt; 0.01. Critically, activity patterns in PrC and alErC did not correlate with estimates from the HMAX model nor with average ratings at any level of object similarity (all Bonferroni-corrected p &gt; 0.05; see <xref ref-type="fig" rid="fig6">Figure 6C,D</xref>). Together, these results suggest that activity in VVS regions posterior to PrC and alErC capture the components of perceived visual similarity structure among exemplars that is shared by observers and that is closely related to object features at the intermediate feature level. At the same time, these neural representations in posterior VVS regions do not appear to allow for differentiation of exemplars at high levels of perceived similarity that tend to be observer specific.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Brain-based representational dissimilarity matrices (RDMs) and their relationship to average perceived visual similarity.</title><p>Brain-based RDMs were correlated with (<bold>A</bold>) the average behaviour-based similarity RDMs and (<bold>B</bold>) the different levels of average similarity RDMs; and with (<bold>C</bold>) the entire RDM derived from the HMAX model (<bold>D</bold>) the RDMs at different levels of similarity derived from the HMAX model. Patterns in EVC and LOC show relationship to average whole perceived similarity ratings **p &lt; 0.01, Bonferroni-corrected based on regions. EVC and LOC also show correlations to the average low, and LOC to the medium level of perceived similarity *p &lt; 0.05, Bonferroni-corrected. EVC = early visual cortex; LOC = lateral occipital complex; PrC = perirhinal cortex; alErC = anterolateral entorhinal cortex; pmErC = posteromedial entorhinal cortex; PhC = parahippocampal cortex; TP = temporal pole.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig6-v2.tif"/></fig></sec><sec id="s2-6"><title>Patterns in PrC and alErC predict fine-grained perceived visual similarity structure among exemplars in an observer-specific manner</title><p>In the final set of analyses, we directly focussed on the variability across participants’ reports of similarity in order to address whether activity patterns in PrC and alErC predict even those perceived similarities with high precision that are unique to individual observers. We reasoned that if neural patterns in a region represent the observer’s <italic>unique</italic> perceived similarity structure, brain–behaviour correlations should be higher when calculated within rather than between participants (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, black versus grey arrows; see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for similar results revealed with a multiple regression approach). In other words, if there are observer-specific relationships, activity patterns should predict participants’ own perceived similarity structure better than someone else’s. Such analyses would reveal that interindividual differences are not only present in the perceptual reports of observers and in their discrimination performance, as demonstrated in our behavioural analyses, but also in corresponding neural representations in PrC and alErC. Indeed, initial analyses of our fMRI data demonstrated the presence of stable observer-specific activity patterns for the object exemplars probed in our study in all regions of interest (see <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). The <italic>i</italic>-index introduced by <xref ref-type="bibr" rid="bib12">Charest et al., 2014</xref>, which directly measures differences in correlations for the same (i.e., own) versus other observers, allowed us to examine which of these observer-specific activation patterns predict observer-specific structure in reports of perceived similarities. These analyses confirmed our expectation that the neural activation patterns in PrC and alErC predict observer-specific perceived visual similarity structure (Bonferroni-corrected p[within-participant <italic>r</italic> &gt; between-participant <italic>r</italic>] &lt; 0.01). In contrast, activation patterns in posterior VVS regions, EVC and LOC (Bonferroni-corrected p[within-participant <italic>r</italic> &gt; between-participant <italic>r</italic>] &gt; 0.05) did not uniquely predict participants’ own perceived similarity structure (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Not surprisingly, regions that did not predict participants’ perceived similarity structure at all (pmErC, PhC, and TP; <xref ref-type="fig" rid="fig3">Figure 3D</xref>), also did not have significantly above zero <italic>i</italic>-indices (Bonferroni-corrected p[within-participant <italic>r</italic> &gt; between-participant <italic>r</italic>] &gt; 0.05). Critically, PrC and alErC showed significant brain–behaviour <italic>i</italic>-indices even when we restricted analyses to fine-grained differentiation, that is, to the subrange of high levels of perceived similarity (Bonferroni-corrected p[within-participant <italic>r</italic> &gt; between-participant <italic>r</italic>] &lt; 0.05). Taken together, these results reveal that activity patterns in PrC and alErC even predict perceived similarities that are unique to individual observers, which are most prevalent in fine-grained structure.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Brain-based representational dissimilarity matrices (RDMs) and their relationship to observer-specific perceived visual similarity.</title><p>(<bold>A</bold>) Brain-based RDMs were correlated with (1) participants’ own behaviour-based similarity RDMs (black double arrows = within subject <italic>r</italic>) and (2) other participants’ behaviour-based similarity RDMs (grey arrows = between subject <italic>r</italic>) for comparison. (<bold>B</bold>) Patterns in PrC and alErC show relationship to perceived similarity ratings that are observer specific as reflected in brain–behaviour <italic>i</italic>-index (i.e., within minus between subject correlation; *p &lt; 0.05, **p &lt; 0.01, Bonferroni-corrected based on regions, with testing against a null distribution created by randomizing subject labels; error bars represent standard error of the mean [SEM] estimated based on randomization). PrC and alErC also show significant higher <italic>i</italic>-index than other regions as indicated with horizontal lines; *p &lt; 0.05, Bonferroni-corrected. (<bold>C</bold>) Only patterns in PrC and alErC show relationship observer-specific perceived similarity ratings at the middle and high levels of perceived similarity; (*p &lt; 0.05, Bonferroni-corrected for regions and levels). Correlations in PrC and alErC were significantly larger than those in EVC and LOC at the medium and high levels of perceived similarity (horizontal lines indicate p &lt; 0.05). EVC = early visual cortex; LOC = lateral occipital complex; PrC = perirhinal cortex; alErC = anterolateral entorhinal cortex; pmErC = posteromedial entorhinal cortex; PhC = parahippocampal cortex; TP = temporal pole; see <xref ref-type="fig" rid="fig5">Figure 5a</xref> for visualization.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Comparison between inferotemporal (IT) cortex and lateral occipital complex (LOC): brain-based representational dissimilarity matrices (RDMs) and their relationship to perceived visual similarity (<bold>A</bold>) at all levels of perceived similarity (**p &lt; 0.01, *p &lt; 0.05) and (<bold>B</bold>) corresponding <italic>i</italic>-index; and (<bold>C</bold>) at different levels of perceived similarity and (<bold>D</bold>) corresponding <italic>i</italic>-index (*p &lt; 0.05).</title><p>These four analyses from the main experiment were included here to compare results in IT cortex with those in LOC. IT cortex is a large swath of cortex that extends across occipital and temporal cortex. This region of interest (similar to the one used in <xref ref-type="bibr" rid="bib12">Charest et al., 2014</xref>) overlaps with regions of interest used in the main experiment including LOC, PhC, and TP. We were specifically interested in LOC – the object selective portion of larger IT cortex – because our stimuli were objects. The results across these four sets of analyses were highly similar between the two regions, with the only noticeable exception that LOC activity patterns predict observer-specific perceived similarity at low levels of similarity while IT patterns do not predict any observer-specific similarity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Relationship between brain-based representational dissimilarity matrices (RDMs) in even and odd runs, within and between participants.</title><p>(<bold>A</bold>) Pairwise pattern dissimilarity percentiles were used to create observer-specific brain-based RDMs for even and odd runs. (<bold>B</bold>) Brain-based RDMs from even runs were correlated with (1) observers’ brain-based RDMs for odd runs (black double arrows = within subject <italic>r</italic>) and (2) other observers’ brain-based RDMs from odd runs (grey arrows = between subject <italic>r</italic>) for comparison. (<bold>C</bold>) Patterns in all region of interests (ROIs) show stable similarity structure as reflected in significant correlations between odd and even run; ***p &lt; 0.001, Bonferroni-corrected based on regions; error bars represent standard error of the mean (SEM). (<bold>D</bold>) Patterns in all ROIs are unique to each individual as reflected in significant brain–brain <italic>i</italic>-index (i.e., within minus between subject correlation; ***p &lt; 0.001, Bonferroni-corrected based on regions, with testing against a null distribution created by randomizing subject labels; error bars represent SEM estimated based on randomization). EVC = early visual cortex; LOC = lateral occipital complex; PrC = perirhinal cortex; alErC = anterolateral entorhinal cortex; pmErC = posteromedial entorhinal cortex; PhC = parahippocampal cortex; TP = temporal pole.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66884-fig7-figsupp2-v2.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Vision neuroscience has made great strides in understanding the hierarchical organization of object representations along the VVS. How VVS representations capture fine-grained differences between objects that observers subjectively perceive has received limited examination so far. In the current study, we addressed this question by focussing on perceived similarities among exemplars of real-world categories. Using a novel Category-Exemplar 1-Back Task, we found that visual discrimination performance is highly sensitive to the visual similarity structure that is reflected in observers’ subjective reports. Combining this task with fMRI scanning at ultra-high-resolution allowed us to show, in line with our general hypotheses, that activity patterns in PrC and alErC predict perceived visual similarities among exemplars with higher precision than any other VVS region, including prediction of those aspects of similarity structure that are unique to individual observers.</p><p>Research that has aimed to characterize the nature of object representations in human PrC with fMRI has shown that the degree of feature overlap between objects is captured by activation patterns in this region. Such a relationship has been revealed in multiple task contexts, with images of real-world objects and with words denoting such objects; moreover, it has been observed for feature overlap at the perceptual as well as the semantic level (<xref ref-type="bibr" rid="bib15">Clarke and Tyler, 2014</xref>; <xref ref-type="bibr" rid="bib21">Erez et al., 2016</xref>; <xref ref-type="bibr" rid="bib6">Bruffaerts et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Martin et al., 2018a</xref>). These findings, in combination with work from neurophysiology in nonhuman animals and from computational modelling, have been interpreted to suggest that PrC integrates features of objects with complex conjunctive coding into representations of whole objects, and that the resulting conjunctive representations allow for differentiation of objects even when they are highly similar due to a high degree of feature overlap (<xref ref-type="bibr" rid="bib9">Bussey et al., 2002</xref>; <xref ref-type="bibr" rid="bib50">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib19">Cowell et al., 2010</xref>). Indeed, it is this type of conjunctive coding that has motivated the central notion of the representational–hierarchical model of VVS organization that PrC, together with alErC (<xref ref-type="bibr" rid="bib18">Connor and Knierim, 2017</xref>; <xref ref-type="bibr" rid="bib65">Yeung et al., 2017</xref>), can be considered the pinnacle of the VVS object-processing hierarchy. Using a metric that was rooted in participants’ subjective reports of perceived visual similarity and that showed a direct relationship to behavioural discrimination performance, the current fMRI findings provide new support for this hierarchical model by revealing increased differentiation of subordinate category exemplars in PrC and alErC, as compared to more posterior VVS regions.</p><p>Findings from lesion studies conducted with oddity-discrimination tasks support the idea that medial temporal lobe structures downstream from IT play a critical role in processes required for the appreciation of fine-grained visual similarities between complex real-world objects that are expressed in perceptual reports (see <xref ref-type="bibr" rid="bib5">Bonnen et al., 2021</xref>, for review). Numerous studies conducted in humans and in other species have shown that performance on such tasks relies on the integrity of PrC when objects with high visual feature overlap must be judged (<xref ref-type="bibr" rid="bib7">Buckley et al., 2001</xref>; <xref ref-type="bibr" rid="bib2">Barense et al., 2007</xref>; <xref ref-type="bibr" rid="bib3">Bartko et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Inhoff et al., 2019</xref>; cf., <xref ref-type="bibr" rid="bib62">Stark and Squire, 2000</xref>; <xref ref-type="bibr" rid="bib39">Levy et al., 2005</xref>; <xref ref-type="bibr" rid="bib29">Hales and Clark, 2015</xref>). For example, <xref ref-type="bibr" rid="bib2">Barense et al., 2007</xref> compared performance on multiple visual oddity tasks between individuals with lesions in the medial temporal lobe that largely spared PrC and ErC, versus individuals with more widespread damage in the medial temporal lobes that included PrC and ErC. Most notably, individuals in the latter but not in the former group showed impairments in identifying the odd-one out item in sets of images of real-world objects that shared a high number of overlapping visual features. While the results of these prior lesion studies are compatible with the conclusions we draw in the current study, they do not allow for characterization of similarity structure of neural representations in PrC and alErC, and their direct comparison with representations in other medial temporal and posterior VVS regions, as provided here.</p><p>The anatomical specificity of our fMRI findings in the medial temporal lobe is striking. While activity patterns that reflected the similarity structure among category exemplars were present in PrC and alErC, they were absent in medial temporal regions that have previously been implicated in visual discrimination of scenes, specifically pmErC and PhC cortex (see <xref ref-type="bibr" rid="bib58">Schultz et al., 2015</xref>, for review). This specificity is noteworthy in light of documented differences in functional connectivity between these regions that have been linked to object versus scene processing, with PrC being connected to alErC, and PhC being connected to pmErC, respectively (<xref ref-type="bibr" rid="bib40">Maass et al., 2015</xref>; see <xref ref-type="bibr" rid="bib57">Schröder et al., 2015</xref> for more broadly distributed differences in functional connectivity between ErC subregions and other cortical structures).</p><p>The higher precision we observed for the representation of perceived similarity relationships in the medial temporal lobe, as compared to more posterior VVS regions, is of particular theoretical interest for the representational–hierarchical model of VVS organization (<xref ref-type="bibr" rid="bib50">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib10">Bussey and Saksida, 2007</xref>; <xref ref-type="bibr" rid="bib19">Cowell et al., 2010</xref>; <xref ref-type="bibr" rid="bib33">Kent et al., 2016</xref>). Most revealing, in this context, is the comparison between PrC and ErC versus LOC, a region that is part of the large swath of cortex that is often referred to as IT in neuroimaging research and that has been linked to processing of object shape in many prior fMRI studies (e.g., <xref ref-type="bibr" rid="bib27">Grill-Spector et al., 2001</xref>; <xref ref-type="bibr" rid="bib35">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Mur et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Cichy et al., 2019</xref>). While activity patterns in LOC predicted some aspects of similarity structure among subordinate category exemplars in the current work, this relationship was observed at a coarser grain than in PrC and alErC; it only held when perceived visual similarity was low, and when performance in behavioural discrimination revealed that objects were easily distinguishable. Indeed, complementary pattern classification analyses revealed that activity patterns associated with exemplars of high perceived similarity were not sufficiently separable in LOC so as to allow for classification as distinct items. By contrast, this classification could be successfully performed based on activity patterns in PrC and alErC. Indeed, our searchlight analyses showed that these two regions in the medial temporal lobe were the only ones in which activity was related to perceived visual similarity at a fine-grained level.</p><p>Our analyses of the relationship between activity patterns and aspects of perceived similarity among exemplars that are tied to objective image characteristics, as estimated by the computational HMAX model, offer support for the central claim of the representational–hierarchical model that the transformation of object representations from IT (specifically, LOC) to medial temporal-lobe structures involves further integration. Notably, the component of perceived similarity structure that was shared by observers showed a statistical relationship to the estimates of the HMAX model, which describes objects at the intermediate feature level and which has been linked to LOC representations in prior work (<xref ref-type="bibr" rid="bib56">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib11">Cadieu et al., 2007</xref>; <xref ref-type="bibr" rid="bib60">Serre et al., 2007</xref>; but see <xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>, <xref ref-type="bibr" rid="bib38">Kubilius et al., 2016</xref> for limitations as compared to deep convolutional neural network models). In the present study, we also found that HMAX estimates of the similarity among the exemplars we employed were correlated with activity patterns in LOC, but only at low and medium levels of similarity. The fine-grained similarity structure among exemplars that led to the largest number of confusion errors in behavioural discrimination on the Category-Exemplar 1-Back Task during scanning was predominantly observer specific, and this fine-grained observer-specific structure was solely predicted by activity patterns in PrC and alErC. The coding of objects in PrC and alErC as fully integrated entities based on complex feature conjunctions arguably affords the flexibility that is required to capture the fine-grained differentiation among exemplars that characterizes the perception of individual observers. .</p><p>Our study was not designed to directly address what factors might drive the variability in perceived similarity structure across observers that were present in their subjective reports, their discrimination performance, and in corresponding activity patterns in PrC and alErC. Prior evidence from fMRI research on neural representations in other VVS regions suggests that past experience and object familiarity may play an important role. <xref ref-type="bibr" rid="bib12">Charest et al., 2014</xref> revealed observer-specific effects in the similarity structure of activity patterns in IT that were tied to participants' reports for highly familiar real-world objects with unique personal meaning (e.g., images of observers’ own car, their own bicycle). Notably, this observer-specific mapping between similarity in activity patterns and reports was not present for unfamiliar objects. There is also evidence from behavioural training studies indicating that prior experience with categories has an impact on how the similarity among its exemplars is perceived. In a recent study by <xref ref-type="bibr" rid="bib16">Collins and Behrmann, 2020</xref>, for example, it was shown that just a few days of repeated exposure can lead to increased differentiation among exemplars, and that these changes are most pronounced at the level of fine-grained similarity structure when observers have had some prior experience with the category in question. This change in similarity structure based on training occurred in the absence of any apparent opportunity to gain new sematic knowledge about the exemplars in question, suggesting it could reflect an increase in perceptual expertise. Indeed in recent behavioural research from our laboratory, we have found that the degree of self-reported exposure to real-world object categories, but not corresponding semantic knowledge, predicts observers’ perceived visual similarity structure among exemplars, and that this relationship is most notable at the level of fine-grained similarity structure (<xref ref-type="bibr" rid="bib45">Minos et al., 2021</xref>). It is possible that the observer specificity in fine-grained differentiation among exemplars that was predicted by activity patterns in PrC and alErC in the current study is tied to similar factors at work; it may reflect interindividual differences in perceptual expertise across observers and categories. Although speculative at present, such an account would be in line with a large body of evidence revealing learning-related plasticity in object representations in these medial temporal lobe structures (see <xref ref-type="bibr" rid="bib1">Banks et al., 2014</xref>, for review). This account can be directly tested with training paradigms that target specific real-world object categories in future fMRI research. Regardless of the outcome of such future research, the current findings highlight the critical value of probing the subjective appreciation of visual object similarities, and their variability across observers, for a complete understanding of the transformation of neural representations from posterior regions to those at the apex of the VVS.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>A total of 29 participants completed the perceived similarity iMDS arrangement task and fMRI experiment (12 females; age range = 18–35 years old; mean age = 24.2 years). All participants were right handed, fluent in English, and had no known history of psychiatric or neurological disorders. Three participants were removed due to excessive head motion above the cutoff of 0.8 mm of framewise displacement, one participant weas removed due to behavioural performance accuracy 2 SD below the average on the fMRI task, and two participants were removed due to poor signal quality in medial temporal-lobe regions, i.e., a temporal signal-to-noise ratio 2 SD below average (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> <xref ref-type="fig" rid="fig1">Figure 1</xref>). Therefore, 23 participants were included in the final analyses. All participants gave informed consent, were debriefed, and received monetary compensation upon completion of the experiment. This study was conducted with Western’s Human Research Ethics Board approval.</p></sec><sec id="s4-2"><title>Stimuli</title><p>In order to investigate object representations at the exemplar level, we selected stimuli with varying normative levels of perceived visual similarity from the Migo Normative Database (<xref ref-type="bibr" rid="bib44">Migo et al., 2013</xref>). We used 40 greyscale images of objects from 10 categories (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Each category in our set was made up of four exemplars that all shared the same name (e.g., apple, lipstick, and stapler). Based on findings from a pilot study (<italic>n</italic> = 40), only stimuli with perceived similarity ratings similar to those from the normative database were selected.</p></sec><sec id="s4-3"><title>Modelling of perceived visual similarity structure</title><p>In order to obtain observer-specific models of perceived visual similarity structure for our stimuli, participants provided reports of perceived similarity between all stimuli on a computer outside of the scanner prior to scanning. Participants were seated in front of a monitor and completed a modified version of the iMDS task (<xref ref-type="bibr" rid="bib36">Kriegeskorte and Mur, 2012</xref>). Specifically, participants were asked to drag-and-drop images into a white circle (i.e., arena), and arrange them according to perceived visual similarity (<xref ref-type="bibr" rid="bib36">Kriegeskorte and Mur, 2012</xref>; see <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Objects perceived to be more visually similar were to be placed closer together, and objects perceived to be less visually similar were to be placed further apart. The iMDS task consisted of two phases. In the first phase, participants arranged all 40 stimuli according to perceived visual similarity. In the second phase, participants completed 10 category-specific trials in which they sorted 4 exemplars from the same category according to their perceived visual similarity. They were instructed to use the entire space within the circle, and make sure they compared each stimulus to every other stimulus. Only data from the second phase were considerd in the current analyses. A MATLAB-based toolbox was used to calculate distances between each pair of exemplars, and to convert these distances to dissimilarity percentiles (<xref ref-type="bibr" rid="bib36">Kriegeskorte and Mur, 2012</xref>). These dissimilarity percentiles were then used to create observer-specific behaviour-based RDMs that represented each observers’ perceived similarity space at the exemplar level. The behaviour-based RDMs for the entire range included 6 dissimilarity percentiles for each of the 10 categories (i.e., 6 × 10 = 60 dissimilarity percentiles).</p><p>These behaviour-based RDMs, which captured the full range of perceived similarity, were used to create three behaviour-based RDMs to reflect three levels of perceived similarity (low, medium, and high). The six pairwise distances (expressed as dissimilarity percentile) per category were sorted into the two largest, two medium, and two smallest dissimilarities to create behaviour-based RDMs for low, medium, and high perceived visual similarity, respectively. These behaviour-based RDMs for each of the levels included 2 dissimilarity percentiles for each of the 10 categories (i.e., 2 × 10 = 20 dissimilarity percentiles). In order to ensure that the different levels of perceived similarity were nonoverlapping, any values that did not allow for at least 0.1 dissimilarity percentile between each of the successive levels (i.e., high-middle and middle-low) was excluded. The range of dissimilarity percentiles did not differ significantly between the different levels of perceived similarity (p &gt; 0.05; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>).</p></sec><sec id="s4-4"><title>Modelling of objective visual similarity: HMAX model</title><p>We obtained estimates of similarities that were derived from a computational model, HMAX (<xref ref-type="bibr" rid="bib56">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib11">Cadieu et al., 2007</xref>; <xref ref-type="bibr" rid="bib60">Serre et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>), developed to describe objects at their intermediate visual feature level, including shape. In this biologically inspired model, simple cell layers, akin to V1 cells, detect local orientation using Gabor filters. These orientation signals are pooled in complex cell layers to extract global features. In this way, HMAX is designed as a four-layer hierarchical feed-forward structure similar to that previously described in the VVS. The output layer of HMAX captures an object’s shape over activation patterns and has been shown to correspond to activation patterns in IT cortex and LOC (<xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>). We used Matlab implementation of HMAX (<ext-link ext-link-type="uri" xlink:href="https://maxlab.neuro.georgetown.edu/hmax.html">https://maxlab.neuro.georgetown.edu/hmax.html</ext-link>) to extract the activations from the C2 layer of the model and compute RDM between 4 exemplars from the 10 categories used in the current study.</p></sec><sec id="s4-5"><title>Category-Exemplar 1-Back Task</title><p>For the main experiment, participants completed a variation of a 1-back task, coined the ‘Category-Exemplar 1-Back’ in the 3T scanner. We created this new 1-back task to ensure that participants were attending carefully to each individual object, given our interest in fine-grained object discrimination. As in a classic 1-back task, participants were shown a stream of individual objects and asked to indicate with a button press when the object was an exact repeat of the object previous to it, and no response was required when the object was from a different category as the one previous (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Our novel twist was the addition of a second response option, whereby participants were asked to indicate with a different button when the object was from the same category as the previous one, but a different exemplar. The two response trial types served as catch trials to ensure participants’ attention focussed on differences between objects across consecutive trials, and to assess behavioural performance. These modifications of the classic 1-back task were introduced to ensure that participants considered category membership and engaged in object processing at the exemplar level. Successful identification of the repetition of different exemplars from the same category could not be based on local low-level features, such as changes in luminance, texture, or shape across consecutive trials. Participants used their right index and middle finger to respond, with response assignment counterbalanced across participants. Of the three trial types—same exemplar, same category, different category—only the no-response trials (i.e., different category) were used in the fMRI analysis to avoid motor confounds associated with button presses. By extension, none of the trials considered for assessment of similarity in activation patterns were immediate neighbours.</p><p>Participants completed a total of eight functional runs that each lasted 4 min (stimulus duration = 1.2 s, intertrial interval = 1 s). Run order was counterbalanced across participants. Within each run, each of the 40 exemplars were presented 3 times as no-response trials, and once as a catch trial, for a total of 24 presentations on no-response trials and 8 catch trials (excluded from fMRI analyses) per exemplar across the entire experiment. Prior to scanning, each participant completed a 5-min practice task with images from categories not included in the functional scanning experiment.</p></sec><sec id="s4-6"><title>fMRI data acquisition</title><p>MRI data were acquired using a 3T MR system (Siemens). A 32-channel head coil was used. Before the fMRI session, a whole head MP-RAGE volume (TE = 2.28 ms, TR = 2400 ms, TI = 1060 ms, resolution = 0.8 × 0.8 × 0.8 mm isometric) was acquired. This was followed by four fMRI runs, each with 300 volumes, which consisted of 42 T2*-weighted slices with a resolution of 1.7 × 1.7 mm (TE = 30 ms, TR = 1000 ms, slice thickness 1.7 mm, FOV 200 mm, parallel imaging with grappa factor 2). T2*-weighted data were collected at this ultra-high resolution so as to optimize differentiation of BOLD signal in anterolateral versus posterior medial entorhinal cortex. The T2* slices were acquired in odd-even interleaved fashion in the anterior to posterior direction. Subsequently, a T2-weighted image (TE = 564 ms, TR = 3200 ms, resolution 0.8 × 0.8 × 0.8 mm isometric) was acquired. Finally, participants then completed four more fMRI runs. Total duration of MRI acquisition was approximately 60 min.</p></sec><sec id="s4-7"><title>Preprocessing and modelling</title><p>MRI data were converted to brain imaging data structure (<xref ref-type="bibr" rid="bib25">Gorgolewski et al., 2016</xref>) and ran through fmriprep-v1.1.8 (<xref ref-type="bibr" rid="bib22">Esteban et al., 2019</xref>). This preprocessing included motion correction, slice time correction, susceptibility distortion correction, registration from EPI to T1w image, and confounds estimated (e.g., tCompCor, aCompCor, and framewise displacement). Component based noise correction was performed using anatomical and temporal CompCor, aCompCor, and tCompCor, by adding these confound estimates as regressors in SPM12 during a first-level general linear model (GLM) (<xref ref-type="bibr" rid="bib4">Behzadi et al., 2007</xref>). Each participant was coregistered to the participant-specific T1w image by fmriprep. First-level analyses were conducted in native space for each participant with no spatial smoothing to preserve ultra-high-resolution patterns of activity for multivariate pattern analyses (MVPA). Exemplar-specific multivoxel activity patterns were estimated in 40 separate general linear models using the mean activity of the no-response trials across runs.</p></sec><sec id="s4-8"><title>ROI definitions for fMRI analyses</title><p>Anatomical regions of interest were defined using multiple techniques. Automated segmentation was employed to delineate PrC, ErC, and PhC (ASHS; <xref ref-type="bibr" rid="bib64">Wisse et al., 2016</xref>). We manually segmented each ERC obtained from ASHS into alErC and pmErC following a protocol developed by <xref ref-type="bibr" rid="bib65">Yeung et al., 2017</xref>, which is derived from a functional connectivity study (<xref ref-type="bibr" rid="bib40">Maass et al., 2015</xref>). A probabilistic atlas was used to define EVC (<xref ref-type="bibr" rid="bib63">Wang et al., 2015</xref>) and TP (<xref ref-type="bibr" rid="bib23">Fischl, 2012</xref>). A functional localizer was used to define LOC as the contiguous voxels located along the lateral extent of the occipital lobe that responded more strongly to intact objects than scrambled objects (p &lt; 0.01, uncorrected; <xref ref-type="bibr" rid="bib55">Proklova et al., 2016</xref>).</p><p>In the VVS, we focussed on lateral occipital complex and the TP as they have previously been linked to object processing (e.g., <xref ref-type="bibr" rid="bib27">Grill-Spector et al., 2001</xref>; <xref ref-type="bibr" rid="bib42">Martin et al., 2018b</xref>), as well as EVC. In the medial temporal lobe (MTL), we included ROIs for the posteromedial ErC and PhC, both of which have been linked to scene processing (e.g., <xref ref-type="bibr" rid="bib40">Maass et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Schröder et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Schultz et al., 2015</xref>; <xref ref-type="bibr" rid="bib20">Epstein and Baker, 2019</xref>).</p></sec><sec id="s4-9"><title>RSAs of fMRI data</title><p>For each ROI, linear correlation distances (Pearson’s <italic>r</italic>) were calculated between all pairs of exemplar-specific multivoxel patterns using CoSMoMVPA toolbox in Matlab (<xref ref-type="bibr" rid="bib51">Oosterhof et al., 2016</xref>) across all voxels. These correlations were used to create participant-specific brain-based RDMs (1 − Pearson’s <italic>r</italic>), which capture the unique neural pattern dissimilarities between all exemplars within each category (<italic>n</italic> = 10), within each region (<italic>n</italic> = 8).</p><p>Whole-volume RSA were conducted using surface-based searchlight analysis (<xref ref-type="bibr" rid="bib35">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib51">Oosterhof et al., 2016</xref>; <xref ref-type="bibr" rid="bib41">Martin et al., 2018a</xref>). Specifically, we defined a 100-voxel neighbourhood around each surface voxel, and computed a brain-based RDM within this region, analogous to the ROI-based RSA. This searchlight was swept across the entire cortical surface (<xref ref-type="bibr" rid="bib35">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib51">Oosterhof et al., 2016</xref>). First, the entire perceived similarity RDM for all within category ratings was compared to each searchlight. These brain–behaviour correlations were Fisher transformed and mapped to the centre of each searchlight for each participant separately. Participant-specific similarity maps were then standardized and group-level statistical analysis was performed. TFCE was used to correct for multiple comparisons with a cluster threshold of p &lt; 0.05 (<xref ref-type="bibr" rid="bib61">Smith and Nichols, 2009</xref>).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Formal analysis, Methodology, Software</p></fn><fn fn-type="con" id="con5"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Data curation, Funding acquisition, Methodology, Software, Supervision</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Formal analysis, Funding acquisition, Methodology, Project administration, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study was approved by the Institutional Review Board at the University of Western Ontario (REB # 0442). Informed consent was obtained from each participant before the experiment, including consent to publish anonymized results.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Behavioural performance on Category-Exemplar 1-Back Task.</title><p>Proportion of correct responses for each trial type are indicated in green.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-66884-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Multiple linear regression: brain RDM ~ (own RDM + average RDM).</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-66884-supp2-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-66884-transrepform1-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analyzed during this study are included in the manuscript and supporting fields. Source data files have been provided for Figures 1, 2, 3, 4, 6,7.</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by a Canadian Institutes for Health Research Project Grant (CIHR Grant # 366062) to AK and SK, a Brain Canada Platform Support Grant, and the Canada First Research Excellence Fund. KF was funded through a Natural Sciences and Engineering Research Council doctoral Canadian Graduate Scholarship (NSERC CGS-D) and an Ontario Graduate Scholarship (OGS). AB was funded through an Ontario Trillium Scholarship for Doctoral study in Canada. We thank Dr. Marieke Mur for her generous help with use of the iMDS task and related Matlab programs.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banks</surname><given-names>PJ</given-names></name><name><surname>Warburton</surname><given-names>EC</given-names></name><name><surname>Brown</surname><given-names>MW</given-names></name><name><surname>Bashir</surname><given-names>ZI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mechanisms of synaptic plasticity and recognition memory in the perirhinal cortex</article-title><source>Progress in Molecular Biology and Translational Science</source><volume>122</volume><fpage>193</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-420170-5.00007-6</pub-id><pub-id pub-id-type="pmid">24484702</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname><given-names>MD</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Graham</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The human medial temporal lobe processes online representations of complex objects</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>2963</fpage><lpage>2974</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.05.023</pub-id><pub-id pub-id-type="pmid">17658561</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartko</surname><given-names>SJ</given-names></name><name><surname>Winters</surname><given-names>BD</given-names></name><name><surname>Cowell</surname><given-names>RA</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Perceptual functions of perirhinal cortex in rats: zero-delay object recognition and simultaneous oddity discriminations</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>2548</fpage><lpage>2559</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5171-06.2007</pub-id><pub-id pub-id-type="pmid">17344392</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Y</given-names></name><name><surname>Restom</surname><given-names>K</given-names></name><name><surname>Liau</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title><source>NeuroImage</source><volume>37</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>T</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>When the ventral visual stream is not enough: A deep learning account of medial temporal lobe involvement in perception</article-title><source>Neuron</source><volume>109</volume><fpage>2755</fpage><lpage>2766</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.018</pub-id><pub-id pub-id-type="pmid">34265252</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruffaerts</surname><given-names>R</given-names></name><name><surname>Dupont</surname><given-names>P</given-names></name><name><surname>Peeters</surname><given-names>R</given-names></name><name><surname>De Deyne</surname><given-names>S</given-names></name><name><surname>Storms</surname><given-names>G</given-names></name><name><surname>Vandenberghe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Similarity of fMRI activity patterns in left perirhinal cortex reflects semantic similarity between words</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>18597</fpage><lpage>18607</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1548-13.2013</pub-id><pub-id pub-id-type="pmid">24259581</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Booth</surname><given-names>MC</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Selective perceptual impairments after perirhinal cortex ablation</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>9824</fpage><lpage>9836</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-24-09824.2001</pub-id><pub-id pub-id-type="pmid">11739590</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Extent of MTL lesions in animals and human patients</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.01.008</pub-id><pub-id pub-id-type="pmid">16469525</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Perirhinal cortex resolves feature ambiguity in complex visual discriminations</article-title><source>European Journal of Neuroscience</source><volume>15</volume><fpage>365</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1046/j.0953-816x.2001.01851.x</pub-id><pub-id pub-id-type="pmid">11849302</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Memory, perception, and the ventral visual‐perirhinal‐hippocampal stream: thinking outside of the boxes</article-title><source>Hippocampus</source><volume>17</volume><fpage>898</fpage><lpage>908</lpage><pub-id pub-id-type="doi">10.1002/hipo.20320</pub-id><pub-id pub-id-type="pmid">17636546</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>C</given-names></name><name><surname>Kouh</surname><given-names>M</given-names></name><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A model of V4 shape selectivity and invariance</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1733</fpage><lpage>1750</lpage><pub-id pub-id-type="doi">10.1152/jn.01265.2006</pub-id><pub-id pub-id-type="pmid">17596412</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name><name><surname>Schmitz</surname><given-names>TW</given-names></name><name><surname>Deca</surname><given-names>D</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Unique semantic space in the brain of each beholder predicts perceived similarity</article-title><source>PNAS</source><volume>111</volume><fpage>14565</fpage><lpage>14570</lpage><pub-id pub-id-type="doi">10.1073/pnas.1402594111</pub-id><pub-id pub-id-type="pmid">25246586</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The brain of the beholder: honouring individual representational idiosyncrasies</article-title><source>Language, Cognition and Neuroscience</source><volume>30</volume><fpage>367</fpage><lpage>379</lpage><pub-id pub-id-type="doi">10.1080/23273798.2014.1002505</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>van den Bosch</surname><given-names>JJF</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The spatiotemporal neural dynamics underlying perceived similarity for real-world objects</article-title><source>NeuroImage</source><volume>194</volume><fpage>12</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.031</pub-id><pub-id pub-id-type="pmid">30894333</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Object-specific semantic coding in human perirhinal cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4766</fpage><lpage>4775</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2828-13.2014</pub-id><pub-id pub-id-type="pmid">24695697</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>E</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Exemplar learning reveals the representational origins of expert category perception</article-title><source>PNAS</source><volume>117</volume><fpage>11167</fpage><lpage>11177</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912734117</pub-id><pub-id pub-id-type="pmid">32366664</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Gors</surname><given-names>J</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Wu</surname><given-names>Y-C</given-names></name><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The representation of biological classes in the human brain</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2608</fpage><lpage>2618</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5547-11.2012</pub-id><pub-id pub-id-type="pmid">22357845</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connor</surname><given-names>CE</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integration of objects and space in perception and memory</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1493</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1038/nn.4657</pub-id><pub-id pub-id-type="pmid">29073645</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowell</surname><given-names>RA</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Components of recognition memory: dissociable cognitive processes or just differences in representational complexity</article-title><source>Hippocampus</source><volume>20</volume><fpage>1245</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1002/hipo.20865</pub-id><pub-id pub-id-type="pmid">20882548</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>RA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Scene perception in the human brain</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>373</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014809</pub-id><pub-id pub-id-type="pmid">31226012</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erez</surname><given-names>J</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Kendall</surname><given-names>W</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Conjunctive coding of complex object features</article-title><source>Cerebral Cortex (New York, N.Y</source><volume>26</volume><fpage>2271</fpage><lpage>2282</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv081</pub-id><pub-id pub-id-type="pmid">25921583</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goldstone</surname><given-names>RL</given-names></name><name><surname>Son</surname><given-names>JY</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Similarity</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199734689.013.0010</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Auer</surname><given-names>T</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name><name><surname>Craddock</surname><given-names>RC</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Duff</surname><given-names>EP</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Glatard</surname><given-names>T</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Handwerker</surname><given-names>DA</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Keator</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Michael</surname><given-names>Z</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>Nichols</surname><given-names>BN</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Pellman</surname><given-names>J</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Schaefer</surname><given-names>G</given-names></name><name><surname>Sochat</surname><given-names>V</given-names></name><name><surname>Triplett</surname><given-names>W</given-names></name><name><surname>Turner</surname><given-names>JA</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title><source>Scientific Data</source><volume>3</volume><elocation-id>160044</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id><pub-id pub-id-type="pmid">27326542</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>KS</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name><name><surname>Lee</surname><given-names>ACH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Going beyond LTM in the MTL: a synthesis of neuropsychological and neuroimaging findings on the role of the medial temporal lobe in memory and perception</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>831</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2010.01.001</pub-id><pub-id pub-id-type="pmid">20074580</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The lateral occipital complex and its role in object recognition</article-title><source>Vision Research</source><volume>41</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hales</surname><given-names>JB</given-names></name><name><surname>Clark</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Organization and structure of brain systems supporting memory</chapter-title><person-group person-group-type="editor"><name><surname>Hales</surname><given-names>JB</given-names></name></person-group><source>In The Maze Book</source><publisher-name>Humana Press</publisher-name><fpage>143</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-2159-1_6</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haushofer</surname><given-names>J</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multivariate patterns in object-selective cortex dissociate perceptual and physical shape similarity</article-title><source>PLOS Biology</source><volume>6</volume><elocation-id>e187</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060187</pub-id><pub-id pub-id-type="pmid">18666833</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</article-title><source>Nature Human Behaviour</source><volume>4</volume><fpage>1173</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00951-3</pub-id><pub-id pub-id-type="pmid">33046861</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inhoff</surname><given-names>MC</given-names></name><name><surname>Heusser</surname><given-names>AC</given-names></name><name><surname>Tambini</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>O’Neil</surname><given-names>EB</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name><name><surname>Meager</surname><given-names>MR</given-names></name><name><surname>Blackmon</surname><given-names>K</given-names></name><name><surname>Vazquez</surname><given-names>B</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Understanding perirhinal contributions to perception and memory: Evidence through the lens of selective perirhinal damage</article-title><source>Neuropsychologia</source><volume>124</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.12.020</pub-id><pub-id pub-id-type="pmid">30594569</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kent</surname><given-names>BA</given-names></name><name><surname>Hvoslef-Eide</surname><given-names>M</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The representational–hierarchical view of pattern separation: Not just hippocampus, not just space, not just memory</article-title><source>Neurobiology of Learning and Memory</source><volume>129</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2016.01.006</pub-id><pub-id pub-id-type="pmid">26836403</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>245</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00245</pub-id><pub-id pub-id-type="pmid">22848204</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id><pub-id pub-id-type="pmid">23876494</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep neural networks as a computational model for human shape sensitivity</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004896</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id><pub-id pub-id-type="pmid">27124699</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>DA</given-names></name><name><surname>Shrager</surname><given-names>Y</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Intact visual discrimination of complex and feature-ambiguous stimuli in the absence of perirhinal cortex</article-title><source>Learning &amp; Memory (Cold Spring Harbor, N.Y.)</source><volume>12</volume><fpage>61</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1101/lm.84405</pub-id><pub-id pub-id-type="pmid">15647593</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>A</given-names></name><name><surname>Berron</surname><given-names>D</given-names></name><name><surname>Libby</surname><given-names>LA</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Düzel</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional subregions of the human entorhinal cortex</article-title><source>eLife</source><volume>4</volume><elocation-id>e06426</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06426</pub-id><pub-id pub-id-type="pmid">26052749</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Douglas</surname><given-names>D</given-names></name><name><surname>Newsome</surname><given-names>RN</given-names></name><name><surname>Man</surname><given-names>LL</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title><source>eLife</source><volume>7</volume><elocation-id>e31873</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31873</pub-id><pub-id pub-id-type="pmid">29393853</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Sullivan</surname><given-names>JA</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>How landmark suitability shapes recognition memory signals for objects in the medial temporal lobes</article-title><source>NeuroImage</source><volume>166</volume><fpage>425</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.004</pub-id><pub-id pub-id-type="pmid">29108942</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medin</surname><given-names>DL</given-names></name><name><surname>Goldstone</surname><given-names>RL</given-names></name><name><surname>Gentner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Respects for similarity</article-title><source>Psychological Review</source><volume>100</volume><fpage>254</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.100.2.254</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Migo</surname><given-names>EM</given-names></name><name><surname>Montaldi</surname><given-names>D</given-names></name><name><surname>Mayes</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A visual object stimulus database with standardized similarity information</article-title><source>Behavior Research Methods</source><volume>45</volume><fpage>344</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.3758/s13428-012-0255-4</pub-id><pub-id pub-id-type="pmid">23055161</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minos</surname><given-names>AN</given-names></name><name><surname>Ferko</surname><given-names>KM</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Expertise predicts perceived visual similarity between exemplars of real-world object categories</article-title><source>Journal of Vision</source><volume>21</volume><elocation-id>2985</elocation-id><pub-id pub-id-type="doi">10.1167/jov.21.9.2985</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Misaki</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title><source>NeuroImage</source><volume>53</volume><fpage>103</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.051</pub-id><pub-id pub-id-type="pmid">20580933</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moutsiana</surname><given-names>C</given-names></name><name><surname>de Haas</surname><given-names>B</given-names></name><name><surname>Papageorgiou</surname><given-names>A</given-names></name><name><surname>van Dijk</surname><given-names>JA</given-names></name><name><surname>Balraj</surname><given-names>A</given-names></name><name><surname>Greenwood</surname><given-names>JA</given-names></name><name><surname>Schwarzkopf</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical idiosyncrasies predict the perception of object size</article-title><source>Nature Communications</source><volume>7</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/ncomms12110</pub-id><pub-id pub-id-type="pmid">27357864</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Revealing representational content with pattern-information fMRI—an introductory guide</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>4</volume><fpage>101</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1093/scan/nsn044</pub-id><pub-id pub-id-type="pmid">19151374</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Meys</surname><given-names>M</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>128</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00128</pub-id><pub-id pub-id-type="pmid">23525516</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Perceptual–mnemonic functions of the perirhinal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>142</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(99)01303-0</pub-id><pub-id pub-id-type="pmid">10322468</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in Matlab/GNU Octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Torfs</surname><given-names>K</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perceived shape similarity among unfamiliar objects and the organization of the human object vision pathway</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>10111</fpage><lpage>10123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2511-08.2008</pub-id><pub-id pub-id-type="pmid">18829969</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Neil</surname><given-names>EB</given-names></name><name><surname>Cate</surname><given-names>AD</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perirhinal cortex contributes to accuracy in recognition memory and perceptual discriminations</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>8329</fpage><lpage>8334</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0374-09.2009</pub-id><pub-id pub-id-type="pmid">19571124</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Neil</surname><given-names>EB</given-names></name><name><surname>Barkley</surname><given-names>VA</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational demands modulate involvement of perirhinal cortex in face processing</article-title><source>Hippocampus</source><volume>23</volume><fpage>592</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1002/hipo.22117</pub-id><pub-id pub-id-type="pmid">23460411</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: The animate–inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schröder</surname><given-names>TN</given-names></name><name><surname>Haak</surname><given-names>KV</given-names></name><name><surname>Jimenez</surname><given-names>NIZ</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional topography of the human entorhinal cortex</article-title><source>eLife</source><volume>4</volume><elocation-id>e06738</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06738.001</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>H</given-names></name><name><surname>Sommer</surname><given-names>T</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The role of the human entorhinal cortex in a representational account of memory</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>628</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00628</pub-id><pub-id pub-id-type="pmid">26635581</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarzkopf</surname><given-names>DS</given-names></name><name><surname>Song</surname><given-names>C</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The surface area of human V1 predicts the subjective experience of object size</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>28</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1038/nn.2706</pub-id><pub-id pub-id-type="pmid">21131954</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id><pub-id pub-id-type="pmid">17404214</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stark</surname><given-names>CE</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Intact visual perceptual discrimination in humans in the absence of perirhinal cortex</article-title><source>Learning &amp; Memory (Cold Spring Harbor, N.Y.)</source><volume>7</volume><fpage>273</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1101/lm.35000</pub-id><pub-id pub-id-type="pmid">11040258</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Mruczek</surname><given-names>REB</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex (New York, N.Y</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wisse</surname><given-names>LEM</given-names></name><name><surname>Kuijf</surname><given-names>HJ</given-names></name><name><surname>Honingh</surname><given-names>AM</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Pluta</surname><given-names>JB</given-names></name><name><surname>Das</surname><given-names>SR</given-names></name><name><surname>Wolk</surname><given-names>DA</given-names></name><name><surname>Zwanenburg</surname><given-names>JJM</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Geerlings</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Automated hippocampal subfield segmentation at 7T MRI</article-title><source>AJNR. American Journal of Neuroradiology</source><volume>37</volume><fpage>1050</fpage><lpage>1057</lpage><pub-id pub-id-type="doi">10.3174/ajnr.A4659</pub-id><pub-id pub-id-type="pmid">26846925</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname><given-names>L-K</given-names></name><name><surname>Olsen</surname><given-names>RK</given-names></name><name><surname>Bild-Enkin</surname><given-names>HEP</given-names></name><name><surname>D’Angelo</surname><given-names>MC</given-names></name><name><surname>Kacollja</surname><given-names>A</given-names></name><name><surname>McQuiggan</surname><given-names>DA</given-names></name><name><surname>Keshabyan</surname><given-names>A</given-names></name><name><surname>Ryan</surname><given-names>JD</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Anterolateral entorhinal cortex volume predicted by altered intra-item configural processing</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>5527</fpage><lpage>5538</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3664-16.2017</pub-id><pub-id pub-id-type="pmid">28473640</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66884.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.01.21.427602" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.01.21.427602"/></front-stub><body><p>Your response has been thorough and thoughtful and we believe this work now represents an important advancement to our understanding of the contributions of anterior temporal lobe regions in visual representations. Your approach affords tremendous specificity in the conclusions one can draw about the relationship between visual similarity and neural similarity along this ventral visual pathway and highlights perirhinal cortex as a potential key region whose neural representational structure relates to subjective behavior.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66884.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Davis</surname><given-names>Simon W</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00py81415</institution-id><institution>Duke University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.01.21.427602">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.01.21.427602v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Activity in perirhinal and entorhinal cortex predicts observer-specific perceived visual similarities between objects&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Christian Büchel as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Simon W Davis (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Both reviewers agreed that the value of the results was clear, the methods are methodologically sound but that there are some theoretical or analytical ambiguities that need to be addressed.</p><p>The work is an important extension of prior work (particularly Charest et al., 2014), that includes adjacent brain areas, most notably the medial temporal lobe. However, the reviewers and I agreed that the theoretical basis and contributions of the results could use some further highlighting to underscore what new insights are gleaned from this work. In particular, we found the results in Figure 4 to be the most unique and important of this work but did not feel the Introduction set up the paper to appreciate this.</p><p>It was suggested that further analyses be conducted to boost the strength of the paper to resolve ambiguities about the subjective similarity measure to better understand what underlies these idiosyncratic representations (visual information? semantic information? familiarity?), and what these results suggest about the perirhinal cortex. Does this challenge or support any pre-existing notions about the perirhinal cortex, or the nature of visual / memory representations in the brain?</p><p>Specifically, please report representational dissimilarity matrices (RDMs) to identify what portions of the variance are idiosyncratic vs. shared across the group and attempt to relate the brain RDMs to visually-based RDMs of the stimuli. It was recognized that the semantic factor is much more constrained (given the use of a carefully controlled stimulus set) and is a strength of the current manuscript.</p><p>Other important potential additions for discussion are outlined in the individual reviewer comments.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Some of my struggle with the scope of the work may be in how it's framed in the introduction. The intro discusses the open question of how the brain represents similarities in objects, but then that question is never answered. It's also not clear to me what the key novelty is of the study beyond prior work, and how that changes our understanding of the human visual system. I'm also not sure why real-world objects are an interesting set or novelty (versus other types of stimuli we'd expect to have more idiosyncratic representations).</p><p>I also have the question of whether the offline object arrangement task could have influenced the similarity structure reflected in the brain.</p><p>Also, most of the analyses looked at correlations between individuals' brain and behavior to look at &quot;perceived visual similarity&quot;. However, the behavioral measure includes both what we can consider the group-shared visual similarity (the mean, perhaps even capturing the &quot;objective visual similarity&quot;), and the individual-specific visual similarity (the variance). I am curious to see what regions still show a correlation with behavior even if you remove the mean -- so, for example, if you calculate an RDM factoring out / partial out the group mean, is it the PRC and alErC that show a correlation with really this individual-specific RDM? That being said, your later analysis showing higher correlations with an individual's behavioral RDM than other participants' RDMs answers this question in a different way. But the method I suggest also isolates the representation specific to the individual.</p><p>With that, I wonder if one can begin to look at the causes of these idiosyncracies. Since the current study is looking at visual similarity, I kept wondering if it would make sense to get an &quot;objective&quot; measure of visual similarity, by creating RDMs formed from some sort of computer vision metric (e.g., pixel similarity, or a DNN metric). You could examine the degree to which individuals agree / disagree with objective similarity, and how that relates to patterns in the brain. I wouldn't say this is necessary, but one potential direction that could expand the impact of the current work.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The authors argue in the Discussion for a modest relationship between perceived and objective measures of similarity (p23), and as such &quot;the coding of whole objects based on complex feature conjunctions in PrC and alErC could afford the flexibility that is required to capture these differences in perception&quot;. What does &quot;complex&quot; mean here? Specifically, it is unclear as to whether the authors are suggesting that &quot;subjectively perceived aspects&quot; of stimuli constitute just another set of features (in addition to objective perceptual/semantic features), or something else entirely.</p><p>One inference the paper makes is that all 10 items contribute to the PRC/ERC effects equally. Do the authors have any evidence for this? How similar were the 10 different item categories? More specifically, did they have similar between-exemplar similarity ratings? This could maybe be inferred maybe from SFig1B for 5 of the items.</p><p>Absent from much of the framing in this paper is the characterization of perceptual verses semantic similarity, in favor of a coarse/fine-grained motif. However, some of these elements may be influencing the Supplementary analysis of centroid similarity. For example, in the first stage of the iMDS when all 10 categories of items are on the screen, it's unlikely that raters can do a purely visual decomposition of the visual similarity, despite the instructions. As such, there may be some &quot;semantic similarity&quot; in these groupings (and the consequent centroids). Could the authors speak to whether their Supplementary analyses of these relationships (SFig4) address this semantic factor?</p><p>Do the authors see any similarities in their LOC result for SFig4B, and the ITC-centered results in Charest et al., 2014? Some mention of this in the Discussion paragraph might help to draw further links between these papers.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66884.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Both reviewers agreed that the value of the results was clear, the methods are methodologically sound but that there are some theoretical or analytical ambiguities that need to be addressed.</p><p>The work is an important extension of prior work (particularly Charest et al., 2014), that includes adjacent brain areas, most notably the medial temporal lobe. However, the reviewers and I agreed that the theoretical basis and contributions of the results could use some further highlighting to underscore what new insights are gleaned from this work. In particular, we found the results in Figure 4 to be the most unique and important of this work but did not feel the Introduction set up the paper to appreciate this.</p></disp-quote><p>Response 1A: We thank the reviewers and editors for highlighting the methodological soundness of our work, and for spelling out what they consider the most important contribution in the presented set of findings. We share the excitement about the findings reported in Figure 4, which reveal that activity patterns in PrC and alErC predict observers’ perceived visual similarity structure among category exemplars with higher precision than more posterior VVS regions. We have introduced substantial revisions to the Introduction and Discussion in order to facilitate appreciation of these findings within the theoretical framework of the representational-hierarchical model of object processing in the VVS. In particular, we now elaborate on predictions as to how PrC and alErC compare with more posterior VVS regions (see pages 4 and 5). We elaborate on the corresponding interpretation of the reported results, while highlighting our new contribution, on p 29, 31, and 32. We also make reference to the inclusion of a new comparison between subjective ratings of perceived similarity and objective image characterization based on computational modeling (for further detail, see Essential Revisions Response 1B, 2B). The added findings we report based on this modeling, as well as new findings obtained with ratings averaged across observers, provide further support for the representational-hierarchical model as discussed in the modified and expanded Discussion on p 33. We also consider on p 28 how our finding that representational structure related to fine-grained perceived similarity in PrC and alErC activity is predominantly observerspecific (idiosyncratic) can be accommodated within this model, when the well documented evidence for plasticity in these medial-temporal regions is brought into play.</p><p>Response 1B: In order to help interpret the results in Figure 4</p><p>(considered most exciting by the Reviewers) in a more theoretically focused manner, and relate them to the observer-specific effects, we conducted additional analyses that address the unique vs shared component of perceived similarity structure in relation to estimates derived from the computational HMAX model at different levels/grains of within-category similarity, as illustrated in the new Figure 6 and the new panel Figure 7C. The HMAX model is valuable in this context as it provides a means to estimate objective image-based visual similarity at the level of intermediate feature descriptions based on prior neurophysiological characterization of the VVS. These new analyses are presented in the new section “Patterns in Posterior VVS Regions Predict Similarity Structure that is Shared by Observers and Tied to Object Characteristics at the Intermediate Feature Level”, with additional expansion of the section “Patterns in PrC and alErC Predict Fine-Grained Perceived Visual Similarity Structure among Exemplars in an Observer-Specific Manner”.</p><p>We summarize the results in the following way: <italic>”</italic>Together, these results suggest that activity in VVS regions posterior to PrC and alErC capture the components of perceived visual similarity structure among exemplars that is shared by observers and that is closely related to object features at the intermediate feature level. At the same time, these neural representations in posterior VVS regions do not appear to allow for differentiation of exemplars at high levels of perceived similarity that tend to be observer-specific<italic>.</italic>”</p><p>Response 1C: We changed the Title of the paper to reflect our new emphasis to: “Activity in perirhinal and entorhinal cortex predicts perceived visual similarities among category exemplars with highest precision”. Moreover, we substantially revised the abstract accordingly.</p><disp-quote content-type="editor-comment"><p>It was suggested that further analyses be conducted to boost the strength of the paper to resolve ambiguities about the subjective similarity measure to better understand what underlies these idiosyncratic representations (visual information? semantic information? familiarity?), and what these results suggest about the perirhinal cortex. Does this challenge or support any pre-existing notions about the perirhinal cortex, or the nature of visual / memory representations in the brain?</p><p>Specifically, please report representational dissimilarity matrices (RDMs) to identify what portions of the variance are idiosyncratic vs. shared across the group and attempt to relate the brain RDMs to visually-based RDMs of the stimuli. It was recognized that the semantic factor is much more constrained (given the use of a carefully controlled stimulus set) and is a strength of the current manuscript.</p></disp-quote><p>Response 2A: Idiosyncratic vs Shared Variance &amp; Computational Model-derived objective visual similarity. To better understand idiosyncratic versus shared aspects of ratings of perceived similarity, we introduced new analyses with an average perceived similarity RDM using the Means of ratings on the iMDS across all participants. We calculated brain-behaviour correlations using these average perceived similarity ratings (page 21-22; new Figure 6A-B p 23), and now present these in addition to the previously included analyses that focus on idiosyncratic aspects, which we expanded to allow for an examination of idiosyncratic structure at different grains (based on i-index as shown in Figure 7A-B). Our analyses revealed that average similarity ratings are not reflected in the representational structure of activity patterns in PrC and alErC but rather in EVC and LOC, and only for low and medium levels of similarity. We also related the brain RDMs to RDMs derived from the influential computational HMAX model that describes visual objects at the intermediate feature level (Riesenhuber, Poggio, 1999; Cadieu et al., 2007; Serre, Olivia, Poggio, 2007; Khaligh-Razavi, Kriegeskorte, 2014; see Figure 6C-D; see Methods section on page 37). Critically, we found that average perceived similarity that is shared across observers is tied to objective image characteristics at the intermediate feature level as estimated by the HMAX model, again only for low and medium levels of similarity though (see new behavioural results on p 8 and new neuroimaging findings presented in Figure 7C-D). At the most fine-grained level of similarity (i.e. among highly similar objects), neither average ratings nor HMAX estimates predicted activity patterns in any VVS region we examined. This pattern of results underscores our point that the fine-grained structure in perceived similarity among category exemplars that is captured by PrC and alErC representations is predominantly idiosyncratic, as supported by new behavioural analyses that focused on intersubject correlations in perceived similarity at different grains (see p 8).</p><p>Response 2B: Computational Model-derived objective visual similarity. We note that we also explored use of a deep convolutional neural network, including AlexNet and VGG-16. However, this approach was not well suited given our stimuli are in greyscale format, leading to limited classification performance in output. For example, apple was classified as “golfball”. In addition, we browsed various discussion boards (e.g., <ext-link ext-link-type="uri" xlink:href="https://stackoverflow.com/questions/44668771/can-we-use-the-weights-of-a-model-trained-on-rgb-images-for-grayscale-images">https://stackoverflow.com/questions/44668771/can-we-use-the-weights-of-a-model-trained-onrgb-images-for-grayscale-images</ext-link>; https://datascience.stackexchange.com/questions/22684/is-itpossible-to-use-grayscale-images-to-existing-model) and explored using a neural network that was trained using greyscale ImageNet images (https://github.com/DaveRichmond-/grayscaleimagenet/blob/master/eval_image_classifier_gray.py). The latter has potential to classify categories correctly, but the network architecture becomes arguably less ecologically valid. Consequently, we limit presentation of computational modeling results to those obtained with HMAX in the current paper, which still help constrain interpretation of our main findings, as discussed above.</p><p>Response 2C: Theoretical interpretation. Please see our extended Discussion section on page 31, 32 for theoretical interpretation of these sets of new results in the context of the representational-hierarchical model of VVS organization and the functional role of PrC and alErC. We argue that our pattern of findings provides evidence in support of a central claim of the representational-hierarchical model, namely that PrC computes more integrated object representations allowing for increased differentiation of objects in visual perception than more posterior VVS regions that code for objects at the intermediate feature level. We also suggest that idiosyncratic fine-grained similarity structure among exemplars in PrC/alErC, perceptual reports, and discrimination behaviour could reflect interindividual differences in perceptual expertise, an account that receives some indirect preliminary support from findings reported in the recent behavioural literature. We discuss this idea and indicate that it can directly be tested in follow-up fMRI research (p 33).</p><disp-quote content-type="editor-comment"><p>Other important potential additions for discussion are outlined in the individual reviewer comments.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>Some of my struggle with the scope of the work may be in how it's framed in the introduction. The intro discusses the open question of how the brain represents similarities in objects, but then that question is never answered. It's also not clear to me what the key novelty is of the study beyond prior work, and how that changes our understanding of the human visual system. I'm also not sure why real-world objects are an interesting set or novelty (versus other types of stimuli we'd expect to have more idiosyncratic representations).</p></disp-quote><p>Please see Essential Revisions – Response 1A and 2C and Reviewer 1 – Response 2 in the Public Review.</p><disp-quote content-type="editor-comment"><p>I also have the question of whether the offline object arrangement task could have influenced the similarity structure reflected in the brain.</p></disp-quote><p>Please see Reviewer 1 – Response 1 in the Public Review.</p><disp-quote content-type="editor-comment"><p>Also, most of the analyses looked at correlations between individuals' brain and behavior to look at &quot;perceived visual similarity&quot;. However, the behavioral measure includes both what we can consider the group-shared visual similarity (the mean, perhaps even capturing the &quot;objective visual similarity&quot;), and the individual-specific visual similarity (the variance). I am curious to see what regions still show a correlation with behavior even if you remove the mean -- so, for example, if you calculate an RDM factoring out / partial out the group mean, is it the PRC and alErC that show a correlation with really this individual-specific RDM? That being said, your later analysis showing higher correlations with an individual's behavioral RDM than other participants' RDMs answers this question in a different way. But the method I suggest also isolates the representation specific to the individual.</p></disp-quote><p>Reviewer 1 – Response 4: In addition to the results derived from the brain-behaviour i-index, we have included supplementary supporting analyses using multiple linear regression of the kind the Reviewer suggests in order to determine the contributions of shared versus observer-specific perceived similarity structure to activity patterns in our ROIs (p 23 main text; supplementary table 2). The results of these analyses for observer-specific effects closely parallel those reported for the i-index and converge on the same conclusions.</p><disp-quote content-type="editor-comment"><p>With that, I wonder if one can begin to look at the causes of these idiosyncracies. Since the current study is looking at visual similarity, I kept wondering if it would make sense to get an &quot;objective&quot; measure of visual similarity, by creating RDMs formed from some sort of computer vision metric (e.g., pixel similarity, or a DNN metric). You could examine the degree to which individuals agree / disagree with objective similarity, and how that relates to patterns in the brain. I wouldn't say this is necessary, but one potential direction that could expand the impact of the current work.</p></disp-quote><p>Reviewer 1 – Response 5: We thank the Reviewer for this insightful comment and promising suggestion. We appreciate the value of contrasting the perceived visual similarity measures with objective ones. Please see Essential Revision – Response 2 above for a summary of the newly added analyses that focused on average similarity (across observers) and HMAX-derived objective similarity, as suggested. By revealing a different relationship to posterior VVS regions versus PrC and alErC, they offer further constraints for interpretation of the transformation of representations between these regions.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The authors argue in the Discussion for a modest relationship between perceived and objective measures of similarity (p23), and as such &quot;the coding of whole objects based on complex feature conjunctions in PrC and alErC could afford the flexibility that is required to capture these differences in perception&quot;. What does &quot;complex&quot; mean here? Specifically, it is unclear as to whether the authors are suggesting that &quot;subjectively perceived aspects&quot; of stimuli constitute just another set of features (in addition to objective perceptual/semantic features), or something else entirely.</p></disp-quote><p>Reviewer 2 – Response 2: We thank the reviewer for this comment – this is a great question. In the representational hierarchical model of VVS organization that guided our work “Complex” refers to the degree of integration based on feature conjunctions as we now discuss in more detail in the Introduction and Discussion. Because similar exemplars of the same category share many features, differentiation among them is possible by way of representing exemplars as a conjunction of many of their features. For example, in order to compare two apples and discriminate them from one another, it may not be sufficient to consider shape or shading in isolation, but necessary to represent differences in shape and shading in a conjoint manner. Several prior fMRI studies and studies in rodents that we cite in the Introduction provide evidence in support of the view that disambiguation of feature overlap is a critical factor in the computation of representations in PrC, and that feature conjunctions play a critical role in this disambiguation. Recent fMRI work also suggests that this applies to perceptual as well as semantic features (Martin et al., Neuron, 2018). The novelty of the approach we took here to test predictions of the representational hierarchical model for differentiation of category exemplars along the VVS, however, was to rely on subjective reports of perceived similarity that could be quantified on a single perceptual dimension rather than based on objective feature statistics. In as much as participants were explicitly asked to provide their ratings of similarity based on visual characteristics in the iMDS task, we argue that they provide a measure of visual similarity and that the corresponding neural representational structure also taps into organization based on visual characteristics. Indeed our newly added results from computational modeling with HMAX provide support for this perspective as they indicate that the reported perceived similarity and related activation patterns in the VVS are correlated with similarity estimates based on objective image characteristics in the visual domain (see Essential Revisions – Response 1B, 2A). By extension, we also suggest in the Discussion (p 33) that the variability in fine-grained representational structure across observers we report for PrC and ErC could also reflect variability in visual representation, specifically variability based on expertise. Here we raise the possibility that it reflects differences in perceptual expertise rather than semantic knowledge and we make reference to pertinent behavioural research in the literature. This interpretation is admittedly speculative, and we present it as an idea that deserves to be tested directly in future fMRI research with training paradigms (see also Reviewer 1 – Response 2).</p><disp-quote content-type="editor-comment"><p>One inference the paper makes is that all 10 items contribute to the PRC/ERC effects equally. Do the authors have any evidence for this? How similar were the 10 different item categories? More specifically, did they have similar between-exemplar similarity ratings? This could maybe be inferred maybe from SFig1B for 5 of the items.</p></disp-quote><p>Reviewer 2 – Response 3: This is an interesting question to ask when considering that we used multiple categories, and that the main analyses focused on examination of structure in matrices that included data for all categories combined. We explored this question in further behavioural analyses that examined each category in isolation, but we did not find noticeable category differences. Figure 1-Supplementary Figure 1B now includes behavioural data for all 10 categories. Given that we focused on a relatively large number of categories with a limited number of exemplars in each of them, however, it is possible that our design did not have sufficient power to uncover category-specific effects. As such we feel the absence of category specific effects requires caution in interpretation, and we decided against bringing them into the main text and against including them in our analyses of similarity structure in different VVS regions.</p><disp-quote content-type="editor-comment"><p>Absent from much of the framing in this paper is the characterization of perceptual verses semantic similarity, in favor of a coarse/fine-grained motif. However, some of these elements may be influencing the Supplementary analysis of centroid similarity. For example, in the first stage of the iMDS when all 10 categories of items are on the screen, it's unlikely that raters can do a purely visual decomposition of the visual similarity, despite the instructions. As such, there may be some &quot;semantic similarity&quot; in these groupings (and the consequent centroids). Could the authors speak to whether their Supplementary analyses of these relationships (SFig4) address this semantic factor?</p></disp-quote><p>Reviewer 2 – Response 4: We appreciate the note of caution expressed by the Reviewer with respect to interpretation of the data for the first stage of the iMDS that required judgment of similarity among exemplars across all categories examined. We agree that the data for this component of the task, which were only used for characterization of category centroids and neural correlates in Supplementary material, may not provide an estimate of similarity that is exclusively tied to visual characteristics (despite instructions asking it), but may be contaminated by semantic factors that become more prominent in categorization as compared to subordinate exemplar discrimination. With this concern in mind, we removed any reference to centroid representations and data from the first phase of the iMDS task in our paper and Supplementary materials. Further, we now spell out more clearly in numerous places, including in the changed Title of our paper, that this study specifically focuses on similarities among (subordinate) category exemplars. For further detail on our perspective on visual vs semantic similarities among exemplars, as probed in our study, please see Reviewer 2 – Response 2.</p><disp-quote content-type="editor-comment"><p>Do the authors see any similarities in their LOC result for SFig4B, and the ITC-centered results in Charest et al., 2014? Some mention of this in the Discussion paragraph might help to draw further links between these papers.</p></disp-quote><p>Reviewer 2 – Response 5: Thank you for this helpful suggestion. We added direct comparison of results in IT and LOC for a more complete documentation of VVS findings in Figure 7Supplementary Figure 1. IT is a large swath of cortex that includes ROIs LOC and PhC in the current study. The stimuli used in our study were objects that varied in shape (as well as texture), and therefore we were specifically interested in probing LOC. PhC was used as a control region for comparison and we did not predict any significant correlations with perceived visual similarity of objects in this region, given its well established role in scene processing. The results across four core sets of analyses were highly similar between IT and LOC, however, with the only noticeable exception being that LOC activity patterns predict observer-specific perceived similarity at low levels of similarity, while IT patterns do not predict any observer-specific similarity at all.</p></body></sub-article></article>