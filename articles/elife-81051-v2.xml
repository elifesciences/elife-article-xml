<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">81051</article-id><article-id pub-id-type="doi">10.7554/eLife.81051</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Medicine</subject></subj-group></article-categories><title-group><article-title>Transparency of research practices in cardiovascular literature</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Heckerman</surname><given-names>Gabriel O</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6420-8909</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Tzng</surname><given-names>Eileen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7513-9192</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Campos-Melendez</surname><given-names>Arely</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0322-1170</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Ekwueme</surname><given-names>Chisomaga</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Mueller</surname><given-names>Adrienne</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9161-5323</contrib-id><email>alm04@stanford.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0446vnd56</institution-id><institution>Western Kentucky University</institution></institution-wrap><addr-line><named-content content-type="city">Bowling Green</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Stanford Cardiovascular Institute</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05bnh6r87</institution-id><institution>Cornell University</institution></institution-wrap><addr-line><named-content content-type="city">Ithaca</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rrcem69</institution-id><institution>University of California, Davis</institution></institution-wrap><addr-line><named-content content-type="city">Davis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mangoni</surname><given-names>Arduino A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kpzv902</institution-id><institution>Flinders Medical Centre and Flinders University</institution></institution-wrap><country>Australia</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Barton</surname><given-names>Matthias</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University of Zurich</institution></institution-wrap><country>Switzerland</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>03</month><year>2025</year></pub-date><volume>14</volume><elocation-id>e81051</elocation-id><history><date date-type="received" iso-8601-date="2022-06-14"><day>14</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2025-03-17"><day>17</day><month>03</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-07-07"><day>07</day><month>07</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.06.498942"/></event></pub-history><permissions><copyright-statement>© 2025, Heckerman, Tzng, Campos-Melendez et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Heckerman, Tzng, Campos-Melendez et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-81051-v2.pdf"/><abstract><sec id="abs1"><title>Background:</title><p>Several fields have described low reproducibility of scientific research and poor accessibility in research reporting practices. Although previous reports have investigated accessible reporting practices that lead to reproducible research in other fields, to date, no study has explored the extent of accessible and reproducible research practices in cardiovascular science literature.</p></sec><sec id="abs2"><title>Methods:</title><p>To study accessibility and reproducibility in cardiovascular research reporting, we screened 639 randomly selected articles published in 2019 in three top cardiovascular science publications: Circulation, the European Heart Journal, and the Journal of the American College of Cardiology (JACC). Of those 639 articles, 393 were empirical research articles. We screened each paper for accessible and reproducible research practices using a set of accessibility criteria including protocol, materials, data, and analysis script availability, as well as accessibility of the publication itself. We also quantified the consistency of open research practices within and across cardiovascular study types and journal formats.</p></sec><sec id="abs3"><title>Results:</title><p>We identified that fewer than 2% of cardiovascular research publications provide sufficient resources (materials, methods, data, and analysis scripts) to fully reproduce their studies. Of the 639 articles screened, 393 were empirical research studies for which reproducibility could be assessed using our protocol, as opposed to commentaries or reviews. After calculating an accessibility score as a measure of the extent to which an article makes its resources available, we also showed that the level of accessibility varies across study types with a score of 0.08 for case studies or case series and 0.39 for clinical trials (p = 5.500E−5) and across journals (0.19 through 0.34, p = 1.230E−2). We further showed that there are significant differences in which study types share which resources.</p></sec><sec id="abs4"><title>Conclusions:</title><p>Although the degree to which reproducible reporting practices are present in publications varies significantly across journals and study types, current cardiovascular science reports frequently do not provide sufficient materials, protocols, data, or analysis information to reproduce a study. In the future, having higher standards of accessibility mandated by either journals or funding bodies will help increase the reproducibility of cardiovascular research.</p></sec><sec id="abs5"><title>Funding:</title><p>Authors Gabriel Heckerman, Arely Campos-Melendez, and Chisomaga Ekwueme were supported by an NIH R25 grant from the National Heart Lung and Blood Institute (R25HL147666). Eileen Tzng was supported by an AHA Institutional Training Award fellowship (18UFEL33960207).</p></sec></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Scientists are under pressure to publish impactful research quickly. In this “publish or perish” culture of science, scientists who are the first to publish a discovery in a well-known journal are often rewarded with credit and more funding. But this publication pressure can have unintended consequences and lead to the publication of incomplete or flawed research. To improve the quality and integrity of research, advocates encourage scientists to share their data and how they came to their conclusions to allow others to verify or replicate their work.</p><p>Several fields, including cancer research, have launched efforts to improve study quality and transparency. But so far, there has been little analysis about the reproducibility and replicability of cardiovascular disease research. Assessing how much information cardiovascular researchers provide about their methods, materials, and data could help determine the quality of heart disease research studies and boost confidence in the field’s discoveries.</p><p>Heckerman et al. found that fewer than 2 percent of heart disease research studies include enough information to be verified by other scientists. In the analysis, Heckerman et al. analyzed 393 cardiovascular disease research studies to determine if the studies provided enough information for others to replicate their work. The amount of information the authors shared varied according to study type. Studies describing the experiences of individuals or small numbers of patients shared the least information about how they came to their conclusions, while larger clinical trials shared more. In some cases, such as when a study used personally identifiable patient information or a drug company’s proprietary data, there may have been reasons to keep the data confidential.</p><p>The findings provide valuable information for both cardiovascular scientists and the public. They show a lot of room to improve trust in cardiovascular disease research by ensuring more studies are verifiable. Scientific journals and research funders could incentivize researchers to share more information about their methods and data to increase trust and transparency in the field.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cardiovascular literature</kwd><kwd>accessibility</kwd><kwd>reproducibility</kwd><kwd>open science</kwd><kwd>data sharing</kwd><kwd>cardiology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R25HL147666</award-id><principal-award-recipient><name><surname>Heckerman</surname><given-names>Gabriel O</given-names></name><name><surname>Campos-Melendez</surname><given-names>Arely</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000968</institution-id><institution>American Heart Association</institution></institution-wrap></funding-source><award-id>18UFEL33960207</award-id><principal-award-recipient><name><surname>Tzng</surname><given-names>Eileen</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Based on a standardized screening process, the simple majority of reviewed cardiovascular research publications lack sufficient information needed for the study to be replicated or reproduced.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Previous studies have reported on the lack of detailed methodology, data, and code provided that would be necessary for individual studies or study sets to be considered replicable and reproducible (<xref ref-type="bibr" rid="bib7">Filazzola and Cahill, 2021</xref>). In addition, scientific publications are often behind paywalls that only users affiliated with a larger institution are readily able to access, meaning that research funded by public money is often inaccessible to the general public without paying a fee. Scientific practices have been evolving over time: data from a study can be complex and require a large amount of storage space, methods can be extremely sophisticated and require specialized equipment, and data analysis can require complex algorithms and code. Journals often do not specify requirements for materials sharing, data sharing, analysis code sharing, and methodological information – all of which have been shown to be important to be able to reproduce or replicate a study (<xref ref-type="bibr" rid="bib9">Hamra et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Munafò et al., 2017</xref>). Note that often there may be legitimate reasons for restricting accessibility, such as privacy concerns, restrictive company policies, and costliness of the process.</p><p>Several efforts to improve transparency and accessibility are already underway. Among these are the Reproducible Evidence: Practices to Enhance and Achieve Transparency (REPEAT) initiative, a program dedicated to the improvement of transparency and reproducibility of database research in the healthcare sector (<xref ref-type="bibr" rid="bib23">Repeat, 2023</xref>). REPEAT contributes to <xref ref-type="bibr" rid="bib30">Wang et al., 2022</xref>, exploring reasons for irreproducibility, responsiveness of corresponding authors, and many more anecdotes in a study attempting to reproduce published results utilizing the same databases as the original investigators. Another example is the Reproducibility Project: Cancer Biology, meant to replicate experiments in cancer biology research papers to analyze their level or replicability (<xref ref-type="bibr" rid="bib4">Davis et al., 2014</xref>). The Reproducibility Project: Cancer Biology demonstrated their work in preclinical cancer biology research replicability by repeating 50 experiments from 23 papers (<xref ref-type="bibr" rid="bib6">Errington et al., 2021b</xref>) and 193 experiments from another 53 papers (<xref ref-type="bibr" rid="bib5">Errington et al., 2021a</xref>), discussing challenges and barriers associated with replicability. In addition, several resources are readily available to aid with the transparency assessment, including but not limited to SciScore (<xref ref-type="bibr" rid="bib16">Menke et al., 2022</xref>), DataSeer (<xref ref-type="bibr" rid="bib3">DataSeer, 2023</xref>), and Ripeta (<xref ref-type="bibr" rid="bib28">Sumner et al., 2021</xref>).</p><p>Although some journals and funding agencies have implemented policies to support the public dissemination of research, we know from recent studies in several disciplines that accessible and reproducible research practices are still far from the norm (<xref ref-type="bibr" rid="bib1">Borghi and Van Gulick, 2018</xref>; <xref ref-type="bibr" rid="bib29">Walters et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Kemper et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Sherry et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Smith et al., 2021</xref>). However, to date, no study has examined the degree of accessible and reproducible practices in cardiovascular science publications. Within the domain of cardiovascular science research, this type of analysis is extremely important for holding the latest cutting-edge discoveries in cardiovascular science to a high standard because of the impact on human health. We therefore investigate reproducible reporting practices using an adapted previously published screening process (<xref ref-type="bibr" rid="bib18">National Academies of Sciences, Engineering, and Medicine, 2019</xref>).</p><p>With this study, we hoped to identify the prevalence of accessible and reproducible research practices in cardiovascular research. To achieve this aim, we defined what information is necessary to recreate research from a published work (reproducible). We screened randomly selected articles published in 2019 in Circulation, the European Heart Journal, and the Journal of the American College of Cardiology (JACC). We tested, both, that the simple majority, more than half of screened publications, would lack one or more specific criteria that facilitate reproducibility or replicability, and that the vast majority, over 90% of screened publications, would lack one or more specific criteria that facilitate reproducibility or replicability. In addition, we predicted that some study types (e.g. clinical trials) would satisfy significantly more accessibility criteria than other study types. We also predicted that some categories of accessibility criteria would be satisfied significantly more frequently than other categories in both study types and across separate journals (e.g. materials availability vs analysis script availability). We also specified that a lack of specific requirements by journals regarding what information to provide would lead to variability in which accessibility criteria are satisfied.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Sampling plan</title><p>To determine the prevalence of accessible and reproducible research practices within cardiovascular literature, data was gathered from a random selection of all published studies in 2019 in the following three leading cardiology journals (<xref ref-type="bibr" rid="bib20">Opthof, 2019</xref>): Circulation, European Heart Journal, and the Journal of the American College of Cardiology (JACC). To limit the scope of our analysis, we only included articles published in the year of 2019. This specific year was selected because it was the most recent full year not influenced by changes in reporting practices due to the COVID-19 pandemic.</p><p>The following PubMed search string was used to obtain the full list of articles screened: ((‘Circulation’[Journal] OR ‘J Am Coll Cardiol’[Journal] OR ‘Eur Heart J’[Journal]) AND 2019/01/01:2019/12/31[Date – Publication]) NOT (Published Erratum[Publication Type]). This search string retrieved 2786 articles. This list was randomized and the first 639 were screened for our study, and researchers screened the listed articles sequentially. A PRISMA flow diagram (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>) of the inclusion and exclusion information is provided in the supplementary materials.</p><p>No blinding was involved in this study and each article was screened at least twice by separate individuals. Some researchers screened a higher proportion of articles than other authors, but each author screened at least 180 articles once. Any ambiguities identified during screening were resolved either through additional review, or through discussion among participating researchers to achieve consensus.</p><p>We initially screened 400 randomly selected articles, however of those articles only 153 were empirical research articles for which we could calculate accessibility measures. To increase the number of empirical research articles in our study, we therefore conducted a second round of screening. All aspects remained the same during additional screening other than generating the following new PubMed search string: (‘Circulation’[Journal] OR ‘J Am Coll Cardiol’[Journal] OR ‘Eur Heart J’[Journal]) AND 2019/01/01:2019/12/31 [Date – Publication] NOT ((review[Publication Type]) OR (systematic review[Publication Type]) OR (editorial[Publication Type]) OR (comment[Publication Type]) OR (‘case reports’[Publication Type]) OR (‘Introductory Journal Article’[Publication Type]) OR (‘Historical Article’[Publication Type]) OR (‘Practice Guideline’[Publication Type])). This search string retrieved 1461 primarily empirical articles. This list was also randomized and the first 239 articles that had not already been screened were added to the first set of 400 screened articles. Researchers screened the new articles sequentially and any remaining non-empirical articles were removed from the list and replaced with an empirical research article.</p></sec><sec id="s2-2"><title>Variables</title><p>Following a modified version of a previously established coding protocol for reproducible and accessible research practices (<xref ref-type="bibr" rid="bib12">Iqbal et al., 2016</xref>), the following criteria were screened during the evaluation process: type of reported study, pre-registration status, protocol availability, materials availability, data availability, analysis script availability, conflict of interest (COI) status, and open access of the article. An article’s ‘accessibility score’ was calculated as the fraction of several accessibility criteria results with the specific criteria satisfied out of the total possible to be satisfied for that specific study type. See <xref ref-type="table" rid="table1">Table 1</xref> for a list of screening criteria that contributed to the calculation of the accessibility score. Note that for papers without the ability to share Materials (e.g. Meta-analyses), the materials criterion was omitted from the accessibility score calculation. Our article coding form is derived from <xref ref-type="bibr" rid="bib12">Iqbal et al., 2016</xref> and can be found as Qualtrics.qsf and Word files in the pre-registration (<xref ref-type="bibr" rid="bib2">Campos-Melendez et al., 2021</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Criteria determining accessibility score and criteria-satisfying responses.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Criteria screened</th><th align="left" valign="bottom">Criteria-satisfying responses</th></tr></thead><tbody><tr><td align="left" valign="bottom">How clearly stated was the study type?</td><td align="left" valign="bottom">Stated (e.g. editorial comment, clinical trial)</td></tr><tr><td align="left" valign="bottom">Does the article state whether or not materials are available?</td><td align="left" valign="bottom">Yes the statement says that the materials (or some of the materials are available).<xref ref-type="table-fn" rid="table1fn1">*</xref></td></tr><tr><td align="left" valign="bottom">Can you access, download, and open the materials files?</td><td align="left" valign="bottom">Yes.<xref ref-type="table-fn" rid="table1fn1">*</xref></td></tr><tr><td align="left" valign="bottom">Does the article state whether or not data are available?</td><td align="left" valign="bottom">Yes – the statement says that the data (or some of the data) are available.</td></tr><tr><td align="left" valign="bottom">Can you access, download, and open the data files?</td><td align="left" valign="bottom">Yes.</td></tr><tr><td align="left" valign="bottom">Are the data files clearly documented?</td><td align="left" valign="bottom">Yes.</td></tr><tr><td align="left" valign="bottom">Do the data files appear to contain all of the raw data necessary to reproduce the reported findings?</td><td align="left" valign="bottom">Yes.</td></tr><tr><td align="left" valign="bottom">Does the article state whether or not analysis scripts are available?</td><td align="left" valign="bottom">Yes – the statement says that the analysis scripts (or some of the analysis scripts) are available.</td></tr><tr><td align="left" valign="bottom">Can you access, download, and open the analysis files?</td><td align="left" valign="bottom">Yes.</td></tr><tr><td align="left" valign="bottom">Does the article state whether or not the study (or some aspect of the study) was pre-registered?</td><td align="left" valign="bottom">Yes – the statement says that there was a pre-registration.</td></tr><tr><td align="left" valign="bottom">Can you access and open the pre-registration?</td><td align="left" valign="bottom">Yes.</td></tr><tr><td align="left" valign="bottom">What aspects of the study appear to be pre-registered? (select all that apply)</td><td align="left" valign="bottom">Hypotheses, Methods, AND Analysis Plan all available.</td></tr><tr><td align="left" valign="bottom">Does the article link to an accessible protocol?</td><td align="left" valign="bottom">Yes.</td></tr><tr><td align="left" valign="bottom">What aspects of the study appear to be included in the protocol? (select all that apply)</td><td align="left" valign="bottom">Hypotheses, Methods, AND Analysis Plan all available.</td></tr><tr><td align="left" valign="bottom">Does the article include a statement indicating whether there were any conflicts of interest?</td><td align="left" valign="bottom">Any of the following can be selected:<break/>Yes – the statement says that there are one or more conflicts of interest.<break/>Yes – the statement says that there is no conflict of interest.</td></tr><tr><td align="left" valign="bottom">Does the article include a statement indicating whether there were funding sources?</td><td align="left" valign="bottom">Any of the following can be selected:<break/>Yes – the statement says that there was funding from a private organization.<break/>Yes – the statement says that there was funding from a public organization.<break/>Yes – the statement says that there was funding from both public and private organizations.<break/>Yes – the statement says that no funding was provided.</td></tr><tr><td align="left" valign="bottom">Is the article open access?</td><td align="left" valign="bottom">Any of the following can be selected:<break/>Yes – found via Open Access Button.<break/>Yes – found via other means.</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>This criterion was not included for publications for which it was not possible to share materials.</p></fn></table-wrap-foot></table-wrap><p>The four criteria defining repeatability – methods (protocol or pre-registration), materials, data, and analysis script availability – are each considered an accessibility category. Each individual criterion in these four categories can be either satisfied or not. For a study to be considered fully ‘replicable’ all three of the following categories must be available: methods, data, and analysis scripts. For a study to be considered fully ‘reproducible’, all three of the following categories must be available: methods, data, and analysis scripts. If an article is either partially replicable or partially reproducible it is considered ‘partially repeatable’. Definitions of replicability and reproducibility were set based on those introduced by the National Academies of Sciences, Engineering and Medicine (NASEM) (2019). We defined the ability to replicate a study as the attempt to obtain the same results as a study by collecting new data. Reproducing a study describes the attempt to obtain the same results as a study by re-analyzing its data.</p></sec><sec id="s2-3"><title>Analysis</title><p>Only articles that were fully screened and for which all ambiguities had been resolved were included in the final analysis. Papers written in a language other than English were also excluded, as well as any articles for which we could not access the full text. Articles without empirical data (e.g. review articles, commentaries, and editorials) were evaluated for author location, language, COI, funding statements, and public access of the article itself, but were otherwise not screened for accessibility criteria because these study types by their nature cannot share, for example, materials, data, or analysis code. We also screened whether studies could share materials or not. For example, meta-analyses are empirical research studies, but do not typically have any shareable materials. One such example is a meta-analysis (<xref ref-type="bibr" rid="bib24">Russo et al., 2019</xref>), which was screened as an empirical paper but could not have materials due to the nature of the research conducted. Our analysis does assume, however, that all empirical research articles involve a protocol, the collection or re-use of data, and analysis of that data, and therefore methods, data, and analysis code or scripts should always be available.</p><p>To determine how frequently published cardiovascular research reports provide sufficient resources to replicate or reproduce their studies, we quantified the fraction of papers that are either partially replicable or partially reproducible – defined as partially repeatable. For all accessibility criteria, we calculated the proportion of papers in our dataset that exhibit each possible outcome. For example, for the accessibility criterion ‘How does the statement indicate the materials are available’, we calculated the proportion of papers that state that materials are available through an online third party repository, a personal or institutional webpage, supplemental information hosted by a journal, or upon request from authors. We also calculated the distribution of accessibility scores across our full dataset and further collected data regarding the location of the corresponding author. We use standard deviations to show the variability of the data.</p><p>To test for inconsistent, or variable, levels of accessibility, we examined our data using multi-way ANOVAs. Our independent variables included ‘study type’ and ‘journal’ and our dependent variable was the accessibility score. For the hypotheses addressing differences between the types of studies, journals, and accessibility scores using multi-way ANOVA, p-values &lt;0.05 were considered to be statistically significant.</p><p>To evaluate the relationship between accessibility criteria (Materials, Methods, Data, and Analysis Code) and study types, we performed several sequential chi-squared tests of proportions. In cases with 0 samples in a group, we instead performed Fisher’s exact tests. Tests were performed using R. We compared the proportion of papers that did and did not allow access to those four categories of information and resources, across all four categories. We also used chi-squared tests to further compare the proportion of papers that provided access to each category of information and which study types shared more or less of a specific type of information compared to other study types. We considered p-values ≤0.05 to indicate significant differences in proportions. We used Bonferroni correction to adjust the p-value for the number of tests performed. Note that this analysis differs slightly from that proposed in our pre-registration: Chi square Automatic Interaction Detection (CHAID).</p></sec><sec id="s2-4"><title>Pre-registration</title><p>This study was pre-registered through the Open Science Foundation and can be accessed at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/QFSTH">https://doi.org/10.17605/OSF.IO/QFSTH</ext-link> (<xref ref-type="bibr" rid="bib2">Campos-Melendez et al., 2021</xref>).</p><p>After obtaining our data, not all exploratory analyses documented in the pre-registration were performed. We did not test the relationship between the articles described as accessible and those actually accessible due to the limited number of articles that exhibited large enough accessibility scores, nor did we perform textual analysis on the COI, funding statements, or the way resources were being shared. We also deviated from evaluating the relationship between corresponding author location and accessibility score given the low sample size for most countries. We also did not perform additional analyses with the accessibility criteria organized as a hierarchy, because of the subjectivity in determining a ranking of screening criteria. We also deviated from the pre-registration by using sequential chi-squared tests instead of the CHAID. CHAID analysis builds a predictive model using a classification tree with multiple levels. In our case, we only compare data across two levels and there is no hierarchical basis for a decision tree. CHAID was therefore an inappropriate method for our study. Lastly, we extended our data collection beyond what we initially proposed in our pre-registration so that we could have more empirical research articles in our dataset.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>We screened a similar proportion of 2019 articles from each of the three top cardiovascular journals: Circulation, European Heart Journal, and the Journal of the American College of Cardiology (226, 193, and 220, respectively, <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Over half (247 out of 400) of the randomly selected articles in our initial screening were review articles or other article types that contained no empirical data (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We ultimately screened a total of 639 articles, of which 393 were empirical research studies. Approximately half of the papers (56%) had easily identifiable study types while the remaining papers required more substantial reading to determine their study type (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). 43% (276 out of 639) of the articles did not have a funding statement (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). 93% of articles included a COI statement, and 57% stated they had a COI (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Notably, 7% of articles did not have any COI statement at all. Additionally, over 90% of articles were publicly accessible (605 out of 639, <xref ref-type="fig" rid="fig1">Figure 1F</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Distribution summary of all screened publications.</title><p>Summary of all screened publications on their (<bold>A</bold>) study type, (<bold>B</bold>) study type clarity, (<bold>C</bold>) journal in which it was published, (<bold>D</bold>) presence of funding source statement, (<bold>E</bold>) presence of conflict of interest (COI) statement, and (<bold>F</bold>) open access status.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81051-fig1-v2.tif"/></fig><sec id="s3-1"><title>Cardiovascular research publications rarely make their resources available</title><p>Looking only at articles that included empirical research (393 out of 639 total articles), the majority had had no pre-registration statement (286) or no linked and accessible protocol (383; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Of the papers that had a pre-registration statement, nearly all of them had accessible and openable pre-registrations (106 out of 107; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Among articles that had an openable and accessible pre-registration or protocol, almost all articles included methods (106 out of 106, 9 out of 10), approximately half included hypotheses (44 out of 106, 7 out of 10), and less than half included analysis plans (27 out of 106, 5 out of 10; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). We define a publication to have all pre-registration aspects if hypotheses, methods, and analysis are pre-registered. Of the 393 screened publications that have empirical research, 53 had 1 out of 3 of the aspects, 35 had 2 out of 4 of the aspects, and 18 studies had all three aspects.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Summary of screened papers for pre-registration, protocol, material, data, and analysis script availability.</title><p>Only study types considered empirical research in which materials are theoretically available were summarized. (<bold>A</bold>) Summary of the presence and accessibility of pre-registrations and protocols and the summary of components (hypotheses, methods, and analysis plan) of pre-registration and protocol for papers that had a pre-registration or protocol statement. N/A represents papers that could not answer the criteria because data, materials, and analysis plans were not available to begin with (top panel). (<bold>B</bold>) Summary of papers and material, data, and analysis script availability. For papers that had a statement, how materials, data, and analysis script were available, whether they were accessible, and whether it was clearly documented or present in its entirety are also summarized.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81051-fig2-v2.tif"/></fig><p>Only study types considered empirical research in which materials are theoretically available are summarized. Only 14% of articles made their materials available (56 out of 393), 31.8% of articles made their data available (125 out of 393), and 10.9% made their analysis scripts available (43 out of 393; <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Across the articles that stated materials, data, and analysis scripts were available, it was most common for them only to be made available upon request from the authors (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Across the articles that stated materials, data, and analysis scripts were available, very few studies made the materials and data readily available for downloading or opening (1, 13) and only one study’s analysis scripts were readily downloadable (1 out of 43; <xref ref-type="fig" rid="fig2">Figure 2B</xref>). However, the majority of data that was downloadable or openable was also clearly documented (9 out of 13; <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>The majority of cardiovascular publications are not readily replicable or reproducible. We predicted that the majority of publications will not provide sufficient resources to replicate or reproduce their studies. We tested both (1) that the simple majority, more than half of screened publications, will be lacking one or more specific replicability or reproducibility criteria, and (2) that the vast majority, over 90% of screened publications, will be lacking one or more specific reproducibility or replicability criteria. We found that 49.6% (195 out of 393) of empirical research studies were not partially reproducible and 49.4% (194 out of 393) were not partially replicable. 2% (7 out of 393) empirical research papers were fully reproducible, but only 5 were fully replicable (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Therefore, the simple majority of empirical research articles were partially reproducible or replicable, but the vast majority of empirical research studies were neither fully reproducible nor fully replicable.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Summary of potential replicability and reproducibility of empirical research studies (<italic>n</italic> = 393 studies).</title><p>An article is considered ‘partially replicable’ if any of material availability, analysis script availability, and methods criteria are satisfied and ‘fully replicable’ if all three criteria are satisfied. An article is considered ‘partially reproducible’ if any of data availability, analysis script availability, and methods are satisfied and ‘fully reproducible’ if all three criteria are satisfied. Note that these data describe the <italic>potential</italic> for a study to be partially or fully reproduced or replicated based on the availability of the study’s resources (methods, materials, data, and code), not whether the study was itself replicated or reproduced.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81051-fig3-v2.tif"/></fig></sec><sec id="s3-2"><title>Accessibility varies across journals and study types</title><p>With the exception of one paper, the entire dataset exhibited low accessibility with scores lower than 0.6; the majority of articles (144 out of 393) had accessibility scores within the 0.20–0.29 range and 18 articles had scores below 0.1. Only one article exhibited an accessibility score of 0.6 or greater (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Accessibility score distribution accross the dataset, by study type and by journal type.</title><p>(<bold>A</bold>) Accessibility score distribution across the entire dataset. The number of articles represented by each bar falls within the specified accessibility score range. No articles obtained an accessibility score fraction greater than 0.60. (<bold>B</bold>) Average accessibility scores across all articles screened, based on study type. Error bars correspond to standard deviation of the collected data points. Sample sizes are noted at the base of each bar. All scores are based on a total possible score of 1. (<bold>C</bold>) Average accessibility score by journal type. Screened articles were obtained from three different journals, including Journals of the American College of Cardiology (JACC), European Heart Journal from the European Society of Cardiology, and American Heart Association (AHA) Circulation. The standard deviation bars are indicative of the range of distribution of the obtained accessibility score fractions for each journal. Sample sizes are noted at the base of each bar. All score fractions are based out of a total possible score of 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81051-fig4-v2.tif"/></fig><p>We also predicted that the level of accessibility will be inconsistent across several dimensions. Specifically, we predicted that average accessibility score would vary across study types and potentially also across journals, depending on the publisher’s requirements. For example, if clinical trials must be registered and open access, clinical trial publications will satisfy more of the article coding form criteria than other studies. We tested this hypothesis by calculating an accessibility score for every article: the sum of the satisfied screening criteria divided by the total possible satisfiable criteria. We then calculated the average and standard deviation in accessibility scores across study type (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Overall accessibility scores were low across all study types, and standard deviations were large, indicating high variability across individual publications. We found that clinical trials had the highest average accessibility score (0.39), and clinical case studies or series had the lowest accessibility scores (0.08). Average accessibility scores were significantly different across study types (ANOVA, d.f. 7, <italic>F</italic>=36.37, p=5.500E−5).</p><p>We also predicted a significant difference in the number of accessibility criteria satisfied across journals due to differences in reporting policies. The largest proportion of articles screened were obtained from Circulation with a total number of 226 articles, and 74% of those were empirical research papers (168 out of 226). 63% of JACC articles (138 out of 220) and 45% of Circulation articles (87 out of 193) were empirical research for which we could calculate an accessibility score. After computing the average accessibility score for all three journals, Circulation had the highest average accessibility score (0.34), followed by the Journals of the American College of Cardiology (0.24), then the European Heart Journal (0.19) (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Accessibility scores vary significantly across journals (ANOVA, d.f. 2, <italic>F</italic> = 80.07, p = 1.230E−2). There was not a significant interaction between study type and journal (p = 0.284).</p></sec><sec id="s3-3"><title>Categories of accessible resources varies across study types</title><p>We also predicted some categories of resources will be accessible significantly more frequently than other categories (e.g. materials availability vs analysis script availability). We found that there was a significant difference in the proportion of papers that shared specific categories of resources (<italic>X</italic><sup>2</sup> = 55.2, p = 6.17e−12). We then further identified whether different study types exhibited differences in the proportions of shared resources, for example, whether there was a difference in the proportion of papers that have accessible data depending on study type. Note that we did not include Clinical Studies with Interventions, Meta-analysis or Systematic Reviews, Non-clinical Secondary Data Analysis, or Laboratory Cell or Molecular Studies in this analysis due to their small sample size. For all four shareable resources (Materials, Methods, Data, and Analysis), we found a significant difference across study types (<xref ref-type="table" rid="table2">Table 2</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>p-values of chi-square tests of study type for each category of resource.</title><p>We tested for a significant relationship between study type and availability of resources. ‘Yes’ refers to a paper including the resource and ‘No’ refers to a paper not including the resource. Observed values were compared to expected values for each category of resource.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Yes</th><th align="left" valign="bottom">No</th><th align="left" valign="bottom">Study type chi-square p-value with Bonferroni correction</th></tr></thead><tbody><tr><td align="left" valign="bottom">Materials</td><td align="char" char="." valign="bottom">46</td><td align="char" char="." valign="bottom">308</td><td align="char" char="hyphen" valign="bottom">3.05E−13</td></tr><tr><td align="left" valign="bottom">Methods</td><td align="char" char="." valign="bottom">107</td><td align="char" char="." valign="bottom">247</td><td align="left" valign="bottom">4.91E−35 (case studies omitted)</td></tr><tr><td align="left" valign="bottom">Data</td><td align="char" char="." valign="bottom">107</td><td align="char" char="." valign="bottom">247</td><td align="char" char="hyphen" valign="bottom">2.31E−12</td></tr><tr><td align="left" valign="bottom">Analysis</td><td align="char" char="." valign="bottom">36</td><td align="char" char="." valign="bottom">318</td><td align="char" char="hyphen" valign="bottom">1.58E−12</td></tr></tbody></table></table-wrap><p>To determine specifically how study types differ in which resources they share, we performed multiple chi-squared tests with Bonferroni correction, comparing each study type with every other study type for a given resource (<xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="table" rid="table3">Table 3</xref>). We found that a significantly higher proportion of laboratory animal studies stated that they would share materials than clinical observational studies or clinical trials. We also, as expected, found a significantly higher proportion of clinical trials shared their methods compared to any other study type. With regard to data sharing, we found that a significantly higher proportion of laboratory animal studies shared their data than any other study type. Lastly, a significantly higher proportion of laboratory animal studies shared their analysis code compared to clinical observational studies and clinical trials.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Proportion of papers with presence (yes) or absence (no) of specific accessibility criteria (Material, Methods, Data, and Analysis Code) for specific study types.</title><p>Methods presence is determined by presence of a pre-registration or linked protocol. Presence is indicated in green, absence in yellow. For each value, both the percentage and the count for that category and study type are shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81051-fig5-v2.tif"/></fig><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>p-values for chi-squared tests and Fisher’s exact tests comparing the presence or absence of specific accessibility categories (Materials, Methods, Data, and Analysis Code) for each study type compared to every other study type (Clinical Case Study or Series, Clinical Observational Study, Clinical Trial, and Laboratory Animal Study).</title><p>p-values significant at an alpha level of 0.05 with Bonferroni correction are shown in bold. Use of Bonferroni correction to adjust the p-values for multiple comparisons resulted in some p-values being greater than 1. Given that a probability greater than 100% cannot occur, any values greater than 1 were adjusted to a ceiling value of 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Materials</th><th align="left" valign="bottom">Methods</th><th align="left" valign="bottom">Data</th><th align="left" valign="bottom">Analysis Code</th></tr></thead><tbody><tr><td align="left" valign="bottom">Clinical Case Study or Series vs Clinical Observational Study</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Clinical Case Study or Series vs Clinical Trial</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom"><bold>3.14E−13</bold></td><td align="left" valign="bottom">0.34</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Clinical Case Study or Series vs Laboratory Animal Study</td><td align="left" valign="bottom"><bold>0.04</bold></td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom"><bold>1.86E−5</bold></td><td align="left" valign="bottom">0.18</td></tr><tr><td align="left" valign="bottom">Clinical Observational Study vs Clinical Trial</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom"><bold>1.35E−26</bold></td><td align="left" valign="bottom">0.25</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Clinical Observational Study vs Laboratory Animal Study</td><td align="left" valign="bottom"><bold>1.58E−12</bold></td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom"><bold>1.74E−11</bold></td><td align="left" valign="bottom"><bold>2.66E−12</bold></td></tr><tr><td align="left" valign="bottom">Clinical Trial vs Laboratory Animal Study</td><td align="left" valign="bottom"><bold>1.42E−3</bold></td><td align="left" valign="bottom"><bold>3.96E−19</bold></td><td align="left" valign="bottom"><bold>2.07E−3</bold></td><td align="left" valign="bottom"><bold>1.92E−3</bold></td></tr></tbody></table></table-wrap></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>We reviewed 639 papers that were published in 2019 in three of the top cardiovascular research journals to determine how accessible, and therefore reproducible, their research was. Our intention with this study was not to diminish existing studies, but to share current practices with the cardiovascular field and identify ways to improve those practices to enhance reproducibility and transparency in the future. We hope to identify opportunities for journals and scientists to adapt their practices to further reproducible science and encourage the use of higher standards and consistent formats, among cardiovascular scientific literature.</p><p>In general, we found that the simple majority, but not the vast majority, of publications are lacking one or more of the resources (materials, methods, data, or analysis scripts) to replicate or reproduce a study. Only 5 out of 393 provided sufficient resources to fully replicate their work, and only 7 out of 393 provided sufficient resources to reproduce their work. Although there were statistically significant differences in accessibility scores across study types and journals, overall, accessibility scores were consistently low.</p><p>In an effort to increase replicability and reproducibility of future published cardiovascular literature, there are some initiatives that could be taken. Journals have implemented practices to incentivize researchers to share their materials, methods, data, and analysis scripts, such as open science badges (<xref ref-type="bibr" rid="bib14">Kidwell et al., 2016</xref>), reporting checklists (<xref ref-type="bibr" rid="bib10">Han et al., 2017</xref>), and checking articles for reproducibility (<xref ref-type="bibr" rid="bib21">Organic Syntheses, 2023</xref>) or replicability (<xref ref-type="bibr" rid="bib19">Nosek et al., 2015</xref>). Our findings suggest increasing these efforts and expanding them to funding agencies would help promote open science practices across the field.</p><sec id="s4-1"><title>Publication accessibility</title><p>In collecting data on the fraction of papers that were publicly accessible, we found that only 34 out of 639 articles (5%) were not publicly accessible (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Although this is a fairly low fraction of inaccessible articles, a study’s publication is in many ways the most tangible output of the research, and having publicly accessible publication should be a minimum standard for research. The NIH’s public access policy has been instrumental in ensuring public access to published research reports, but reports funded purely through private sources are not currently under the same reporting mandate.</p></sec><sec id="s4-2"><title>Study type</title><p>It is notable that over half of the 400 papers we initially randomly selected for screening were non-empirical research, e.g. reviews, editorials, and commentaries. Although these emissions may be effective ways to communicate quickly and effectively with larger audiences, they may not go through the same rigor of review process. We also occasionally screened articles that were simplified summaries of the original research study, geared toward a lay audience. This type of article is valuable in that it makes research more accessible to a broader audience; however, because methods were absent and results were condensed, it was ambiguous whether the studies being reported in these articles had undergone full scientific review. Readers run the risk of assuming that these articles describe a full scientific story as opposed to a news highlight.</p><p>In addition, publications of replication studies, or studies that included a replication study, were virtually absent from our dataset (4 out of 393 empirical research publications) – suggesting the field of cardiovascular research puts very little emphasis on replication work. Of concern, several studies also stated outright that their data was not available for replication, stating for example ‘The investigators will not make the data, methods used in the analysis, and materials used to conduct the research available to any researcher for purposes of reproducing the results or replicating the procedure.’ Although there can be legitimate reasons why data sharing is not possible, these statements are typically not accompanied by any justification.</p></sec><sec id="s4-3"><title>Materials, methods, data, and analysis script sharing</title><p>Although data was shared more frequently than materials or analysis scripts, there were still only a total of 13 out of 393 empirical research papers for which data was readily openable to a reader. We acknowledge that authors will often have legitimate reasons for not being able to share resources, including patient privacy. However, it has also been shown that patients are in general very willing to make their data available to further research (<xref ref-type="bibr" rid="bib25">Seltzer et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Kim et al., 2019</xref>). We advocate for research studies actively seeking consent from human subject participants to make their data available and if that consent cannot be obtained to specify that justification for not sharing data in their report.</p><p>Although basic research studies more frequently shared materials, data, and analysis scripts than clinical trials, that sharing was frequently ‘upon request from the authors’, which previous studies have shown to be hit-or-miss in terms of yield (<xref ref-type="bibr" rid="bib11">Hardwicke et al., 2018</xref>). Clinical trials had by far the most consistent availability of a resource category: methods, in the form of pre-registrations. Because pre-registrations are mandated by the FDA to conduct a clinical trial, our results suggest that resource-sharing requirements by funding or approval agencies are effective means of changing practices. It should be noted however, that a recent study by <xref ref-type="bibr" rid="bib8">Goldacre et al., 2019</xref> showed that in trials with pre-registrations, only 76% of pre-specified trial outcomes were correctly reported. Therefore, even when trials are pre-registered variable switching is common when reporting results.</p></sec><sec id="s4-4"><title>COI and funding statements</title><p>Publications are a critical form of communicating research, and the content and results of publications are often prioritized to the point that COI and funding statements are easy to overlook. As a community, we are often more interested in the results described in publications than the process behind getting them. Personal interests and finances are undeniably a part of experimental integrity and can impact experimental design and therefore COI and funding statements should be given as much attention as other components of the publication. Although it was beyond the scope of this study to perform a rigorous analysis of COI and funding statements, our screening process did reveal numerous cases of ambiguous COI and funding declarations. In <xref ref-type="table" rid="table4">Table 4</xref>, we capture both problematic and positive examples of COI and funding statements. For example, many COI and funding statements were vague in that they lacked details on how different funders of interests specifically influenced the study, which is important for interpretation of the results. As another example, we also identified ‘Disclosures: None’ as a problematic statement, because it could be interpreted as the authors having no disclosures or that the authors declined to list their disclosures. The relationship between funders and COIs is also ambiguous. Authors frequently listed funders and declared ‘no conflict of interest’; however, funders frequently do have an interest in and impact on the study and therefore represent a COI. If nothing else, it is in the interest of researchers to produce compelling results to maintain good relations and receive future funding from agencies, even if they were funded solely through public organizations.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Summary table of problematic and informative COI and funding statement examples that we repeatedly came across.</title><p>Ambiguous or problematic statements are in red and clear statements are in green.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Conflict of interest (COI) statements</th><th align="left" valign="bottom"/></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Example</bold></td><td align="left" valign="bottom"><bold>Comment</bold></td></tr><tr><td style="background-color: #E57373;">When the COI is mixed with funding when disclosures are listed as ‘none’.</td><td style="background-color: #E57373;">While funding can be considered a conflict of interest, the statements should be carefully separated if ‘disclosures’ and ‘funding’ are listed separately. Support for a specific author may not always be directly funding the project but may influence the project as a conflict of interest.</td></tr><tr><td style="background-color: #E57373;">‘Conflict of interest: the author has received fees or grants from COMPANY.’</td><td style="background-color: #E57373;">‘Fees’ and ‘grants’ are two different elements and should be clarified as both can influence a study. It is unclear whether these fees or grants funded the published study and how they influenced the study if at all.</td></tr><tr><td style="background-color: #E57373;">‘Both authors have reported that they have no relationships relevant to the contents of this paper to disclose.’ ‘The authors have reported that they have no relationships relevant to the contents of this paper to disclose.’ ‘Conflicts of interest: none.’ ‘Disclosures: none.’</td><td style="background-color: #E57373;">This statement suggests that there are potential conflicts of interests and leaves the reader wondering if there are possible conflicts of interests left out. It does not leave room to be confident that there are no conflicts of interests.</td></tr><tr><td style="background-color: #E57373;">‘Dr. NAME has received research grants from NAME; and has received honoraria for NAME. Dr. NAME has received research grants from NAME. Drs. NAMES are founders of COMPANY and as such have received modest honoraria from COMPANY.’‘Dr. NAME is related through family to a member of COMPANY but neither she, nor her spouse, nor children have financial involvement or equity interest in and have received no financial assistance, support, or grants from the aforementioned.’</td><td style="background-color: #E57373;">It is highly ambiguous as to whether this is a funding or COI statement. It is unclear if funders played any role in designing or performing the experiment.</td></tr><tr><td style="background-color: #E57373;">There is no COI but there is an acknowledgement.</td><td style="background-color: #E57373;">Acknowledgements and COI should be separate sections as these two sections have different purposes. Including COI as acknowledgement allows COIs to be easily overlooked.</td></tr><tr><td style="background-color: #E57373;">No COI or funding statement.</td><td style="background-color: #E57373;">COI and funding statements provide additional factors that can impact a study outside of experimental factors. This information should always be included to fully inform readers, particularly when concerns are raised about certain studies or when the information is applied in real-world applications.</td></tr><tr><td style="background-color: #C5E1A5;">‘NAME served as the Guest Associate Editor for this paper.’</td><td style="background-color: #C5E1A5;">Although many authors will not serve as editors for the journals they are applying to, it is helpful to acknowledge when that is the case and the potential conflict of interest.</td></tr><tr><td style="background-color: #C5E1A5;">The authors specifically state that they have no conflicts of interest.</td><td style="background-color: #C5E1A5;">This is a clear and definitive statement that there are no conflicts of interests and readers are not left wondering if there are conflicts of interests that are not mentioned.</td></tr><tr><td style="background-color: #C5E1A5;">Itemization of funders with explicit listing of the ways funders did not contribute to the study. ‘The authors have reported that they have no relationships relevant to the contents of this paper to disclose.’</td><td style="background-color: #C5E1A5;">We believe this is an excellent way of recognizing that funding can be a form of conflict of interest. The authors also specifically state that they have no conflict of interests.</td></tr><tr><th align="left" valign="top" colspan="2">Funding statements</th></tr><tr><td align="left" valign="bottom"><bold>Example</bold></td><td align="left" valign="bottom"><bold>Comment</bold></td></tr><tr><td style="background-color: #E57373;">Funding in acknowledgements.</td><td style="background-color: #E57373;">Funding should be separate from acknowledgements. When funding is included in acknowledgements, it is easily overlooked.</td></tr><tr><td style="background-color: #E57373;">Long list of affiliations without any statement that the list is funding or COI.</td><td style="background-color: #E57373;">Funding and COI should be considered as an opportunity to share how factors outside of the experiment influenced the study. Listing affiliations is not sufficient and should be followed by explanations of why they are listed.</td></tr><tr><td style="background-color: #E57373;">Funding statement system, particularly when papers list ‘funding on page PAGENUMBER’.</td><td style="background-color: #E57373;">Listing ‘funding on page PAGENUMBER’ is ambiguous, leading to issues with finding the funding statement. We experienced cases where we either could not find the funding statement, or it was difficult to access. Funding statements should be listed with their respective articles.</td></tr><tr><td style="background-color: #E57373;">Funding statement found on pages outside of their respective articles ‘Sources of Funding, see page PAGENUMBER.’</td><td style="background-color: #E57373;">Funding statements should be directly associated with their corresponding article. Although including a funding statement elsewhere in a journal issue is better than no funding statement, the location is distant and disconnected from its corresponding article. If a pay wall is present, access may differ between the funding statement and the original article.</td></tr><tr><td style="background-color: #E57373;">‘Acknowledgements: Dr. NAME is a recipient of a grant from the ORGANIZATION in support of SPECIFIC research.’</td><td style="background-color: #E57373;">We believe that funding should be separate from acknowledgements, as these two sections have their own purposes.</td></tr><tr><td style="background-color: #E57373;">‘Sources of Funding: none.’ ‘Disclosures: Drs. NAMES received modest consulting fees from COMPANY for the conduct of this research. NAME is funded by grants from COMPANY. Dr. NAME reports a charitable grant from the ORGANIZATION, and personal fees from COMPANY. The other authors report no conflicts.’</td><td style="background-color: #E57373;">There are no funding sources listed, but numerous avenues of fees, grants, and more are then listed as disclosures. It is not clear that these fees and grants did not influence the study in any way, including whether those fees and grants were used to partially fund the study.</td></tr><tr><td style="background-color: #E57373;">Under ‘sources of funding’, no sources of funding are listed, but the COI statement refers to (explicit) statements that describe funding for the study.</td><td style="background-color: #E57373;">There should not be discrepancies between reports on funding and COI. This makes it difficult for the reader to gauge how factors are truly impacting the study.</td></tr><tr><td style="background-color: #E57373;">Funding and COI are condensed into a single section below author associations. Funding statement and COI are listed together as ‘Footnotes’.</td><td style="background-color: #E57373;">COI, funding, and author associations should be listed separately for easy understanding. By listing COI, funding, and author associations together, it is difficult to understand who impacts the study in what ways.</td></tr><tr><td style="background-color: #C5E1A5;">‘Dr. NAME is supported by FUNDING from the COMPANY/ORGANIZATION. The funding source had no role in the design and conduct of the study; collection, management, analysis, and interpretation of the data; preparation, review, or approval of the article; and decision to submit the article for publication.’</td><td style="background-color: #C5E1A5;">This statement clearly acknowledges how funders can play a role in a study and explicitly states that funders had no role in the experiment and publication. The reader is not left wondering if there are additional conflicts.</td></tr></tbody></table></table-wrap><p>To avoid these issues, we advocate that journals require more complete and standardized COI and funding information that recognizes their overlap, including: a clear description on how each funding entity supported the work including whether they influenced the experiments, analysis, or dissemination of the work. We recommend that journals require authors to complete a consistent and transparent funding and COI questionnaire or decision tree that is clearly and prominently associated with the article.</p></sec><sec id="s4-5"><title>Authorship</title><p>Although we did not have sufficient articles from different countries of corresponding authors to determine whether there was a significant difference in accessibility practices across countries, we did see a wide range in accessibility score values. In terms of reporting, it was in general very straightforward to identify the corresponding author of an article. However, sometimes corresponding authorship was ambiguous in that no contact information was given, or was listed as, ‘Published on behalf of […]. All rights reserved. The Author(s) 2019. For permissions, please email: […].’ In these circumstances, it is unclear who the corresponding author is and who is ultimately responsible for questions regarding the research.</p></sec><sec id="s4-6"><title>Limitations</title><p>Although we attempted to achieve completeness and consistency in screening by ensuring each article was screened by two separate individuals, we acknowledge we may have missed or misinterpreted statements in screened articles. For example, in capturing the type of funding for a study as public, private, or a combination of both, we may have misidentified the funding type for some organizations, especially for foreign funding bodies. Another example would be identifying whether the study type was clearly stated versus inferred with further reading of the article. These determinations are subjective and may vary with individuals’ experience with different study types and interpretation of language used in articles. Note that regular readers of publications will face the same challenges.</p><p>Furthermore, the accessibility score we used could be further developed to better represent the diversity of publications. For example, there may be types of studies that we identified as not providing materials, because no materials statement was given, but for which all materials may already be adequately described in the text and therefore actually readily available. In these cases, our accessibility score is not accurate. Case studies and case series are the study type most likely to be affected by this limitation due to their limited data and analysis. Only 21 out of 393 empirical research studies (5%) were case studies or case series. Similarly, our current screening criteria only gives credit to papers that state they share a protocol and that protocol is actually linked. Our screening protocol does not capture articles that have a protocol availability statement but do not provide a link.</p><p>In the future, the accessibility score could be improved to be more specific to different article types. For example, case studies typically represent patient cases where replicating or reproducing the study would not necessarily be expected. Lastly, this study also only used data from some of the highest ranking cardiovascular research journals. These journals are likely to have higher reporting standards than other journals in the field, so our results are likely to overestimate the reporting and sharing practices of publications in cardiovascular research in general.</p></sec><sec id="s4-7"><title>Future work and conclusion</title><p>The data collected from 639 screened papers provides numerous future directions for not only exploratory analysis on the existing dataset, but also for new projects assessing accessibility and reproducibility of scientific literature. The accessibility scores we calculated are a rough, quantitative estimate of an article’s actual accessibility and further work is required to more fully describe how accessible and reproducible cardiovascular literature is. For example, future work could identify which criteria are the biggest needs for the field, and then evaluate the quality of an article’s accessibility by weighting those criteria or organizing the criteria into a hierarchy of importance. Future studies could also investigate text excerpts describing how resources are or are not being made available to determine the causes that promote or undermine accessible research practices.</p><p>Our study shows that there is a high degree of variability in the resources cardiovascular research publications make available – across study types and journals. Universally, however, publications almost never provide sufficient materials, protocol information, data, or analysis scripts for another group to fully replicate or reproduce their work. When federal policies mandate the open research practices, such as ensuring publications are publicly accessible or clinical trials are pre-registered, these practices are adopted. To ensure the highest caliber of research in the future, we urge journals and funding agencies to require higher standards in their materials, protocol, data, and analysis script sharing – regardless of the study type or the funding source.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Validation, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-81051-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All materials, data, and analysis scripts associated with this study are available on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/FUDKA">Open Science Framework</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Tzng</surname><given-names>E</given-names></name><name><surname>Heckerman</surname><given-names>G</given-names></name><name><surname>Campos-Melendez</surname><given-names>A</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Ekwueme</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Transparency of research practices in cardiovascular literature</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/FUDKA</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghi</surname><given-names>JA</given-names></name><name><surname>Van Gulick</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Data management and sharing in neuroimaging: practices and perceptions of MRI researchers</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0200562</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0200562</pub-id><pub-id pub-id-type="pmid">30011302</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campos-Melendez</surname><given-names>A</given-names></name><name><surname>Ekwueme</surname><given-names>CS</given-names></name><name><surname>Heckerman</surname><given-names>GO</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Assessment of accessibility and reproducible research practices in cardiovascular literature</article-title><source>OSF</source><volume>1</volume><elocation-id>QFSTH</elocation-id><pub-id pub-id-type="doi">10.17605/OSF.IO/QFSTH</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="web"><person-group person-group-type="author"><collab>DataSeer</collab></person-group><year iso-8601-date="2023">2023</year><article-title>DataSeer</article-title><ext-link ext-link-type="uri" xlink:href="https://dataseer.ai/">https://dataseer.ai/</ext-link><date-in-citation iso-8601-date="2023-01-06">January 6, 2023</date-in-citation></element-citation></ref><ref id="bib4"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>RJ</given-names></name><name><surname>Espinosa</surname><given-names>J</given-names></name><name><surname>Green</surname><given-names>MR</given-names></name><name><surname>Massagué</surname><given-names>J</given-names></name><name><surname>Duojia</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reproducibility Project: Cancer Biology</article-title><ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology">https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology</ext-link><date-in-citation iso-8601-date="2023-01-06">January 6, 2023</date-in-citation></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Mathur</surname><given-names>M</given-names></name><name><surname>Soderberg</surname><given-names>CK</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name><name><surname>Iorns</surname><given-names>E</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Investigating the replicability of preclinical cancer biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e71601</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71601</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Denis</surname><given-names>A</given-names></name><name><surname>Perfito</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Reproducibility in cancer biology: challenges for assessing replicability in preclinical cancer biology</article-title><source>eLife</source><volume>10</volume><elocation-id>e67995</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67995</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Filazzola</surname><given-names>A</given-names></name><name><surname>Cahill</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Replication in field ecology: Identifying challenges and proposing solutions</article-title><source>Methods in Ecology and Evolution</source><volume>12</volume><fpage>1780</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.13657</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldacre</surname><given-names>B</given-names></name><name><surname>Drysdale</surname><given-names>H</given-names></name><name><surname>Dale</surname><given-names>A</given-names></name><name><surname>Milosevic</surname><given-names>I</given-names></name><name><surname>Slade</surname><given-names>E</given-names></name><name><surname>Hartley</surname><given-names>P</given-names></name><name><surname>Marston</surname><given-names>C</given-names></name><name><surname>Powell-Smith</surname><given-names>A</given-names></name><name><surname>Heneghan</surname><given-names>C</given-names></name><name><surname>Mahtani</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Compare: a prospective cohort study correcting and monitoring 58 misreported trials in real time</article-title><source>Trials</source><volume>20</volume><elocation-id>118</elocation-id><pub-id pub-id-type="doi">10.1186/s13063-019-3173-2</pub-id><pub-id pub-id-type="pmid">30760329</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamra</surname><given-names>GB</given-names></name><name><surname>Goldstein</surname><given-names>ND</given-names></name><name><surname>Harper</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Resource sharing to improve research quality</article-title><source>Journal of the American Heart Association</source><volume>8</volume><elocation-id>e012292</elocation-id><pub-id pub-id-type="doi">10.1161/JAHA.119.012292</pub-id><pub-id pub-id-type="pmid">31364452</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Olonisakin</surname><given-names>TF</given-names></name><name><surname>Pribis</surname><given-names>JP</given-names></name><name><surname>Zupetic</surname><given-names>J</given-names></name><name><surname>Yoon</surname><given-names>JH</given-names></name><name><surname>Holleran</surname><given-names>KM</given-names></name><name><surname>Jeong</surname><given-names>K</given-names></name><name><surname>Shaikh</surname><given-names>N</given-names></name><name><surname>Rubio</surname><given-names>DM</given-names></name><name><surname>Lee</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A checklist is associated with increased quality of reporting preclinical biomedical research: A systematic review</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0183591</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0183591</pub-id><pub-id pub-id-type="pmid">28902887</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardwicke</surname><given-names>TE</given-names></name><name><surname>Mathur</surname><given-names>MB</given-names></name><name><surname>MacDonald</surname><given-names>K</given-names></name><name><surname>Nilsonne</surname><given-names>G</given-names></name><name><surname>Banks</surname><given-names>GC</given-names></name><name><surname>Kidwell</surname><given-names>MC</given-names></name><name><surname>Hofelich Mohr</surname><given-names>A</given-names></name><name><surname>Clayton</surname><given-names>E</given-names></name><name><surname>Yoon</surname><given-names>EJ</given-names></name><name><surname>Henry Tessler</surname><given-names>M</given-names></name><name><surname>Lenne</surname><given-names>RL</given-names></name><name><surname>Altman</surname><given-names>S</given-names></name><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal <italic>Cognition</italic></article-title><source>Royal Society Open Science</source><volume>5</volume><elocation-id>180448</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.180448</pub-id><pub-id pub-id-type="pmid">30225032</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iqbal</surname><given-names>SA</given-names></name><name><surname>Wallach</surname><given-names>JD</given-names></name><name><surname>Khoury</surname><given-names>MJ</given-names></name><name><surname>Schully</surname><given-names>SD</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reproducible research practices and transparency across the biomedical literature</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002333</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002333</pub-id><pub-id pub-id-type="pmid">26726926</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemper</surname><given-names>JM</given-names></name><name><surname>Rolnik</surname><given-names>DL</given-names></name><name><surname>Mol</surname><given-names>BWJ</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reproducible research practices and transparency in reproductive endocrinology and infertility articles</article-title><source>Fertility and Sterility</source><volume>114</volume><fpage>1322</fpage><lpage>1329</lpage><pub-id pub-id-type="doi">10.1016/j.fertnstert.2020.05.020</pub-id><pub-id pub-id-type="pmid">32771255</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kidwell</surname><given-names>MC</given-names></name><name><surname>Lazarević</surname><given-names>LB</given-names></name><name><surname>Baranski</surname><given-names>E</given-names></name><name><surname>Hardwicke</surname><given-names>TE</given-names></name><name><surname>Piechowski</surname><given-names>S</given-names></name><name><surname>Falkenberg</surname><given-names>LS</given-names></name><name><surname>Kennett</surname><given-names>C</given-names></name><name><surname>Slowik</surname><given-names>A</given-names></name><name><surname>Sonnleitner</surname><given-names>C</given-names></name><name><surname>Hess-Holden</surname><given-names>C</given-names></name><name><surname>Errington</surname><given-names>TM</given-names></name><name><surname>Fiedler</surname><given-names>S</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Badges to acknowledge open practices: a simple, low-cost, effective method for increasing transparency</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002456</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002456</pub-id><pub-id pub-id-type="pmid">27171007</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Bell</surname><given-names>E</given-names></name><name><surname>Bath</surname><given-names>T</given-names></name><name><surname>Paul</surname><given-names>P</given-names></name><name><surname>Pham</surname><given-names>A</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Zheng</surname><given-names>K</given-names></name><name><surname>Ohno-Machado</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Patient perspectives about decisions to share medical data and biospecimens for research</article-title><source>JAMA Network Open</source><volume>2</volume><elocation-id>e199550</elocation-id><pub-id pub-id-type="doi">10.1001/jamanetworkopen.2019.9550</pub-id><pub-id pub-id-type="pmid">31433479</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menke</surname><given-names>J</given-names></name><name><surname>Eckmann</surname><given-names>P</given-names></name><name><surname>Ozyurt</surname><given-names>IB</given-names></name><name><surname>Roelandse</surname><given-names>M</given-names></name><name><surname>Anderson</surname><given-names>N</given-names></name><name><surname>Grethe</surname><given-names>J</given-names></name><name><surname>Gamst</surname><given-names>A</given-names></name><name><surname>Bandrowski</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Establishing institutional scores with the rigor and transparency index: large-scale analysis of scientific reporting quality</article-title><source>Journal of Medical Internet Research</source><volume>24</volume><elocation-id>e37324</elocation-id><pub-id pub-id-type="doi">10.2196/37324</pub-id><pub-id pub-id-type="pmid">35759334</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munafò</surname><given-names>MR</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Bishop</surname><given-names>DVM</given-names></name><name><surname>Button</surname><given-names>KS</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>du Sert</surname><given-names>NP</given-names></name><name><surname>Simonsohn</surname><given-names>U</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Ware</surname><given-names>JJ</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A manifesto for reproducible science</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0021</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-016-0021</pub-id><pub-id pub-id-type="pmid">33954258</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><collab>National Academies of Sciences, Engineering, and Medicine</collab></person-group><year iso-8601-date="2019">2019</year><source>Reproducibility and Replicability in Science</source><publisher-loc>Washington, DC</publisher-loc><publisher-name>The National Academies Press</publisher-name><pub-id pub-id-type="doi">10.17226/25303</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Alter</surname><given-names>G</given-names></name><name><surname>Banks</surname><given-names>GC</given-names></name><name><surname>Borsboom</surname><given-names>D</given-names></name><name><surname>Bowman</surname><given-names>SD</given-names></name><name><surname>Breckler</surname><given-names>SJ</given-names></name><name><surname>Buck</surname><given-names>S</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>Chin</surname><given-names>G</given-names></name><name><surname>Christensen</surname><given-names>G</given-names></name><name><surname>Contestabile</surname><given-names>M</given-names></name><name><surname>Dafoe</surname><given-names>A</given-names></name><name><surname>Eich</surname><given-names>E</given-names></name><name><surname>Freese</surname><given-names>J</given-names></name><name><surname>Glennerster</surname><given-names>R</given-names></name><name><surname>Goroff</surname><given-names>D</given-names></name><name><surname>Green</surname><given-names>DP</given-names></name><name><surname>Hesse</surname><given-names>B</given-names></name><name><surname>Humphreys</surname><given-names>M</given-names></name><name><surname>Ishiyama</surname><given-names>J</given-names></name><name><surname>Karlan</surname><given-names>D</given-names></name><name><surname>Kraut</surname><given-names>A</given-names></name><name><surname>Lupia</surname><given-names>A</given-names></name><name><surname>Mabry</surname><given-names>P</given-names></name><name><surname>Madon</surname><given-names>T</given-names></name><name><surname>Malhotra</surname><given-names>N</given-names></name><name><surname>Mayo-Wilson</surname><given-names>E</given-names></name><name><surname>McNutt</surname><given-names>M</given-names></name><name><surname>Miguel</surname><given-names>E</given-names></name><name><surname>Paluck</surname><given-names>EL</given-names></name><name><surname>Simonsohn</surname><given-names>U</given-names></name><name><surname>Soderberg</surname><given-names>C</given-names></name><name><surname>Spellman</surname><given-names>BA</given-names></name><name><surname>Turitto</surname><given-names>J</given-names></name><name><surname>VandenBos</surname><given-names>G</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Wilson</surname><given-names>R</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Promoting an open research culture</article-title><source>Science</source><volume>348</volume><fpage>1422</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Opthof</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Comparison of the impact factors of the most-cited cardiovascular journals</article-title><source>Circulation Research</source><volume>124</volume><fpage>1718</fpage><lpage>1724</lpage><pub-id pub-id-type="doi">10.1161/CIRCRESAHA.119.315249</pub-id><pub-id pub-id-type="pmid">31170041</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Organic Syntheses</collab></person-group><year iso-8601-date="2023">2023</year><article-title>Organic Syntheses: A publication of Reliable Methods for the Preparation of Organic Compounds</article-title><ext-link ext-link-type="uri" xlink:href="https://www.orgsyn.org/">https://www.orgsyn.org/</ext-link><date-in-citation iso-8601-date="2023-02-10">February 10, 2023</date-in-citation></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname><given-names>MJ</given-names></name><name><surname>McKenzie</surname><given-names>JE</given-names></name><name><surname>Bossuyt</surname><given-names>PM</given-names></name><name><surname>Boutron</surname><given-names>I</given-names></name><name><surname>Hoffmann</surname><given-names>TC</given-names></name><name><surname>Mulrow</surname><given-names>CD</given-names></name><name><surname>Shamseer</surname><given-names>L</given-names></name><name><surname>Tetzlaff</surname><given-names>JM</given-names></name><name><surname>Akl</surname><given-names>EA</given-names></name><name><surname>Brennan</surname><given-names>SE</given-names></name><name><surname>Chou</surname><given-names>R</given-names></name><name><surname>Glanville</surname><given-names>J</given-names></name><name><surname>Grimshaw</surname><given-names>JM</given-names></name><name><surname>Hróbjartsson</surname><given-names>A</given-names></name><name><surname>Lalu</surname><given-names>MM</given-names></name><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Loder</surname><given-names>EW</given-names></name><name><surname>Mayo-Wilson</surname><given-names>E</given-names></name><name><surname>McDonald</surname><given-names>S</given-names></name><name><surname>McGuinness</surname><given-names>LA</given-names></name><name><surname>Stewart</surname><given-names>LA</given-names></name><name><surname>Thomas</surname><given-names>J</given-names></name><name><surname>Tricco</surname><given-names>AC</given-names></name><name><surname>Welch</surname><given-names>VA</given-names></name><name><surname>Whiting</surname><given-names>P</given-names></name><name><surname>Moher</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The PRISMA 2020 statement: an updated guideline for reporting systematic reviews</article-title><source>BMJ</source><volume>372</volume><elocation-id>n71</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.n71</pub-id><pub-id pub-id-type="pmid">33782057</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Repeat</collab></person-group><year iso-8601-date="2023">2023</year><article-title>Reproducible Evidence: Practices to Enhance and Achieve Transparency</article-title><ext-link ext-link-type="uri" xlink:href="https://www.repeatinitiative.org/">https://www.repeatinitiative.org/</ext-link><date-in-citation iso-8601-date="2023-01-06">January 6, 2023</date-in-citation></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>JJ</given-names></name><name><surname>Aleksova</surname><given-names>N</given-names></name><name><surname>Pitcher</surname><given-names>I</given-names></name><name><surname>Couture</surname><given-names>E</given-names></name><name><surname>Parlow</surname><given-names>S</given-names></name><name><surname>Faraz</surname><given-names>M</given-names></name><name><surname>Visintini</surname><given-names>S</given-names></name><name><surname>Simard</surname><given-names>T</given-names></name><name><surname>Di Santo</surname><given-names>P</given-names></name><name><surname>Mathew</surname><given-names>R</given-names></name><name><surname>So</surname><given-names>DY</given-names></name><name><surname>Takeda</surname><given-names>K</given-names></name><name><surname>Garan</surname><given-names>AR</given-names></name><name><surname>Karmpaliotis</surname><given-names>D</given-names></name><name><surname>Takayama</surname><given-names>H</given-names></name><name><surname>Kirtane</surname><given-names>AJ</given-names></name><name><surname>Hibbert</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Left ventricular unloading during extracorporeal membrane oxygenation in patients with cardiogenic shock</article-title><source>Journal of the American College of Cardiology</source><volume>73</volume><fpage>654</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1016/j.jacc.2018.10.085</pub-id><pub-id pub-id-type="pmid">30765031</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seltzer</surname><given-names>E</given-names></name><name><surname>Goldshear</surname><given-names>J</given-names></name><name><surname>Guntuku</surname><given-names>SC</given-names></name><name><surname>Grande</surname><given-names>D</given-names></name><name><surname>Asch</surname><given-names>DA</given-names></name><name><surname>Klinger</surname><given-names>EV</given-names></name><name><surname>Merchant</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Patients’ willingness to share digital health and non-health data for research: a cross-sectional study</article-title><source>BMC Medical Informatics and Decision Making</source><volume>19</volume><elocation-id>157</elocation-id><pub-id pub-id-type="doi">10.1186/s12911-019-0886-9</pub-id><pub-id pub-id-type="pmid">31395102</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherry</surname><given-names>CE</given-names></name><name><surname>Pollard</surname><given-names>JZ</given-names></name><name><surname>Tritz</surname><given-names>D</given-names></name><name><surname>Carr</surname><given-names>BK</given-names></name><name><surname>Pierce</surname><given-names>A</given-names></name><name><surname>Vassar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Assessment of transparent and reproducible research practices in the psychiatry literature</article-title><source>General Psychiatry</source><volume>33</volume><elocation-id>e100149</elocation-id><pub-id pub-id-type="doi">10.1136/gpsych-2019-100149</pub-id><pub-id pub-id-type="pmid">32175523</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>CA</given-names></name><name><surname>Nolan</surname><given-names>J</given-names></name><name><surname>Tritz</surname><given-names>DJ</given-names></name><name><surname>Heavener</surname><given-names>TE</given-names></name><name><surname>Pelton</surname><given-names>J</given-names></name><name><surname>Cook</surname><given-names>K</given-names></name><name><surname>Vassar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Evaluation of reproducible and transparent research practices in pulmonology</article-title><source>Pulmonology</source><volume>27</volume><fpage>134</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1016/j.pulmoe.2020.07.001</pub-id><pub-id pub-id-type="pmid">32739326</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sumner</surname><given-names>JQ</given-names></name><name><surname>Vitale</surname><given-names>CH</given-names></name><name><surname>McIntosh</surname><given-names>LD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>RipetaScore: measuring the quality, transparency, and trustworthiness of a scientific work</article-title><source>Frontiers in Research Metrics and Analytics</source><volume>6</volume><elocation-id>751734</elocation-id><pub-id pub-id-type="doi">10.3389/frma.2021.751734</pub-id><pub-id pub-id-type="pmid">35128302</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walters</surname><given-names>C</given-names></name><name><surname>Harter</surname><given-names>ZJ</given-names></name><name><surname>Wayant</surname><given-names>C</given-names></name><name><surname>Vo</surname><given-names>N</given-names></name><name><surname>Warren</surname><given-names>M</given-names></name><name><surname>Chronister</surname><given-names>J</given-names></name><name><surname>Tritz</surname><given-names>D</given-names></name><name><surname>Vassar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Do oncology researchers adhere to reproducible and transparent principles? A cross-sectional survey of published oncology literature</article-title><source>BMJ Open</source><volume>9</volume><elocation-id>e033962</elocation-id><pub-id pub-id-type="doi">10.1136/bmjopen-2019-033962</pub-id><pub-id pub-id-type="pmid">31892667</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>SV</given-names></name><name><surname>Sreedhara</surname><given-names>SK</given-names></name><name><surname>Schneeweiss</surname><given-names>S</given-names></name><collab>REPEAT Initiative</collab></person-group><year iso-8601-date="2022">2022</year><article-title>Reproducibility of real-world evidence studies using clinical practice data to inform regulatory and coverage decisions</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>5126</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-32310-3</pub-id><pub-id pub-id-type="pmid">36045130</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>PRISMA 2020 flow diagram showing the inclusion and exclusion criteria for our screening process (<xref ref-type="bibr" rid="bib22">Page et al., 2021</xref>).</title><p>*Consider, if feasible to do so, reporting the number of records identified from each database or register searched (rather than the total number across all databases/registers). **If automation tools were used, indicate how many records were excluded by a human and how many were excluded by automation tools.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81051-app1-fig1-v2.tif"/></fig></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81051.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mangoni</surname><given-names>Arduino A</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kpzv902</institution-id><institution>Flinders Medical Centre and Flinders University</institution></institution-wrap><country>Australia</country></aff></contrib></contrib-group></front-stub><body><p>This paper in the field of metascience reports important findings on the levels of accessibility and reproducible research practices in the field of cardiovascular science. As such, it provides a solid benchmarks against which future work could be assessed. The article is of broad interest to basic and clinical cardiovascular scientists.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81051.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mangoni</surname><given-names>Arduino A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kpzv902</institution-id><institution>Flinders Medical Centre and Flinders University</institution></institution-wrap><country>Australia</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Errington</surname><given-names>Timothy M</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05d5mza29</institution-id><institution>Center for Open Science</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Bishop</surname><given-names>Dorothy VM</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.12.01.518752">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.12.01.518752v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Accessibility and Reproducible Research Practices in Cardiovascular Literature&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Matthias Barton as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Timothy M Errington (Reviewer #1); Dorothy VM Bishop (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions (for the authors):</p><p>The analysis states that to be fully reproducible, publications must include sufficient resources (materials, methods, data and analysis scripts). But how about cases where materials are not required to reproduce the work? In line 128-129 it is noted that the materials criterion was omitted for meta-analyses, but what about other types of study where materials may be either described adequately in the text, readily available (eg published questionnaires), or impossible to share (e.g. experimental animals). To see how valid these concerns might be, the first 4 papers were assessed in the deposited 'EmpricalResearchOnly.csv' file. Two had been coded as 'No Materials availability statement' and for two the value was blank.</p><p>Study 1 used registry data and was coded as missing a Materials statement. The only materials that might be useful to have might be 'standardized case report forms' that were referred to. But the authors did note that the Registry methods were fully documented elsewhere.</p><p>Study 2 was a short surgical case report – for this one the Materials field was left blank by the coder.</p><p>Study 3 was a meta-analysis; the Materials field was blank by the coder</p><p>Study 4 was again coded as lacking a Material statement. It presented a model predicting outcome for cardiac arrhythmias. The definitions of the predictor variables were provided in supplementary materials. It is not clear what other materials might be needed.</p><p>These four cases suggest that it is rather misleading to treat lack of a Materials statement as contributing to an index of irreproducibility. Certainly, there are many studies where this is the case, but it will vary from study to study depending on the nature of the research. Indeed, this may also be true for other components of the irreproducibility index: for instance, in a case study, there may be no analysis script because no statistical analysis was done. And in some papers, the raw data may all be present in the text already – that may be less common, but it is likely to be so for case studies, for instance.</p><p>A related point concerns the criteria for selecting papers for screening: it was surprising that the requirement for studies to have empirical data was not imposed at the outset: it should be possible to screen these out early on by specifying 'publication type'; instead, they were included and that means that the numbers used for the actual analysis are well below 400. The large number of non-empirical papers is not of particular relevance for the research questions considered here. In the Discussion, the authors expressed surprise at the large number of non-empirical papers they found; it would have been reasonable for them to depart from their preregistered plan on discovering this, and to review further papers to bring the number up to 400, restricting consideration to empirical papers only – also excluding case reports, which pose their own problems in this kind of analysis.</p><p>The analysis presented may create a backlash against metascientific analyses like this because it appears unfair on authors to use a metric based on criteria that may not apply to their study. If you are going to evaluate papers as to whether they include things like materials/data/ availability statements, then you need to have a N/A option. However, it may not be possible to rely on authors' self-evaluation of N/A and that means that metascientists doing an evaluation would need to read enough of the paper to judge whether such a statement should apply.</p><p>Some of the analyses could be dropped. The analysis of authorship by country, Figure 6, had too few cases for many countries to allow for sensible analysis.</p><p>It would be good to put into context efforts to replicate and reproduce papers and accessibility of materials, methods, data, code, etc – such as REPEAT initiative (https://www.repeatinitiative.org), the Reproducibility Project: Cancer Biology (https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology). Additionally, the discussion could benefit from emerging automated tools to assess transparency of materials, methods, data, code, etc (e.g., SciScore (https://www.jmir.org/2022/6/e37324), DataSeer (https://dataseer.ai/), and Ripeta (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8814593/)).</p><p>The abstract should specify that while 400 articles were screened, less than half were able to be assessed for the main findings reported – currently it is unclear.</p><p>The discussion could benefit from some concrete next steps. Such as what could be done at the journal level to make this more available? Items such as better metadata, incentivizing researchers (e.g., open science badges – https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456), reporting checklists (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5597130/), or checking articles for reproducibility (e.g., https://www.insidehighered.com/blogs/rethinking-research/should-journals-be-responsible-reproducibility) or replicability (e.g., http://www.orgsyn.org/). An overarching theme to expand on would be on what journals can do for improving their policies and incentive (e.g., https://www.science.org/doi/full/10.1126/science.aab2374). Similarly, this could be done at the funder and author level (and editor/reviewer).</p><p>Please consider reviewing the title as the authors study does not investigate reproducibility directly, but rather indirectly through a prerequisite of accessibility. So maybe consider reframing to have the title focused on the transparency 'audit' the authors did so readers do not think it should include a reproducibility 'audit'.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Transparency of Research Practices in Cardiovascular Literature&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by a Reviewing Editor and a Senior Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>(1) There is still a strong tendency to have the language center on replicability and reproducibility even though the article is entirely on transparency/accessibility of the aspects that would be needed for assessing replicability or reproducibility. While the title and abstract reflect this, it is not consistent throughout. For example, lines 277-282 still anchor on reproducible and replicable even though it's not tested – instead the authors are assessing if the materials needed for reproducibility and replicability are accessible from the published articles. Same with the figures (e.g., Figure 3).</p><p>(2) While the addition of additional projects is helpful for context, the authors might consider articles instead of websites. For example, REPEAT could be this article (https://www.nature.com/articles/s41467-022-32310-3). The reproducibility project in cancer biology could be this article (https://elifesciences.org/articles/67995) or this article (https://elifesciences.org/articles/71601). These would match the references to SciScore and Ripeta.</p><p>(3) Line 240 – it should be &quot;Open Science Framework&quot; not &quot;Open Science Foundation&quot;.</p><p>(4) Lines 352-360 should have references instead of urls to the papers.</p><p>(5) In Table 3 – I think the authors should list any p-value over 1 as 1.0 – there is no additional value in reporting them as over 1.</p><p>(6) Figure 1F, 2A, 2B – at times the authors present data where the bar graph stops even though the number presented is less than the value (e.g., 1F goes to 564 for open access button even though the axis looks like it's less than 400). The authors seem to be adjusting their axis for clarity purposes, however, it is suggested the authors adjust further for increased clarity and decreased confusion – they should show the bar and axis as broken with the axis properly reflecting the value. Alternatively, the authors should create the graphs so they are reporting the entire range. Furthermore:</p><p>a) Figure 1 could more economically be shown in a single table.</p><p>b) If Figure 2 used stacked barplots rather than single bars, then it would be possible to show the relationship between study type and the dependent variables within the same plot – this would make it easier to gain an intuitive sense of what the chi square tests were showing</p><p>c) Figure 3 is unnecessary – the numbers reported could be more economically described in the text.</p><p>d) Figures 4 and 5 again miss an opportunity to show the interesting interactions between variables, by just plotting variables one at a time. For instance, Figure 4 panel A could use stacked bars to differentiate journals, which would make panel C unnecessary. Figure 5 could use stacked bars with study type denoted by colour, which would make it easier to economically show the 4 types of accessibility criteria in a single plot (ie one bar for each criterion, with the study types stacked in a bar). This website is v useful for showing how to do this in R: http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization</p><p>(7) There are issues with reproducibility. The file EmpiricalResearchOnly(3).csv could be open but It was not immediately clear how the Accessibility.Score.Fraction had been computed. Ideally, this should be done within the script. The criteria are defined in Table 1, but it is not easy to recompute this from the available script and file for two reasons: the responses are coded as text rather than numerical, and it is unclear how NA responses (which are very frequent for some items) are handled. When trying to recreate the Accessibility Score and the Accessibility Score proportion from the raw data, results were a little different (script appended below).</p><p>As far as this reviewer could see, the problem regarding cases where Materials may not be appropriate was only partially addressed. There is a code for Study.Type.With.No.Materials, but studies coded that way are also coded as &quot;No&quot; for Materials. Flag, and it seems that it is the latter that is still used to index the Repeatable/Reproducible variables?</p><p>It's possible this has all been addressed, but it is not at all clear. It would seem appropriate to ignore the Materials Flag when computing reproducibility etc. if the study type did not require materials – I checked the first study on file, and it was of this type. It was an analysis of mortality in data records, and was coded as Study.Type.With.No.Materials. As mentioned above, though, it isn't crystal clear how these computations were done and a revised script to automate the calculations within R is required, I think, for an article focused on reproducibility.</p><p>(8) The Fully.Reproducible and Fully.Repeatable variables are identical – and indeed that is how they are described on p. 4.</p><p>(9) Para 1 of Analysis says &quot;We also screened whether studies could share Materials or not. For example, meta-analyses are empirical research studies, but do not typically have any shareable materials&quot;. Because the focus is now on empirical articles, it would be better to give an example of an empirical article that has no materials – the first study on the.csv file is of this type.</p><p>(10) Line 231 : should this be &quot;across two levels&quot;</p><p>(11) Line 266: &quot;Only 14% of articles made their materials available (56 out of 393)&quot; – need to break this down to show the proportion of papers after excluding those with no materials.</p><p>(12) Line 323; need comma or : rather than stop before &quot;For example&quot;</p><p>(13) Line 410: may be worth citing the work of Goldacre, showing that even when trials are pre-registered, variable switching is common when reporting results: Goldacre, B., et al. (2019). COMPare: A prospective cohort study correcting and monitoring 58 misreported trials in real time. Trials, 20(1), 118. https://doi.org/10.1186/s13063-019-3173-2</p><p>(14) Table 2 – it is not clear what the Yes vs No numbers are, or how these chi square values were computed. Could not find this analysis in the R script. what hypothesis is being tested here?</p><p>(15) Table 3 – because p-values greater than 1 make no sense, it may be better to follow the precedent of SPSS, and just censor these values so that the ceiling is 1. This could be explained in the legend.</p><p>Simple R script for trying to reproduce accessibility score</p><p>mydf &lt;- read_csv(paste0(here(&quot;EmpiricalResearchOnly_3.csv&quot;)))</p><p>#csv file read from OSF: 'here' just ensures we look in working directory</p><p>#Use table 1 to create binary variables a1 to a17</p><p>mydf$a1 &lt;- 1</p><p>mydf$a1[mydf$Study.Type.Clarity==&quot;Inferred (needed to read)&quot;]&lt;-0 #no NA cases here</p><p>mydf$a2 &lt;- 0</p><p>mydf$a2[mydf$Materials.Availability.Statement==&quot;Yes the statement says that the materials (or some of the materials) are available.&quot;]&lt;-1 #nb this turns NA to 0</p><p>mydf$a3&lt;-0</p><p>mydf$a3[mydf$Data.Downloadable.Openable==&quot;Yes&quot;]&lt;-1 #nb this turns NA to 0</p><p>mydf$a4&lt;-1</p><p>mydf$a4[mydf$Data.Availability.Statement==&quot;No – there is no data availability statement.&quot;]&lt;-0</p><p>mydf$a5&lt;-0</p><p>mydf$a5[mydf$Data.Downloadable.Openable==&quot;Yes&quot;]&lt;-1 #nb this turns NA to 0</p><p>mydf$a6&lt;-0</p><p>mydf$a6[mydf$Data.Clearly.Documented==&quot;Yes&quot;]&lt;-1 #most here are NA</p><p>mydf$a7&lt;-0</p><p>mydf$a7[mydf$Data.Contain.All.Raw.Data==&quot;Yes&quot;]&lt;-1 #most here are NA</p><p>mydf$a8&lt;-1</p><p>mydf$a8[mydf$Analysis.Script.Availability.Statement == &quot;No – there is no analysis script availability statement.&quot;]&lt;-0 #no NA here</p><p>mydf$a9&lt;-0</p><p>mydf$a9[mydf$Analysis.Files.Downloadable.Openable==&quot;Yes&quot;]&lt;-1 #many NA</p><p>mydf$a10&lt;-0</p><p>mydf$a10[mydf$Pre.registered.Statement==&quot;Yes – the statement says that there was a pre­-registration.&quot;]&lt;-1 #no NA</p><p>mydf$a11=0</p><p>mydf$a11[mydf$Pre.registration.Accessible.Openable==&quot;Yes&quot;]&lt;-1 #many NA</p><p>mydf$a12a=0</p><p>mydf$a12a[mydf$Pre.registration.Aspect…Hypothesis==&quot;Hypotheses&quot;]&lt;-1</p><p>mydf$a12b=0</p><p>mydf$a12b[mydf$Pre.registration.Aspect…Methods==&quot;Methods&quot;]&lt;-1</p><p>mydf$a12c=0</p><p>mydf$a12c[mydf$Pre.registration.Aspect…Methods==&quot;Analysis.Plan&quot;]&lt;-1</p><p>mydf$a12&lt;-mydf$a12a+mydf$a12b+mydf$a12c</p><p>mydf$a12[mydf$a12&lt;3]&lt;-0 #only code if all 3 are present</p><p>mydf$a12[mydf$a12==3]&lt;-1</p><p>mydf$a13&lt;-0</p><p>mydf$a13[mydf$Accessible.Protocol.Linked==&quot;Yes&quot;]&lt;-1</p><p>mydf$a14a=0</p><p>mydf$a14a[mydf$Protocol.Aspect…Analysis.Plan==&quot;Analysis plan&quot;]&lt;-1 #most are NA</p><p>mydf$a14b=0</p><p>mydf$a14b[mydf$Protocol.Aspect…Hypothesis==&quot;Hypotheses&quot;]&lt;-1 #most are NA</p><p>mydf$a14c=0</p><p>mydf$a14c[mydf$Protocol.Aspect…Methods==&quot;Methods&quot;]&lt;-1</p><p>mydf$a14&lt;-mydf$a14a+mydf$a14b+mydf$a14c</p><p>mydf$a14[mydf$a14&lt;3]&lt;-0 #only code if all 3 are present</p><p>mydf$a14[mydf$a14==3]&lt;-1</p><p>mydf$a15 &lt;-1</p><p>mydf$a15[mydf$COI.Indicated==&quot;No – there is no conflict of interest statement, and disclosures are NOT listed.&quot;]&lt;-0 #no NA</p><p>mydf$a16&lt;-0</p><p>myans&lt;-substring(mydf$Funding.Sources.Statement,1,3)</p><p>w&lt;-which(myans == &quot;Yes&quot;)</p><p>mydf$a16[w]&lt;-1</p><p>mydf$a17&lt;-1</p><p>mydf$a17[mydf$Open.Access.Article==&quot;No could not access article other than through paywall&quot;]&lt;-0</p><p>#no NA</p><p>mydf$allacc &lt;-mydf$a1+mydf$a2+mydf$a3+mydf$a4+mydf$a5+mydf$a6+mydf$a7+mydf$a8+mydf$a9+mydf$a10+mydf$a11+mydf$a12+mydf$a13+mydf$a14+mydf$a15+mydf$a16+mydf$a17</p><p>mydf$accprop&lt;-mydf$allacc/17</p><p>#Adjust for study type with no materials</p><p>w&lt;-which(mydf$Study.Type.With.No.Materials==&quot;Yes&quot;)</p><p>mydf$allacc[w] &lt;- mydf$allacc[w]-mydf$a2[w]-mydf$a3[w] #remove items a2 and a3</p><p>mydf$accprop[w] &lt;-mydf$allacc[w]/15 #adjust proportion</p><p>#visualise if computed accessibility score is same as Accessibility.Score (it isn't)</p><p>plot(mydf$allacc,mydf$Accessibility.Score)</p><p>abline(a=0,b=1)</p><p>plot(mydf$accprop,mydf$Accessibility.Score.Fraction)</p><p>abline(a=0,b=1)</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Transparency of Research Practices in Cardiovascular Literature&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by a Senior Editor and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>- Please reconsider the alternative visualizations of the figures. They don't have to remove their current figures, instead they can present alternative figure visualizations as supplementary figures so readers can see both versions.</p><p>- Figure legend 3, it should be &quot;An article is considered 'partially reproducible' if any of data availability, analysis script….&quot; (i.e., the 'if any' is missing).</p><p>- It is not sufficient to refer to an online calculator for the calculation of chi-square values. These are trivial to compute in R, and if they were part of the script, then it would be possible to work out which numbers had gone into the calculation. As it is, table 2 remains confusing because it shows numbers and a p-value for each of 4 categories of resource, but states that the chi-square test is used to test a 2-way relationship: between study type and resource. It seems we are shown only one dimension of the two-way table, which is not helpful. Furthermore, it is not clear where those numbers came from because it was unclear what variables were used.</p><p>- Given the paper's focus on reproducibility, there should be a script that generates all the tables in the paper. That allows readers to check on exactly what was done and is particularly useful if, for instance, an error is found at some point: the calculations can then be rerun to regenerate all tables and statistics (and ideally also the figures).</p><p>- Here's a bit of code that will at least do a chi-square test, and give the two-way data that is needed to interpret the relationship.</p><p>wantcat &lt;- c('Clinical Case Study or Series','Clinical Observational Study','Clinical Trial','Laboratory Animal Study')</p><p>shortdf&lt;-mydf[mydf$Study.Type %in% wantcat,] #filter Study.Type data</p><p>mytab &lt;- table(shortdf$Study.Type,shortdf$Materials.Availability.Statement.Binary) #two way table</p><p>mychi &lt;- chisq.test(mytab)</p><p>pcorr &lt;- mychi$p.value*4 #multiply by 4 for Bonferroni-corrected value</p><p>#Display 2 way table and corrected pvalue for Table 2</p><p>mytab</p><p>pcorr</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81051.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions (for the authors):</p><p>The analysis states that to be fully reproducible, publications must include sufficient resources (materials, methods, data and analysis scripts). But how about cases where materials are not required to reproduce the work? In line 128-129 it is noted that the materials criterion was omitted for meta-analyses, but what about other types of study where materials may be either described adequately in the text, readily available (eg published questionnaires), or impossible to share (e.g. experimental animals). To see how valid these concerns might be, the first 4 papers were assessed in the deposited 'EmpricalResearchOnly.csv' file. Two had been coded as 'No Materials availability statement' and for two the value was blank.</p><p>Study 1 used registry data and was coded as missing a Materials statement. The only materials that might be useful to have might be 'standardized case report forms' that were referred to. But the authors did note that the Registry methods were fully documented elsewhere.</p><p>Study 2 was a short surgical case report – for this one the Materials field was left blank by the coder.</p><p>Study 3 was a meta-analysis; the Materials field was blank by the coder</p><p>Study 4 was again coded as lacking a Material statement. It presented a model predicting outcome for cardiac arrhythmias. The definitions of the predictor variables were provided in supplementary materials. It is not clear what other materials might be needed.</p><p>These four cases suggest that it is rather misleading to treat lack of a Materials statement as contributing to an index of irreproducibility. Certainly, there are many studies where this is the case, but it will vary from study to study depending on the nature of the research. Indeed, this may also be true for other components of the irreproducibility index: for instance, in a case study, there may be no analysis script because no statistical analysis was done. And in some papers, the raw data may all be present in the text already – that may be less common, but it is likely to be so for case studies, for instance.</p></disp-quote><p>We very much appreciate the reviewer’s point about how the calculated accessibility score may be inaccurate for articles that would not, by their nature, have materials, or, in some cases, analyses. Our screening protocol was directly adapted from Iqbal et al. (2016) with very few changes. The reviewer requested clarification on how we decided a study would be expected or not expected to be able to make materials available. We determined whether articles could have material availability based on their study type and by interpretation of the abstract. For example, meta analysis were a study type that was never expected to have materials available. We did make the assumption that all empirical research articles would include methods, data, and some form of analysis. We have now updated the text to more clearly explain our definition so the readers will understand how the data was generated (see page 3). We have also included an additional paragraph in the Limitations section (see page 10) to better explain how the accessibility score derived in this study is an imperfect indicator for some studies.</p><disp-quote content-type="editor-comment"><p>A related point concerns the criteria for selecting papers for screening: it was surprising that the requirement for studies to have empirical data was not imposed at the outset: it should be possible to screen these out early on by specifying 'publication type'; instead, they were included and that means that the numbers used for the actual analysis are well below 400. The large number of non-empirical papers is not of particular relevance for the research questions considered here. In the Discussion, the authors expressed surprise at the large number of non-empirical papers they found; it would have been reasonable for them to depart from their preregistered plan on discovering this, and to review further papers to bring the number up to 400, restricting consideration to empirical papers only – also excluding case reports, which pose their own problems in this kind of analysis.</p></disp-quote><p>We appreciate the reviewer’s perspective regarding our effective sample size for empirical research papers and we agree that it is reasonable to depart from our pre-registered plan upon analysis of a large number of non-empirical papers. An additional 239 papers were screened to bring the total number of empirical papers to 393. As an introductory paper describing a developing accessibility score in cardiovascular literature, we also included some data regarding the number of non-empirical studies in a random selection of publications, and their funding and conflict of interest information.</p><disp-quote content-type="editor-comment"><p>The analysis presented may create a backlash against metascientific analyses like this because it appears unfair on authors to use a metric based on criteria that may not apply to their study. If you are going to evaluate papers as to whether they include things like materials/data/ availability statements, then you need to have a N/A option. However, it may not be possible to rely on authors' self-evaluation of N/A and that means that metascientists doing an evaluation would need to read enough of the paper to judge whether such a statement should apply.</p></disp-quote><p>Our coding form allowed a ‘N/A’ option for the inclusion of materials for empirical research studies, but our study did make the assumption that all empirical research would include a shareable protocol, data, and analysis. We agree that this assumption may not be valid for a small proportion of articles and we have expanded our discussion to more thoroughly describe this limitation. All screeners were prepared to read enough of the paper to judge whether the selection of ‘N/A’ for ‘Materials availability’ could apply.</p><disp-quote content-type="editor-comment"><p>Some of the analyses could be dropped. The analysis of authorship by country, Figure 6, had too few cases for many countries to allow for sensible analysis.</p></disp-quote><p>We appreciate and agree with the reviewer’s concern. We have now removed figure six and any discussion or analysis of the author country of origin. We have now updated the Methods section to explain why we are not including this analysis.</p><disp-quote content-type="editor-comment"><p>It would be good to put into context efforts to replicate and reproduce papers and accessibility of materials, methods, data, code, etc – such as REPEAT initiative (https://www.repeatinitiative.org), the Reproducibility Project: Cancer Biology (https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology). Additionally, the discussion could benefit from emerging automated tools to assess transparency of materials, methods, data, code, etc (e.g., SciScore (https://www.jmir.org/2022/6/e37324), DataSeer (https://dataseer.ai/), and Ripeta (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8814593/)).</p></disp-quote><p>We appreciate the reviewer’s making us aware of these studies and tools. We have now expanded the introduction to describe current efforts to replicate and reproduce papers and emerging tools to assess transparency (page 2.)</p><disp-quote content-type="editor-comment"><p>The abstract should specify that while 400 articles were screened, less than half were able to be assessed for the main findings reported – currently it is unclear.</p></disp-quote><p>We have now updated the abstract to more clearly explain the number of articles screened. We also increased the number of empirical research articles screened to 393 (page 1).</p><disp-quote content-type="editor-comment"><p>The discussion could benefit from some concrete next steps. Such as what could be done at the journal level to make this more available? Items such as better metadata, incentivizing researchers (e.g., open science badges – https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456), reporting checklists (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5597130/), or checking articles for reproducibility (e.g., https://www.insidehighered.com/blogs/rethinking-research/should-journals-be-responsible-reproducibility) or replicability (e.g., http://www.orgsyn.org/). An overarching theme to expand on would be on what journals can do for improving their policies and incentive (e.g., https://www.science.org/doi/full/10.1126/science.aab2374). Similarly, this could be done at the funder and author level (and editor/reviewer).</p></disp-quote><p>We completely agree and we have now expanded the Discussion section to emphasize the steps that journals and funders can take to improve transparent research practices (page 9).</p><disp-quote content-type="editor-comment"><p>Please consider reviewing the title as the authors study does not investigate reproducibility directly, but rather indirectly through a prerequisite of accessibility. So maybe consider reframing to have the title focused on the transparency 'audit' the authors did so readers do not think it should include a reproducibility 'audit'.</p></disp-quote><p>We have changed the title from “Accessibility and Reproducible Research Practices in Cardiovascular Literature” to “Transparency of Research Practices in Cardiovascular Literature.” (page 1).</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>(1) There is still a strong tendency to have the language center on replicability and reproducibility even though the article is entirely on transparency/accessibility of the aspects that would be needed for assessing replicability or reproducibility. While the title and abstract reflect this, it is not consistent throughout. For example, lines 277-282 still anchor on reproducible and replicable even though it's not tested – instead the authors are assessing if the materials needed for reproducibility and replicability are accessible from the published articles.</p></disp-quote><p>We have changed the language in the introduction to clarify that our study provides information about the availability of materials needed to facilitate reproduction/replication of a research study, not the reproducibility/replication study itself (page 3).</p><disp-quote content-type="editor-comment"><p>Same with the figures (e.g., Figure 3).</p></disp-quote><p>We have similarly clarified the language in the figure 3 caption (page 17).</p><disp-quote content-type="editor-comment"><p>(2) While the addition of additional projects is helpful for context, the authors might consider articles instead of websites. For example, REPEAT could be this article (https://www.nature.com/articles/s41467-022-32310-3). The reproducibility project in cancer biology could be this article (https://elifesciences.org/articles/67995) or this article (https://elifesciences.org/articles/71601). These would match the references to SciScore and Ripeta.</p></disp-quote><p>We thank the reviewers for additional references. We have expanded the introduction to incorporate all suggested references to match to SciScore, Ripeta, and REPEAT (page 2).</p><disp-quote content-type="editor-comment"><p>(3) Line 240 – it should be &quot;Open Science Framework&quot; not &quot;Open Science Foundation&quot;.</p></disp-quote><p>We changed “Open Science Foundation” to “Open Science Framework” (page 6).</p><disp-quote content-type="editor-comment"><p>(4) Lines 352-360 should have references instead of urls to the papers.</p></disp-quote><p>We have converted all urls into references, and added new references to the citations (page 8).</p><disp-quote content-type="editor-comment"><p>(5) In Table 3 – I think the authors should list any p-value over 1 as 1.0 – there is no additional value in reporting them as over 1.</p></disp-quote><p>We have updated the p-values in table 3 to have a ceiling of 1 (page 13).</p><disp-quote content-type="editor-comment"><p>(6) Figure 1F, 2A, 2B – at times the authors present data where the bar graph stops even though the number presented is less than the value (e.g., 1F goes to 564 for open access button even though the axis looks like it's less than 400). The authors seem to be adjusting their axis for clarity purposes, however, it is suggested the authors adjust further for increased clarity and decreased confusion – they should show the bar and axis as broken with the axis properly reflecting the value. Alternatively, the authors should create the graphs so they are reporting the entire range. Furthermore:</p><p>a) Figure 1 could more economically be shown in a single table.</p></disp-quote><p>We have now fixed the axes for figure 1. Although we agree that this data could also be shown in a table, we prefer to display the information with a figure to help with readability. Additionally, because each element (Panels A-F) has different variables, we would need to use a similar arrangement of six subtables.</p><disp-quote content-type="editor-comment"><p>b) If Figure 2 used stacked barplots rather than single bars, then it would be possible to show the relationship between study type and the dependent variables within the same plot – this would make it easier to gain an intuitive sense of what the chi square tests were showing</p></disp-quote><p>We appreciate the reviewer’s observation and we have updated the figure axes for accuracy. Although stacked barplots may help show relationships across variables, we prefer to show individual bars to ensure readability.</p><disp-quote content-type="editor-comment"><p>c) Figure 3 is unnecessary – the numbers reported could be more economically described in the text.</p></disp-quote><p>Although we report the numbers for figure 3 in the text, we prefer to also show the data graphically to help with readability.</p><disp-quote content-type="editor-comment"><p>d) Figures 4 and 5 again miss an opportunity to show the interesting interactions between variables, by just plotting variables one at a time. For instance, Figure 4 panel A could use stacked bars to differentiate journals, which would make panel C unnecessary. Figure 5 could use stacked bars with study type denoted by colour, which would make it easier to economically show the 4 types of accessibility criteria in a single plot (ie one bar for each criterion, with the study types stacked in a bar). This website is v useful for showing how to do this in R: http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization</p></disp-quote><p>We again appreciate the reviewer’s suggestion to use stacked bars, however we prefer to use single bars to help with readability. It is very hard for readers to compare values across stacked bars because of the lack of axis references for all but the outer- and inner-most bar segments.</p><disp-quote content-type="editor-comment"><p>(7) There are issues with reproducibility. The file EmpiricalResearchOnly(3).csv could be open but It was not immediately clear how the Accessibility.Score.Fraction had been computed. Ideally, this should be done within the script. The criteria are defined in Table 1, but it is not easy to recompute this from the available script and file for two reasons: the responses are coded as text rather than numerical, and it is unclear how NA responses (which are very frequent for some items) are handled. When trying to recreate the Accessibility Score and the Accessibility Score proportion from the raw data, results were a little different (script appended below).</p><p>As far as this reviewer could see, the problem regarding cases where Materials may not be appropriate was only partially addressed. There is a code for Study.Type.With.No.Materials, but studies coded that way are also coded as &quot;No&quot; for Materials. Flag, and it seems that it is the latter that is still used to index the Repeatable/Reproducible variables?</p><p>It's possible this has all been addressed, but it is not at all clear. It would seem appropriate to ignore the Materials Flag when computing reproducibility etc. if the study type did not require materials – I checked the first study on file, and it was of this type. It was an analysis of mortality in data records, and was coded as Study.Type.With.No.Materials. As mentioned above, though, it isn't crystal clear how these computations were done and a revised script to automate the calculations within R is required, I think, for an article focused on reproducibility.</p></disp-quote><p>We thank the reviewer for their insights and for taking the time to develop an R script to calculate the Accessibility Score. We had previously calculated this score using Tableau, but we agree it would be more transparent and reproducible to calculate this variable using a script. We have therefore created an R script to calculate the Accessibility Score and made this code available on OSF.</p><p>Creating this code did reveal a discrepancy in our Tableau-calculation for the accessibility score, and our current calculation now aligns with that generated by the code provided by the reviewer. We have updated the figures and manuscript to reflect the changed data.</p><disp-quote content-type="editor-comment"><p>(8) The Fully.Reproducible and Fully.Repeatable variables are identical – and indeed that is how they are described on p. 4.</p></disp-quote><p>We have corrected the statement to reflect our original screening criteria and definitions of reproducible and repeatability (page 4).</p><disp-quote content-type="editor-comment"><p>(9) Para 1 of Analysis says &quot;We also screened whether studies could share Materials or not. For example, meta-analyses are empirical research studies, but do not typically have any shareable materials&quot;. Because the focus is now on empirical articles, it would be better to give an example of an empirical article that has no materials – the first study on the.csv file is of this type.</p></disp-quote><p>We have included an example of an empirical article that has no materials (page 4) and included a citation (page 19).</p><disp-quote content-type="editor-comment"><p>(10) Line 231 : should this be &quot;across two levels&quot;</p></disp-quote><p>We have corrected this typo (page 5).</p><disp-quote content-type="editor-comment"><p>(11) Line 266: &quot;Only 14% of articles made their materials available (56 out of 393)&quot; – need to break this down to show the proportion of papers after excluding those with no materials.</p></disp-quote><p>We made an additional clarification statement that the 393 papers mentioned in this statement have theoretically available materials, meaning these articles do not include papers with no materials (page 6). This statement is also made in the caption of Figure 2B.</p><disp-quote content-type="editor-comment"><p>(12) Line 323; need comma or : rather than stop before &quot;For example&quot;</p></disp-quote><p>We have added a comma (page 7).</p><disp-quote content-type="editor-comment"><p>(13) Line 410: may be worth citing the work of Goldacre, showing that even when trials are pre-registered, variable switching is common when reporting results: Goldacre, B., et al. (2019). COMPare: A prospective cohort study correcting and monitoring 58 misreported trials in real time. Trials, 20(1), 118. https://doi.org/10.1186/s13063-019-3173-2</p></disp-quote><p>We agree and now reference this study in the Discussion (page 9).</p><disp-quote content-type="editor-comment"><p>(14) Table 2 – it is not clear what the Yes vs No numbers are, or how these chi square values were computed. Could not find this analysis in the R script. what hypothesis is being tested here?</p></disp-quote><p>We are testing the hypothesis that there is a relationship between study type and the presence of materials, methods, data, and analysis. We added this to our table caption and clarified that “yes” refers to the number of articles that have the resource and “no” refers to the number of articles that do not have the resource. We used an online calculator, not an R script to calculate the scores. We have updated the methods and references to cite this tool (page 13).</p><disp-quote content-type="editor-comment"><p>(15) Table 3 – because p-values greater than 1 make no sense, it may be better to follow the precedent of SPSS, and just censor these values so that the ceiling is 1. This could be explained in the legend.</p></disp-quote><p>We followed the precedent of SPSS as suggested and explained why values over 1 were censored in the caption (page 13).</p><p>[Editors’ note: what follows is the authors’ response to the third round of review.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>- Please reconsider the alternative visualizations of the figures. They don't have to remove their current figures, instead they can present alternative figure visualizations as supplementary figures so readers can see both versions.</p></disp-quote><p>We appreciate the reviewer’s suggestion, but we respectfully decline remaking alternative visualizations of the figures. Because supplementary figures are typically used to show additional data, as opposed to alternative visualizations or data that is already shown, we think that additional visualizations of the same data may confuse the readers.</p><disp-quote content-type="editor-comment"><p>- Figure legend 3, it should be &quot;An article is considered 'partially reproducible' if any of data availability, analysis script….&quot; (i.e., the 'if any' is missing).</p></disp-quote><p>Figure legend 3 was corrected to include “if any of.” (page 17)</p><disp-quote content-type="editor-comment"><p>- It is not sufficient to refer to an online calculator for the calculation of chi-square values. These are trivial to compute in R, and if they were part of the script, then it would be possible to work out which numbers had gone into the calculation. As it is, table 2 remains confusing because it shows numbers and a p-value for each of 4 categories of resource, but states that the chi-square test is used to test a 2-way relationship: between study type and resource. It seems we are shown only one dimension of the two-way table, which is not helpful. Furthermore, it is not clear where those numbers came from because it was unclear what variables were used.</p></disp-quote><p>We sincerely appreciate the reviewer’s concerns regarding the reproducibility of the statistical analyses. We now provide R code to accompany the paper that calculate the chi-square values used in Table 2 and Table 3. Note that upon recalculating these tests in R, we obtained slightly different values than in the previous manuscript submission; however none of the implications have changed. We have updated the methods (page 5), and the tables themselves (p13) accordingly.</p><disp-quote content-type="editor-comment"><p>- Given the paper's focus on reproducibility, there should be a script that generates all the tables in the paper. That allows readers to check on exactly what was done and is particularly useful if, for instance, an error is found at some point: the calculations can then be rerun to regenerate all tables and statistics (and ideally also the figures).</p><p>- Here's a bit of code that will at least do a chi-square test, and give the two-way data that is needed to interpret the relationship.</p><p>wantcat &lt;- c('Clinical Case Study or Series','Clinical Observational Study','Clinical Trial','Laboratory Animal Study')</p><p>shortdf&lt;-mydf[mydf$Study.Type %in% wantcat,] #filter Study.Type data</p><p>mytab &lt;- table(shortdf$Study.Type,shortdf$Materials.Availability.Statement.Binary) #two way table</p><p>mychi &lt;- chisq.test(mytab)</p><p>pcorr &lt;- mychi$p.value*4 #multiply by 4 for Bonferroni-corrected value</p><p>#Display 2 way table and corrected pvalue for Table 2</p><p>mytab</p><p>pcorr</p></disp-quote><p>We appreciate the reviewer’s perspective regarding the reproducibility of the analysis methods. We now include R code that calculates the values for tables 2 and 3, as well as the associated chi-square tests. Table 1 and Table 4 are not generated from the original data and cannot be generated in R.</p></body></sub-article></article>