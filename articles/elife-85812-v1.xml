<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">85812</article-id><article-id pub-id-type="doi">10.7554/eLife.85812</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Towards biologically plausible phosphene simulation for the differentiable optimization of visual cortical prostheses</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-301836"><name><surname>van der Grinten</surname><given-names>Maureen</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-301848"><name><surname>de Ruyter van Steveninck</surname><given-names>Jaap</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2711-0889</contrib-id><email>jaap.deruyter@donders.ru.nl</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-301838"><name><surname>Lozano</surname><given-names>Antonio</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4508-1484</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-301839"><name><surname>Pijnacker</surname><given-names>Laura</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-301840"><name><surname>Rueckauer</surname><given-names>Bodo</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-33135"><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1625-0034</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-113897"><name><surname>van Gerven</surname><given-names>Marcel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2206-9098</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-301841"><name><surname>van Wezel</surname><given-names>Richard</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-271755"><name><surname>Güçlü</surname><given-names>Umut</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-301842"><name><surname>Güçlütürk</surname><given-names>Yağmur</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05csn2x06</institution-id><institution>Netherlands Institute for Neuroscience, Vrije Universiteit</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Donders Institute for Brain Cognition and Behaviour, Radboud University Nijmegen</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006hf6230</institution-id><institution>Biomedical Signals and Systems Group, University of Twente</institution></institution-wrap><addr-line><named-content content-type="city">Enschede</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>02</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>13</volume><elocation-id>e85812</elocation-id><history><date date-type="received" iso-8601-date="2022-12-28"><day>28</day><month>12</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-01-21"><day>21</day><month>01</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-12-23"><day>23</day><month>12</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.12.23.521749"/></event></pub-history><permissions><copyright-statement>© 2024, van der Grinten, de Ruyter van Steveninck, Lozano et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>van der Grinten, de Ruyter van Steveninck, Lozano et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-85812-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-85812-figures-v1.pdf"/><abstract><p>Blindness affects millions of people around the world. A promising solution to restoring a form of vision for some individuals are cortical visual prostheses, which bypass part of the impaired visual pathway by converting camera input to electrical stimulation of the visual system. The artificially induced visual percept (a pattern of localized light flashes, or ‘phosphenes’) has limited resolution, and a great portion of the field’s research is devoted to optimizing the efficacy, efficiency, and practical usefulness of the encoding of visual information. A commonly exploited method is non-invasive functional evaluation in sighted subjects or with computational models by using simulated prosthetic vision (SPV) pipelines. An important challenge in this approach is to balance enhanced perceptual realism, biologically plausibility, and real-time performance in the simulation of cortical prosthetic vision. We present a biologically plausible, PyTorch-based phosphene simulator that can run in real-time and uses differentiable operations to allow for gradient-based computational optimization of phosphene encoding models. The simulator integrates a wide range of clinical results with neurophysiological evidence in humans and non-human primates. The pipeline includes a model of the retinotopic organization and cortical magnification of the visual cortex. Moreover, the quantitative effects of stimulation parameters and temporal dynamics on phosphene characteristics are incorporated. Our results demonstrate the simulator’s suitability for both computational applications such as end-to-end deep learning-based prosthetic vision optimization as well as behavioral experiments. The modular and open-source software provides a flexible simulation framework for computational, clinical, and behavioral neuroscientists working on visual neuroprosthetics.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>simulated prosthetic vision</kwd><kwd>cortical stimulation</kwd><kwd>bionic vision</kwd><kwd>blindness</kwd><kwd>deep learning</kwd><kwd>neurotechnology</kwd><kwd>neural implants</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>P15-42 'NESTOR'</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name><name><surname>van Wezel</surname><given-names>Richard</given-names></name><name><surname>van Gerven</surname><given-names>Marcel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>17619 'INTENSE'</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name><name><surname>Güçlütürk</surname><given-names>Yağmur</given-names></name><name><surname>van Wezel</surname><given-names>Richard</given-names></name><name><surname>van Gerven</surname><given-names>Marcel</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100019188</institution-id><institution>HORIZON EUROPE Excellent Science</institution></institution-wrap></funding-source><award-id>899287 'NeuraViper'</award-id><principal-award-recipient><name><surname>Rueckauer</surname><given-names>Bodo</given-names></name><name><surname>Güçlütürk</surname><given-names>Yağmur</given-names></name><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>650003</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>024.005.022 'DBI2'</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name><name><surname>van Gerven</surname><given-names>Marcel</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>823-02-010</award-id><principal-award-recipient><name><surname>Roelfsema</surname><given-names>Pieter</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The provided framework based on biological models and clinical literature can aid in the optimization of visual cortical prostheses through computational or behavioral simulation experiments, serving as a flexible tool for computational, clinical, and behavioral neuroscientists working on visual neuroprosthetics.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Globally, as per 2020, an estimated 43.3 million people were blind (<xref ref-type="bibr" rid="bib13">Bourne et al., 2021</xref>). For some cases of blindness, visual prosthetics may provide a promising solution. These devices aim to restore a rudimentary form of vision by interacting with the visual system using electrical stimulation (<xref ref-type="bibr" rid="bib34">Fernandez, 2018</xref>; <xref ref-type="bibr" rid="bib9">Bloch et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Nowik et al., 2020</xref>). In particular, our work concerns prosthetic devices that target the primary visual cortex (V1). Despite recent advances in the field, more research is required before cortical prosthesis will become clinically available. Besides research into the improvement of the safety and durability of cortical implants (<xref ref-type="bibr" rid="bib21">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>), a great portion of the research attention is devoted to optimizing the efficacy, efficiency, and practical usefulness of the prosthetic percepts. The artificially induced visual percepts consist of patterns of localized light flashes (‘phosphenes’) with a limited resolution. To achieve a functional level of vision, scene-processing is required to condense complex visual information from the surroundings in an intelligible pattern of phosphenes (<xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>; <xref ref-type="bibr" rid="bib57">Lozano et al., 2020</xref>; <xref ref-type="bibr" rid="bib68">Normann et al., 2009</xref>; <xref ref-type="bibr" rid="bib84">Sanchez-Garcia et al., 2020</xref>; <xref ref-type="bibr" rid="bib99">Troyk, 2017</xref>; <xref ref-type="bibr" rid="bib43">Granley et al., 2022a</xref>; <xref ref-type="bibr" rid="bib47">Han et al., 2021</xref>). Many studies employ a SPV paradigm to non-invasively evaluate the functional quality of the prosthetic vision with the help of sighted subjects (<xref ref-type="bibr" rid="bib26">de Ruyter van Steveninck et al., 2022b</xref>; <xref ref-type="bibr" rid="bib7">Beyeler et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Normann et al., 2009</xref>; <xref ref-type="bibr" rid="bib18">Cha et al., 1992a</xref>; <xref ref-type="bibr" rid="bib22">Dagnelie et al., 2006</xref>; <xref ref-type="bibr" rid="bib23">Dagnelie et al., 2007</xref>; <xref ref-type="bibr" rid="bib47">Han et al., 2021</xref>; <xref ref-type="bibr" rid="bib102">Vergnieux et al., 2014</xref>; <xref ref-type="bibr" rid="bib92">Srivastava et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">Cha et al., 1992b</xref>; <xref ref-type="bibr" rid="bib91">Sommerhalder et al., 2004</xref>; <xref ref-type="bibr" rid="bib11">Bollen et al., 2019</xref>; <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib77">Pezaris and Reid, 2008</xref>; <xref ref-type="bibr" rid="bib105">Vurro et al., 2014</xref>; <xref ref-type="bibr" rid="bib81">Rassia et al., 2022</xref>) or through ‘end-to-end’ approaches, using in silico models (<xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>; <xref ref-type="bibr" rid="bib52">Küçükoğlu et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Granley et al., 2022a</xref>). Although the aforementioned SPV literature has provided us with important insights, the perceptual realism of electrically generated phosphenes and some aspects of the biological plausibility of the simulations can be further improved by integrating knowledge of phosphene vision and its underlying physiology. Given the steadily expanding empirical literature on cortically-induced phosphene vision, it is both feasible and desirable to have a more phenomenologically accurate model of cortical prosthetic vision. Such an accurate simulator has already been developed for retinal prostheses (<xref ref-type="bibr" rid="bib7">Beyeler et al., 2017</xref>), which has formed an inspiration for our work on simulation of cortical prosthetic vision. Thus, in this current work, we propose a realistic, biologically inspired computational model for the simulation of cortical prosthetic vision. Biological plausibility, in our work’s context, points to the simulation’s ability to capture essential biological features of the visual system in a manner consistent with empirical findings: our simulator integrates quantitative findings and models from the literature on cortical stimulation in V1. The elements that are modeled in our simulator include cortical magnification, current-dependent spread of activation, and charge-dependent activation thresholds. Furthermore, our simulator models the effects of specific stimulation parameters, accounting for temporal dynamics. A schematic overview of the pipeline and some example outputs are displayed in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Our simulator runs in real-time, is open-source, and makes use of fully differentiable functions, which is an essential requirement for the gradient-based optimization of phosphene encoding models with machine learning. This design enables both simulations with sighted participants, as well as end-to-end optimization in machine-learning frameworks, thus fitting the needs of fundamental, clinical, and computational scientists working on neuroprosthetic vision. All of the underlying models in the simulator are flexibly and modularly implementend allowing for easy integration with external software (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Although the simulator, by default, assumes round phosphene percepts that are independently activated, it can be tailored to simulate custom alternative scenarios: the phosphene maps can be adjusted to simulate different shapes (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), and it is possible to simulate arbitrary interactions between electrodes with minor adaptations to our code (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). The modular design of the simulator allows for future extensions to simulate brain stimulation in other regions such as lateral geniculate nucleus (LGN) or higher visual areas (<xref ref-type="bibr" rid="bib65">Murphey and Maunsell, 2007</xref>; <xref ref-type="bibr" rid="bib76">Pezaris and Reid, 2007</xref>; <xref ref-type="bibr" rid="bib71">Panetsos et al., 2011</xref>; <xref ref-type="bibr" rid="bib63">Mirochnik and Pezaris, 2019</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Left: schematic illustration of our simulator pipeline.</title><p>Our simulator is initialized with electrode locations on a visuotopic map of the visual cortex. Each frame, the simulator takes a set of stimulation parameters that for each electrode specify the amplitude, pulse width, and frequency of electrical stimulation. Based on the electrode locations on the cortical map and the stimulation parameters, the phosphene characteristics are estimated and for each phosphene the effects are rendered on a map of the visual field. Finally, the phosphene renderings are summed to obtain the resulting simulated prosthetic percept. Right: Example renderings after initializing the simulator with four 10 × 10 electrode arrays (indicated with roman numerals) placed in the right hemisphere (electrode spacing: 0.4 mm, in correspondence with the commonly used ‘Utah array’ <xref ref-type="bibr" rid="bib58">Maynard et al., 1997</xref>). The output is visualized for 166 ms pulse trains with stimulation amplitudes of 40, 80, and 120 µA, a pulse width of 170 ms, and a frequency of 300Hz. In these example frames, we can observe the effects of cortical magnification, thresholds for activation, current-dependent spread (size) and proportion (brightness) of cortical tissue activation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Demonstration of the use of the simulator in conjunction with external software for receptive field prediction (<xref ref-type="bibr" rid="bib41">Goebel et al., 2006</xref>).</title><p>(<bold>A</bold>) On the left, a 3D brain model with receptive field mapping (color indicates eccentricity in the visual field) is used to simulate the placement of four 8-by-8 Utah-like electrode arrays on the accessible parts of visual cortex (V1) in the left hemisphere. The inset shows a close-up of the electrodes and the expected locations of the visual receptive fields (color indicates eccentricity). (<bold>B</bold>) On the right, we see the resulting simulation of stimulating these electrodes with a current amplitude of 70 µA, after 167 ms of continuous stimulation. To simulate imperfect knowledge of the electrode or phosphene locations, a small normally distributed noise (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn><mml:mi>deg</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>) was added to the determined receptive fields. Note that as the simulator is initialized with the phosphene locations in the visual field, any assumptions about feasible electrode locations, uncertainties, and other sources of noise can be incorporated flexibly.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Example simulation of irregular phosphene shapes.</title><p>In our software, it is relatively straightforward to change the shape of individual phosphenes. The simulator includes the functionality of initializing with elongated phosphenes and the phosphene maps can also be manually adjusted to simulate arbitrary shapes. (<bold>A</bold>) Schematic illustration of the modeled visual field location. (<bold>B</bold>) The simulation output after initialization with 50 elongated phosphenes with random orientations. (<bold>C</bold>) Simulation output with manually customized phosphene shapes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Example simulations of irregular phosphene percepts that can arise upon multi-electrode stimulation of three electrodes.</title><p>By default, the simulator assumes independence between electrodes and their corresponding percepts. However, the code can be easily customized to simulate interactions between electrodes. In each of these examples, the base input is a stimulation vector describing the activation of three electrodes with a current of 100 µA. The pulse width was set to 170 µs, and the frequency was set to 300 Hz. Before being sent to the simulator, the stimulation vector is processed by different custom interaction models. (<bold>A</bold>) <italic>No interactions</italic>: the input is multiplied with the identity matrix before feeding to the simulator, resulting in phosphenes I, II, and III. (<bold>B</bold>) <italic>Coactivation</italic>: phosphenes I and II are activated by a weighted sum of the first two electrodes, simulating a ‘leak’ between the electrodes, resulting in brighter phosphenes. (<bold>C</bold>) <italic>Merging</italic>: a non-linear interaction model is used consisting of a two-layer fully connected network with ReLU activation. When simultaneously activated, phosphenes I and II are replaced by a phosphene that falls in between the original locations. (<bold>D</bold>) <italic>Arbitrary percept</italic>: similar to panel (C), the percepts of phosphenes I and II are replaced by a new percept when simultaneously activated. The new percept is an oblique line.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig1-figsupp3-v1.tif"/></fig></fig-group><sec id="s1-1"><title>Background and related work</title><sec id="s1-1-1"><title>Cortical prostheses</title><p>Early attempts by Brindley and Lewin, and Dobelle successfully reported the ability to reliably induce the perception of phosphenes (described as localized round flashes of light) via electrical stimulation of the cortical surface (<xref ref-type="bibr" rid="bib15">Brindley and Lewin, 1968</xref>; <xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>). More recent preclinical studies demonstrate promising results concerning the safety and efficacy of long-term stimulation in the primary visual cortex, either via surface electrodes (<xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Beauchamp et al., 2020</xref>) or with intracortical electrodes (<xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>; <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>). Other studies that performed V1 stimulation in sighted subjects (<xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>) and non-human primates (<xref ref-type="bibr" rid="bib87">Schiller et al., 2011</xref>; <xref ref-type="bibr" rid="bib21">Chen et al., 2020</xref>) have shown similar success. Some milestones include the implantation of over 1000 electrodes in a monkey’s visual cortex (<xref ref-type="bibr" rid="bib21">Chen et al., 2020</xref>), and the testing of a preliminary artificial vision system that presents visual information from the surroundings to a blind subject using a penetrating multi-electrode array in the visual cortex (<xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>). Taken together, the previous literature provides strong evidence for the clinical potential of cortical prostheses for the blind.</p></sec><sec id="s1-1-2"><title>Perceptual reports on cortical prosthetic vision</title><p>In our simulator, we integrate empirical findings and quantitative models from existing literature on electrical stimulation in the visual cortex. Stimulation in V1 with intracortical electrodes is estimated to activate tens to thousands of neurons (<xref ref-type="bibr" rid="bib48">Histed et al., 2009</xref>), resulting in the perception of often ‘featureless’ white dots of light with a circular shape (<xref ref-type="bibr" rid="bib15">Brindley and Lewin, 1968</xref>; <xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>; <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>; <xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>). Due to the cortical magnification (the foveal information is represented by a relatively large surface area in the visual cortex as a result of variation of retinal RF size) the size of the phosphene increases with its eccentricity (<xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>). Furthermore, phosphene size, stimulation threshold (defined as the minimum current to produce a visible phosphene 50% of the time) and brightness depend on stimulation parameters such as the pulse width, train length, amplitude, and frequency of stimulation (<xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>; <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>; <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>). To account for these effects, we integrated and adapted previously proposed quantitative models that estimate the charge-dependent activation level of cortical tissue (<xref ref-type="bibr" rid="bib95">Tehovnik and Slocum, 2007</xref>; <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Geddes, 2004</xref>; <xref ref-type="bibr" rid="bib16">Bruce et al., 1999</xref>; <xref ref-type="bibr" rid="bib50">Kim et al., 2017</xref>). Furthermore, our simulator includes a model of the temporal dynamics, observed by <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>, accounting for response-attenuation after prolonged or repeated stimulation, as well as the delayed ‘offset’ of phosphene perception.</p></sec><sec id="s1-1-3"><title>Simulated prosthetic vision</title><p>A wide range of previous studies has employed SPV with sighted subjects to non-invasively investigate the usefulness of prosthetic vision in everyday tasks, such as mobility (<xref ref-type="bibr" rid="bib18">Cha et al., 1992a</xref>; <xref ref-type="bibr" rid="bib23">Dagnelie et al., 2007</xref>; <xref ref-type="bibr" rid="bib26">de Ruyter van Steveninck et al., 2022b</xref>; <xref ref-type="bibr" rid="bib47">Han et al., 2021</xref>; <xref ref-type="bibr" rid="bib103">Vergnieux et al., 2017</xref>), hand-eye coordination (<xref ref-type="bibr" rid="bib92">Srivastava et al., 2009</xref>), reading (<xref ref-type="bibr" rid="bib18">Cha et al., 1992a</xref>; <xref ref-type="bibr" rid="bib91">Sommerhalder et al., 2004</xref>), or face recognition (<xref ref-type="bibr" rid="bib11">Bollen et al., 2019</xref>; <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>). Several studies have examined the effect of the number of phosphenes, spacing between phosphenes and the visual angle over which the phosphenes are spread (e.g. <xref ref-type="bibr" rid="bib26">de Ruyter van Steveninck et al., 2022b</xref>; <xref ref-type="bibr" rid="bib97">Thorn et al., 2020</xref>; <xref ref-type="bibr" rid="bib85">Sanchez-Garcia et al., 2022</xref>; <xref ref-type="bibr" rid="bib92">Srivastava et al., 2009</xref>; <xref ref-type="bibr" rid="bib74">Parikh et al., 2013</xref>). The results of these studies vary widely, which could be explained by the difference in the implemented tasks, or, more importantly, by the differences in the simulation of phosphene vision. The aforementioned studies used varying degrees of simplification of phosphene vision in their simulations. For instance, many included equally-sized phosphenes that were uniformly distributed over the visual field (informally referred to as the ‘scoreboard model’). Furthermore, most studies assumed either full control over phosphene brightness or used binary levels of brightness (e.g. ‘on’ / ‘off’), but did not provide a description of the associated electrical stimulation parameters. Several studies have explicitly made steps towards more realistic phosphene simulations, by taking into account cortical magnification or using visuotopic maps (<xref ref-type="bibr" rid="bib108">Wong et al., 2010</xref>; <xref ref-type="bibr" rid="bib55">Li, 2013</xref>; <xref ref-type="bibr" rid="bib92">Srivastava et al., 2009</xref>; <xref ref-type="bibr" rid="bib73">Paraskevoudi and Pezaris, 2021</xref>), simulating noise and electrode dropout (<xref ref-type="bibr" rid="bib23">Dagnelie et al., 2007</xref>), or using varying levels of brightness (<xref ref-type="bibr" rid="bib103">Vergnieux et al., 2017</xref>; <xref ref-type="bibr" rid="bib85">Sanchez-Garcia et al., 2022</xref>; <xref ref-type="bibr" rid="bib74">Parikh et al., 2013</xref>). However, no phosphene simulations have modeled temporal dynamics or provided a description of the parameters used for electrical stimulation. Some recent studies developed descriptive models of the phosphene size or brightness as a function of the stimulation parameters (<xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>). Another very recent study has developed a deep-learning based model for predicting a realistic phosphene percept for single stimulating electrodes (<xref ref-type="bibr" rid="bib44">Granley et al., 2022b</xref>). These studies have made important contributions to improve our understanding of the effects of different stimulation parameters. The present work builds on these previous insights to provide a full simulation model that can be used for the functional evaluation of cortical visual prosthetic systems. Meanwhile, a realistic and biologically-plausible simulator has been developed for retinal prosthetic vision (Pulse2Percept, <xref ref-type="bibr" rid="bib7">Beyeler et al., 2017</xref>), which takes into account the axonal spread of activation along ganglion cells and temporal nonlinearities to construct plausible simulations of stimulation patterns. Even though scientists increasingly realize that more realistic models of phosphene vision are required to narrow the gap between simulations and reality (<xref ref-type="bibr" rid="bib47">Han et al., 2021</xref>; <xref ref-type="bibr" rid="bib24">Dagnelie, 2008</xref>), a biophysically-grounded simulation model for the functional evaluation of cortical prosthetic vision remains to be developed. Realistic SPV can aid technological developments by allowing neuroscientists, clinicians, and engineers to test the perceptual effects of changes in stimulation protocols, and subsequently select stimulation parameters that yield the desired phosphene percepts without the need for extensive testing in blind volunteers. A realistic simulator could also be used as support in the rehabilitation process, assisting clinicians and caregivers in identifying potential problematic situations and adapt preprocessing or stimulation protocols accordingly (<xref ref-type="bibr" rid="bib24">Dagnelie, 2008</xref>).</p></sec><sec id="s1-1-4"><title>Deep learning-based optimization of prosthetic vision</title><p>SPV is often employed to develop, optimize, and test encoding strategies for capturing our complex visual surroundings in an informative phosphene representation. Numerous scene-processing methods have been proposed in the literature, ranging from basic edge detection, or contour-detection algorithms (<xref ref-type="bibr" rid="bib14">Boyle et al., 2001</xref>; <xref ref-type="bibr" rid="bib31">Dowling et al., 2004</xref>; <xref ref-type="bibr" rid="bib45">Guo et al., 2018</xref>) to more intelligent deep learning-based approaches, which can be tailored to meet task-specific demands (<xref ref-type="bibr" rid="bib84">Sanchez-Garcia et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Han et al., 2021</xref>; <xref ref-type="bibr" rid="bib79">Rasla and Beyeler, 2022</xref>; <xref ref-type="bibr" rid="bib11">Bollen et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>; <xref ref-type="bibr" rid="bib57">Lozano et al., 2020</xref>; <xref ref-type="bibr" rid="bib56">Lozano et al., 2018</xref>). The advantages of deep learning-based methods are clear: more intelligent and flexible extraction of useful information in camera input leads to less noise or unimportant information in the low-resolution phosphene representation, allowing for more successful completion of tasks. Some recent studies demonstrated that the simulation of prosthetic vision can even be incorporated directly into the deep learning-based optimization pipeline for end-to-end optimization (<xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>; <xref ref-type="bibr" rid="bib52">Küçükoğlu et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Granley et al., 2022a</xref>). With end-to-end optimization, the image processing can be tailored to an individual user or specific tasks. Here, the usefulness of the prosthetic vision is evaluated by a computational agent, or decoder model, which assists in the direct optimization of the stimulation parameters required to optimally encode the present image. Again, an important drawback is that the current computational studies use simplified simulations of cortical prosthetic vision. This problem is addressed in the current study. Note that end-to-end machine learning pipelines rely on gradient propagation to update the parameters of the phosphene encoding model. Consequently, a crucial requirement is that the simulator makes use of differentiable operations to convert the stimulation parameters to an image of phosphenes - a requirement that is met by the proposed simulator. To evaluate the practical usability of our simulator in an end-to-end framework, we replicated and adapted the experiments by <xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>, replacing their simulator with ours. The currently proposed simulator can be compared to the simulator that was used in the aforementioned work: our simulator can handle temporal sequences and our experiments explore a more biologically grounded simulation of phosphene size and locations. Furthermore, instead of a more abstract or qualitative description of the required stimulation (‘on’ / ‘off’), we included biologically inspired methods to model the perceptual effects of different stimulation parameters such as the current amplitude, the duration, the pulse width, and the frequency. This opens new doors for optimization of the stimulation parameters in realistic ranges: although technological developments advance the state-of-the-art hardware capabilities rapidly, cortical prosthesis devices will be operating under energy constraints, due to both hardware limitations as well as safety limits regarding neurostimulation (<xref ref-type="bibr" rid="bib90">Shannon, 1992</xref>; <xref ref-type="bibr" rid="bib59">McCreery et al., 2002</xref>). Deep learning methods trained in tandem with a biologically plausible phosphene simulator can be leveraged to produce constrained optimal stimulation paradigms that take these limitations into account, allowing for safe and viable stimulation protocols to be developed.</p></sec></sec></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><p>Our simulator is implemented in Python, using the PyTorch deep learning library (<xref ref-type="bibr" rid="bib75">Paszke et al., 2019</xref>). The simulator makes use of differentiable functions which, given the entire set of phosphenes and their modeled properties, calculate the brightness of each pixel in the output image in parallel. This architecture makes our model memory intensive, but allows for fast computations that can be executed on a GPU. Each frame, the simulator maps electrical stimulation parameters (stimulation current, pulse width, and frequency) to an estimated phosphene perception, taking into account the stimulation history. In the sections below, we discuss the different components of the simulator model, followed by a description of some showcase experiments that assess the ability to fit clinical data and the practical usability of our simulator in simulation experiments. Our simulator can be imported as a python package and the source code is available on GitHub. The source code of our simulator can be retrieved from <ext-link ext-link-type="uri" xlink:href="https://github.com/neuralcodinglab/dynaphos">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib27">de Ruyter van Steveninck, 2024a</xref>). The latest stable release can be installed using pip:</p><p><monospace>$ pip install dynaphos</monospace>.</p><sec id="s2-1"><title>Visuotopic mapping</title><p>Our simulator can be flexibly initialized with a list of electrode locations or phosphene locations to the simulator to base the simulations on clinical data. We also provide the code for generating a random list of phosphene locations based on equally-distant electrode locations on a flattened cortical map of V1. For mapping phosphene locations from the cortical electrode locations to the neuroprosthesis user’s visual field, our simulator uses the reverse wedge-dipole visuotopic model of V1, proposed by <xref ref-type="bibr" rid="bib78">Polimeni et al., 2006</xref>. This model maps a complex polar coordinate <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in the visual field, to a cortical location <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> in one hemisphere of V1, following the visuotopic relationship<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>α</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>α</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>≤</mml:mo><mml:mi>θ</mml:mi><mml:mo>≤</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> are the eccentricity and azimuth of the point in the visual field, the parameter <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the shear of the ‘wedge map’, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a scaling factor that scales the mapping to realistic proportions in cortical distance (millimeters), and <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are parameters that control the singularities of the dipole model. For the mapping from cortical coordinates to phosphene location, we use the inverse of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, which is given by<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>for the inverse shearing operation<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mfrac><mml:mi>θ</mml:mi><mml:mi>α</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The visuotopic model also provides us with the cortical magnification <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which defines the relative amount of cortical tissue that is involved in processing of visual information, depending on the eccentricity in the visual field. The cortical magnification is given by the derivative of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> along the horizontal meridian:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> is given in millimetres of cortical surface per degree of visual angle. The parameters of the models are configurable. The default values are specified below, in the ‘Parameter estimates’ section. Note that in our simulation software, we provide the option of substituting <xref ref-type="disp-formula" rid="equ1 equ2 equ3 equ4">Equations 1, 2, 3 and 4</xref>, with other estimates described such as the mono- or dipole model in <xref ref-type="bibr" rid="bib78">Polimeni et al., 2006</xref>; <xref ref-type="bibr" rid="bib89">Schwartz, 1983</xref>. Moreover, to simulate imperfect knowledge of electrode or phosphene locations, and malfunctioning electrodes, the cortical mapping methods include parameters for the introduction of noise and electrode dropout. We note, however, that the framework is compatible with the retinotopic maps of other structures, such as the LGN, which is the structure providing input to the primary visual cortex.</p></sec><sec id="s2-2"><title>Phosphene size</title><p>Based on a model by <xref ref-type="bibr" rid="bib95">Tehovnik and Slocum, 2007</xref>, the phosphene size (in degrees),<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>D</mml:mi><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>is obtained via an estimation of the current spread from the stimulating electrodes, where<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mi>I</mml:mi><mml:mi>K</mml:mi></mml:mfrac></mml:msqrt></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>is the diameter of the activated cortical tissue (in mm), for stimulation current <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (in µA) and excitability constant <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (in <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>). Note that the cortical magnification factor <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is obtained in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>. The default value for <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is specified below, in the ‘Parameter estimates’ section. In our simulation software, we provide the option to substitute <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> with an estimate by <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>. Based on verbal descriptions (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>; <xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>), phosphenes are shown as Gaussian blobs with two standard deviations set equal to the phosphene size <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, such that 95% of the Gaussian falls within the fitted phosphene size.</p></sec><sec id="s2-3"><title>Phosphene brightness</title><p>The brightness and detection threshold of each phosphene are based on a model of the intracortical tissue activation in response to electrical stimulation with biphasic square pulse trains. The model assumes brightness and detection thresholds of phosphene perception to be primarily correlated with the deposited charge, and accounts for the relative inefficiency of higher stimulation frequencies, longer pulse widths, and longer train durations, as found in <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>. We model the combined effects of these stimulation parameters as follows: First, we subtract from the stimulation amplitude <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> a leak current <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which represents the ineffective component of the stimulation input, and a memory trace <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (further explained in section ‘Temporal dynamics’) that accounts for the decreased neural excitability after prior stimulation. <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is set equal to the rheobase current (the absolute threshold for continuous stimulation at infinite duration), following prior literature on the strength-duration relationship of neural tissue activation for isolated single-pulse trials (<xref ref-type="bibr" rid="bib37">Geddes, 2004</xref>). To calculate the effective stimulation current of trains of pulses, the remaining current amplitude is multiplied with the duty cycle of the stimulation signal (<inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>w</mml:mi><mml:mo>⋅</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the fraction of one period in which the signal is active), such that<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mi>w</mml:mi><mml:mo>⋅</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>for pulse width <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and frequency <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then, the cortical tissue activation is estimated by integrating the effective input current over multiple frames, using a leaky integrator model. By integrating over time, this model additionally implements the delayed on- and offset as described by several studies (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>; <xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>). For each frame with duration <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the estimated cortical activation is updated as<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>A</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the time constant of the activation decay in seconds and <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a parameter that scales the duration of the stimulation relative to the frame duration. By default, <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula> is set to 1 to simulate a stimulation duration equal to the frame duration, where the total pulse train duration is controlled with the number of successive frames in which stimulation is provided to the simulator. Finally, if the cortical activation reaches the detection threshold (explained in the following section), the phosphene is activated with a brightness equal to the sigmoidal activation<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the slope of the sigmoidal curve and <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the value of <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for which the phosphene reaches half the maximum brightness.</p></sec><sec id="s2-4"><title>Stimulation threshold</title><p>Our simulator uses a thresholding model based on psychometric data from <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>. Phosphenes are only generated when the cortical tissue activation reaches the activation threshold <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is obtained for each electrode separately upon initialization of the simulator. To introduce a degree of variability between electrodes, <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is sampled from the normal distribution<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The default values of the 50% probability threshold <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the standard deviation <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are fit on data from <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref> and can be found below, in the ‘Parameter estimates’ section. Note that, by default, the detection thresholds remain constant after initialization. However, in accordance to the user requirements, the values can be flexibly adjusted or re-initialized manually.</p></sec><sec id="s2-5"><title>Temporal dynamics</title><p>Using a memory trace of the stimulation history, the simulator accounts for basic accommodation effects on brightness for prolonged or repeated stimulation, as described in prior work (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>). Each frame, the memory trace is dynamically updated as follows:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>B</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the time constant of the trace decay in seconds, and the parameter <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>κ</mml:mi></mml:mstyle></mml:math></inline-formula> controls the input effect. Note that the memory trace is used for the phosphene brightness and not the phosphene size. Because there is little experimental data on the temporal dynamics of phosphene size in relation to the accumulated charge, only the instantaneous current is used in the calculation of the phosphene size.</p></sec><sec id="s2-6"><title>Parameter estimates</title><p>By default, our model uses the parameters specified below. Unless stated otherwise, these parameter estimates were obtained by fitting our model to experimental data using the SciPy Python package, version 1.9.0 (<xref ref-type="bibr" rid="bib104">Virtanen et al., 2020</xref>). More details on the comparison between the models’ estimates and the experimental data can be found in the Results section. Note that the parameter settings may strongly depend on the specific experimental conditions (such as the type of electrodes).</p><list list-type="bullet"><list-item><p>In <xref ref-type="disp-formula" rid="equ1 equ2 equ3 equ4">Equations 1, 2, 3 and 4</xref> we use <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>17.3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, based on a fit by <xref ref-type="bibr" rid="bib78">Polimeni et al., 2006</xref> on data of the human V1 from <xref ref-type="bibr" rid="bib49">Horton, 1991</xref>.</p></list-item><list-item><p>In <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, the parameter <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is set to 675 μA mm<sup>–2</sup>, following an estimate by <xref ref-type="bibr" rid="bib94">Tehovnik et al., 2006</xref>, who measured the responses to intracortical stimulation in V1 at different current levels.</p></list-item><list-item><p>In <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, we use a rheobase current <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of 23.9 μA based on a fit on data from <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>. Here, we used the strength-duration curve for tissue-excitability <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with minimal input charge <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, chronaxie parameter <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> and total stimulation duration <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> as described in <xref ref-type="bibr" rid="bib37">Geddes, 2004</xref>.</p></list-item><list-item><p>In <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, the parameter <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is set equal to 1. The parameter <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is set equal to 0.111 s to reflect qualitative descriptions found in <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>. Note: this parameter is not obtained by fitting to experimental data.</p></list-item><list-item><p>In <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>, we use a slope <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>19.2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and offset <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.06</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for the brightness curve, based on a fit of our model on data by <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>.</p></list-item><list-item><p>The descriptive parameters of the distribution 11, are set to <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>9.14</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>6.72</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, based on a fit on psychometric data by <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>.</p></list-item><list-item><p>In <xref ref-type="disp-formula" rid="equ1 equ13">Equations 12 and 13</xref>, we use <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> set to 1.97 × 10<sup>3</sup> s and <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> set to 14.0. These values are based on a fit of our model to data from <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>.</p></list-item></list></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>In this section, we present the results of computational experiments and comparisons with the literature to verify and validate the biological realism, the performance, and the practical usability of the simulator.</p><sec id="s3-1"><title>Biological plausibility</title><p>Here, we report on experimental data obtained from the literature, and evaluate the capacity of our simulator of fitting these empirical data. Using <xref ref-type="disp-formula" rid="equ7 equ8 equ9 equ10">Equations 7, 8, 9 and 10</xref>, the simulator accurately reproduces the relative phosphene brightness that was reported in the previous study for different stimulation amplitudes (<inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.950</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; verification on the same data that was used to fit the model; <xref ref-type="fig" rid="fig2">Figure 2</xref>). <xref ref-type="fig" rid="fig3">Figure 3</xref> visualizes the effect of changing the stimulation parameters on the probability of phosphene perception, as estimated by our model. We compare our estimates with data reported by <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>. The model accurately fits the reported effect of pulse width, frequency and train duration on the probability of phosphenes perception. To evaluate the robustness of this fit, a threefold cross-validation was performed (<xref ref-type="fig" rid="fig3">Figure 3</xref> panel (d)) where part of the data were held-out (i.e. the data in this panel were predicted and not in the data set used for the fit). In this more strict analysis, the prediction performance is still accurate (average <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.844</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> (not part of the main analysis) displays the cross-validation results after fitting the simulator to thresholding data from clinical studies that used cortical surface electrodes (<xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>; <xref ref-type="bibr" rid="bib40">Girvin et al., 1979</xref>; <xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Estimate of the relative phosphene brightness for different stimulation amplitudes.</title><p>The simulator was provided with a stimulation train of 166 ms with a pulse width of 170 µs at a frequency of 300 Hz (see <xref ref-type="disp-formula" rid="equ7 equ8 equ9 equ10">Equations 7, 8, 9 and 10)</xref>. Left: the fitted brightness levels reproduced by our model (red) and psychometric data reported by <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref> (light blue). Note that for stimulation amplitudes of 20 µA and lower, the simulator generated no phosphenes as the threshold for activation was not reached. Right: the modeled tissue activation and brightness response over time.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig2-v1.tif"/></fig><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Probability of phosphene perception for different stimulation parameters.</title><p>(<bold>A–C</bold>) Psychometric curves (solid lines) overlaid on experimental data (dashed lines) (<xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>; <xref ref-type="fig" rid="fig2">Figure 2a and b</xref>). The model’s probability of phosphene perception is visualized as a function of charge per phase for (<bold>A</bold>) different pulse widths, (<bold>B</bold>) different frequencies, and (<bold>C</bold>) different train durations. Note that rather than the total charge per trial, we report the charge per phase to facilitate easy comparison with aforementioned experimental data. In panel (<bold>D</bold>) the probabilities of phosphene perception reproduced with our model are compared to the detection probabilities reported in (<xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>; <xref ref-type="fig" rid="fig2">Figure 2a and b</xref>). Predicted probabilities in panel (D) are the results of a threefold cross-validation on held-out test data. Colors conform to the conditions in panels A, B, and C.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Sixfold cross-validation results of fitting the simulator to data from several clinical studies that used cortical surface electrodes (<xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>; <xref ref-type="bibr" rid="bib40">Girvin et al., 1979</xref>; <xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>).</title><p>The predicted current threshold (<bold>A</bold>) and total charge threshold (<bold>B</bold>) are displayed as a function of the effective stimulation duration. The model fit (dashed lines) is overlaid on several sources of experimental data (solid points) from <xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref> (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.873</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), Dobelle and Mladejovsky (<inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.616</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and <xref ref-type="bibr" rid="bib40">Girvin et al., 1979</xref> (<inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.919</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). In the respective publications, experimental data were read out from graphs (<xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Girvin et al., 1979</xref>) and tables (<xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Demonstration of the effect of the model’s temporal dynamics over time.</title><p>Stimulation currents between 5 and 180 µA were simulated for 7s, using a pulse width of 170 µs at a frequency of 300 Hz. The figures show the lower left quadrant of the visual field, where 130 phosphenes are located. (<bold>A</bold>) The first five frames of the simulation, show the delayed onset of phosphenes and increase in brightness. (<bold>B</bold>) Frames at subsequent regular intervals, showing gradual phosphene fading.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig3-figsupp2-v1.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig4">Figure 4</xref> displays the simulator’s fit on the temporal dynamics found in a previous published study by <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>. Here, cross-validation was not feasible due to the limited amount of quantitative data. For repeated stimulation at different timescales (intervals of 4 s, and intervals of 200 s), the brightness of a single phosphene is evaluated after fitting the memory trace parameters. The observed accommodation effects in the simulator are compared to the data from <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>. <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> shows the effect of the modeled temporal dynamics on the simulator’s output for continuous stimulation over 7 s.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Relative brightness of a phosphene in response to repeated stimulation, overlaid on experimental results by <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>.</title><p>The stimulation sequence consisted of 50 pulse trains at a 4 s stimulation interval, followed by five pulse trains at an interval of 200 s to test recovery. The simulator was provided with a stimulation train of 125 ms with a pulse width of 100 µs at a frequency of 200 Hz using a stimulation amplitude of 90 µA. Please notice the split x-axis with variable scaling.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig4-v1.tif"/></fig></sec><sec id="s3-2"><title>Performance</title><p>We tested the computational efficiency of our simulator, by converting a pre-processed example video (1504 frames) into simulated phosphene images, for different numbers of phosphenes, and at varying image resolutions. The used example video can be downloaded via this link. The simulator was run on a CUDA-enabled graphics card (NVIDIA A30) and each setting was run five times. The results are displayed in <xref ref-type="fig" rid="fig5">Figure 5</xref>. The lowest measured frame rate (10.000 phosphenes at a resolution of 256 × 256) was 28.7 frames per second. Note that the missing combinations in <xref ref-type="fig" rid="fig5">Figure 5</xref> indicate that the required memory exceeded the capacity of our GPU, as the simulation of large numbers of phosphenes at high resolutions can be memory intensive. Notably, even on a consumer-grade GPU (e.g. a 2016 model GeForce GTX 1080) the simulator still reaches real-time processing speeds (&gt;100 fps) for simulations with 1000 phosphenes at 256 × 256 resolution. Some additional example videos are included in the online version of this article (<xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Performance as a function of resolution and number of phosphenes.</title><p>The data is based on five runs of 1540 frames per condition, with batch size equal to one frame. Simulation was run with an NVIDIA A30 GPU (memory size: 24 GB). Crosses indicate missing conditions. Note that these data are presented only for evaluating the software-performance. For some combinations of phosphene count and image resolution (e.g. 10.000 phosphenes in a 64 × 64 image) there are fewer pixels than phosphenes. The error bars indicate the 95-percent confidence interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig5-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-85812-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Example phosphene simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-85812-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Example phosphene simulation.</title></caption></media></sec><sec id="s3-3"><title>Usability in a deep learning SPV pipeline</title><p>To validate that the simulator can conveniently be incorporated in a machine learning pipeline, we replicated an existing SPV pipeline by <xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>, replacing the simulator of that study with our biologically plausible simulator. We performed several phosphene encoding optimization experiments, described below. In this pipeline, a convolutional neural network encoder is trained to process images or video frames and generate adequate electrode stimulation parameters. To train the encoder, a simulation of the prosthetic percept is generated by the phosphene simulator. This simulated percept is evaluated by a second convolutional neural network, the decoder, which decodes the simulated percept into a reconstruction of the original input image (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The quality of the phosphene encoding is optimized by iteratively updating the network parameters of the encoder and decoder (simultaneously) using backpropagation of the reconstruction error. In addition to the reconstruction error, which measures similarity between the reconstruction and the input, we used a regularization term that measures similarity between the phosphenes and the input. For a more detailed description of the end-to-end optimization pipeline, see <xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Schematic illustration of the end-to-end machine-learning pipeline adapted from <xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>.</title><p>A convolutional neural network encoder is trained to convert input images or video frames into a suitable electrical stimulation protocol. In the training procedure, the simulator generates a simulation of the expected prosthetic percept, which is evaluated by a second convolutional neural network that decodes a reconstruction of the input image. The quality of the encoding is iteratively optimized by updating the network parameters using back-propagation. Different loss terms can be used to constrain the phosphene encoding, such as the reconstruction error between the reconstruction and the input, a regularization loss between the phosphenes and the input, or a supervised loss term between the reconstructions and some ground-truth labeled data (not depicted here). Note that the internal parameters of the simulator (e.g. the estimated tissue activation) can also be used as loss terms.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig6-v1.tif"/></fig><sec id="s3-3-1"><title>Dynamic end-to-end encoding of videos</title><p>In the first experiment, we explored the potential of using our simulator in a dynamic end-to-end encoding pipeline. The simulator is initialized with 1000 possible phosphenes in both hemifields, covering a field of view of 16° of visual angle. Note that the simulated electrode density and placement differs from current prototype implants and the simulation can be considered to be an ambitious scenario from a surgical point of view, given the folding of the visual cortex and the part of the retinotopic map in V1 that is buried in the calcarine sulcus. We extended the previously published pipeline with 3D-convolutions (with an additional temporal dimension) to enable encoding of subsequent video frames. The model was trained on a basic video dataset with moving white digits on a black background (Moving MNIST Dataset, <xref ref-type="bibr" rid="bib93">Srivastava et al., 2015</xref>). We used video sequences of five frames. The framerate of the simulation was set at five frames per second. We used a combination of two equally-weighted mean squared error (MSE) loss functions: the MSE loss between reconstruction and input, and the MSE loss between the simulated phosphene representation and the input. <xref ref-type="fig" rid="fig7">Figure 7</xref> displays several frames after training for 45 epochs (for a total of 810,000 training examples). We can observe that the model has successfully learned to represent the original input frames in phosphene vision over time, and the decoder is able to approximately reconstruct the original input.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Results of training the end-to-end pipeline on video sequences from the moving MNIST dataset (<xref ref-type="bibr" rid="bib93">Srivastava et al., 2015</xref>).</title><p>Columns indicate different frames. Top row: the input frames; middle row: the simulated phosphene representations; bottom row: the decoded reconstructions of the input.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig7-v1.tif"/></fig></sec><sec id="s3-3-2"><title>Constrained stimulation and naturalistic scenes</title><p>In a second experiment, we trained the end-to-end model with a more challenging dataset containing complex images of naturalistic scenes (the ADE20K dataset <xref ref-type="bibr" rid="bib109">Zhou et al., 2019</xref>). In this experiment, we implemented the original pipeline described in <xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref> (experiment 4), with the same phosphene coverage as the previously described experiment. The images were normalized and converted to grayscale, and we applied a circular mask such that the corners (outside the field covered by phosphenes) were ignored in the reconstruction task. The experiment consisted of three training runs, in which we tested different conditions: a free optimization condition, a constrained optimization condition, and a supervised boundary reconstruction condition. In the free optimization condition, the model was trained using an equally weighted combination of a MSE reconstruction loss between input and reconstruction, and a MSE regularization loss between the phosphenes and input images. After six epochs the model found an optimal encoding strategy that can accurately represent the scene and allows the decoder to accurately reconstruct pixel intensities while qualitatively maintaining the image structure (see <xref ref-type="fig" rid="fig8">Figure 8</xref>). Importantly, the encoder encoded brighter areas of the input picture with large stimulation amplitudes (over 2000 µA). The encoding strategy found in such an unconstrained optimization scheme is not feasible for real-life applications. In practice, the electrical stimulation protocol will need to satisfy safety bounds and it will need to comply with technical requirements and limitations of the stimulator hardware. For instance, rather than continuous stimulation intensities it is likely that the stimulator will allow for stimulation with only a number of (discrete) amplitudes. To evaluate whether our end-to-end pipeline can be harnessed to optimize the encoding in a constrained context, we performed a second training run (the constrained condition) where we reconfigured the encoder to output 10 discrete values between 0 and 128 µA. We used straight-through estimation with a smooth staircase function to estimate the gradients during backpropagation. To compensate for the relative sparsity of phosphenes in the SPV representation, we increased the training stability by taking the regularization loss as the MSE between the pixel brightness at the phosphene centers and the corresponding pixel brightness in the input image. Furthermore, to encourage large spatial correspondence with input stimuli, we adapted the relative weights of the reconstruction loss and the regularization loss to 0.00001 and 0.99999, respectively. Note that the regularization loss only promotes similarity between the phosphene encoding and the input and the decoder is unaffected by the regularization loss. The results of the safety-constrained training after six epochs are visualized in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Note that overall, the resulting phosphenes are less bright and smaller due to the lower stimulation amplitudes. Nevertheless, the decoder is able to accurately reconstruct the original input. One limitation is that we did not test the subjective interpretability for human observers. As not all information in the scene is equally important, it may be informative to further constrain the phosphene representation to encode specific task-relevant features. In a third training run (the supervised boundary condition) we validated whether our simulator can be used in a supervised machine learning pipeline for the reconstruction of specific target features, such as the object boundaries. Instead of using the input image as a reference, now the MSE is used between the reconstruction and a ground truth target image, and between the pixel brightness at the phosphene centers and the corresponding pixel brightness in the target image. The ground truth semantic boundary targets were obtained by performing canny edge detection and subsequent line thickening on the semantic segmentation labels provided with the dataset. The results after training for 16 epochs are visualized in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Note that the model successfully converged to a sparse phosphene encoding that selectively represents the object boundaries. Enlarged and inverted visualizations can be found in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref> and <xref ref-type="fig" rid="fig8s2">2</xref>.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Results of training our simulator in an end-to-end pipeline on naturalistic images from the ADE20K dataset (<xref ref-type="bibr" rid="bib109">Zhou et al., 2019</xref>).</title><p>In the constrained optimization condition and the supervised boundary reconstruction condition, the encoder was configured to output 10 discrete stimulation amplitudes within the safe range of stimulation (0 to 128 µA). The selected images represent the first three categories in the validation dataset (‘Abbey’, ‘Access Road,’ ‘Airbase’). Note that the brightness is enhanced in the phosphene images of the constrained optimization and the supervised boundary condition by 40%. (See <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> and <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref> for enlarged and inverted simulated prosthetic vision (SPV) images).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Enlarged version of the simulated prosthetic vision (SPV) representations in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</title><p>From left to right, the columns indicate: the input images; the SPV representations in the unconstrained optimization condition; the SPV representations in the constrained optimization condition; the SPV representations in the supervised boundary reconstruction condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig8-figsupp1-v1.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Inverted grayscale version of the simulated prosthetic vision (SPV) representations in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</title><p>From left to right, the columns indicate: the input images; the SPV representations in the unconstrained optimization condition; the SPV representations in the constrained optimization condition; the SPV representations in the supervised boundary reconstruction condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig8-figsupp2-v1.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 3.</label><caption><title>Example results after training an end-to-end model with a small electrode count.</title><p>The simulator was initialized with a random subset of 60 electrodes from a 10 × 10 array with electrode spacing of 0.4 mm, matching the location and spacing of array no. IV in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The model was trained on an image dataset of white characters on a black background. The top row displays three example inputs, and the bottom row shows the simulated prosthetic vision SPV encoding found by the encoder model after training.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig8-figsupp3-v1.tif"/></fig><fig id="fig8s4" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 4.</label><caption><title>Results after training an end-to-end model on a small electrode count (60 electrodes) with different interaction conditions.</title><p>In the <italic>baseline condition</italic>, the model was trained without electrode interactions. In the <italic>costimulation loss condition</italic>, no electrode interactions were simulated, but an additional loss component was introduced to discourage simultaneous activation of neighboring electrode pairs. In the <italic>coactivation condition</italic>, no additional loss function was used, but an interaction model was implemented in the simulator. For each active electrode, a coactivation leak current was added from neighboring electrodes. Left panel: the percentage of active electrode pairs with a relative distance smaller than 1 mm. <italic>Costimulation loss</italic> resulted in significantly smaller number of active neighboring electrode pairs. Adding a <italic>coactivation</italic> current resulted in a significant increase of simultaneously active neighboring electrodes. (p &lt; 0.001 for both comparisons with the baseline condition). The error bars indicate the 95-percent confidence interval. Right panel: the coactivation matrix characterizing the magnitude of the leak current between electrodes in the <italic>coactivation condition</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-fig8-figsupp4-v1.tif"/></fig></fig-group></sec><sec id="s3-3-3"><title>Small electrode counts and interactions</title><p>The previously described experiments simulate implant designs with many electrodes. As a supplementary experiment, we verified that the end-to-end model can also be trained with smaller phosphene counts. The simulator was initialized with a random subset of 60 electrodes from a 10 × 10 array with electrode spacing 0.4 mm, matching the location and spacing of array no. IV in <xref ref-type="fig" rid="fig1">Figure 1</xref>. An image dataset was used with white characters on a black background. We tested three conditions to address potential interaction effects between neighboring electrodes. Besides a baseline training, we evaluated training the model with an additional loss component to avoid unexpected interactions by discouraging simultaneous activation of neighboring electrode pairs. This co-stimulation loss component is defined as<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>for stimulation currents <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> the squared distance between the electrodes in mm. In a final training condition, instead of avoiding interactions using a loss component, we explicitly included an interaction model in the phosphene simulation. An electrode coactivation interaction was implemented, where current of active electrodes ‘leaks’ to neighboring activated electrodes based on their distance. For each active electrode pair <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> we added the coactivation current<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>100</mml:mn><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>to the stimulation current <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> used in the simulation. The results are visualized in <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplements 3</xref> and <xref ref-type="fig" rid="fig8s4">4</xref>. The found encoding strategy resulted in distinct letter shapes. The letters are, however, poorly recognizable which is unsurprising with the minimal electrode resolution. The costimulation loss resulted in a lower percentage of active neighboring electrodes (at a distance &lt; 1 mm). The electrode coactivation resulted in a higher percentage of active neighboring electrodes compared to the baseline, suggesting that the encoder learns to make use of the leak current.</p></sec></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>The aim of this study is to present a biologically plausible phosphene simulator, which takes realistic ranges of stimulation parameters, and generates a phenomenologically accurate representation of phosphene vision using differentiable functions. In order to achieve this, we have modeled and incorporated an extensive body of work regarding the psychophysics of phosphene perception. The results indicate that our simulator is able to produce phosphene percepts that match the descriptions of phosphene vision that were gathered in basic and clinical visual neuroprosthetics studies over the past decades. When we used a GPU, the simulator ran in real-time, and as such, it could be used in experiments with sighted volunteers. Furthermore, our proof-of-principle computational experiments demonstrate the suitability of the simulator for machine learning pipelines, aimed at improving the image processing and stimulation strategies. Here, we discuss some implications of our findings.</p><sec id="s4-1"><title>Validation experiments</title><sec id="s4-1-1"><title>Visuotopic mapping</title><p>The results presented in <xref ref-type="fig" rid="fig1">Figure 1</xref> illustrate the value of including a visuotopic model based on the spread of cortical activation to realistically estimate phosphene locations and size. Some previous studies have used a model of cortical magnification (<xref ref-type="bibr" rid="bib92">Srivastava et al., 2009</xref>; <xref ref-type="bibr" rid="bib73">Paraskevoudi and Pezaris, 2021</xref>) or visuotopic mapping (<xref ref-type="bibr" rid="bib108">Wong et al., 2010</xref>; <xref ref-type="bibr" rid="bib55">Li, 2013</xref>) in their phosphene simulations. However, our simulator is the first to incorporate empirical models of the current spread in cortical tissue (<xref ref-type="bibr" rid="bib95">Tehovnik and Slocum, 2007</xref>; <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>) to simulate the effects of stimulation current on the phosphene size. The accurate modeling of this biophysical relationship can help to increase the validity of simulation studies and brings fundamental SPV research closer to addressing questions regarding the practical real-life requirements of a visual prosthesis. Furthermore, the explicit link between the modeled area of cortical activation and the simulated phosphene sizes and locations makes our software suitable for including new receptive field modeling results (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for an example simulation based on 3D receptive field modelling using third party software). Future studies that target other structures than V1 that contain a retinotopic map, such as the LGN, can also use the simulator by replacing the V1 map with a retinotopic map of the respective brain structure. Collaborative international projects such as the PRIMatE Resource Exchange (PRIME-RE) offer advanced tools which allow to fit probabilistic retinotopic maps generated from large samples to any individual NHP brain (<xref ref-type="bibr" rid="bib51">Klink et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Messinger et al., 2021</xref>) and it is currently possible to accurately predict human cortical receptive field mapping based on anatomical scans (<xref ref-type="bibr" rid="bib5">Benson et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Benson et al., 2014</xref>), or other retinotopic mapping strategies that do not rely on visual input (<xref ref-type="bibr" rid="bib42">Goebel et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Bock et al., 2015</xref>). This opens new doors for future research into the functionality of visual prostheses with limited visual field coverage. Thanks to the machine learning compatibility, the model can also be used for the pre-operative optimization of the implant placement and design (<xref ref-type="bibr" rid="bib100">van Hoof, 2022</xref>).</p></sec><sec id="s4-1-2"><title>Threshold and brightness</title><p>The results presented in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref> indicate that our simulator closely models existing psychophysical data on the stimulation thresholds and phosphene brightness, for different electrical stimulation settings. Note that the effects found by <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref> (that were modeled by us) are consistent with findings by other studies, which report brighter phosphenes for higher stimulation strengths (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>), and a lower stimulation efficiency (i.e. higher total charge thresholds) for longer pulse trains or higher pulse widths and frequencies (<xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>). While the simulator is developed for intracortical electrodes and stimulation amplitudes in the range of micro-amperes, there is a wide range of literature describing the use of electrodes which are placed on the cortical surface (e.g. <xref ref-type="bibr" rid="bib15">Brindley and Lewin, 1968</xref>; <xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>; <xref ref-type="bibr" rid="bib40">Girvin et al., 1979</xref>; <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Niketeghad et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Beauchamp et al., 2020</xref>). These electrodes require higher currents to elicit phosphenes (in the range of milli-amperes), but the mechanisms underlying the generation of phosphenes are presumably similar to those of intracortical electrodes. The results from <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, suggest that the implemented thresholding model for intracortical stimulation also generalizes to surface stimulation. Moreover, our results are in line with other computational models for detection thresholds in the somatosensory cortex (<xref ref-type="bibr" rid="bib36">Fridman et al., 2010</xref>; <xref ref-type="bibr" rid="bib50">Kim et al., 2017</xref>). Our results indicate how a leaky integrator model and normally-distributed activation thresholds, provide a suitable approximation of the tissue activation in cortical prostheses. Note that alternative, more complex, models can possibly predict the psychometric data more accurately. However, most probably, this will entail a trade-off with the simplicity and modularity of the current simulator. Future research may further improve our understanding of the neural processing underlying the conscious perception of phosphenes, possibly borrowing insights from the domain of natural vision. More elaborate theories on this matter have been developed and tested in <xref ref-type="bibr" rid="bib101">van Vugt et al., 2018</xref>. More specific limitations and suggestions for future adaptations are discussed below, in the section ‘General limitations and future directions’.</p></sec><sec id="s4-1-3"><title>Temporal dynamics</title><p>The results presented in <xref ref-type="fig" rid="fig4">Figure 4</xref> reveal that the model accounts for experimental data on the accommodation in response to repeated stimulation in time periods up to 200 s. However, in contrast to the findings by <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>, our simulator predicts a moderate recovery over the next 1000s. Although we cannot provide an explanation for this difference, the modeled recovery largely stays within the 95% confidence interval of the experimental data. Similar to the other components of our simulator, the memory trace was chosen as a basic, yet effective, model of neural habituation. Possibly, a more complex, non-linear, model can more accurately fit the neurophysiological data. However, the presented model has the benefit of simplicity (there are only three parameters). Also, note that there is still some ambivalence in the clinical data. In contrast to the findings by <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>, some studies have found accommodation over different time scales and sometimes no accommodation at all (<xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>; <xref ref-type="bibr" rid="bib3">Bartlett et al., 1977</xref>; <xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>). More research is required for a better understanding of the neural response after repeated or prolonged stimulation.</p></sec><sec id="s4-1-4"><title>Phosphene shape and appearance</title><p>The appearance of phosphenes in our simulation (white, round, soft dots of light) are largely in line with previous reports on intracortical stimulation (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>; <xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>; <xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>). However, more elongated shapes and more complex shapes have been reported as well (<xref ref-type="bibr" rid="bib12">Bosking et al., 2017</xref>; <xref ref-type="bibr" rid="bib107">Winawer and Parvizi, 2016</xref>; <xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>). By using separately generated phosphene renderings, our simulator enables easy adjustments of the appearance of individual phosphenes. Additionally, we incorporated the possibility to change the default Gaussian blob appearance into Gabor patches with a specific frequency and orientation. Regarding the color of phosphenes, there is still some ambivalence in the reports, including descriptions of phosphene color ranging from black or white to different tones of color (<xref ref-type="bibr" rid="bib95">Tehovnik and Slocum, 2007</xref>; <xref ref-type="bibr" rid="bib96">Tehovnik et al., 2009</xref>; <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>). Notably, increasing the stimulation amplitudes can lead the appearance to shift from colored to yellowish or white (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>). This effect may be explained by the increased current spread for higher stimulation amplitudes, which is predicted to span multiple cortical columns coding for different visual characteristics (e.g. orientation or color), thus giving rise to phosphenes with amalgamated features (<xref ref-type="bibr" rid="bib95">Tehovnik and Slocum, 2007</xref>). Currently, the limited amount of systematic data render it difficult to enable more accurate simulations of the variability in phosphene appearance.</p></sec></sec><sec id="s4-2"><title>End-to-end optimization</title><sec id="s4-2-1"><title>Dynamic encoding</title><p>The results presented in <xref ref-type="fig" rid="fig7">Figure 7</xref> demonstrate that our proposed realistic phosphene simulator is well-suited for the dynamic optimization in an end-to-end architecture. Our proof-of-principle video-encoding experiments are the first to explicitly optimize the stimulation across the temporal domain. This provides a basis for the further exploration of computationally-optimized dynamic stimulation patterns. Dynamic optimization of the stimulation may be necessary to counteract unwanted effects such as response fading due to accommodation after repeated or prolonged stimulation (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>), or delayed phosphene perception after stimulation on- and offset. The inclusion of a realistic simulator in the optimization pipeline enables researchers to exploit the optimal combination of stimulation parameters to obtain precise control over the required perception. Moreover, besides acquiring optimal control over the transfer function from stimulation to phosphenes, dynamic phosphene encoding could also prove useful to expand the encoded information along the temporal domain (<xref ref-type="bibr" rid="bib4">Beauchamp et al., 2020</xref>). Although this was not in the scope of the current study, our software is well-suited for simulation experiments that further investigate dynamic stimulation. Note that there remain some challenging perceptual considerations for the design of useful dynamical stimulation patterns (for an excellent review on asynchronous stimulation in relation to flicker fusion, form vision, and apparent motion perception, please see <xref ref-type="bibr" rid="bib64">Moleirinho et al., 2021</xref>).</p></sec><sec id="s4-2-2"><title>Constrained, efficient stimulation for natural stimuli</title><p>Our second optimization experiment addressed a more natural and realistic context. The results presented in <xref ref-type="fig" rid="fig8">Figure 8</xref> demonstrate that our simulator is well-suited for the optimization of prosthetic vision to natural stimuli and that it can be configured to comply with constraints of the stimulation protocol. Note that the quality of the reconstructions for the constrained version of the encoder indicate that the model can still find an efficient information encoding strategy using a limited set of stimulation amplitudes (10 discrete values between 0 and 128 μA). These results are in line with previous results on constrained end-to-end optimization, indicating that task-relevant information can be maximized under sparsity constraints (<xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>). While in the current experiments the stimulation amplitude is maximized for the individual electrodes, future studies could investigate other sparsity constraints, such as a maximum total charge delivered per second across all electrodes. Ultimately, a visual prosthesis may need to prioritize task-relevant information, rather than providing an accurate description of the visual surroundings. For this reason, in recent SPV research with sighted human observers much attention is devoted to semantic (boundary) segmentation for discriminating the important information from irrelevant background (<xref ref-type="bibr" rid="bib84">Sanchez-Garcia et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Han et al., 2021</xref>; <xref ref-type="bibr" rid="bib79">Rasla and Beyeler, 2022</xref>). Note that the explored image processing strategies in these behavioral studies are equally compatible with the automated optimization through an end-to-end machine learning pipeline. Our experiments exemplify how supervision targets obtained from semantic segmentation data can be adopted to promote task-relevant information in the phosphene representation. Furthermore, in addition to reconstruction of the input or labeled targets, another recent study experimented with different decoding tasks, including more interactive, goal-driven tasks in virtual game environments (<xref ref-type="bibr" rid="bib52">Küçükoğlu et al., 2022</xref>). Although these proof-of-principle results remain to be translated to real-world tasks and environments, they provide a valuable basis for further exploration. Computational optimization approaches can also aid in the development of safe stimulation protocols, because they allow a faster exploration of the large parameter space and enable task-driven optimization of image processing strategies (<xref ref-type="bibr" rid="bib43">Granley et al., 2022a</xref>; <xref ref-type="bibr" rid="bib32">Fauvel and Chalk, 2022</xref>; <xref ref-type="bibr" rid="bib106">White et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Küçükoğlu et al., 2022</xref>; <xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>; <xref ref-type="bibr" rid="bib38">Ghaffari et al., 2021</xref>). Ultimately, the development of task-relevant scene-processing algorithms will likely benefit both from computational optimization experiments as well as exploratory SPV studies with human observers. The results presented in <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplements 3</xref> and <xref ref-type="fig" rid="fig8s4">4</xref> demonstrate that the end-to-end pipeline can also be used for low electrode counts and that exploratory experimentation with non-linear interaction models is possible. With the presented simulator we aim to contribute a flexible toolkit that can be tailored to specific use cases.</p></sec><sec id="s4-2-3"><title>Interpretability and perceptual correspondence</title><p>Besides the encoding efficiency (characterized by the computational decodability of task-relevant information), it is important to consider the subjective interpretablitity of the simulated phosphene representation. In <xref ref-type="fig" rid="fig8">Figure 8</xref> it can be observed that the model has successfully learned to preserve correspondences between the phosphene representation and the input image in all of the training conditions. However, as a more formal analysis was outside the scope of this study, we did not further quantify the subjective interpretability. In our model the subjective interpretability was promoted through the regularization loss between the simulated phosphenes and the input image. Similarly, a recent study adapted an auto-encoder architecture designed to directly maximize the perceptual correspondence between a target representation and the simulated percept in a retinal prosthesis, using basic stimuli (<xref ref-type="bibr" rid="bib43">Granley et al., 2022a</xref>). The preservation of subjective interpretability in an automated optimization pipeline remains a non-trivial challenge, especially when using natural stimuli. This may be even more important for cortical prostheses, as the distinct punctate phosphenes are in nature very dissimilar from natural images, possibly hampering perceptual similarity metrics that rely on low-level feature correspondence. Eventually, the functional quality of the artificial vision will not only depend on the correspondence between the visual environment and the phosphene encoding, but also on the implant recipient’s ability to extract that information into a usable percept. The functional quality of end-to-end generated phosphene encodings in daily life tasks will need to be evaluated in future experiments. Regardless of the implementation, it will always be important to include human observers (both sighted experimental subjects and actual prosthetic implant users) in the optimization cycle to ensure subjective interpretability for the end user (<xref ref-type="bibr" rid="bib32">Fauvel and Chalk, 2022</xref>; <xref ref-type="bibr" rid="bib8">Beyeler and Sanchez-Garcia, 2022</xref>).</p></sec></sec><sec id="s4-3"><title>General limitations and future directions</title><sec id="s4-3-1"><title>Performance and hardware</title><p>There are some remaining practical limitations and challenges for future research. We identify three considerations related to the the performance of our model and the required hardware for implementation in an experimental setup. First, although our model runs in real-time and is faster than the state-of-the art realistic simulation for retinal prostheses (<xref ref-type="bibr" rid="bib7">Beyeler et al., 2017</xref>), there is a trade-off between speed and the memory demand. Therefore, for higher resolutions and larger numbers of phosphenes, future experimental research may need to adapt a simplified version of our model - although most of the simulation conditions can be run easily with common graphical cards. While several simulators exist for cortical prostheses that run in real-time without requiring a dedicated graphics card (<xref ref-type="bibr" rid="bib55">Li, 2013</xref>; <xref ref-type="bibr" rid="bib108">Wong et al., 2010</xref>), none of these incorporate current spread-based models of phosphene size, realistic stimulation parameter ranges or temporal dynamics. An effort can be made to balance more lightweight hardware requirements with more realistic phosphene characteristics. Second, a suggestion for follow-up research, is to combine our simulator with the latest developments in mixed reality (XR) to enable immersive simulation in virtual environments. More specifically, a convenient direction would be the implementation of our simulator using the Cg shader programming language for graphics processing, which is used in 3D game engines like Unreal Engine, or Unity 3D, as previously demonstrated for epiretinal simulations by <xref ref-type="bibr" rid="bib97">Thorn et al., 2020</xref>. Third and lastly, future studies could explore the effects of using eye-tracking technology with our simulation software. Even after loss of vision, the brain integrates eye movements for the localization of visual stimuli (<xref ref-type="bibr" rid="bib82">Reuschel et al., 2012</xref>), and in cortical prostheses the position of the artificially induced percept will shift along with eye movements (<xref ref-type="bibr" rid="bib15">Brindley and Lewin, 1968</xref>; <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>). Therefore, in prostheses with a head-mounted camera, misalignment between the camera orientation and the pupillary axes can induce localization problems (<xref ref-type="bibr" rid="bib17">Caspi et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Paraskevoudi and Pezaris, 2018</xref>; <xref ref-type="bibr" rid="bib83">Sabbah et al., 2014</xref>; <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>). Previous SPV studies have demonstrated that eye-tracking can be implemented to simulate the gaze-coupled perception of phosphenes (<xref ref-type="bibr" rid="bib20">Cha et al., 1992c</xref>; <xref ref-type="bibr" rid="bib91">Sommerhalder et al., 2004</xref>; <xref ref-type="bibr" rid="bib22">Dagnelie et al., 2006</xref>; <xref ref-type="bibr" rid="bib60">McIntosh et al., 2013</xref>; <xref ref-type="bibr" rid="bib73">Paraskevoudi and Pezaris, 2021</xref>; <xref ref-type="bibr" rid="bib80">Rassia and Pezaris, 2018</xref>; <xref ref-type="bibr" rid="bib98">Titchener et al., 2018</xref>; <xref ref-type="bibr" rid="bib92">Srivastava et al., 2009</xref>). Note that some of the cited studies implemented a simulation condition where not only the simulated phosphene locations, but also the stimulation protocol depended on the gaze direction. More specifically, instead of representing the head-centered camera input, the stimulation pattern was chosen to encode the external environment at the location where the gaze was directed. While further research is required, there is some preliminary evidence that such a gaze-contingent image processing can improve the functional and subjective quality of prosthetic vision (<xref ref-type="bibr" rid="bib17">Caspi et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Paraskevoudi and Pezaris, 2021</xref>; <xref ref-type="bibr" rid="bib80">Rassia and Pezaris, 2018</xref>; <xref ref-type="bibr" rid="bib98">Titchener et al., 2018</xref>). Some example videos of gaze-contingent simulated prosthetic vision can be retrieved from our repository <ext-link ext-link-type="uri" xlink:href="https://github.com/neuralcodinglab/dynaphos/blob/main/examples">here</ext-link>. Note that an eye-tracker will be required to produce gaze-contingent image processing in visual prostheses and there might be unforeseen complexities in the clinical implementation thereof. The study of oculomotor behavior in blind individuals (with or without a visual prosthesis) is still an ongoing line of research (<xref ref-type="bibr" rid="bib17">Caspi et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Kwon et al., 2013</xref>; <xref ref-type="bibr" rid="bib83">Sabbah et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Hafed et al., 2016</xref>).</p></sec><sec id="s4-3-2"><title>Complexity and realism of the simulation</title><p>There are some remaining challenges regarding the realistic simulation of the effects of neural stimulation. A complicating factor is that cortical neuroprostheses are still in the early stages of development. Neurostimulation hardware and stimulation protocols are continuously being improved (<xref ref-type="bibr" rid="bib4">Beauchamp et al., 2020</xref>), and clinical trials with cortical visual neuroprostheses are often limited to small numbers of blind volunteers (<xref ref-type="bibr" rid="bib33">Fernández and Normann, 2017</xref>; <xref ref-type="bibr" rid="bib99">Troyk, 2017</xref>). Therefore, it is no surprise that the amount of data that is available at the present moment is limited, often open for multiple interpretations, and sometimes contains apparent contradictory information. Notably, the trade-off between model complexity and accurate psychophysical fits or predictions is a recurrent theme in the verification and validation of the components implemented in our simulator. Our approach aims to comprehensively integrate a set of biologically plausible models, while striking a balance between real-time performance, flexibility, and biological realism. Combining models of current spread and knowledge about the retinotopic organization of the visual cortex with psychophysics allows us to link the space of electrical stimulation parameters with clinical perceptual reports as well as physiological knowledge in the NHP and clinical literature. These design choices play a role in some of the potential limitations of our current simulator. Here, we name a few of the important limitations and some interesting directions for future research. First, in our simulator, phosphenes are only rendered when the activation is above threshold. This might be an inaccurate depiction of the perceptual experience of an implant user, and in reality the distinction may be less strict. The conscious perception of phosphenes requires considerable training and the detection process is influenced by attention (<xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>). Although our implementation is effective for modeling the psychometric data, alternative implementations could also be considered. The perceptual effect of different simulated phosphene threshold implementations for sighted subjects remains to be evaluated in future SPV work. Second, the leaky integrator and the memory trace that are implemented in our simulator might be an oversimplified model of tissue activation in the visual cortex and some non-linear dynamics might be missed. For instance, all data used in this study to fit and validate the model used symmetric, biphasic pulse trains, while other pulse shapes might lead to different neural or behavioral responses (<xref ref-type="bibr" rid="bib61">Merrill et al., 2005</xref>). Also, several studies reported that higher stimulation amplitudes may give rise to double phosphenes (<xref ref-type="bibr" rid="bib15">Brindley and Lewin, 1968</xref>; <xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>; <xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>; <xref ref-type="bibr" rid="bib70">Oswalt et al., 2021</xref>), or a reduction of phosphene brightness (<xref ref-type="bibr" rid="bib88">Schmidt et al., 1996</xref>). Furthermore, in contrast to the assumptions of our model, interactions between simultaneous stimulation of multiple electrodes can have an effect on the phosphene size and sometimes lead to unexpected percepts (<xref ref-type="bibr" rid="bib35">Fernández et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Dobelle and Mladejovsky, 1974</xref>; <xref ref-type="bibr" rid="bib2">Bak et al., 1990</xref>). Although our software supports basic exploratory experimentation of non-linear interactions (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>, <xref ref-type="fig" rid="fig8s4">Figure 8—figure supplement 4</xref>), by default, our simulator assumes independence between electrodes. Multi-phosphene percepts are modeled using linear summation of the independent percepts. These assumptions seem to hold for intracortical electrodes separated by more than 1 mm (<xref ref-type="bibr" rid="bib39">Ghose and Maunsell, 2012</xref>), but may underestimate the complexities observed when electrodes are nearer. Further clinical and theoretical modeling work could help to improve our understanding of these non-linear dynamics. A third limitation is that our simulator currently only models responses of V1 stimulation. Future studies could explore the possible extension of modeling micro-stimulation of the LGN (<xref ref-type="bibr" rid="bib76">Pezaris and Reid, 2007</xref>) and higher visual areas, such as V2, V3, V4, or inferotemporal cortex (IT). In previous NHP research, reliable phosphene thresholds could be obtained with the stimulation of the LGN, V1, V2, V3A, and middle temporal visual area (MT) (<xref ref-type="bibr" rid="bib76">Pezaris and Reid, 2007</xref>; <xref ref-type="bibr" rid="bib65">Murphey and Maunsell, 2007</xref>). Furthermore, IT stimulation has been shown to bias face perception (<xref ref-type="bibr" rid="bib1">Afraz et al., 2006</xref>). Similar effects have been confirmed in human subjects, and previous work has demonstrated that electrical stimulation of higher-order visual areas can elicit a range of feature-specific percepts (<xref ref-type="bibr" rid="bib66">Murphey et al., 2009</xref>; <xref ref-type="bibr" rid="bib54">Lee et al., 2000</xref>; <xref ref-type="bibr" rid="bib86">Schalk et al., 2017</xref>). Our simulator could be extended with maps of other visual areas with clear retinotopy, and an interesting direction for future research will be the implementation of feature-specific percepts, including texture, shape, and color.</p></sec></sec><sec id="s4-4"><title>Conclusion</title><p>We present a framework for the biologically plausible simulation of phosphene vision. The simulator models psychophysical and neurophysiological findings in a wide array of experimental results. Its phenomenologically accurate simulations allow for the optimization of visual cortical prostheses in a manner that drastically narrows the gap between simulation and reality, compared to previous studies of simulated phosphene vision. It can operate in real-time, making it a viable option for behavioral experiments with sighted volunteers. Additionally, owing to the PyTorch implementation and the differentiable operations, it is a good choice for machine learning approaches to study and optimize phosphene vision. The modular design of the simulator allows for straightforward adaptation of novel insights and improved models of cortical activation. In summary, our open-source, biologically plausible phosphene simulator aims to provide an accessible bedrock software platform that fits the needs of fundamental, clinical, and computational vision scientists working on cortical neuroprosthetic vision. With this work, we aspire to contribute to increasing the field’s translational impact.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Supervision, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Supervision, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Software, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Supervision, Funding acquisition, Methodology</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Supervision, Funding acquisition, Methodology</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-85812-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The source code of our simulator can be retreived from <ext-link ext-link-type="uri" xlink:href="https://github.com/neuralcodinglab/dynaphos">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib27">de Ruyter van Steveninck, 2024a</xref>). It is licensed under the GNU General Public License. The latest stable release can be installed using pip: $ pip install dynaphos. The code and data that were used for experimental modelling and analysis in this paper can be retreived from <ext-link ext-link-type="uri" xlink:href="https://github.com/neuralcodinglab/dynaphos-experiments">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib28">de Ruyter van Steveninck, 2024b</xref>). The computational optimization experiments in this paper were run using an adapted version of a previously published end-to-end learning pipeline (<xref ref-type="bibr" rid="bib25">de Ruyter van Steveninck et al., 2022a</xref>). The code can be retrieved from <ext-link ext-link-type="uri" xlink:href="https://github.com/neuralcodinglab/viseon">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib29">de Ruyter van Steveninck, 2024c</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by three grants of the Dutch Organization for Scientific Research (NWO): STW grant number P15-42 'NESTOR', ALW grant number 823-02-010 and Cross-over grant number 17619 'INTENSE' and grant number 024.005.022 'DBI2', a Gravitation program of the Dutch Ministry of Science, Education and Culture; the European Union’s Horizon 2020 research and innovation programme: grant number 899287, 'NeuraViper'; the Human Brain Project, grant number 650003. We thank Xing Chen for help with the compilation and reviewing of relevant literature regarding phosphene perception.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afraz</surname><given-names>S-R</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Microstimulation of inferotemporal cortex influences face categorization</article-title><source>Nature</source><volume>442</volume><fpage>692</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1038/nature04982</pub-id><pub-id pub-id-type="pmid">16878143</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bak</surname><given-names>M</given-names></name><name><surname>Girvin</surname><given-names>JP</given-names></name><name><surname>Hambrecht</surname><given-names>FT</given-names></name><name><surname>Kufta</surname><given-names>CV</given-names></name><name><surname>Loeb</surname><given-names>GE</given-names></name><name><surname>Schmidt</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Visual sensations produced by intracortical microstimulation of the human occipital cortex</article-title><source>Medical &amp; Biological Engineering &amp; Computing</source><volume>28</volume><fpage>257</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1007/BF02442682</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlett</surname><given-names>JR</given-names></name><name><surname>Doty, sr.</surname><given-names>RW</given-names></name><name><surname>Lee</surname><given-names>BB</given-names></name><name><surname>Negrão</surname><given-names>N</given-names></name><name><surname>Overman, jr.</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Deleterious effects of prolonged electrical excitation of striate cortex in macaques</article-title><source>Brain, Behavior and Evolution</source><volume>14</volume><fpage>46</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1159/000125575</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Oswalt</surname><given-names>D</given-names></name><name><surname>Sun</surname><given-names>P</given-names></name><name><surname>Foster</surname><given-names>BL</given-names></name><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Niketeghad</surname><given-names>S</given-names></name><name><surname>Pouratian</surname><given-names>N</given-names></name><name><surname>Bosking</surname><given-names>WH</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamic stimulation of visual cortex produces form vision in sighted and blind humans</article-title><source>Cell</source><volume>181</volume><fpage>774</fpage><lpage>783</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.04.033</pub-id><pub-id pub-id-type="pmid">32413298</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Butt</surname><given-names>OH</given-names></name><name><surname>Datta</surname><given-names>R</given-names></name><name><surname>Radoeva</surname><given-names>PD</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Aguirre</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The retinotopic organization of striate cortex is well predicted by surface topology</article-title><source>Current Biology</source><volume>22</volume><fpage>2081</fpage><lpage>2085</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.09.014</pub-id><pub-id pub-id-type="pmid">23041195</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Butt</surname><given-names>OH</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Aguirre</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Correction of distortion in flattened representations of the cortical surface allows prediction of V1-V3 functional organization from anatomy</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003538</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003538</pub-id><pub-id pub-id-type="pmid">24676149</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Beyeler</surname><given-names>M</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name><name><surname>Fine</surname><given-names>I</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pulse2percept: A Python-Based Simulation Framework for Bionic Vision</article-title><conf-name>Python in Science Conference</conf-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beyeler</surname><given-names>M</given-names></name><name><surname>Sanchez-Garcia</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Towards asmart bionic eye: ai-powered artificial vision for the treatment of incurable blindness</article-title><source>Journal of Neural Engineering</source><volume>19</volume><elocation-id>aca69d</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aca69d</pub-id><pub-id pub-id-type="pmid">36541463</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloch</surname><given-names>E</given-names></name><name><surname>Luo</surname><given-names>Y</given-names></name><name><surname>da Cruz</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Advances in retinal prosthesis systems</article-title><source>Therapeutic Advances in Ophthalmology</source><volume>11</volume><elocation-id>17501</elocation-id><pub-id pub-id-type="doi">10.1177/2515841418817501</pub-id><pub-id pub-id-type="pmid">30729233</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>AS</given-names></name><name><surname>Binda</surname><given-names>P</given-names></name><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Bridge</surname><given-names>H</given-names></name><name><surname>Watkins</surname><given-names>KE</given-names></name><name><surname>Fine</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Resting-state retinotopic organization in the absence of retinal input and visual experience</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>12366</fpage><lpage>12382</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4715-14.2015</pub-id><pub-id pub-id-type="pmid">26354906</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bollen</surname><given-names>CJM</given-names></name><name><surname>Guclu</surname><given-names>U</given-names></name><name><surname>van Wezel</surname><given-names>RJA</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Gucluturk</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simulating neuroprosthetic vision for emotion recognition</article-title><conf-name>2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW</conf-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosking</surname><given-names>WH</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Electrical stimulation of visual cortex: relevance for the development of visual cortical prosthetics</article-title><source>Annual Review of Vision Science</source><volume>3</volume><fpage>141</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114525</pub-id><pub-id pub-id-type="pmid">28753382</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourne</surname><given-names>R</given-names></name><name><surname>Steinmetz</surname><given-names>JD</given-names></name><name><surname>Flaxman</surname><given-names>S</given-names></name><name><surname>Briant</surname><given-names>PS</given-names></name><name><surname>Taylor</surname><given-names>HR</given-names></name><name><surname>Resnikoff</surname><given-names>S</given-names></name><name><surname>Casson</surname><given-names>RJ</given-names></name><name><surname>Abdoli</surname><given-names>A</given-names></name><name><surname>Abu-Gharbieh</surname><given-names>E</given-names></name><name><surname>Afshin</surname><given-names>A</given-names></name><name><surname>Ahmadieh</surname><given-names>H</given-names></name><name><surname>Akalu</surname><given-names>Y</given-names></name><name><surname>Alamneh</surname><given-names>AA</given-names></name><name><surname>Alemayehu</surname><given-names>W</given-names></name><name><surname>Alfaar</surname><given-names>AS</given-names></name><name><surname>Alipour</surname><given-names>V</given-names></name><name><surname>Anbesu</surname><given-names>EW</given-names></name><name><surname>Androudi</surname><given-names>S</given-names></name><name><surname>Arabloo</surname><given-names>J</given-names></name><name><surname>Arditi</surname><given-names>A</given-names></name><name><surname>Asaad</surname><given-names>M</given-names></name><name><surname>Bagli</surname><given-names>E</given-names></name><name><surname>Baig</surname><given-names>AA</given-names></name><name><surname>Bärnighausen</surname><given-names>TW</given-names></name><name><surname>Battaglia Parodi</surname><given-names>M</given-names></name><name><surname>Bhagavathula</surname><given-names>AS</given-names></name><name><surname>Bhardwaj</surname><given-names>N</given-names></name><name><surname>Bhardwaj</surname><given-names>P</given-names></name><name><surname>Bhattacharyya</surname><given-names>K</given-names></name><name><surname>Bijani</surname><given-names>A</given-names></name><name><surname>Bikbov</surname><given-names>M</given-names></name><name><surname>Bottone</surname><given-names>M</given-names></name><name><surname>Braithwaite</surname><given-names>T</given-names></name><name><surname>Bron</surname><given-names>AM</given-names></name><name><surname>Butt</surname><given-names>ZA</given-names></name><name><surname>Cheng</surname><given-names>C-Y</given-names></name><name><surname>Chu</surname><given-names>D-T</given-names></name><name><surname>Cicinelli</surname><given-names>MV</given-names></name><name><surname>Coelho</surname><given-names>JM</given-names></name><name><surname>Dagnew</surname><given-names>B</given-names></name><name><surname>Dai</surname><given-names>X</given-names></name><name><surname>Dana</surname><given-names>R</given-names></name><name><surname>Dandona</surname><given-names>L</given-names></name><name><surname>Dandona</surname><given-names>R</given-names></name><name><surname>Del Monte</surname><given-names>MA</given-names></name><name><surname>Deva</surname><given-names>JP</given-names></name><name><surname>Diaz</surname><given-names>D</given-names></name><name><surname>Djalalinia</surname><given-names>S</given-names></name><name><surname>Dreer</surname><given-names>LE</given-names></name><name><surname>Ehrlich</surname><given-names>JR</given-names></name><name><surname>Ellwein</surname><given-names>LB</given-names></name><name><surname>Emamian</surname><given-names>MH</given-names></name><name><surname>Fernandes</surname><given-names>AG</given-names></name><name><surname>Fischer</surname><given-names>F</given-names></name><name><surname>Friedman</surname><given-names>DS</given-names></name><name><surname>Furtado</surname><given-names>JM</given-names></name><name><surname>Gaidhane</surname><given-names>AM</given-names></name><name><surname>Gaidhane</surname><given-names>S</given-names></name><name><surname>Gazzard</surname><given-names>G</given-names></name><name><surname>Gebremichael</surname><given-names>B</given-names></name><name><surname>George</surname><given-names>R</given-names></name><name><surname>Ghashghaee</surname><given-names>A</given-names></name><name><surname>Golechha</surname><given-names>M</given-names></name><name><surname>Hamidi</surname><given-names>S</given-names></name><name><surname>Hammond</surname><given-names>BR</given-names></name><name><surname>Hartnett</surname><given-names>MER</given-names></name><name><surname>Hartono</surname><given-names>RK</given-names></name><name><surname>Hay</surname><given-names>SI</given-names></name><name><surname>Heidari</surname><given-names>G</given-names></name><name><surname>Ho</surname><given-names>HC</given-names></name><name><surname>Hoang</surname><given-names>CL</given-names></name><name><surname>Househ</surname><given-names>M</given-names></name><name><surname>Ibitoye</surname><given-names>SE</given-names></name><name><surname>Ilic</surname><given-names>IM</given-names></name><name><surname>Ilic</surname><given-names>MD</given-names></name><name><surname>Ingram</surname><given-names>AD</given-names></name><name><surname>Irvani</surname><given-names>SSN</given-names></name><name><surname>Jha</surname><given-names>RP</given-names></name><name><surname>Kahloun</surname><given-names>R</given-names></name><name><surname>Kandel</surname><given-names>H</given-names></name><name><surname>Kasa</surname><given-names>AS</given-names></name><name><surname>Kempen</surname><given-names>JH</given-names></name><name><surname>Keramati</surname><given-names>M</given-names></name><name><surname>Khairallah</surname><given-names>M</given-names></name><name><surname>Khan</surname><given-names>EA</given-names></name><name><surname>Khanna</surname><given-names>RC</given-names></name><name><surname>Khatib</surname><given-names>MN</given-names></name><name><surname>Kim</surname><given-names>JE</given-names></name><name><surname>Kim</surname><given-names>YJ</given-names></name><name><surname>Kisa</surname><given-names>S</given-names></name><name><surname>Kisa</surname><given-names>A</given-names></name><name><surname>Koyanagi</surname><given-names>A</given-names></name><name><surname>Kurmi</surname><given-names>OP</given-names></name><name><surname>Lansingh</surname><given-names>VC</given-names></name><name><surname>Leasher</surname><given-names>JL</given-names></name><name><surname>Leveziel</surname><given-names>N</given-names></name><name><surname>Limburg</surname><given-names>H</given-names></name><name><surname>Majdan</surname><given-names>M</given-names></name><name><surname>Manafi</surname><given-names>N</given-names></name><name><surname>Mansouri</surname><given-names>K</given-names></name><name><surname>McAlinden</surname><given-names>C</given-names></name><name><surname>Mohammadi</surname><given-names>SF</given-names></name><name><surname>Mohammadian-Hafshejani</surname><given-names>A</given-names></name><name><surname>Mohammadpourhodki</surname><given-names>R</given-names></name><name><surname>Mokdad</surname><given-names>AH</given-names></name><name><surname>Moosavi</surname><given-names>D</given-names></name><name><surname>Morse</surname><given-names>AR</given-names></name><name><surname>Naderi</surname><given-names>M</given-names></name><name><surname>Naidoo</surname><given-names>KS</given-names></name><name><surname>Nangia</surname><given-names>V</given-names></name><name><surname>Nguyen</surname><given-names>CT</given-names></name><name><surname>Nguyen</surname><given-names>HLT</given-names></name><name><surname>Ogundimu</surname><given-names>K</given-names></name><name><surname>Olagunju</surname><given-names>AT</given-names></name><name><surname>Ostroff</surname><given-names>SM</given-names></name><name><surname>Panda-Jonas</surname><given-names>S</given-names></name><name><surname>Pesudovs</surname><given-names>K</given-names></name><name><surname>Peto</surname><given-names>T</given-names></name><name><surname>Quazi Syed</surname><given-names>Z</given-names></name><name><surname>Rahman</surname><given-names>MHU</given-names></name><name><surname>Ramulu</surname><given-names>PY</given-names></name><name><surname>Rawaf</surname><given-names>S</given-names></name><name><surname>Rawaf</surname><given-names>DL</given-names></name><name><surname>Reinig</surname><given-names>N</given-names></name><name><surname>Robin</surname><given-names>AL</given-names></name><name><surname>Rossetti</surname><given-names>L</given-names></name><name><surname>Safi</surname><given-names>S</given-names></name><name><surname>Sahebkar</surname><given-names>A</given-names></name><name><surname>Samy</surname><given-names>AM</given-names></name><name><surname>Saxena</surname><given-names>D</given-names></name><name><surname>Serle</surname><given-names>JB</given-names></name><name><surname>Shaikh</surname><given-names>MA</given-names></name><name><surname>Shen</surname><given-names>TT</given-names></name><name><surname>Shibuya</surname><given-names>K</given-names></name><name><surname>Shin</surname><given-names>JI</given-names></name><name><surname>Silva</surname><given-names>JC</given-names></name><name><surname>Silvester</surname><given-names>A</given-names></name><name><surname>Singh</surname><given-names>JA</given-names></name><name><surname>Singhal</surname><given-names>D</given-names></name><name><surname>Sitorus</surname><given-names>RS</given-names></name><name><surname>Skiadaresi</surname><given-names>E</given-names></name><name><surname>Skirbekk</surname><given-names>V</given-names></name><name><surname>Soheili</surname><given-names>A</given-names></name><name><surname>Sousa</surname><given-names>RARC</given-names></name><name><surname>Spurlock</surname><given-names>EE</given-names></name><name><surname>Stambolian</surname><given-names>D</given-names></name><name><surname>Taddele</surname><given-names>BW</given-names></name><name><surname>Tadesse</surname><given-names>EG</given-names></name><name><surname>Tahhan</surname><given-names>N</given-names></name><name><surname>Tareque</surname><given-names>MI</given-names></name><name><surname>Topouzis</surname><given-names>F</given-names></name><name><surname>Tran</surname><given-names>BX</given-names></name><name><surname>Travillian</surname><given-names>RS</given-names></name><name><surname>Tsilimbaris</surname><given-names>MK</given-names></name><name><surname>Varma</surname><given-names>R</given-names></name><name><surname>Virgili</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>YX</given-names></name><name><surname>Wang</surname><given-names>N</given-names></name><name><surname>West</surname><given-names>SK</given-names></name><name><surname>Wong</surname><given-names>TY</given-names></name><name><surname>Zaidi</surname><given-names>Z</given-names></name><name><surname>Zewdie</surname><given-names>KA</given-names></name><name><surname>Jonas</surname><given-names>JB</given-names></name><name><surname>Vos</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Trends in prevalence of blindness and distance and near vision impairment over 30 years: an analysis for the global burden of disease study</article-title><source>The Lancet Global Health</source><volume>9</volume><fpage>e130</fpage><lpage>e143</lpage><pub-id pub-id-type="doi">10.1016/S2214-109X(20)30425-3</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Boyle</surname><given-names>J</given-names></name><name><surname>Maeder</surname><given-names>A</given-names></name><name><surname>Boles</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Static image simulation of electronic visual prostheses</article-title><conf-name>ANZIIS 2001. Proceedings of the Seventh Australian and New Zealand Intelligent Information Systems Conference</conf-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brindley</surname><given-names>GS</given-names></name><name><surname>Lewin</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>The sensations produced by electrical stimulation of the visual cortex</article-title><source>The Journal of Physiology</source><volume>196</volume><fpage>479</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008519</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>IC</given-names></name><name><surname>White</surname><given-names>MW</given-names></name><name><surname>Irlicht</surname><given-names>LS</given-names></name><name><surname>O’Leary</surname><given-names>SJ</given-names></name><name><surname>Dynes</surname><given-names>S</given-names></name><name><surname>Javel</surname><given-names>E</given-names></name><name><surname>Clark</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A stochastic model of the electrically stimulated auditory nerve: single-pulse response</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>46</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1109/10.764938</pub-id><pub-id pub-id-type="pmid">10356868</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspi</surname><given-names>A</given-names></name><name><surname>Roy</surname><given-names>A</given-names></name><name><surname>Wuyyuru</surname><given-names>V</given-names></name><name><surname>Rosendall</surname><given-names>PE</given-names></name><name><surname>Harper</surname><given-names>JW</given-names></name><name><surname>Katyal</surname><given-names>KD</given-names></name><name><surname>Barry</surname><given-names>MP</given-names></name><name><surname>Dagnelie</surname><given-names>G</given-names></name><name><surname>Greenberg</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eye movement control in the argus ii retinal-prosthesis enables reduced head movement and better localization precision</article-title><source>Investigative Opthalmology &amp; Visual Science</source><volume>59</volume><elocation-id>792</elocation-id><pub-id pub-id-type="doi">10.1167/iovs.17-22377</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cha</surname><given-names>K</given-names></name><name><surname>Horch</surname><given-names>KW</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1992">1992a</year><article-title>Mobility performance with a pixelized vision system</article-title><source>Vision Research</source><volume>32</volume><fpage>1367</fpage><lpage>1372</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(92)90229-c</pub-id><pub-id pub-id-type="pmid">1455709</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cha</surname><given-names>K</given-names></name><name><surname>Horch</surname><given-names>K</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1992">1992b</year><article-title>Simulation of a phosphene-based visual field: visual acuity in a pixelized vision system</article-title><source>Annals of Biomedical Engineering</source><volume>20</volume><fpage>439</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1007/BF02368135</pub-id><pub-id pub-id-type="pmid">1510295</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cha</surname><given-names>K</given-names></name><name><surname>Boman</surname><given-names>DK</given-names></name><name><surname>Horch</surname><given-names>KW</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1992">1992c</year><article-title>Reading speed with a pixelized vision system</article-title><source>Journal of the Optical Society of America A</source><volume>9</volume><elocation-id>673</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.9.000673</pub-id><pub-id pub-id-type="pmid">17301853</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Fernandez</surname><given-names>E</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Shape perception via a high-channel-count neuroprosthesis in monkey visual cortex</article-title><source>Science</source><volume>370</volume><fpage>1191</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1126/science.abd7435</pub-id><pub-id pub-id-type="pmid">33273097</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dagnelie</surname><given-names>G</given-names></name><name><surname>Walter</surname><given-names>M</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Playing checkers: detection and eye–hand coordination in simulated prosthetic vision</article-title><source>Journal of Modern Optics</source><volume>53</volume><fpage>1325</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.1080/09500340600619197</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dagnelie</surname><given-names>G</given-names></name><name><surname>Keane</surname><given-names>P</given-names></name><name><surname>Narla</surname><given-names>V</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name><name><surname>Weiland</surname><given-names>J</given-names></name><name><surname>Humayun</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Real and virtual mobility performance in simulated prosthetic vision</article-title><source>Journal of Neural Engineering</source><volume>4</volume><fpage>S92</fpage><lpage>S101</lpage><pub-id pub-id-type="doi">10.1088/1741-2560/4/1/S11</pub-id><pub-id pub-id-type="pmid">17325421</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dagnelie</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Psychophysical evaluation for visual prosthesis</article-title><source>Annual Review of Biomedical Engineering</source><volume>10</volume><fpage>339</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1146/annurev.bioeng.10.061807.160529</pub-id><pub-id pub-id-type="pmid">18429703</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Ruyter van Steveninck</surname><given-names>J</given-names></name><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Wezel</surname><given-names>R</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>End-to-end optimization of prosthetic vision</article-title><source>Journal of Vision</source><volume>22</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1167/jov.22.2.20</pub-id><pub-id pub-id-type="pmid">35703408</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Ruyter van Steveninck</surname><given-names>J</given-names></name><name><surname>van Gestel</surname><given-names>T</given-names></name><name><surname>Koenders</surname><given-names>P</given-names></name><name><surname>van der Ham</surname><given-names>G</given-names></name><name><surname>Vereecken</surname><given-names>F</given-names></name><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name><name><surname>Güçlütürk</surname><given-names>Y</given-names></name><name><surname>van Wezel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Real-world indoor mobility with simulated prosthetic vision: The benefits and feasibility of contour-based scene simplification at different phosphene resolutions</article-title><source>Journal of Vision</source><volume>22</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/jov.22.2.1</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>de Ruyter van Steveninck</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024a</year><data-title>Dynaphos</data-title><version designator="swh:1:rev:5eae4dc5cec02d9003711afa171a4e61d152a9ee">swh:1:rev:5eae4dc5cec02d9003711afa171a4e61d152a9ee</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:6361d4358f2783e28ea7b006738c71d7e5def0b8;origin=https://github.com/neuralcodinglab/dynaphos;visit=swh:1:snp:568a0d165c813c6e9f8bda7286e759e264c4438f;anchor=swh:1:rev:5eae4dc5cec02d9003711afa171a4e61d152a9ee">https://archive.softwareheritage.org/swh:1:dir:6361d4358f2783e28ea7b006738c71d7e5def0b8;origin=https://github.com/neuralcodinglab/dynaphos;visit=swh:1:snp:568a0d165c813c6e9f8bda7286e759e264c4438f;anchor=swh:1:rev:5eae4dc5cec02d9003711afa171a4e61d152a9ee</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>de Ruyter van Steveninck</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024b</year><data-title>dynaphos-experiments</data-title><version designator="swh:1:rev:c4adc805e77e1e2fef7f8544fe3967931d5b1fea">swh:1:rev:c4adc805e77e1e2fef7f8544fe3967931d5b1fea</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:862fd96680f91a92920b46e24b874af1ff96ec62;origin=https://github.com/neuralcodinglab/dynaphos-experiments;visit=swh:1:snp:2f3b75ff819b3ad4a335a867331c332d6856c942;anchor=swh:1:rev:c4adc805e77e1e2fef7f8544fe3967931d5b1fea">https://archive.softwareheritage.org/swh:1:dir:862fd96680f91a92920b46e24b874af1ff96ec62;origin=https://github.com/neuralcodinglab/dynaphos-experiments;visit=swh:1:snp:2f3b75ff819b3ad4a335a867331c332d6856c942;anchor=swh:1:rev:c4adc805e77e1e2fef7f8544fe3967931d5b1fea</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>de Ruyter van Steveninck</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024c</year><data-title>viseon</data-title><version designator="swh:1:rev:f66d05748917e339633ac5e4ee48b799355ec6bf">swh:1:rev:f66d05748917e339633ac5e4ee48b799355ec6bf</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:8b26cfb3c9489b3c50a0bc74bbbfa72c2c529ce2;origin=https://github.com/neuralcodinglab/viseon;visit=swh:1:snp:a501a58ca9ee0c9d242b88bc646a27aafbc8fb5a;anchor=swh:1:rev:f66d05748917e339633ac5e4ee48b799355ec6bf">https://archive.softwareheritage.org/swh:1:dir:8b26cfb3c9489b3c50a0bc74bbbfa72c2c529ce2;origin=https://github.com/neuralcodinglab/viseon;visit=swh:1:snp:a501a58ca9ee0c9d242b88bc646a27aafbc8fb5a;anchor=swh:1:rev:f66d05748917e339633ac5e4ee48b799355ec6bf</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobelle</surname><given-names>WH</given-names></name><name><surname>Mladejovsky</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Phosphenes produced by electrical stimulation of human occipital cortex, and their application to the development of a prosthesis for the blind</article-title><source>The Journal of Physiology</source><volume>243</volume><fpage>553</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1974.sp010766</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dowling</surname><given-names>JA</given-names></name><name><surname>Amini</surname><given-names>AA</given-names></name><name><surname>Manduca</surname><given-names>A</given-names></name><name><surname>Maeder</surname><given-names>A</given-names></name><name><surname>Boles</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Mobility enhancement and assessment for a visual prosthesis</article-title><conf-name>Medical Imaging 2004</conf-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fauvel</surname><given-names>T</given-names></name><name><surname>Chalk</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Human-in-the-loop optimization of visual prosthetic stimulation</article-title><source>Journal of Neural Engineering</source><volume>19</volume><elocation-id>7615</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/ac7615</pub-id><pub-id pub-id-type="pmid">35667363</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fernández</surname><given-names>E</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Cortivis approach for an Intracortical visual Prostheses</chapter-title><person-group person-group-type="editor"><name><surname>Fernández</surname><given-names>E</given-names></name></person-group><source>In Artificial Vision</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>191</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-41876-6_15</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Development of visual Neuroprostheses: trends and challenges</article-title><source>Bioelectronic Medicine</source><volume>4</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1186/s42234-018-0013-8</pub-id><pub-id pub-id-type="pmid">32232088</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernández</surname><given-names>E</given-names></name><name><surname>Alfaro</surname><given-names>A</given-names></name><name><surname>Soto-Sánchez</surname><given-names>C</given-names></name><name><surname>Gonzalez-Lopez</surname><given-names>P</given-names></name><name><surname>Lozano</surname><given-names>AM</given-names></name><name><surname>Peña</surname><given-names>S</given-names></name><name><surname>Grima</surname><given-names>MD</given-names></name><name><surname>Rodil</surname><given-names>A</given-names></name><name><surname>Gómez</surname><given-names>B</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Rolston</surname><given-names>JD</given-names></name><name><surname>Davis</surname><given-names>TS</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Visual percepts evoked with an intracortical 96-channel microelectrode array inserted in human occipital cortex</article-title><source>The Journal of Clinical Investigation</source><volume>131</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1172/JCI151331</pub-id><pub-id pub-id-type="pmid">34665780</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fridman</surname><given-names>GY</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Blaisdell</surname><given-names>AP</given-names></name><name><surname>Judy</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceived intensity of somatosensory cortical electrical stimulation</article-title><source>Experimental Brain Research</source><volume>203</volume><fpage>499</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1007/s00221-010-2254-y</pub-id><pub-id pub-id-type="pmid">20440610</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geddes</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Accuracy limitations of chronaxie values</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>51</volume><fpage>176</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1109/TBME.2003.820340</pub-id><pub-id pub-id-type="pmid">14723507</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ghaffari</surname><given-names>DH</given-names></name><name><surname>Chang</surname><given-names>YC</given-names></name><name><surname>Mirzakhalili</surname><given-names>E</given-names></name><name><surname>Weiland</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Closed-loop optimization of retinal ganglion cell responses to epiretinal stimulation: a computational study</article-title><conf-name>2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)</conf-name><pub-id pub-id-type="doi">10.1109/NER49283.2021.9441437</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghose</surname><given-names>K</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A strong constraint to the joint processing of pairs of cortical signals</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>15922</fpage><lpage>15933</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2186-12.2012</pub-id><pub-id pub-id-type="pmid">23136430</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girvin</surname><given-names>JP</given-names></name><name><surname>Evans</surname><given-names>JR</given-names></name><name><surname>Dobelle</surname><given-names>WH</given-names></name><name><surname>Mladejovsky</surname><given-names>MG</given-names></name><name><surname>Henderson</surname><given-names>DC</given-names></name><name><surname>Abramov</surname><given-names>I</given-names></name><name><surname>Gordon</surname><given-names>J</given-names></name><name><surname>Turkel</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Electrical stimulation of human visual cortex: the effect of stimulus parameters on phosphene threshold</article-title><source>Sensory Processes</source><volume>3</volume><fpage>66</fpage><lpage>81</lpage><pub-id pub-id-type="pmid">515742</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Esposito</surname><given-names>F</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Analysis of functional image analysis contest (FIAC) data with brainvoyager QX: From single-subject to cortically aligned group general linear model analysis and self-organizing group independent component analysis</article-title><source>Human Brain Mapping</source><volume>27</volume><fpage>392</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1002/hbm.20249</pub-id><pub-id pub-id-type="pmid">16596654</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>van Hoof</surname><given-names>R</given-names></name><name><surname>Bhat</surname><given-names>S</given-names></name><name><surname>Luhrs</surname><given-names>M</given-names></name><name><surname>Senden</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reading imagined letter shapes from the mind’s eye using real-time 7 tesla fMRI</article-title><conf-name>2022 10th International Winter Conference on Brain-Computer Interface (BCI</conf-name></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granley</surname><given-names>J</given-names></name><name><surname>Relic</surname><given-names>L</given-names></name><name><surname>Beyeler</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Hybrid neural autoencoders for stimulus encoding in visual and other sensory neuroprostheses</article-title><source>Advances in Neural Information Processing Systems</source><volume>35</volume><fpage>22671</fpage><lpage>22685</lpage><pub-id pub-id-type="pmid">37719469</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Granley</surname><given-names>J</given-names></name><name><surname>Riedel</surname><given-names>A</given-names></name><name><surname>Beyeler</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Adapting brain-like neural networks for modeling cortical visual prostheses</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2209.13561">https://arxiv.org/abs/2209.13561</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>F</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Optimization of visual information presentation for visual prosthesis</article-title><source>International Journal of Biomedical Imaging</source><volume>2018</volume><elocation-id>3198342</elocation-id><pub-id pub-id-type="doi">10.1155/2018/3198342</pub-id><pub-id pub-id-type="pmid">29731769</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafed</surname><given-names>ZM</given-names></name><name><surname>Stingl</surname><given-names>K</given-names></name><name><surname>Bartz-Schmidt</surname><given-names>K-U</given-names></name><name><surname>Gekeler</surname><given-names>F</given-names></name><name><surname>Zrenner</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Oculomotor behavior of blind patients seeing with a subretinal visual implant</article-title><source>Vision Research</source><volume>118</volume><fpage>119</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2015.04.006</pub-id><pub-id pub-id-type="pmid">25906684</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Han</surname><given-names>N</given-names></name><name><surname>Srivastava</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>A</given-names></name><name><surname>Klein</surname><given-names>D</given-names></name><name><surname>Beyeler</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep learning–based scene simplification for bionic vision</article-title><conf-name>AHs ’21</conf-name></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Histed</surname><given-names>MH</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Direct activation of sparse, distributed populations of cortical neurons by electrical microstimulation</article-title><source>Neuron</source><volume>63</volume><fpage>508</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.016</pub-id><pub-id pub-id-type="pmid">19709632</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horton</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The representation of the visual field in human striate cortex</article-title><source>Archives of Ophthalmology</source><volume>109</volume><elocation-id>816</elocation-id><pub-id pub-id-type="doi">10.1001/archopht.1991.01080060080030</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Callier</surname><given-names>T</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A computational model that predicts behavioral sensitivity to intracortical microstimulation</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>016012</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/14/1/016012</pub-id><pub-id pub-id-type="pmid">27977419</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Population receptive fields in nonhuman primates from whole-brain fMRI and large-scale neurophysiology in visual cortex</article-title><source>eLife</source><volume>10</volume><elocation-id>e67304</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67304</pub-id><pub-id pub-id-type="pmid">34730515</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Küçükoğlu</surname><given-names>B</given-names></name><name><surname>Rueckauer</surname><given-names>B</given-names></name><name><surname>Ahmad</surname><given-names>N</given-names></name><name><surname>van Steveninck</surname><given-names>J</given-names></name><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Optimization of neuroprosthetic vision via end-to-end deep reinforcement learning</article-title><source>International Journal of Neural Systems</source><volume>32</volume><elocation-id>ISSN</elocation-id><pub-id pub-id-type="doi">10.1142/S0129065722500526</pub-id><pub-id pub-id-type="pmid">36328967</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwon</surname><given-names>M</given-names></name><name><surname>Nandy</surname><given-names>AS</given-names></name><name><surname>Tjan</surname><given-names>BS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid and persistent adaptability of human oculomotor control in response to simulated central vision loss</article-title><source>Current Biology</source><volume>23</volume><fpage>1663</fpage><lpage>1669</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.06.056</pub-id><pub-id pub-id-type="pmid">23954427</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>HW</given-names></name><name><surname>Hong</surname><given-names>SB</given-names></name><name><surname>Seo</surname><given-names>DW</given-names></name><name><surname>Tae</surname><given-names>WS</given-names></name><name><surname>Hong</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mapping of functional organization in human visual cortex: electrical cortical stimulation</article-title><source>Neurology</source><volume>54</volume><fpage>849</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1212/wnl.54.4.849</pub-id><pub-id pub-id-type="pmid">10690975</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Wearable computer vision systems for a cortical visual prosthesis</article-title><conf-name>Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops</conf-name><conf-loc>Sydney, Australia</conf-loc><pub-id pub-id-type="doi">10.1109/ICCVW.2013.63</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lozano</surname><given-names>A</given-names></name><name><surname>Soto-Sánchez</surname><given-names>C</given-names></name><name><surname>Garrigós</surname><given-names>J</given-names></name><name><surname>Martínez</surname><given-names>JJ</given-names></name><name><surname>Ferrández</surname><given-names>JM</given-names></name><name><surname>Fernández</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A 3D convolutional neural network to model retinal ganglion cell’s responses to light patterns in mice</article-title><source>International Journal of Neural Systems</source><volume>28</volume><elocation-id>1850043</elocation-id><pub-id pub-id-type="doi">10.1142/S0129065718500430</pub-id><pub-id pub-id-type="pmid">30556459</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lozano</surname><given-names>A</given-names></name><name><surname>Suárez</surname><given-names>JS</given-names></name><name><surname>Soto-Sánchez</surname><given-names>C</given-names></name><name><surname>Garrigós</surname><given-names>J</given-names></name><name><surname>Martínez-Alvarez</surname><given-names>JJ</given-names></name><name><surname>Ferrández</surname><given-names>JM</given-names></name><name><surname>Fernández</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neurolight: a deep learning neural interface for cortical visual prostheses</article-title><source>International Journal of Neural Systems</source><volume>30</volume><elocation-id>ISSN</elocation-id><pub-id pub-id-type="doi">10.1142/S0129065720500458</pub-id><pub-id pub-id-type="pmid">32689842</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maynard</surname><given-names>EM</given-names></name><name><surname>Nordhausen</surname><given-names>CT</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The utah intracortical electrode array: a recording structure for potential brain-computer interfaces</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>102</volume><fpage>228</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/S0013-4694(96)95176-0</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCreery</surname><given-names>DB</given-names></name><name><surname>Agnew</surname><given-names>WF</given-names></name><name><surname>Bullara</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The effects of prolonged intracortical microstimulation on the excitability of pyramidal tract neurons in the cat</article-title><source>Annals of Biomedical Engineering</source><volume>30</volume><fpage>107</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1114/1.1430748</pub-id><pub-id pub-id-type="pmid">11874134</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McIntosh</surname><given-names>BP</given-names></name><name><surname>Stiles</surname><given-names>NRB</given-names></name><name><surname>Humayun</surname><given-names>MS</given-names></name><name><surname>Tanguay</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Effects of foveation on visual search task with visual prosthesis simulation</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>685</elocation-id><pub-id pub-id-type="doi">10.1167/13.9.685</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merrill</surname><given-names>DR</given-names></name><name><surname>Bikson</surname><given-names>M</given-names></name><name><surname>Jefferys</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Electrical stimulation of excitable tissue: design of efficacious and safe protocols</article-title><source>Journal of Neuroscience Methods</source><volume>141</volume><fpage>171</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2004.10.020</pub-id><pub-id pub-id-type="pmid">15661300</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Messinger</surname><given-names>A</given-names></name><name><surname>Sirmpilatze</surname><given-names>N</given-names></name><name><surname>Heuer</surname><given-names>K</given-names></name><name><surname>Loh</surname><given-names>KK</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Sein</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name><name><surname>Glen</surname><given-names>D</given-names></name><name><surname>Jung</surname><given-names>B</given-names></name><name><surname>Seidlitz</surname><given-names>J</given-names></name><name><surname>Taylor</surname><given-names>P</given-names></name><name><surname>Toro</surname><given-names>R</given-names></name><name><surname>Garza-Villarreal</surname><given-names>EA</given-names></name><name><surname>Sponheim</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Benn</surname><given-names>RA</given-names></name><name><surname>Cagna</surname><given-names>B</given-names></name><name><surname>Dadarwal</surname><given-names>R</given-names></name><name><surname>Evrard</surname><given-names>HC</given-names></name><name><surname>Garcia-Saldivar</surname><given-names>P</given-names></name><name><surname>Giavasis</surname><given-names>S</given-names></name><name><surname>Hartig</surname><given-names>R</given-names></name><name><surname>Lepage</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Majka</surname><given-names>P</given-names></name><name><surname>Merchant</surname><given-names>H</given-names></name><name><surname>Milham</surname><given-names>MP</given-names></name><name><surname>Rosa</surname><given-names>MGP</given-names></name><name><surname>Tasserie</surname><given-names>J</given-names></name><name><surname>Uhrig</surname><given-names>L</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A collaborative resource platform for non-human primate neuroimaging</article-title><source>NeuroImage</source><volume>226</volume><elocation-id>117519</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117519</pub-id><pub-id pub-id-type="pmid">33227425</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirochnik</surname><given-names>RM</given-names></name><name><surname>Pezaris</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Contemporary approaches to visual prostheses</article-title><source>Military Medical Research</source><volume>6</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1186/s40779-019-0206-9</pub-id><pub-id pub-id-type="pmid">31167653</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moleirinho</surname><given-names>S</given-names></name><name><surname>Whalen</surname><given-names>AJ</given-names></name><name><surname>Fried</surname><given-names>SI</given-names></name><name><surname>Pezaris</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The impact of synchronous versus asynchronous electrical stimulation in artificial vision</article-title><source>Journal of Neural Engineering</source><volume>18</volume><elocation-id>abecf1</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/abecf1</pub-id><pub-id pub-id-type="pmid">33900206</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphey</surname><given-names>DK</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Behavioral detection of electrical microstimulation in different cortical visual areas</article-title><source>Current Biology</source><volume>17</volume><fpage>862</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.03.066</pub-id><pub-id pub-id-type="pmid">17462895</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphey</surname><given-names>DK</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perceiving electrical stimulation of identified human visual areas</article-title><source>PNAS</source><volume>106</volume><fpage>5389</fpage><lpage>5393</lpage><pub-id pub-id-type="doi">10.1073/pnas.0804998106</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niketeghad</surname><given-names>S</given-names></name><name><surname>Muralidharan</surname><given-names>A</given-names></name><name><surname>Patel</surname><given-names>U</given-names></name><name><surname>Dorn</surname><given-names>JD</given-names></name><name><surname>Bonelli</surname><given-names>L</given-names></name><name><surname>Greenberg</surname><given-names>RJ</given-names></name><name><surname>Pouratian</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Phosphene perceptions and safety of chronic visual cortex stimulation in a blind subject</article-title><source>Journal of Neurosurgery</source><volume>132</volume><fpage>2000</fpage><lpage>2007</lpage><pub-id pub-id-type="doi">10.3171/2019.3.JNS182774</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Normann</surname><given-names>RA</given-names></name><name><surname>Greger</surname><given-names>BA</given-names></name><name><surname>House</surname><given-names>P</given-names></name><name><surname>Romero</surname><given-names>SF</given-names></name><name><surname>Pelayo</surname><given-names>F</given-names></name><name><surname>Fernandez</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Toward the development of a cortically based visual neuroprosthesis</article-title><source>Journal of Neural Engineering</source><volume>6</volume><elocation-id>035001</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/6/3/035001</pub-id><pub-id pub-id-type="pmid">19458403</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nowik</surname><given-names>K</given-names></name><name><surname>Langwińska-Wośko</surname><given-names>E</given-names></name><name><surname>Skopiński</surname><given-names>P</given-names></name><name><surname>Nowik</surname><given-names>KE</given-names></name><name><surname>Szaflik</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bionic eye review - An update</article-title><source>Journal of Clinical Neuroscience</source><volume>78</volume><fpage>8</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.jocn.2020.05.041</pub-id><pub-id pub-id-type="pmid">32571603</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oswalt</surname><given-names>D</given-names></name><name><surname>Bosking</surname><given-names>W</given-names></name><name><surname>Sun</surname><given-names>P</given-names></name><name><surname>Sheth</surname><given-names>SA</given-names></name><name><surname>Niketeghad</surname><given-names>S</given-names></name><name><surname>Salas</surname><given-names>MA</given-names></name><name><surname>Patel</surname><given-names>U</given-names></name><name><surname>Greenberg</surname><given-names>R</given-names></name><name><surname>Dorn</surname><given-names>J</given-names></name><name><surname>Pouratian</surname><given-names>N</given-names></name><name><surname>Beauchamp</surname><given-names>M</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multi-electrode stimulation evokes consistent spatial patterns of phosphenes and improves phosphene mapping in blind subjects</article-title><source>Brain Stimulation</source><volume>14</volume><fpage>1356</fpage><lpage>1372</lpage><pub-id pub-id-type="doi">10.1016/j.brs.2021.08.024</pub-id><pub-id pub-id-type="pmid">34482000</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panetsos</surname><given-names>F</given-names></name><name><surname>Sanchez-Jimenez</surname><given-names>A</given-names></name><name><surname>Rodrigo-Diaz</surname><given-names>E</given-names></name><name><surname>Diaz-Guemes</surname><given-names>I</given-names></name><name><surname>Sanchez</surname><given-names>FM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Consistent phosphenes generated by electrical microstimulation of the visual thalamus. An experimental approach for thalamic visual neuroprostheses</article-title><source>Frontiers in Neuroscience</source><volume>5</volume><elocation-id>84</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2011.00084</pub-id><pub-id pub-id-type="pmid">21779233</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paraskevoudi</surname><given-names>N</given-names></name><name><surname>Pezaris</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Eye movement compensation and spatial updating in visual prosthetics: mechanisms, limitations and future directions</article-title><source>Frontiers in Systems Neuroscience</source><volume>12</volume><elocation-id>73</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2018.00073</pub-id><pub-id pub-id-type="pmid">30774585</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paraskevoudi</surname><given-names>N</given-names></name><name><surname>Pezaris</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Full gaze contingency provides better reading performance than head steering alone in a simulation of prosthetic vision</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>11121</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-86996-4</pub-id><pub-id pub-id-type="pmid">34045485</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parikh</surname><given-names>N</given-names></name><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Humayun</surname><given-names>M</given-names></name><name><surname>Weiland</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Performance of visually guided tasks using simulated prosthetic vision and saliency-based cues</article-title><source>Journal of Neural Engineering</source><volume>10</volume><elocation-id>026017</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/10/2/026017</pub-id><pub-id pub-id-type="pmid">23449023</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Köpf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pytorch: An Imperative Style, High-Performance Deep Learning Library</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</ext-link></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezaris</surname><given-names>JS</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Demonstration of artificial visual percepts generated through thalamic microstimulation</article-title><source>PNAS</source><volume>104</volume><fpage>7670</fpage><lpage>7675</lpage><pub-id pub-id-type="doi">10.1073/pnas.0608563104</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezaris</surname><given-names>JS</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Simulations of electrode placement for a thalamic visual prosthesis</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>56</volume><fpage>172</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1109/TBME.2008.2005973</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Balasubramanian</surname><given-names>M</given-names></name><name><surname>Schwartz</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Multi-area visuotopic map complexes in macaque striate and extra-striate cortex</article-title><source>Vision Research</source><volume>46</volume><fpage>3336</fpage><lpage>3359</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.03.006</pub-id><pub-id pub-id-type="pmid">16831455</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rasla</surname><given-names>A</given-names></name><name><surname>Beyeler</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The relative importance of depth cues and semantic edges for indoor mobility using simulated prosthetic vision in immersive virtual reality</article-title><conf-name>VRST ’22</conf-name><conf-loc>Tsukuba Japan</conf-loc><pub-id pub-id-type="doi">10.1145/3562939.3565620</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rassia</surname><given-names>KEK</given-names></name><name><surname>Pezaris</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Improvement in reading performance through training with simulated thalamic visual prostheses</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>16310</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-31435-0</pub-id><pub-id pub-id-type="pmid">30397211</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rassia</surname><given-names>KEK</given-names></name><name><surname>Moutoussis</surname><given-names>K</given-names></name><name><surname>Pezaris</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reading text works better than watching videos to improve acuity in a simulation of artificial vision</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>12953</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-10719-6</pub-id><pub-id pub-id-type="pmid">35902596</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reuschel</surname><given-names>J</given-names></name><name><surname>Rösler</surname><given-names>F</given-names></name><name><surname>Henriques</surname><given-names>DY</given-names></name><name><surname>Fiehler</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spatial updating depends on gaze direction even after loss of vision</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2422</fpage><lpage>2429</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2714-11.2012</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabbah</surname><given-names>N</given-names></name><name><surname>Authie</surname><given-names>CN</given-names></name><name><surname>Sanda</surname><given-names>N</given-names></name><name><surname>Mohand-Said</surname><given-names>S</given-names></name><name><surname>Sahel</surname><given-names>J-A</given-names></name><name><surname>Safran</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Importance of eye position on spatial localization in blind subjects wearing an argus ii retinal prosthesis</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><volume>55</volume><fpage>8259</fpage><lpage>8266</lpage><pub-id pub-id-type="doi">10.1167/iovs.14-15392</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez-Garcia</surname><given-names>M</given-names></name><name><surname>Martinez-Cantin</surname><given-names>R</given-names></name><name><surname>Guerrero</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Semantic and structural image segmentation for prosthetic vision</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0227677</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0227677</pub-id><pub-id pub-id-type="pmid">31995568</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sanchez-Garcia</surname><given-names>M</given-names></name><name><surname>Morollon-Ruiz</surname><given-names>R</given-names></name><name><surname>Martinez-Cantin</surname><given-names>R</given-names></name><name><surname>Guerrero</surname><given-names>JJ</given-names></name><name><surname>Fernandez-Jover</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Assessing visual acuity in visual prostheses through a virtual-reality system</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2205.10395">https://arxiv.org/abs/2205.10395</ext-link></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>Kapeller</surname><given-names>C</given-names></name><name><surname>Guger</surname><given-names>C</given-names></name><name><surname>Ogawa</surname><given-names>H</given-names></name><name><surname>Hiroshima</surname><given-names>S</given-names></name><name><surname>Lafer-Sousa</surname><given-names>R</given-names></name><name><surname>Saygin</surname><given-names>ZM</given-names></name><name><surname>Kamada</surname><given-names>K</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Facephenes and rainbows: Causal evidence for functional and anatomical specificity of face and color processing in the human brain</article-title><source>PNAS</source><volume>114</volume><fpage>12285</fpage><lpage>12290</lpage><pub-id pub-id-type="doi">10.1073/pnas.1713447114</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>PH</given-names></name><name><surname>Slocum</surname><given-names>WM</given-names></name><name><surname>Kwak</surname><given-names>MC</given-names></name><name><surname>Kendall</surname><given-names>GL</given-names></name><name><surname>Tehovnik</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>New methods devised specify the size and color of the spots monkeys see when striate cortex (area V1) is electrically stimulated</article-title><source>PNAS</source><volume>108</volume><fpage>17809</fpage><lpage>17814</lpage><pub-id pub-id-type="doi">10.1073/pnas.1108337108</pub-id><pub-id pub-id-type="pmid">21987821</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>EM</given-names></name><name><surname>Bak</surname><given-names>MJ</given-names></name><name><surname>Hambrecht</surname><given-names>FT</given-names></name><name><surname>Kufta</surname><given-names>CV</given-names></name><name><surname>O’Rourke</surname><given-names>DK</given-names></name><name><surname>Vallabhanath</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Feasibility of a visual prosthesis for the blind based on intracortical microstimulation of the visual cortex</article-title><source>Brain</source><volume>119 (Pt 2)</volume><fpage>507</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1093/brain/119.2.507</pub-id><pub-id pub-id-type="pmid">8800945</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Cortical mapping and perceptual invariance: A reply to Cavanagh</article-title><source>Vision Research</source><volume>23</volume><fpage>831</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(83)90206-7</pub-id><pub-id pub-id-type="pmid">6623943</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>RV</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A model of safe levels for electrical stimulation</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>39</volume><fpage>424</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1109/10.126616</pub-id><pub-id pub-id-type="pmid">1592409</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sommerhalder</surname><given-names>J</given-names></name><name><surname>Rappaz</surname><given-names>B</given-names></name><name><surname>de Haller</surname><given-names>R</given-names></name><name><surname>Fornos</surname><given-names>AP</given-names></name><name><surname>Safran</surname><given-names>AB</given-names></name><name><surname>Pelizzone</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Simulation of artificial vision: II. Eccentric reading of full-page text and the learning of this task</article-title><source>Vision Research</source><volume>44</volume><fpage>1693</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.01.017</pub-id><pub-id pub-id-type="pmid">15136004</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>NR</given-names></name><name><surname>Troyk</surname><given-names>PR</given-names></name><name><surname>Dagnelie</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Detection, eye-hand coordination and virtual mobility performance in simulated vision for a cortical visual prosthesis device</article-title><source>Journal of Neural Engineering</source><volume>6</volume><elocation-id>035008</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/6/3/035008</pub-id><pub-id pub-id-type="pmid">19458397</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Mansimov</surname><given-names>E</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unsupervised learning of video representations using lstms</article-title><conf-name>32nd International Conference on Machine Learning, ICML</conf-name><fpage>843</fpage><lpage>852</lpage></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tehovnik</surname><given-names>EJ</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Sultan</surname><given-names>F</given-names></name><name><surname>Slocum</surname><given-names>WM</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Direct and indirect activation of cortical neurons by electrical microstimulation</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>512</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1152/jn.00126.2006</pub-id><pub-id pub-id-type="pmid">16835359</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tehovnik</surname><given-names>EJ</given-names></name><name><surname>Slocum</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phosphene induction by microstimulation of macaque V1</article-title><source>Brain Research Reviews</source><volume>53</volume><fpage>337</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1016/j.brainresrev.2006.11.001</pub-id><pub-id pub-id-type="pmid">17173976</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tehovnik</surname><given-names>EJ</given-names></name><name><surname>Slocum</surname><given-names>WM</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Microstimulation of visual cortex to restore vision</article-title><source>Progress in Brain Research</source><volume>175</volume><fpage>347</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(09)17524-6</pub-id><pub-id pub-id-type="pmid">19660667</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorn</surname><given-names>JT</given-names></name><name><surname>Migliorini</surname><given-names>E</given-names></name><name><surname>Ghezzi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Virtual reality simulation of epiretinal stimulation highlights the relevance of the visual angle in prosthetic vision</article-title><source>Journal of Neural Engineering</source><volume>17</volume><elocation-id>056019</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/abb5bc</pub-id><pub-id pub-id-type="pmid">33146146</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Titchener</surname><given-names>SA</given-names></name><name><surname>Shivdasani</surname><given-names>MN</given-names></name><name><surname>Fallon</surname><given-names>JB</given-names></name><name><surname>Petoe</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gaze compensation as a technique for improving hand–eye coordination in prosthetic vision</article-title><source>Translational Vision Science &amp; Technology</source><volume>7</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/tvst.7.1.2</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Troyk</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Artificial Vision</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-41876-6</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>van Hoof</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The brain as image processor and generator</article-title><publisher-name>Maastricht University</publisher-name></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vugt</surname><given-names>B</given-names></name><name><surname>Dagnino</surname><given-names>B</given-names></name><name><surname>Vartak</surname><given-names>D</given-names></name><name><surname>Safaai</surname><given-names>H</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The threshold for conscious report: Signal loss and response bias in visual and frontal cortex</article-title><source>Science</source><volume>360</volume><fpage>537</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1126/science.aar7186</pub-id><pub-id pub-id-type="pmid">29567809</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vergnieux</surname><given-names>V</given-names></name><name><surname>Mace</surname><given-names>MJ-M</given-names></name><name><surname>Jouffrais</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Wayfinding with simulated prosthetic vision: Performance comparison with regular and structure-enhanced renderings</article-title><conf-name>2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC</conf-name><conf-loc>Chicago, IL</conf-loc><pub-id pub-id-type="doi">10.1109/EMBC.2014.6944151</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergnieux</surname><given-names>V</given-names></name><name><surname>Macé</surname><given-names>MJM</given-names></name><name><surname>Jouffrais</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Simplification of visual rendering in simulated prosthetic vision facilitates navigation</article-title><source>Artificial Organs</source><volume>41</volume><fpage>852</fpage><lpage>861</lpage><pub-id pub-id-type="doi">10.1111/aor.12868</pub-id><pub-id pub-id-type="pmid">28321887</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Joshua Wilson</surname><given-names>KJM</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Eric Larson</surname><given-names>CJC</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Ian Henriksen</surname><given-names>EAQ</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Vijaykumar</surname><given-names>A</given-names></name><name><surname>Bardelli</surname><given-names>AP</given-names></name><name><surname>Rothberg</surname><given-names>A</given-names></name><name><surname>Hilboll</surname><given-names>A</given-names></name><name><surname>Kloeckner</surname><given-names>A</given-names></name><name><surname>Scopatz</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>A</given-names></name><name><surname>Ariel Rokem</surname><given-names>CNW</given-names></name><name><surname>Fulton</surname><given-names>C</given-names></name><name><surname>Masson</surname><given-names>C</given-names></name><name><surname>Häggström</surname><given-names>C</given-names></name><name><surname>Fitzgerald</surname><given-names>C</given-names></name><name><surname>Nicholson</surname><given-names>DA</given-names></name><name><surname>Hagen</surname><given-names>DR</given-names></name><name><surname>Pasechnik</surname><given-names>DV</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Martin</surname><given-names>E</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Silva</surname><given-names>F</given-names></name><name><surname>Lenders</surname><given-names>F</given-names></name><name><surname>Florian Wilhelm</surname><given-names>GY</given-names></name><name><surname>Price</surname><given-names>GA</given-names></name><name><surname>Ingold</surname><given-names>GL</given-names></name><name><surname>Allen</surname><given-names>GE</given-names></name><name><surname>Lee</surname><given-names>GR</given-names></name><name><surname>Audren</surname><given-names>H</given-names></name><name><surname>Probst</surname><given-names>I</given-names></name><name><surname>Dietrich</surname><given-names>JP</given-names></name><name><surname>Silterra</surname><given-names>J</given-names></name><name><surname>Webber</surname><given-names>JT</given-names></name><name><surname>Slavič</surname><given-names>J</given-names></name><name><surname>Nothman</surname><given-names>J</given-names></name><name><surname>Buchner</surname><given-names>J</given-names></name><name><surname>Kulick</surname><given-names>J</given-names></name><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Harrington</surname><given-names>J</given-names></name><name><surname>Rodríguez</surname><given-names>JLC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vurro</surname><given-names>M</given-names></name><name><surname>Crowell</surname><given-names>AM</given-names></name><name><surname>Pezaris</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Simulation of thalamic prosthetic vision: reading accuracy, speed, and acuity in sighted humans</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>816</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00816</pub-id><pub-id pub-id-type="pmid">25408641</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>White</surname><given-names>J</given-names></name><name><surname>Kameneva</surname><given-names>T</given-names></name><name><surname>McCarthy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep reinforcement learning for task-based feature learning in prosthetic vision</article-title><conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><fpage>2809</fpage><lpage>2812</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2019.8856541</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Linking electrical stimulation of human primary visual cortex, size of affected cortical area, neuronal responses, and subjective experience</article-title><source>Neuron</source><volume>92</volume><fpage>1213</fpage><lpage>1219</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.11.008</pub-id><pub-id pub-id-type="pmid">27939584</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>KW</given-names></name><name><surname>Mendis</surname><given-names>BSU</given-names></name><name><surname>Bouzerdoum</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Real-Time Simulation of Phosphene Images Evoked by Electrical Stimulation of the Visual Cortex</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-17537-4</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Puig</surname><given-names>X</given-names></name><name><surname>Xiao</surname><given-names>T</given-names></name><name><surname>Fidler</surname><given-names>S</given-names></name><name><surname>Barriuso</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Semantic understanding of scenes through the ADE20K dataset</article-title><source>International Journal of Computer Vision</source><volume>127</volume><fpage>302</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1007/s11263-018-1140-0</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85812.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.12.23.521749" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.12.23.521749"/></front-stub><body><p>This important study presents a simulator for prosthetic vision (with open source code) whose design is informed by previous psychophysical and neuroanatomical work. The simulation is convincing and demonstrates significant improvements over past visual prosthesis simulations. This work will be of interest to those investigating the impact of cortical stimulation on perception, particularly those developing visual prostheses.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85812.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Barry</surname><given-names>Michael P</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/037t3ry66</institution-id><institution>Illinois Institute of Technology</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.12.23.521749">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.12.23.521749v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Biologically plausible phosphene simulation for the differentiable optimization of visual cortical prostheses&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Chris Baker as the Reviewing Editor/Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Michael P Barry (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers all think the work has great potential and will be useful to the field, but also highlight a number of major limitations. These are all laid out clearly in their individual comments (below).</p><p>In a revision, we would like to see the modeling work extended with a clear acknowledgement of some of the limitations. In particular, a revision should address concerns about:</p><p>1) Model validation with quantitative approaches.</p><p>2) Cortical folding.</p><p>3) Phosphene mapping.</p><p>4) Multiple electrode stimulation and electrode interactions.</p><p>We anticipate this will require substantial revisions to the modeling and the manuscript and not just a discussion of these issues.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The authors should clarify which data was used to fit the model and which data was used to test its predictive ability. As presented, it appears to be more of a descriptive &quot;one size fits all&quot; model than a predictive one that could be used to generalize to new patients and data. To that end, the paper should also make an effort to evaluate the model more thoroughly and more quantitatively.</p><p>The paper claims that based on CORTIVIS results, phosphenes are Gaussian blobs. However, this is true only for single-electrode percepts. Ref. 6 clearly states that multi-electrode stimulation does not produce a linear summation of Gaussian blobs. This is therefore quite a strong assumption of the model that needs to be clearly stated and discussed – it makes it unlikely that the results presented in Figure 8 would translate to real patients.</p><p>It is puzzling to me that the paper would place such an emphasis on the regularization loss, which was thought to promote &quot;subjective interpretability&quot;, but then use a relative weighting of 0.999 for the reconstruction loss and only 0.001 for regularization. I suspect the low weight has to do with training instabilities, but with a relative weight of 0.001 it is hard to argue that this term had any influence on the results at all.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>– The mapping algorithm for visual field to the cortical surface ignores cortical folding</p><p>– It appears the stimulation models also do not take cortical folding into account</p><p>– Given the authors' previous report in JoV, 2022, it isn't clear how much of this paper is an advance.</p><p>– While the authors repeatedly describe the advances that an end-to-end mode confers, the practicality of such a system is not discussed. In particular, end-to-end optimization can only demonstrate that the critical intermediate representation -- the phosphene brightnesses over time -- retains sufficient information for decoding an approximation of the original image. It says little about the hypothetical implant recipient's ability to extract that information into a usable percept.</p><p>– Figure 3 -- the dashed lines are difficult to visually parse.</p><p>– The simulator appears to assume perfect knowledge of phosphene location in the visual field; this assumption is implausible</p><p>– Some of the claims for performance are questionable: 10,000 phosphenes on an image of 64x64 pixels is not a reasonable assessment.</p><p>– Speed is good, but requires substantial hardware to achieve the claimed performance. Other, similar simulations from the literature report as-good or better performance without a multi-thousand dollar GPU. Although the published simulations do not include as detailed models as included here, the substantial difference gives one pause.</p><p>– Focusing on cortical stimulation leaves a significant portion of the literature unexplored, an unreasonable narrowing as the majority of simulation literature is fundamentally agnostic as to the targeted brain area, or at least adaptable to different areas with minimal effort, a claim that the authors here also make of their system.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Towards biologically plausible phosphene simulation for the differentiable optimization of visual cortical prostheses&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Chris Baker as Senior and Reviewing Editor.</p><p>The revised manuscript was re-evaluated by two of the initial reviewers. While the manuscript has been improved there are some remaining issues that need to be addressed, as outlined below:</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Thank you for updating the manuscript and further demonstrating the capabilities of your simulator. The manuscript would be improved by addressing the below issues:</p><p>1) Overall, the manuscript should focus more on simulator demonstrations that reflect existing visual prosthetic technology, instead of highlighting examples with hundreds or thousands of noninteracting phosphenes in Figures 1 and 6-8. Examples using 10-60 phosphenes with nontrivial interactions should be prominent in the main manuscript. Even when only considering available channels, Fernández et al.'s system, as tested, could only stimulate up to 16 channels simultaneously, and the Orion device only has 60 channels. The ability to optimize hundreds of independent phosphenes will be very important in the future when devices are shown to be able to create that many phosphenes simultaneously. Until the field reaches that point, however, emphasizing examples with such large collections of phosphenes encourages misconceptions regarding the capabilities and existing challenges of visual prostheses.</p><p>2) The authors provide nice examples of how nonlinear interactions between single-electrode phosphenes can be rendered in Figure 1—figure supplement 3. While this is a nice demonstration, the authors should put more emphasis on this capability of the simulator. Alongside the end-to-end demonstrations of how the simulator performs assuming independent electrodes and phosphenes, the authors should include at least one end-to-end demonstration of how the simulator performs assuming nontrivial nonlinearities with fewer than 60 phosphenes. If such constraints appear to eliminate the meaningful utility of the simulator and its optimization process, the authors should thoroughly discuss this issue.</p><p>3) This simulator attempts to take numerous biological factors into account to translate electrode locations and stimulation parameters into simulated phosphenes, but also offers many points at which users can make manual adjustments. As the ideas of biological plausibility and simulator flexibility are both raised frequently, it would be good for the authors to specify what aspects of biological plausibility might be lost or maintained when users take advantage of different forms of flexibility in the simulator. For example, for the &quot;most basic mode&quot; of phosphene mapping described in lines 283-286, how much of the biological modeling is bypassed? Are V1 stimulation locations assumed based on the phosphene locations to calculate other phosphene characteristics?</p><p>4) Do the additions of nonlinear phosphene interactions, such as the ones in Figure 1—figure supplement 3, have any significant effect on simulator speed?</p><p>5) Aside from the instances of cross-validation, the authors frequently use the terms &quot;validate&quot; or &quot;validation&quot; when &quot;verify&quot; or &quot;verification&quot; would be more accurate. Particularly when the authors are demonstrating that model output reasonably matches the data for which it was configured to fit, the authors should not use the term validation.</p><p>6) The authors refer to cortical-surface electrodes generally as ECoG electrodes, but only a subset of the referenced studies used electrocorticography arrays. The Brindley and Lewin, Dobelle, and Orion systems did not record neural activity, and thus would not be classified as ECoG systems.</p><p>7) It would be useful for the authors to provide an example of how brightness accommodation is taken into account by the simulation over time.</p><p>8) In the right panel of Figure 2, it is unclear what &quot;1e-7&quot; at the top of the panel signifies.</p><p>9) It can be confusing how both memory trace and minimal input charge use the symbol Q.</p><p>10) In Figure 1—figure supplement 1, noise from a normal distribution with σ = 0.03 degrees is probably a bad example for representing uncertainty in phosphene location. Pointing responses from implantees can have standard deviations on the order of 3 degrees, so achieving a standard error of 0.03 degrees would require around 1000 localization trials per phosphene. Although this is just an example and the simulator can use any level of uncertainty, a more meaningful example of noise might use σ around 0.5-1.0 degrees.</p><p>11) Lines 141-142: The definition for stimulation threshold is vague. The threshold of perception is usually defined as the level at which the probability of stimulus detection is 50%, or sometimes 75%. Is there a specific probability associated with &quot;reliably produce a visible phosphene&quot;?</p><p>12) Line 270: The model is described as memory intensive. What ranges of memory are required?</p><p>13) Lines 323-329: Phosphene size is calculated based on the current provided, but not total charge. How is pulse width taken into account for phosphene sizes?</p><p>14) Lines 381-384: Activation thresholds are determined purely on a per-electrode basis. The authors should discuss how reduced charge-per-electrode thresholds with multi-electrode stimulation can be included in the simulator (e.g., less charge per electrode required when using 2 or 4 adjacent electrodes instead of a single electrode).</p><p>15) Line 453: The reference to the coefficient of determination should be paired with a clarification that this was only a verification of the parameter fitting process and not a demonstration of how the simulator matches unseen data.</p><p>16) Lines 758-759: The text mentions considerations for phosphene-perception delays after stimulation onset. Stimulation strategies will also be important for addressing phosphene perception persisting after stimulation offset. Is such undesired persistence modeled at all by the simulation?</p><p>17) Line 1023: The linked repository appears to be inaccessible: https://github.com/neuralcodinglab/dynaphos-experiments</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85812.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>The authors should clarify which data was used to fit the model and which data was used to test its predictive ability. As presented, it appears to be more of a descriptive &quot;one size fits all&quot; model than a predictive one that could be used to generalize to new patients and data. To that end, the paper should also make an effort to evaluate the model more thoroughly and more quantitatively.</p></disp-quote><p>As described in the public review response, we have adapted our model evaluations in several ways:</p><list list-type="bullet"><list-item><p>The fit of the thresholding mechanism was adapted to include a 3-fold cross validation, where part of the data was excluded during the fitting. The results of the cross-validation are now presented in panel D of Figure 3.</p></list-item><list-item><p>For the components of the model calculating the phosphene size and brightness, the text was adapted to present these elements as fits and not predictions.</p></list-item><list-item><p>To demonstrate model generalizability, the thresholding model was fit, using cross validation, to surface electrode data from several other clinical studies (Dobelle and Mladejovsky, 1974; Girvin et al., 1979; Niketeghad et al., 2019). Model predictions for these validation experiments are presented in Figure 3—figure supplement 1.</p></list-item></list><disp-quote content-type="editor-comment"><p>The paper claims that based on CORTIVIS results, phosphenes are Gaussian blobs. However, this is true only for single-electrode percepts. Ref. 6 clearly states that multi-electrode stimulation does not produce a linear summation of Gaussian blobs. This is therefore quite a strong assumption of the model that needs to be clearly stated and discussed – it makes it unlikely that the results presented in Figure 8 would translate to real patients.</p></disp-quote><p>We agree with the reviewer that the assumption of linear summation in our simulator does not correspond with all clinical observations in the literature: we now mention that linear summation is more likely when the distance between intracortical electrodes is larger than 1 mm (Ghose and Maunsell, 2012) and that the interference patterns are more likely for smaller distances.</p><p>Based on the reviewer’s suggestions, we have made several textual adjustments to improve on the clarity about our model assumptions and limitations. Furthermore, in Figure 1—figure supplement 2 and Figure 1—figure supplement 3 we now also demonstrate an example approach of how our simulator could be adapted to simulate arbitrary phosphene shapes and electrode interactions.</p><p>Adjustments:</p><list list-type="bullet"><list-item><p>Added Figure 1—figure supplement 2 on irregular phosphene percepts</p></list-item><list-item><p>Lines 957-970: Furthermore, <italic>in contrast to the assumptions of our model</italic>, interactions between simultaneous stimulation of multiple electrodes can have an effect on the phosphene size and sometimes lead to unexpected percepts (Fernandez <italic>et al.</italic>, 2021, Dobelle and Mladejovsky 1974, Bak <italic>et al.</italic>, 1990). Although our software supports basic exploratory experimentation of non-linear interactions (see Figure 1—figure supplement 3), by default, our simulator assumes independence between electrodes. Multiphosphene percepts are modeled using linear summation of the independent percepts. These assumptions seem to hold for intracortical electrodes separated by more than 1 mm (Ghose and Maunsell, 2012), but may underestimate the complexities observed when electrodes are nearer. Further clinical and theoretical modeling work could help to improve our understanding of these non-linear dynamics.</p></list-item></list><disp-quote content-type="editor-comment"><p>It is puzzling to me that the paper would place such an emphasis on the regularization loss, which was thought to promote &quot;subjective interpretability&quot;, but then use a relative weighting of 0.999 for the reconstruction loss and only 0.001 for regularization. I suspect the low weight has to do with training instabilities, but with a relative weight of 0.001 it is hard to argue that this term had any influence on the results at all.</p></disp-quote><p>We thank the reviewer for this remark. Mistakenly, we have reported the wrong numbers. The reconstruction loss and the regularization loss were weighted 0.00001 and 0.99999, respectively. Also, we have now specified that here the regularization loss is measured as the MSE between the brightness of the phosphene center pixels and the corresponding brightness in the input image. The high relative regularization loss ensures that the phosphenes look similar to the input image. Note that the regularization loss is calculated by comparing the input with the phosphene simulation and backpropagated through the encoder. Because only the encoder and not the decoder is affected by the regularization loss, the training of the decoder is driven entirely by the reconstruction loss. In summary:</p><list list-type="bullet"><list-item><p>The encoder is trained using a very small reconstruction loss component and a big regularization component to drive phosphene encodings that look similar to the input.</p></list-item><list-item><p>The decoder is trained using the reconstruction component.</p></list-item></list><p>Accordingly, we have made the following textual adjustment:</p><p>Lines 578-586: We increased the training stability by taking the regularisation loss as the MSE between the pixel brightness at the phosphene centers and the corresponding pixel brightness in the input image. Furthermore, we adapted the relative weights of the reconstruction loss and the regularization loss to 0.00001 and 0.99999, respectively. Note that the regularization loss merely promotes similarity between the phosphene encoding and the input and the decoder is unaffected by the regularization loss.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>– The mapping algorithm for visual field to the cortical surface ignores cortical folding</p></disp-quote><p>Cortical folding is indeed an issue that cortical implants will need to consider. However, we chose to make the simulator as generally applicable as possible and the model can simulate any set of phosphene locations, also those produced when the calcarine sulcus is not implanted. As illustration we added a supplementary figure (Figure 1—figure supplement 1) demonstrating a feasible electrode grid placement on a 3D brain, generating the phosphene locations from estimated receptive field maps.</p><disp-quote content-type="editor-comment"><p>– It appears the stimulation models also do not take cortical folding into account</p></disp-quote><p>We thank the reviewer for this remark. We indeed did not include cortical folding into account in the stimulation models, as we assume that the area of activation will be on such a small scale that cortical folding will not have a notable impact (Ghose and Maunsell, 2012). However, such effects could be incorporated in a straightforward way to finetune the simulator to a specific patient’s perceptual experiences. Figure 1—figure supplement 3 displays an example where our code is finetuned to simulate irregular phosphene percepts,</p><disp-quote content-type="editor-comment"><p>– Given the authors' previous report in JoV, 2022, it isn't clear how much of this paper is an advance.</p></disp-quote><p>The improvements of our simulator compared to simulators that are presented in earlier studies are now described in the introduction.</p><p>The two studies had different aims: while the previous paper presents a pipeline for the endto-end optimization of phosphene encodings, our current study presents a simulator that can be used both in simulation experiments with human subjects and as well as in an existing end-to-end pipeline. We have improved the description of the differences between the two studies.</p><p>We have made the following textual changes:</p><list list-type="bullet"><list-item><p>Lines 232-253: Again, an important drawback is that current computational studies use simplified simulations of cortical prosthetic vision. This problem is addressed in the current study. Note that end-to-end machine learning pipelines rely on gradient propagation to update the parameters of the phosphene encoding model. Consequently, a crucial requirement is that the simulator makes use of differentiable operations to convert the stimulation parameters to an image of phosphenes – a requirement that is met by the proposed simulator. To evaluate the practical usability of our simulator in an end-to-end framework, we replicated and adapted the experiments by (de Ruyter van Steveninck, Güçlü et al., 2022), replacing their simulator with ours. The currently proposed simulator can be compared to the simulator that was used in the aforementioned work: Our simulator can handle temporal sequences and our experiments explore a more biologically grounded simulation of phosphene size and locations. Furthermore, instead of a more abstract or qualitative description of the required stimulation ('on' / 'off'), we included biologically inspired methods to model the perceptual effects of different stimulation parameters such as the current amplitude, the duration, the pulse width and the frequency.</p></list-item><list-item><p>Lines 494-497: To validate that the simulator can conveniently be incorporated in a machine learning pipeline, we replicated an existing SPV pipeline by (de Ruyter van Steveninck, Güçlü et al., 2022), replacing the simulator with our biologically plausible simulator.</p></list-item></list><disp-quote content-type="editor-comment"><p>– While the authors repeatedly describe the advances that an end-to-end mode confers, the practicality of such a system is not discussed. In particular, end-to-end optimization can only demonstrate that the critical intermediate representation -- the phosphene brightnesses over time -- retains sufficient information for decoding an approximation of the original image. It says little about the hypothetical implant recipient's ability to extract that information into a usable percept.</p></disp-quote><p>The reviewer raises a valid point. On the one hand, prior literature (Granley <italic>et al.</italic>, 2022; Fauvel <italic>et al.</italic>, 2022; White <italic>et al.</italic>, 2019; Küçükoglü <italic>et al.</italic> 2022; de Ruyter van Steveninck, Güçlü <italic>et al.</italic>, 2022; Ghaffari <italic>et al.</italic>, 2021) indicate that computational optimization pipelines can be beneficial in general. On the other hand, simulation models differ from reality. The present study aims to reduce the gap between simulation models and the clinical reality. We do not yet know the experience of the end user. We now avoid claims about the benefits of end-to-end optimization, as the evaluation thereof is outside the scope of the current paper. We have now made these nuances more explicit and have further clarified the distinction between our own study aims and the hypothesized benefits stated in prior work.</p><list list-type="bullet"><list-item><p>Lines 842-853: Eventually, the functional quality of the artificial vision will not only depend on the correspondence between the visual environment and the phosphene encoding, but also on the implant recipient's ability to extract that information into a usable percept. The functional quality of end-to-end generated phosphene encodings in daily life tasks will need to be evaluated in future experiments. Regardless of the implementation, it will always be important to include human observers (both sighted experimental subjects and actual prosthetic implant users) in the optimization cycle to ensure subjective interpretability for the end user (Fauvel et al., 2022; Beyeler and Sanchez-Garcia, 2022).</p></list-item><list-item><p>Lines 810-819: Computational optimization approaches can also aid in the development of safe stimulation protocols, because they allow a faster exploration of the large parameter space and enable task-driven optimization of image processing strategies (Granley et al., 2022; Fauvel et al., 2022; White et al., 2019; Küçükoglü et al. 2022; de Ruyter van Steveninck, Güçlü et al., 2022; Ghaffari et al., 2021). Ultimately, the development of taskrelevant scene-processing algorithms will likely benefit both from computational optimization experiments as well as exploratory SPV studies with human observers. With the presented simulator we aim to contribute a flexible toolkit for such experiments.</p></list-item></list><disp-quote content-type="editor-comment"><p>– Figure 3 -- the dashed lines are difficult to visually parse.</p></disp-quote><p>We have now removed the distinction between solid/dashed lines in the right panel, and the corresponding lines in the figure caption. (Note that this is now Figure 2 in the revised manuscript)</p><disp-quote content-type="editor-comment"><p>– The simulator appears to assume perfect knowledge of phosphene location in the visual field; this assumption is implausible</p></disp-quote><p>We agree that the assumption of having perfect knowledge of phosphene locations is implausible. For this reason, the mechanism for specifying electrode locations on the simplified cortical map includes noise. The simulator does, however, assume that phosphene locations are stable over time (e.g. Bak et al. 1990, Chen et al. 2020, Fernández et al. 2021). In that sense, the uncertainty of phosphene locations is taken into account once, at initialization of the simulator, and phosphene locations are predictable after this point. Furthermore, the simulator can be initialized with any arbitrary phosphene map, allowing researchers to include various levels of uncertainty in their simulations.</p><p>We adapted the description of the cortical mapping process to further clarify our assumptions regarding the sources of noise. Moreover, a supplementary figure has been added to showcase the possibility of using other methods for electrode and phosphene location initializations. The description of this figure includes an explicit mention of where in the pipeline uncertainty is assumed and noise added.</p><list list-type="bullet"><list-item><p>Lines 315-319: Moreover, to simulate imperfect knowledge of electrode or phosphene locations, and malfunctioning electrodes, the cortical mapping methods include parameters for the introduction of noise and electrode dropout.</p></list-item><list-item><p>Figure 1—figure supplement 1 caption: To simulate imperfect knowledge of the electrode or phosphene locations, a small normally distributed noise (σ = 0.03°) was added to the determined receptive fields. Note that as the simulator is initialized with the phosphene locations in the visual field, any assumptions about feasible electrode locations, uncertainties and other sources of noise can be incorporated flexibly.</p></list-item></list><disp-quote content-type="editor-comment"><p>– Some of the claims for performance are questionable: 10,000 phosphenes on an image of 64x64 pixels is not a reasonable assessment.</p></disp-quote><p>We agree, our aim was to test the performance limits of the software and for completeness we evaluated all combinations of phosphene counts (between 10 and 10.000) and image resolutions (between 64x64 and 1024x1024). These permutations give insight in how the phosphene counts and image resolution impacts on the processing speed of the simulator. Our computational experiments simulated 1000 phosphenes and 256 x 256 pixels.</p><p>To avoid any confusion, we have now incorporated the following changes:</p><list list-type="bullet"><list-item><p>We have now moved the validation experiments (that evaluate the biological plausibility) to the beginning of the Results section as these are the most relevant results. The performance experiments (of secondary importance) are now presented afterwards.</p></list-item></list><list list-type="bullet"><list-item><p>We have added the following explanatory sentence to the figure caption (Figure 5 in the revised manuscript): “Note that these data are presented only for evaluating the software performance. For some combinations of phosphene count and image resolution (e.g. 10.000 phosphenes in a 64 x 64 image) there are fewer pixels then phosphenes”</p></list-item></list><disp-quote content-type="editor-comment"><p>– Speed is good, but requires substantial hardware to achieve the claimed performance. Other, similar simulations from the literature report as-good or better performance without a multi-thousand dollar GPU. Although the published simulations do not include as detailed models as included here, the substantial difference gives one pause.</p></disp-quote><p>Unquestionably, there are alternative strategies for the simulation of phosphene vision which can be run in real-time on a CPU. However, our simulator provides two important advantages that justify the requirement of parallel processing:</p><list list-type="bullet"><list-item><p>It has improved realism and biological plausibility. This is the most important motivation for our work.</p></list-item><list-item><p>It allows the propagation of gradients, which enable the use in gradient-based optimization frameworks, including end-to-end experiments.</p></list-item></list><p>Note that the presented results in Figure 2 (index refers to the originally submitted manuscript) might have given the false impression that our simulator requires a multithousand dollar GPU. However, our simulator can also be run on a range of different devices, and the level of detail of the simulation is up to the user. See point 4 of the public responses: even on a commonly-used GPU that is priced around 600 euros our simulator runs detailed simulations with real-time performance.</p><p>Furthermore, GPUs are becoming more widespread because many behavioral and computational paradigms rely on them, for instance, to render virtual environments, or to train deep learning models.</p><p>We have incorporated the following textual adjustment:</p><p>Lines 860-873: Firstly, although our model runs in real-time and is faster than the state-of-the art realistic simulation for retinal prostheses (Beyeler et al., 2017), there is a tradeoff between speed and the memory demand. Therefore, for higher resolutions and larger number of phosphenes, future experimental research may need to adapt a simplified version of our model – although most of the simulation conditions can be run easily with common graphical cards. While several simulators exist for cortical prostheses that run in real time without requiring a dedicated graphics card (e.g. Li et al., 2013; Fehervari et al., 2010), none of these incorporate current spread-based models of phosphene size, realistic stimulation parameter ranges or temporal dynamics. An effort can be made to balance more lightweight hardware requirements with more realistic phosphene characteristics.</p><disp-quote content-type="editor-comment"><p>– Focusing on cortical stimulation leaves a significant portion of the literature unexplored, an unreasonable narrowing as the majority of simulation literature is fundamentally agnostic as to the targeted brain area, or at least adaptable to different areas with minimal effort, a claim that the authors here also make of their system.</p></disp-quote><p>We agree, and the simulator could also be adapted to emulate thalamic stimulation (Pezaris and Reid, 2007). However, retinal stimulation has been shown to produce distinct visual percepts due to the organization of axonal pathways, and the modeling and simulation of these processes is well addressed by Beyeler et al. (2017). We are unsure about optic nerve stimulation, because it may produce percepts with a different dependence on stimulation parameters because the location of phosphenes changes if stimulation frequency and amplitude is varied (Delbeke et al. 2003).</p><p>In addition, following the reviewer's advice, we have now explored the possibility of fitting our simulator to results from the clinical literature on cortical surface electrodes. The results are presented in Figure 3—figure supplement 1.</p><p>Textual additions:</p><list list-type="bullet"><list-item><p>Lines 107-110: The modular design of the simulator allows for future extensions to simulate brain stimulation in other regions such as lateral geniculate nucleus (LGN) or higher visual areas (Murphey et al., 2007; Pezaris and Reid, 2007; Panetsos et al., 2011; Mirochnik and Pezaris, 2019)</p></list-item><list-item><p>Lines 319-322: We note, however, that the framework is compatible with the retinotopic maps of other structures, such as the LGN, which is the structure providing input to the primary visual cortex.</p></list-item><list-item><p>Lines 649-652: Future studies that target other structures than V1 that contain a retinotopic map, such as the LGN, can also use the simulator by replacing the V1 map with a retinotopic map of the respective brain structure.</p></list-item></list><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>The revised manuscript was re-evaluated by two of the initial reviewers. While the manuscript has been improved there are some remaining issues that need to be addressed, as outlined below:</p><p>Reviewer #3 (Recommendations for the authors):</p><p>Thank you for updating the manuscript and further demonstrating the capabilities of your simulator. The manuscript would be improved by addressing the below issues:</p><p>1) Overall, the manuscript should focus more on simulator demonstrations that reflect existing visual prosthetic technology, instead of highlighting examples with hundreds or thousands of noninteracting phosphenes in Figures 1 and 6-8. Examples using 10-60 phosphenes with nontrivial interactions should be prominent in the main manuscript. Even when only considering available channels, Fernández et al.'s system, as tested, could only stimulate up to 16 channels simultaneously, and the Orion device only has 60 channels. The ability to optimize hundreds of independent phosphenes will be very important in the future when devices are shown to be able to create that many phosphenes simultaneously. Until the field reaches that point, however, emphasizing examples with such large collections of phosphenes encourages misconceptions regarding the capabilities and existing challenges of visual prostheses.</p></disp-quote><p>Indeed, many of the demonstrations in our study focus on the ability to optimize hundreds of independent phosphenes. This is an important future challenge that requires further exploration, especially given recent advances demonstrating higher channel-count visual prostheses (see Chen et. 2020, Science). Nevertheless, we fully agree with the reviewer on the importance of addressing limitations of existing contemporary devices. Depending on the requirements of the user, our simulator can be configured to simulate low-channel-count prostheses just as well as high-channelcount prostheses.</p><p>We thank the reviewer for the concrete suggestions. To demonstrate that our simulator is not limited to large electrode counts and can simulate low-resolution vision of contemporary prostheses, we have added a supplementary experiment with only 60 available electrodes.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-sa2-fig1-v1.tif"/></fig><p>Furthermore, we implemented a basic interaction model in the end-to-end pipeline. The results are visualized in Figure 8—figure supplement 3 and Figure 8—figure supplement 4.</p><disp-quote content-type="editor-comment"><p>2) The authors provide nice examples of how nonlinear interactions between single-electrode phosphenes can be rendered in Figure 1—figure supplement 3. While this is a nice demonstration, the authors should put more emphasis on this capability of the simulator. Alongside the end-to-end demonstrations of how the simulator performs assuming independent electrodes and phosphenes, the authors should include at least one end-to-end demonstration of how the simulator performs assuming nontrivial nonlinearities with fewer than 60 phosphenes. If such constraints appear to eliminate the meaningful utility of the simulator and its optimization process, the authors should thoroughly discuss this issue.</p></disp-quote><p>We thank the reviewer for the suggestion of putting extra emphasis on nonlinear interactions, and point back to the response on point 1. The presented additional experiments demonstrate that the exploration of stimulation effects with custom nonlinear electrode interactions is possible.</p><disp-quote content-type="editor-comment"><p>3) This simulator attempts to take numerous biological factors into account to translate electrode locations and stimulation parameters into simulated phosphenes, but also offers many points at which users can make manual adjustments. As the ideas of biological plausibility and simulator flexibility are both raised frequently, it would be good for the authors to specify what aspects of biological plausibility might be lost or maintained when users take advantage of different forms of flexibility in the simulator. For example, for the &quot;most basic mode&quot; of phosphene mapping described in lines 283-286, how much of the biological modeling is bypassed? Are V1 stimulation locations assumed based on the phosphene locations to calculate other phosphene characteristics?</p></disp-quote><p>The flexibility of the simulator lies in the modular code implementation, and it is up to the user to remove or reconfigure specific modeling features for certain goals. However, for the work presented in the current manuscript, we always used the entire simulator’s features including all the biologically plausible modules.</p><p>We believe that the phrasing ‘in the most basic mode’ may have been a bit misleading, since it suggests that the simulator uses several predefined modes. What we meant to say was: users of the simulator can provide a list of electrode locations or phosphene locations to the simulator to base the simulations on clinical data. We also provide the code for generating a random list of phosphene locations based on equally-distant electrode locations on a flattened cortical map of V1.</p><p>The phrasing has been adapted in the manuscript for clarification.</p><disp-quote content-type="editor-comment"><p>4) Do the additions of nonlinear phosphene interactions, such as the ones in Figure 1—figure supplement 3, have any significant effect on simulator speed?</p></disp-quote><p>When measuring timings, we found that calculating the interactions in figure 3 took 0.02ms on average, thus the impact on the simulation speed can be considered neglectable.</p><disp-quote content-type="editor-comment"><p>5) Aside from the instances of cross-validation, the authors frequently use the terms &quot;validate&quot; or &quot;validation&quot; when &quot;verify&quot; or &quot;verification&quot; would be more accurate. Particularly when the authors are demonstrating that model output reasonably matches the data for which it was configured to fit, the authors should not use the term validation.</p></disp-quote><p>We thank the reviewer for the suggestion to clarify the text, and we have added the term ‘verification’ at several locations in the text where validation was mentioned, to indicate that not all parts of the model were validated in the strict sense of the word.</p><disp-quote content-type="editor-comment"><p>6) The authors refer to cortical-surface electrodes generally as ECoG electrodes, but only a subset of the referenced studies used electrocorticography arrays. The Brindley and Lewin, Dobelle, and Orion systems did not record neural activity, and thus would not be classified as ECoG systems.</p></disp-quote><p>We have adapted the text to clarify that the electrodes described are placed on the cortical surface (as opposed to intracortical electrodes), and we have removed the</p><p>“ECoG” indication.</p><disp-quote content-type="editor-comment"><p>7) It would be useful for the authors to provide an example of how brightness accommodation is taken into account by the simulation over time.</p></disp-quote><p>We agree that such an example would be quite useful to the reader. We have added Figure 4—figure supplement 1 as an example showcasing the effects of brightness accommodation as well as delayed phosphene onset in response to 7 seconds of continuous stimulation.</p><disp-quote content-type="editor-comment"><p>8) In the right panel of Figure 2, it is unclear what &quot;1e-7&quot; at the top of the panel signifies.</p></disp-quote><p>“1e-7” has been changed to “10^-7” to clarify the range of the activation values in this figure.</p><disp-quote content-type="editor-comment"><p>9) It can be confusing how both memory trace and minimal input charge use the symbol Q.</p></disp-quote><p>We agree, and we have therefore changed the symbol for the memory trace to B.</p><disp-quote content-type="editor-comment"><p>10) In Figure 1—figure supplement 1, noise from a normal distribution with σ = 0.03 degrees is probably a bad example for representing uncertainty in phosphene location. Pointing responses from implantees can have standard deviations on the order of 3 degrees, so achieving a standard error of 0.03 degrees would require around 1000 localization trials per phosphene. Although this is just an example and the simulator can use any level of uncertainty, a more meaningful example of noise might use σ around 0.5-1.0 degrees.</p></disp-quote><p>We appreciate the reviewer's remark indicating that uncertainty in the experimental estimation of phosphene locations can be of a higher magnitude as the one used in our paper. However, the mentioned pointing error reflects a <italic>reporting error</italic> rather than the <italic>perceptual spread</italic> of phosphene locations. Our simulator simulates the <italic>perceptual experience</italic> of the implant user, and incorporates uncertainty and noise due to implantation inaccuracies and visuotopic map irregularities, and not <italic>reporting inaccuracies</italic>.</p><p>In other words: the simulated phosphene image in Figure 1—figure supplement 1 shows what the user would <italic>see</italic> as in contrast to what the person would manually <italic>report</italic>. Therefore, we disagree with the reviewer that increasing the noise level would give a more meaningful example. As such, we have chosen to not change the figure.</p><p>Note that the noise levels can be easily adjusted depending on the users requirements. An illustration of the result of setting the noise to 0.5 degrees is included here:</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85812-sa2-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>11) Lines 141-142: The definition for stimulation threshold is vague. The threshold of perception is usually defined as the level at which the probability of stimulus detection is 50%, or sometimes 75%. Is there a specific probability associated with &quot;reliably produce a visible phosphene&quot;?</p></disp-quote><p>We agree that this definition is imprecise and have changed the text to “produce a visible phosphene 50% of the time”.</p><disp-quote content-type="editor-comment"><p>12) Line 270: The model is described as memory intensive. What ranges of memory are required?</p></disp-quote><p>We have added the memory size (24GB) of the used GPU in the caption of figure 5. Note that simulations up to 265x265 pixels with up to 1000 phosphenes are also possible on relatively basic GPUs with a memory size of 2GB.</p><disp-quote content-type="editor-comment"><p>13) Lines 323-329: Phosphene size is calculated based on the current provided, but not total charge. How is pulse width taken into account for phosphene sizes?</p></disp-quote><p>Pulse width is not taken into account, as we did not find quantitative data describing the effects of either pulse width or frequency on the size of phosphenes, from intracortical electrodes trials. We made the choice to use the models described by Bosking and Tehovnik that are based only on stimulation current amplitude. We hope that more quantitative experimental data on phosphene size and the effects of several stimulation parameters become available, so that it will be possible to adapt the model to include these.</p><disp-quote content-type="editor-comment"><p>14) Lines 381-384: Activation thresholds are determined purely on a per-electrode basis. The authors should discuss how reduced charge-per-electrode thresholds with multi-electrode stimulation can be included in the simulator (e.g., less charge per electrode required when using 2 or 4 adjacent electrodes instead of a single electrode).</p></disp-quote><p>See the response on point 1. By default our simulator does not model these electrode interactions, as there is no sufficient literature on the underlying mechanistic models. However, we demonstrate that custom interaction models can be easily added (see Figure 1—figure supplement 2; Figure 1—figure supplement 3; Section ‘Small electrode counts and interactions’ ; Figure 8—figure supplement 4).</p><disp-quote content-type="editor-comment"><p>15) Line 453: The reference to the coefficient of determination should be paired with a clarification that this was only a verification of the parameter fitting process and not a demonstration of how the simulator matches unseen data.</p></disp-quote><p>We thank the reviewer for the suggestion: such a clarification has been added.</p><disp-quote content-type="editor-comment"><p>16) Lines 758-759: The text mentions considerations for phosphene-perception delays after stimulation onset. Stimulation strategies will also be important for addressing phosphene perception persisting after stimulation offset. Is such undesired persistence modeled at all by the simulation?</p></disp-quote><p>The leaky integrator model as described in the Materials and methods section implements both the delayed on- and offset. A sentence has been added in this section to clarify this: “By integrating over time, this model additionally implements the delayed on- and offset as described by several studies (38,39).” Furthermore, the right panel in Figure 2 displays the delayed phosphene offset of the activation and brightness after stimulation offset.</p><disp-quote content-type="editor-comment"><p>17) Line 1023: The linked repository appears to be inaccessible: https://github.com/neuralcodinglab/dynaphos-experiments</p></disp-quote><p>We thank the reviewer for this notification. We will make the repository publicly available upon publication of the manuscript. Meanwhile, all code that was used for the experiments is shared in the submission portal in order to make it accessible for the reviewers.</p></body></sub-article></article>