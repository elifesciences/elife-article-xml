<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">81217</article-id><article-id pub-id-type="doi">10.7554/eLife.81217</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A generalizable brain extraction net (BEN) for multimodal MRI data from rodents, nonhuman primates, and humans</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-283667"><name><surname>Yu</surname><given-names>Ziqi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8201-5481</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-286706"><name><surname>Han</surname><given-names>Xiaoyang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3007-6079</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-298144"><name><surname>Xu</surname><given-names>Wenjing</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-168618"><name><surname>Zhang</surname><given-names>Jie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-95047"><name><surname>Marr</surname><given-names>Carsten</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2154-4552</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-286704"><name><surname>Shen</surname><given-names>Dinggang</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" corresp="yes" id="author-286705"><name><surname>Peng</surname><given-names>Tingying</given-names></name><email>tingying.peng@helmholtz-muenchen.de</email><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-248169"><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8965-1077</contrib-id><email>xiaoyong_zhang@fudan.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-105630"><name><surname>Feng</surname><given-names>Jianfeng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5987-2258</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013q1eq08</institution-id><institution>Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013q1eq08</institution-id><institution>MOE Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence, Fudan University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013q1eq08</institution-id><institution>MOE Frontiers Center for Brain Science, Fudan University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cfam450</institution-id><institution>Institute of AI for Health (AIH), Helmholtz Zentrum München</institution></institution-wrap><addr-line><named-content content-type="city">Neuherberg</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/030bhh786</institution-id><institution>School of Biomedical Engineering, ShanghaiTech University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution>Shanghai United Imaging Intelligence Co., Ltd</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff7"><label>7</label><institution>Shanghai Clinical Research and Trial Center</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cfam450</institution-id><institution>Helmholtz AI, Helmholtz Zentrum München</institution></institution-wrap><addr-line><named-content content-type="city">Neuherberg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Jbabdi</surname><given-names>Saad</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e81217</elocation-id><history><date date-type="received" iso-8601-date="2022-06-20"><day>20</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-12-21"><day>21</day><month>12</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-05-26"><day>26</day><month>05</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.05.25.492956"/></event></pub-history><permissions><copyright-statement>© 2022, Yu et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Yu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-81217-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-81217-figures-v3.pdf"/><abstract><p>Accurate brain tissue extraction on magnetic resonance imaging (MRI) data is crucial for analyzing brain structure and function. While several conventional tools have been optimized to handle human brain data, there have been no generalizable methods to extract brain tissues for multimodal MRI data from rodents, nonhuman primates, and humans. Therefore, developing a flexible and generalizable method for extracting whole brain tissue across species would allow researchers to analyze and compare experiment results more efficiently. Here, we propose a domain-adaptive and semi-supervised deep neural network, named the Brain Extraction Net (BEN), to extract brain tissues across species, MRI modalities, and MR scanners. We have evaluated BEN on 18 independent datasets, including 783 rodent MRI scans, 246 nonhuman primate MRI scans, and 4601 human MRI scans, covering five species, four modalities, and six MR scanners with various magnetic field strengths. Compared to conventional toolboxes, the superiority of BEN is illustrated by its robustness, accuracy, and generalizability. Our proposed method not only provides a generalized solution for extracting brain tissue across species but also significantly improves the accuracy of atlas registration, thereby benefiting the downstream processing tasks. As a novel fully automated deep-learning method, BEN is designed as an open-source software to enable high-throughput processing of neuroimaging data across species in preclinical and clinical applications.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Magnetic resonance imaging (MRI) is an ideal way to obtain high-resolution images of the whole brain of rodents and primates (including humans) non-invasively. A critical step in processing MRI data is brain tissue extraction, which consists on removing the signal from the non-neural tissues around the brain, such as the skull or fat, from the images. If this step is done incorrectly, it can lead to images with signals that do not correspond to the brain, which can compromise downstream analysis, and lead to errors when comparing samples from similar species. Although several traditional toolboxes to perform brain extraction are available, most of them focus on human brains, and no standardized methods are available for other mammals, such as rodents and monkeys.</p><p>To bridge this gap, Yu et al. developed a computational method based on deep learning (a type of machine learning that imitates how humans learn certain types of information) named the Brain Extraction Net (BEN). BEN can extract brain tissues across species, MRI modalities, and scanners to provide a generalizable toolbox for neuroimaging using MRI. Next, Yu et al. demonstrated BEN’s functionality in a large-scale experiment involving brain tissue extraction in eighteen different MRI datasets from different species. In these experiments, BEN was shown to improve the robustness and accuracy of processing brain magnetic resonance imaging data.</p><p>Brain tissue extraction is essential for MRI-based neuroimaging studies, so BEN can benefit both the neuroimaging and the neuroscience communities. Importantly, the tool is an open-source software, allowing other researchers to use it freely. Additionally, it is an extensible tool that allows users to provide their own data and pre-trained networks to further improve BEN’s generalization. Yu et al. have also designed interfaces to support other popular neuroimaging processing pipelines and to directly deal with external datasets, enabling scientists to use it to extract brain tissue in their own experiments.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>brain tissue extraction</kwd><kwd>domain-adaptive deep neural network</kwd><kwd>magnetic resonance imaging</kwd><kwd>rodents</kwd><kwd>primates</kwd><kwd>nonhuman primates</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Rat</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>82171903</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003347</institution-id><institution>Fudan University</institution></institution-wrap></funding-source><award-id>the Office of Global Partnerships (Key Projects Development Fund)</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Shanghai Municipal Science and Technology Major Project</institution></institution-wrap></funding-source><award-id>No.2018SHZDZX01</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name><name><surname>Feng</surname><given-names>Jianfeng</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>81873893</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>92043301</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xiao-Yong</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The Brain Extraction Network (BEN) provides a robust, accurate, and generalizable solution not only for extracting brain tissue from multimodal MRI data in rodents, non-human primates, and humans, but also for improving the accuracy of downstream neuroimaging processing tasks.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Cross-species studies from mice, nonhuman primates (NHPs) to humans are important for studying evolution, development, and the treatment of neurological disorders. As an ideal non-invasive imaging tool, magnetic resonance imaging (MRI) provides high-resolution, multimodal whole brain imaging of mice, NHPs, and humans. Brain tissue extraction on MRI data is a key processing step for the analysis of brain structure, function, and metabolism, as inaccurate brain extraction can not only lead to non-brain signal contamination or misleading registration, compromising the accuracy of downstream analyses, but also lead to cross-species comparison bias.</p><p>Currently, several brain extraction tools have been specifically designed for certain species or MRI modalities. These tools can be categorized into three groups based on their target species: (i) humans, (ii) NHPs, and (iii) rodents. Most well-established tools developed for humans, such as FreeSurfer (<xref ref-type="bibr" rid="bib14">Fischl, 2012</xref>), Analysis of Functional NeuroImages (AFNI) (<xref ref-type="bibr" rid="bib9">Cox, 2012</xref>), and the FMRIB Software Library (FSL) (<xref ref-type="bibr" rid="bib22">Jenkinson et al., 2012</xref>), have been routinely used and integrated into standard preprocessing pipelines, for example, fMRIPrep (<xref ref-type="bibr" rid="bib12">Esteban et al., 2019</xref>) and Data Processing and Analysis for Brain Imaging (DPABI) (<xref ref-type="bibr" rid="bib44">Yan et al., 2016</xref>). In contrast, brain extraction in animals is far from being standardized or fully automated. Although a few atlas- or morphometry-based tools have been developed, for example, for NHPs (<xref ref-type="bibr" rid="bib4">Beare et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Lohmeier et al., 2019</xref>) and for rodents (<xref ref-type="bibr" rid="bib7">Chang et al., 2021</xref>; <xref ref-type="bibr" rid="bib26">Liu et al., 2020</xref>; <xref ref-type="bibr" rid="bib33">Nie and Shen, 2013</xref>; <xref ref-type="bibr" rid="bib34">Oguz et al., 2014</xref>), their performance is still limited.</p><p>Recently, deep learning (DL)-based algorithms have been developed to enable more accurate brain tissue segmentation in animals. For example, U-Net-based algorithms have been proposed to perform skull stripping automatically for NHPs (<xref ref-type="bibr" rid="bib15">Garcia-Saldivar et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Wang et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Zhao et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Zhong et al., 2021</xref>) and for rodents (<xref ref-type="bibr" rid="bib10">De Feo et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Hsu et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Valverde et al., 2020</xref>). Although these DL-based approaches outperform traditional tools such as FreeSurfer or FSL in NHP and rodent brains, they have mostly been evaluated on a single species or a single MRI modality, and suffer severe performance degradation if they are applied to other species or modalities that differ from those represented in their training data. Despite the great diversity in neuroimaging studies in terms of species, MRI modalities, and platforms (e.g. different magnetic field strengths), accurately extracting brain tissue from images collected via multiple MRI modalities or platforms using a single tool remains very difficult, even without additionally attempting to cover various species comprising rodents, NHPs, and humans. In summary, a comprehensive and generalizable method to address these complex challenges is still lacking because current methods (1) are inflexible, which means that the available toolboxes are designed for certain species, modalities, or platforms and are not suitable for cross-species, cross-modality, or cross-platform application and (2) lack automatic quality assessment to evaluate the segmentation results, which means labor-intensive manual curation is required when erroneous segmentation occurs.</p><p>To address these issues, we propose a domain-adaptive and semi-supervised deep neural network, named the Brain Extraction Net (BEN, <xref ref-type="fig" rid="fig1">Figure 1</xref>), to perform the challenging task of accurately extracting brain tissues across different species (mouse, rat, marmoset, macaque, and human), across different MRI platforms with various magnetic field strengths (from 1.5T to 11.7T), and across different MRI modalities (structural MRI and functional MRI). Being flexible and generalizable, BEN is also considerably faster for brain extraction than using a traditional toolbox. Another feature of BEN is that it incorporates an uncertainty-based assessment module that allows it to autonomously control the quality of brain segmentation mask outputs. In contrast, when using a traditional toolbox, experts need to manually check hundreds of extracted brain scans which is very time-consuming. It is worth noting that this laborious manual inspection is not a one-time procedure; it must be repeated after making parameter changes when handling new data. We believe that an automated and generalizable brain extraction tool such as BEN can reduce intra- and interrater variability while also dramatically increasing throughput for the labor-intensive brain extraction task, hence facilitating many neuroimaging studies.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>BEN renovates the brain extraction workflow to adapt to multiple species, modalities and platforms.</title><p>The BEN has the following advantages: (1) Transferability and flexibility: BEN can adapt to different species, modalities and platforms through its adaptive batch normalization module and semi-supervised learning module. (2) Automatic quality assessment: Unlike traditional toolboxes, which rely on manual inspection to assess the brain extraction quality, BEN incorporates a quality assessment module to automatically evaluate its brain extraction performance. (3) Speed: As a DL-based method, BEN can process an MRI volume faster (&lt;1 s) than traditional toolboxes (several minutes or longer).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig1-v3.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Model design</title><p>In this study, we have collected 18 datasets (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>) to evaluate the generalizability of BEN, from different research institutions/cohorts worldwide, including 783 rodent MRI scans, 246 NHP MRI scans, and 4601 human MRI scans, totaling over 900,000 images. To the best of our knowledge, the datasets used in our study represent the largest-scale deep learning application of brain extraction reported to date in terms of the diversity in species, field strengths, and modalities.</p><p>Based on the complexity of our datasets, we designed a domain-adaptive and semi-supervised network (BEN, <xref ref-type="fig" rid="fig2">Figure 2A</xref>) to segment brain MRI volumes into brain and nonbrain tissues. The backbone of BEN is a U-shaped network with nonlocal attention architecture (NL-U-Net, <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Unlike previous approaches (<xref ref-type="bibr" rid="bib15">Garcia-Saldivar et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Zhao et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Zhong et al., 2021</xref>; <xref ref-type="bibr" rid="bib10">De Feo et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Hsu et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Valverde et al., 2020</xref>), the segmentation network of BEN is trained not from scratch but by leveraging domain transfer techniques achieved via two critical strategies: (i) an adaptive batch normalization (<xref ref-type="bibr" rid="bib25">Li et al., 2018</xref>) (AdaBN, <xref ref-type="fig" rid="fig2">Figure 2C</xref>) strategy for adapting the statistical parameters of the batch normalization (BN) layers in a network trained on the source domain to the new data distribution of the target domain and (ii) an iterative pseudo-labeling procedure for semi-supervised learning in which a Monte Carlo quality assessment (MCQA, <xref ref-type="fig" rid="fig2">Figure 2D</xref>) method is proposed to assess the quality of the segmentations generated by the present network and to select the optimal pseudo-labels to be added to the training set for the next iteration. These two strategies are fully automatic and do not require any human intervention. After being trained on a source domain with abundant available annotations, BEN incorporates above domain transfer modules to achieve high segmentation accuracy in each target domain while requiring zero or only a limited number of target-domain annotations. For a detailed explanation of each model component, refer to the subsequent section.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The architecture of our proposed BEN demonstrates its generalizability.</title><p>(<bold>A</bold>) The domain transfer workflow. BEN is initially trained on the Mouse-T2-11.7T dataset (representing the source domain) and then transferred to many target domains that differ from the source domain in either the species, MRI modality, magnetic field strength, or some combination thereof. Efficient domain transfer is achieved via an adaptive batch normalization (AdaBN) strategy and a Monte Carlo quality assessment (MCQA) module. (<bold>B</bold>) The backbone of BEN is the nonlocal U-Net (NL-U-Net) used for brain extraction. Similar to the classic U-Net architecture, NL-U-Net also contains a symmetrical encoding and decoding path, with an additional nonlocal attention module to tell the network where to look, thus maximizing the capabilities of the model. (<bold>C</bold>) Illustration of the AdaBN strategy. The batch normalization (BN) layers in the network are first trained on the source domain. When transferring to a target domain, the statistical parameters in the BN layers are updated in accordance with the new data distribution in the target domain. (<bold>D</bold>) Illustration of the MCQA process. We use Monte Carlo dropout sampling during the inference phase to obtain multiple predictions in a stochastic fashion. The Monte Carlo predictions are then used to generate aleatoric and epistemic uncertainties that represent the confidence of the segmentations predicted by the network, and we screen out the optimal segmentations with minimal uncertainty in each batch and use them as pseudo-labels for semi-supervised learning.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig2-v3.tif"/></fig></sec><sec id="s2-2"><title>Experimental setup</title><p>Since T2WI is the most commonly used modality for rodent brain imaging and we have accumulated extensive brain scans with high-quality annotations in our previous research studies (<xref ref-type="bibr" rid="bib18">Han et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Yu et al., 2021</xref>), we first trained the model on the Mouse-T2WI-11.7T dataset, which served as the source-domain dataset, following the conventional fully supervised training strategy. Here, we used fivefold cross-validation with a training/testing split of 80%/20% (194/49 out of a total of 243 scans) to evaluate the performance of our trained model. The remaining 17 datasets serve as the target-domain datasets (one dataset corresponds to one domain). As shown in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>, BEN obtained a Dice score of 0.99 and a 95% Hausdorff distance (HD95) of 0.14 mm, outperforming other state-of-the-art (SOTA) approaches. The success of BEN in the source domain ensures that it has the potential to excel at a variety of downstream tasks.</p><p>For fair and comprehensive comparisons, we execute BEN and two benchmark settings, namely, training from scratch (training the model only on the target-domain dataset) and fine-tuning (first pretraining on the source-domain dataset and then adjusting the parameters for the target domain), on three domain transfer tasks: across species, across modalities and across MRI scanners with various magnetic fields. Among these three tasks, cross-species domain transfer is the most challenging because of the large anatomical and structural variations between different species.</p></sec><sec id="s2-3"><title>Transfer performance of BEN across species</title><p>We evaluated BEN’s transferability from mouse MRI scans to scans from four other species: rat, marmoset, macaque and human. <xref ref-type="fig" rid="fig3">Figure 3A–D</xref> shows that the segmentation performance (quantified in terms of the Dice score and HD95) improves as the amount of labeled data increases in each of the four target species domains, as expected. In particular, BEN requires only two labeled rat MRI volumes, three labeled human volumes, and five labeled marmoset or macaque volumes to reach a Dice score of 95%, indicating satisfactory segmentation performance. In contrast, the two baseline methods, training from scratch and fine-tuning (<xref ref-type="bibr" rid="bib19">Hsu et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Wang et al., 2021</xref>), require a minimum of 10–15 labeled volumes in the rat, marmoset or macaque domain to reach the same threshold, i.e., almost five times as many labels as BEN. The only exception is human brain extraction, a relatively simple task compared to animal brain extraction, for which all three methods need only three labeled volumes to reach a Dice score of 95%. The more rapid progression to this performance threshold indicates a lower annotation cost, which is achieved by BEN through more efficient transfer learning. Additionally, visual assessment of several representative segmented brain volumes (<xref ref-type="fig" rid="fig3">Figure 3E–H</xref>) illustrates that BEN’s segmentations are more consistent with the reference ground truth than those of other methods. For example, either training from scratch or fine-tuning often leads to false positive errors in the cranial and caudal regions of the brain, which could be caused by regional heterogeneity of the scans.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Performance comparison of BEN with two benchmark settings on the task of cross-species domain transfer.</title><p>(<bold>A - D</bold>) Curve plots representing domain transfer tasks across species, showing the variation in the segmentation performance in terms of the Dice score and 95% Hausdorff distance (HD95) (y axis) as a function of the number of labeled volumes (x axis) for training from scratch (black dotted lines), fine-tuning (black solid lines) and BEN (red lines). From top to bottom: (<bold>A</bold>) mouse to rat, (<bold>B</bold>) mouse to marmoset, (<bold>C</bold>) mouse to macaque, and (<bold>D</bold>) mouse to human (ABCD) with an increasing amount of labeled training data (n=1, 2, 3, …, as indicated on the x axis in each panel). Both the Dice scores and the HD95 values of all three methods reach saturation when the number of labels is sufficient (n&gt;20 labels); however, BEN outperforms the other methods, especially when limited labels are available (n≤5 labels). Error bars represent the mean with a 95% confidence interval (CI) for all curves. The blue dotted line corresponds to y(Dice)=0.95, which can be considered to represent qualified performance for brain extraction. (<bold>E - H</bold>) 3D renderings of representative segmentation results (the number (<bold>n</bold>) of labels used for each method is indicated in each panel). Images with fewer colored regions represent better segmentation results. (gray: true positive; brown: false positive; blue: false negative). Sample size for these datasets: Mouse-T2WI-11.7T (N=243), Rat-T2WI-11.7T (N=132), Marmoset-T2WI-9.4T (N=62), Macaque-T1WI (N=76) and Human-ABCD (N=963).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Performance comparison of BEN with two benchmark settings on the task of cross-modality domain transfer.</title><p>(<bold>A - C</bold>) The curve plots representing domain transfer tasks across modality, showing the variation in the segmentation performance in terms of the Dice score and 95% Hausdorff distance (HD95) (y axis) as a function of the number of labeled volumes (x axis) for training from scratch (black dotted lines), fine-tuning (black solid lines) and BEN (red lines). From top to bottom: (<bold>A</bold>) T2WI to EPI, (<bold>B</bold>) T2WI to SWI and (<bold>C</bold>) T2WI to ASL with an increasing amount of labeled training data (n=1, 2, 3, 5, 8, 10, as indicated on the x axis in each panel). BEN consistently surpasses other methods with less labeled data required. Note that, BEN achieves acceptable performance via zero-shot inference (n=0), where no label is used in the target domain. In this case, fine-tuning directly infer on target datasets and training from scratch failed to perform. Error bars represent the mean with a 95% confidence interval (CI) for all curves. The blue dotted line corresponds to <italic>y(Dice</italic>)=0.95, which can be considered to represent qualified performance for brain extraction. (<bold>D - F</bold>) 3D renderings of representative segmentation results (the number (<bold>n</bold>) of labels used for each method is indicated in each panel). Images with fewer colored regions represent better segmentation results. (gray: true positive; brown: false positive; blue: false negative). Sample size for these datasets: Mouse-T2WI-11.7T (N=243), Mouse-EPI-11.7T (N=54), Mouse-SWI-11.7T (N=50) and Mouse-ASL-11.7T (N=58).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig3-figsupp1-v3.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Performance comparison of BEN with two benchmark settings on the task of cross-platform domain transfer.</title><p>(<bold>A - C</bold>) Curve plots representing domain transfer tasks across platforms with different magnetic field strengths, showing the variation in the segmentation performance in terms of the Dice score and 95% Hausdorff distance (HD95) (y axis) as a function of the number of labeled volumes (x axis) for training from scratch (black dotted lines), fine-tuning (black solid lines) and BEN (red lines). From top to bottom: (<bold>A</bold>) 11.7T to 9.4T, (<bold>B</bold>) 11.7T to 7T and (<bold>C</bold>) 9.4T to 7T with an increasing amount of labeled training data (n=1, 2, 3,..., as indicated on the x axis in each panel). BEN reaches saturation point much earlier in all three tasks than other methods, indicating that fewer labels are required for BEN than for other methods. Error bars represent the mean with a 95% confidence interval (CI) for all curves. The blue dotted line corresponds to <italic>y(Dice</italic>)=0.95, which can be considered to represent qualified performance for brain extraction. (<bold>D - F</bold>) 3D renderings of representative segmentation results (the number (<bold>n</bold>) of labels used for each method is indicated in each panel). Note that due to MRI acquisition, the anterior and posterior of brains in the 7T dataset are not included. Images with fewer colored regions represent better segmentation results. (gray: true positive; brown: false positive; blue: false negative). Sample size for these datasets: Mouse-T2WI-11.7T (N=243), Mouse-T2WI-9.4T (N=14), Mouse-T2WI-7T (N=14).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig3-figsupp2-v3.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>BEN’s transferability is not dependent on specific source dataset.</title><p>In our previous experiments, the mouse-T2WI-11.7T dataset was used as the source domain and transferred the model to other species, including humans. Here, we reselect the human as the source domain and transfer the model to the mouse dataset. (<bold>A</bold>) Human to Mouse with an increasing amount of labeled training data (n=1, 2, 3,..., as indicated on the x axis in each panel). Curve plots representing domain transfer tasks across species, showing the variation in the segmentation performance in terms of HD95 (y axis) as a function of the number of labeled volumes (x axis) for training from scratch (black dotted lines), fine-tuning (black solid lines), and BEN (red lines). (<bold>B</bold>) 3D renderings of representative segmentation results (the number (<bold>n</bold>) of labels used for each method is indicated in each panel). Images with fewer colored regions represent better segmentation results. (gray: true positive; brown: false positive; blue: false negative). Sample size for these datasets: Mouse-T2WI-11.7T (N=243) and Human-ABCD (N=3250).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig3-figsupp3-v3.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>UMAP visualizes BEN’s transfer learning.</title><p>(<bold>A - C</bold>) Scatter plot of neural network feature map clusters. An unsupervised clustering algorithm (UMAP) was used to visualize the semantic features. Different colors in the scatter plot indicate different clusters (green: brain, red: non-brain). Visualization of feature maps via UMAP during the following three intermediate processes: (<bold>A</bold>) semantic features of brain and non-brain are separate in the source domain; (<bold>B</bold>) these features are intermingled in the target domain without transfer learning; (<bold>C</bold>) features are separated again in the target domain after BEN’s domain transferring strategy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig3-figsupp4-v3.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Transfer performance across modalities</title><p>We further evaluated BEN’s transferability across different MRI modalities, particularly between structural and functional modalities. Again, we used the structural Mouse-T2-11.7T dataset as the source domain, and we transferred the trained model to three other commonly used MRI modalities: EPI, SWI and ASL (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Encouragingly, we found that even without labels, BEN could generalize well, with Dice scores of 0.93–0.95 on all three target domains, despite the different imaging sequences and parameters between these modalities (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A–C</xref>). This is an impressive result, as zero-shot learning is generally a challenging problem (<xref ref-type="bibr" rid="bib28">Ma et al., 2021</xref>; <xref ref-type="bibr" rid="bib41">Wang et al., 2019b</xref>). By comparison, a fine-tuning strategy with zero labeled volumes, that is, a direct evaluation of the source-domain-trained model on the target domains, can achieve Dice scores of only 0.40–0.77, significantly lower than those of BEN (p&lt;0.001 using the Mann–Whitney test). Note that training from scratch is impossible to implement without available labels in the target domain. Similar to the case of cross-species domain transfer, with fewer than five labeled volumes, BEN can achieve Dice &gt;0.95 in the target domains in these modality transfer tasks (two volumes for T2WI to EPI, one volume for T2WI to SWI and five volumes for T2WI to ASL).</p></sec><sec id="s2-5"><title>Transfer performance across MR scanners with various magnetic field strengths</title><p>We also examined BEN’s transferability to MRI datasets acquired from multiple MRI scanners with various magnetic field strengths (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). The obtained image contrast varies depending on the magnetic field strength. Even with the same magnetic field strength, MRI scanners from different manufacturers can produce different image contrast, which further increases the complexity of the domain shift. Without domain transfer techniques, the direct application of a model trained on a certain source domain to a different target domain strongly suffers due to the differences in imaging contrast/quality produced by the different scanners; specifically, Dice scores of only 0.42, 0.81, and 0.61 are obtained for the 11.7 T to 9.4 T, 11.7 T to 7 T, and 9.4 T to 7 T transfer tasks, respectively. In comparison, without additional labeled data in the target domains, BEN outperforms these benchmarks by notable margins, with Dice scores of 0.97, 0.95, and 0.91, respectively. Apparently, BEN shows the capacity to generalize well to other domains without additional labeled data, while the baseline approaches overfit the source domain. With sufficient labeled data in the target domain, all three methods perform well, as anticipated.</p></sec><sec id="s2-6"><title>Comparison with conventional neuroimage processing toolboxes</title><p>We compared BEN with several conventional SOTA toolboxes that are widely used in the neuroimaging research field, namely, AFNI, FSL, FreeSurfer, and Sherm (<xref ref-type="bibr" rid="bib26">Liu et al., 2020</xref>), on all 18 datasets. Similar to the previous experiments, we used the Mouse-T2-11.7T dataset as the only source-domain dataset for BEN, with the remaining 17 datasets serving as the target-domain datasets. For each target domain, we used five labeled MRI volumes for the domain transfer task.</p><p>As shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, despite consistent performance on human MRI scans, the AFNI, FSL, and FreeSurfer toolboxes show large intra- and inter-dataset variations on the four animal species, suggesting that these methods are not generalizable to animals. Sherm, a toolbox that was specifically developed for rodent MRI, consequently achieves better results for mouse and rat scans but cannot be used for primates. In contrast, BEN is substantially superior to these conventional SOTA methods, and its performance remains consistent on both animal and human datasets, with median Dice scores of 0.97–0.99 for structural images (T2WI) (<xref ref-type="fig" rid="fig4">Figure 4A–E</xref>) and functional images (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) acquired at MRI scanners with various magnetic field strengths (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Better brain segmentation leads to a more accurate estimation of the brain volume: as shown in <xref ref-type="fig" rid="fig4">Figure 4F–J</xref>, BEN achieves almost perfect linear regression coefficients (LRCs) of approximately 1.00 (red lines) when compared to the corresponding expert-labeled brain volumes. In contrast, AFNI, FSL and Sherm exhibit poor consistency in animals (LRC = 0.06–0.10 for mouse scans, 0.01–0.52 for rat scans, 0.41–0.48 for marmoset scans and 0.00–0.59 for macaque scans). Additionally, in Bland–Altman plots (<xref ref-type="fig" rid="fig4">Figure 4K–N</xref>), BEN shows higher agreement between the predicted and manually annotated volumes than the other toolboxes. The error maps further confirm that BEN has better performance compared to other methods (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). The only exception is for human MRI data, with AFNI and FSL achieving excellent agreement with the ground truth on three human datasets, namely, the ABCD, UK Biobank and ZIB datasets (with LRC around 1.00). This is not surprising, as these conventional tools are well designed for human brain extraction. Encouragingly, BEN also exhibits comparable or even better performance for the human brain (<xref ref-type="fig" rid="fig4">Figure 4E, J, O</xref>), with an accelerated speed of 0.6 s per scan on average. This speed is two orders of magnitude greater than that of conventional toolboxes (several minutes on average) and three orders of magnitude greater than that of manual annotation (an average of 20–30 min; <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>), suggesting the potential of applying BEN in high-throughput studies. Taking into consideration that in practice, UK Biobank and Human Connectome Project (HCP) use more time-consuming registration-based approaches in their preprocess pipelines, which further emphasize the requirement for a fast and robust brain extraction toolbox.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>BEN outperforms traditional SOTA methods and advantageously adapts to datasets from various domains across multiple species, modalities, and field strengths.</title><p>(<bold>A - E</bold>) Violin plots and inner box plots showing the Dice scores of each method for (<bold>A</bold>) mouse (n=345), (<bold>B</bold>) rat (n=330), (<bold>C</bold>) marmoset (n=112), (<bold>D</bold>) macaque (n=134), and (<bold>E</bold>) human (n=4601) MRI scans acquired with different magnetic field strengths. The field strength is illustrated with different markers above each panel, and the results for each method are shown in similar hues. The median values of the data are represented by the white hollow dots in the violin plots, the first and third quartiles are represented by the black boxes, and the interquartile range beyond 1.5 times the first and the third quartiles is represented by the black lines. ‘N.A.’ indicates a failure of the method on the corresponding dataset. (<bold>F - J</bold>) Comparisons of the volumetric segmentations obtained with each method relative to the ground truth for five species. For better visualization, we select magnetic field strengths of 11.7T for mouse scans (<bold>F</bold>), 7T for rat scans (<bold>G</bold>), 9.4T for marmoset scans (<bold>H</bold>), and 3T for both macaque (<bold>I</bold>) and human (<bold>J</bold>) scans. Plots for other field strengths can be found in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. The linear regression coefficients (LRCs) and 95% CIs are displayed above each graph. Each dot in a graph represents one sample in the dataset. The error bands in the plots represent the 95% CIs. (<bold>K - O</bold>) Bland–Altman analysis showing high consistency between the BEN results and expert annotations. The biases between two observers and the 95% CIs for the differences are shown in the tables above each plot. Δvolume = predicted volume <italic>minus</italic> annotated volume. Each method is shown using the same hue in all plots (gray: AFNI; blue: FSL; green: FreeSurfer; purple: Sherm; red: BEN). Different field strengths are shown with different markers (●: 11.7T; ×: 9.4T; ◆: 7T; ★: 4.7T; +: 3T; ▲: 1.5T). The Dice scores compute the overlap between the segmentation results and manual annotations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>BEN outperforms traditional SOTA methods in functional MRI scans acquired from multiple species on different magnetic field strengths.</title><p>(<bold>A - D</bold>) Violin plots and inner box plots showing the Dice scores of each method for (<bold>A</bold>) mouse (n=54), (<bold>B</bold>) rat (n=55), (<bold>C</bold>) marmoset (n=50), and (<bold>d</bold>) macaque (n=58) MRI images acquired. The field strength is illustrated with different markers above each panel, and the results for each method are shown in similar hues. The median values of the data are represented by the white hollow dots in the violin plots, the first and third quartiles are represented by the black boxes, and the interquartile range beyond 1.5 times the first and the third quartiles is represented by the black lines. ‘N.A.’ indicates failure of the method on the corresponding dataset. (gray: AFNI; blue: FSL; green: Freesurfer; purple: Sherm; red: BEN).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig4-figsupp1-v3.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Linear regression coefficients and Bland–Altman analysis demonstration of the rest datasets.</title><p>(<bold>A–D, E–H</bold>) The segmentation accuracy is assessed using the linear regression coefficients (LRCs) (<italic>n</italic> of volumes used are shown above the figure panels). Compared with other methods, BEN achieves better performance. The magnetic field strengths are also shown in different markers according to the legend. Each dot in a graph represents one sample in the dataset. The error bands represent 95% CI in plots. (<bold>I - L, M - P</bold>) Bland–Altman analysis showing high consistency between the BEN results and expert annotations. The biases between two observers and the 95% CIs for the differences are shown in the tables above each plot. Each method is shown using the same hue in all plots (gray: AFNI; blue: FSL; green: FreeSurfer; purple: Sherm; red: BEN). Different field strengths are shown with different markers (●: 11.7T; ×: 9.4T; ◆: 7T; ★: 4.7T; +: 3T; ▲: 1.5T).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig4-figsupp2-v3.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Error maps of BEN and SOTA methods.</title><p>Heatmap projections of average false negative (FN) and false positive (FP) for each species (mouse, rat, marmoset, and macaque). From the first row to the third row: axial, coronal, and sagittal view. Compared with other methods, BEN shows much less FN and FP errors. The upper extreme represents a high systematic number of FNs and FPs. (mouse: n=243 in Mouse-T2WI-11.7T dataset; rat: n=132 in Rat-T2WI-11.7T dataset; marmoset: n=62 in Marmoset-T2WI-9.4T dataset; macaque: n=76 in Macaque-T1WI dataset).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig4-figsupp3-v3.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Execution time comparison of BEN with other methods.</title><p>Compared with other conventional methods, BEN has unequaled faster processing speed. The plot is in log scale and represents average processing time to segment one 3D volume.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig4-figsupp4-v3.tif"/></fig></fig-group><p>In addition, we also compared BEN with published DL-based SOTA brain extraction methods (<xref ref-type="bibr" rid="bib8">Chou et al., 2011</xref>; <xref ref-type="bibr" rid="bib19">Hsu et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Wang et al., 2021</xref>) as well as non DL-based methods (RATS (<xref ref-type="bibr" rid="bib34">Oguz et al., 2014</xref>), Sherm (<xref ref-type="bibr" rid="bib26">Liu et al., 2020</xref>), AFNI, FSL, and FreeSurfer), on two public datasets, CARMI (<ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002870/versions/1.0.0">https://openneuro.org/datasets/ds002870/versions/1.0.0</ext-link>) and PRIME-DE (<ext-link ext-link-type="uri" xlink:href="https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html">https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html</ext-link>). As shown in <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>, the segmentation performance of BEN, as evaluated using Dice score and the Hausdorff distance, surpasses other methods.</p></sec><sec id="s2-7"><title>BEN improves the atlas-to-target registration and brain volumetric quantification</title><p>Accurate brain extraction is a key component of many neuroimaging pipelines, facilitating subsequent procedures in longitudinal or cohort analyses. We therefore designed experiments to demonstrate BEN’s application for downstream processing and analysis in neuroimaging studies of rodents and humans (<xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), including atlas registration (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and volumetric quantification (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). The registration quality was evaluated in terms of the Dice scores between the brain structural segmentations of the warped MRI brain images and the atlas. <xref ref-type="fig" rid="fig5">Figure 5B, F, J</xref> plot the workflow of atlas registration, in which mice are registered to Dorr (<xref ref-type="bibr" rid="bib11">Dorr et al., 2008</xref>), rats to SIGMA (<xref ref-type="bibr" rid="bib2">Barrière et al., 2019</xref>), and humans to MNI (<xref ref-type="bibr" rid="bib13">Evans et al., 2005</xref>). Quantitative evaluations of two critical brain structures, the thalamus and hippocampus (<xref ref-type="fig" rid="fig5">Figure 5C, D, G, H, K, L</xref>), demonstrate that BEN can tremendously enhance the registration quality; registration with AFNI achieves a Dice score of only 0.2–0.6, which can be improved to approximately 0.8 when using BEN for brain extraction (p&lt;0.001).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>BEN improves the accuracy of atlas registration by producing high-quality brain extraction results.</title><p>(<bold>A</bold>) The Dorr space, (<bold>E</bold>) the SIGMA space and (<bold>I</bold>) the MNI space, corresponding to the mouse, rat and human atlases, respectively. (<bold>B, F, J</bold>) Integration of BEN into the registration workflow: (<bold>i</bold>) Three representative samples from a mouse (n=157), a rat (n=88), and human (n=144) in the native space. (ii) The BEN-segmented brain MRI volumes in the native space as registered into the Dorr/SIGMA/MNI space using the Advanced Normalization Tools (ANTs) toolbox, for comparison with the registration of AFNI-segmented/original MRI volumes with respect to the corresponding atlas. (iii) The warped volumes in the Dorr/SIGMA/MNI spaces. (iv) Error maps showing the fixed atlas in green and the moving warped volumes in magenta. The common areas where the two volumes are similar in intensity are shown in gray. (<bold>v</bold>) The brain structures in the warped volumes shown in the atlas spaces. In our experiment, BEN significantly improves the alignment between the propagated annotations and the atlas, as confirmed by the improved Dice scores in the (<bold>C, G, K</bold>) thalamic and (<bold>D, H, L</bold>) hippocampal regions (box plots: purple for BEN, green for w/o BEN; volumes: n=157 for the mouse, n=88 for the rat, n=144 for the human; statistics: paired t-test, n.s.: no significance, *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Atlas registration with BEN benefits brain volumetric quantification in longitudinal studies.</title><p>In our two examples, (<bold>A</bold>) adult mice (n=40) were imaged at weeks 8, 12, 20, and 32. (<bold>B</bold>) With BEN (purple boxplots), the volume statistics of the thalamus and hippocampus remain almost constant, which is plausible, as these two brain regions do not further enlarge in adulthood. In comparison, (<bold>D</bold>) the volume changes of the thalamus and hippocampus in (<bold>C</bold>) juvenile rats (n=23, imaged at weeks 3, 6, 9, and 12) suggest continuous growth with time, particularly between weeks 3 and 9. In contrast, with AFNI, due to poor atlas registration and consequently inaccurate volume statistics (green box plots), these crucial brain growth trends during development may be missed.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig5-figsupp1-v3.tif"/></fig></fig-group><p>As the next step, we examined the contribution of BEN to brain volumetric quantification in rodent longitudinal studies. Two longitudinal MRI datasets, representing adult mice (8, 12, 20, and 32 weeks old, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>) and adolescent rats (3, 6, 9, and 12 weeks old, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>), were collected. As shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>, after registration with BEN, atlas-based morphometry analysis showed that the volumes of two representative brain regions, the thalamus and hippocampus, remained stable for the adult mice from 8 weeks old to 32 weeks old (purple lines). In contrast, calculating the volumes of these brain regions without BEN led to unreliable quantifications (green lines) due to poor atlas registration resulting in the propagation of error to the volumetric quantifications. Similarly, BEN improved brain volumetric quantification in adolescent rats and yielded a plausible pattern of growth from 3 weeks old to 12 weeks old (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D</xref>, purple boxes), suggesting rapid development of these two critical brain regions during this period, consistent with findings in the literature (<xref ref-type="bibr" rid="bib5">Calabrese et al., 2013</xref>; <xref ref-type="bibr" rid="bib17">Hamezah et al., 2017</xref>). In comparison, the atlas-based segmentations were poor when without BEN, and no obvious trend in the volumetric statistics was apparent (green boxes). These results indicate that BEN not only is able to tremendously improve the registration accuracy and contribute to routine brain MRI processing but also is critical for longitudinal MRI studies, as it improves volumetric quantification.</p></sec><sec id="s2-8"><title>BEN represents interrater variations in the form of uncertainty maps</title><p>Due to different levels of annotation experience, disagreements or disputes between human annotators remain a problem. To quantitatively assess the bias of interrater disagreements, we carried out a case study of the annotation of brain functional MRI images from mice, rats, marmosets, and macaques. A total of 9 raters participated in the experiments: two senior raters (defined as having &gt;5 years of experience) and seven junior raters (defined as having 1–2 years of experience). The consensus region of the two senior raters’ annotations was taken as the ground truth (reference) for comparison with the annotations of the seven junior raters. BEN generated an uncertainty map through Monte Carlo sampling during the inference process, without additional adjustment of the network structure.</p><p>As shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>, the segmentations produced by BEN match the ground truth very well across different species. The attention maps and uncertainty maps display complementary information about the brain morphology; specifically, the attention maps focus on the image signal within the brain tissue in MRI images (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), while the uncertainty maps provide information on the boundaries between the brain and nonbrain tissues (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Interestingly, for each rater’s annotations, disputes always occur at the brain boundaries (<xref ref-type="fig" rid="fig6">Figure 6E</xref>), similar to BEN’s uncertainty map. The concurrence between the projections of BEN’s uncertainty maps and the rater disagreement maps (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>) further demonstrates that BEN has the potential to interrogate its own predictions and that its uncertainty measure can be deemed an alternative proxy for rater divergence. From an intra-observer perspective, the nature of the slice-wise approach potentially impacts the consistency of attention maps for human scans with isotropic spatial resolution. One ideal solution is to extend BEN to the 2.5D method or use 3D convolution kernels for these imaging cohorts.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>BEN provides a measure of uncertainty that potentially reflects rater disagreement.</title><p>(<bold>A</bold>) Representative EPI images from four species (in each column from left to right, mouse, rat, marmoset, and macaque). (<bold>B</bold>) BEN segmentations (red semi-transparent areas) overlaid with expert consensus annotations (red opaque lines), where the latter are considered the ground truth. (<bold>C</bold>) Attention maps showing the key semantic features in images as captured by BEN. (<bold>D</bold>) Uncertainty maps showing the regions where BEN has less confidence. The uncertainty values are normalized (0–1) for better visualization. (<bold>E</bold>) Annotations by junior raters (n=7) shown as opaque lines of different colors. (<bold>F</bold>) Bar plots showing the Dice score comparisons between the ground truth and the BEN segmentation results (red) as well as the ground truth and the junior raters’ annotations (gray) for all species (gray: raters, n=7, red: Monte Carlo samples from BEN, n=7; statistics: Mann–Whitney test, **p&lt;0.01). Each dot represents one rater or one sample. Values are represented as the mean and 95% CI. (<bold>G</bold>) Correlation of the linear regression plot between the Dice score and the normalized uncertainty. The error band represents the 95% CI. Each dot represents one volume (species: mouse; modality: T2WI; field strength: 11.7T; n=133; <italic>r</italic>=−0.75).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig6-v3.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>BEN’s uncertainty is consistent with interrater' disagreement.</title><p>Heat map projections of raters’ disagreement (orange hues) and BEN uncertainty (blue hues) across four species using the ground truth as the reference. From left to right: Sagittal, coronal and axial view. These disagree maps and uncertainty maps share similar feature distribution patterns.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig6-figsupp1-v3.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Interrater disagreement.</title><p>Heatmaps representing the segmentation Dice scores of each junior rater as calculated using the labels from one of the others as the ground truth on the rat images and the macaque images.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig6-figsupp2-v3.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>BEN provides a measure of uncertainty that potentially reflects the disagreement of conventional toolboxes in human data.</title><p>(<bold>A</bold>) Representative structural MR images of the human in the ABCD dataset (n=3250). (<bold>B</bold>) BEN’s segmentation (red semi-transparent areas) overlaid with annotation of experts’ consensus, which is considered as ground truth (red opaque lines). (<bold>C</bold>) Attention map shows the key semantic features in images where BEN captures. (<bold>D</bold>) Uncertainty map shows the regions where BEN has less confidence. The uncertainty values are normalized (0~1) for better visualization. (<bold>E</bold>) Segmentations obtained by toolboxes (n=3) are shown in different color opaque lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81217-fig6-figsupp3-v3.tif"/></fig></fig-group><p>Furthermore, to evaluate the annotation variability between different raters in a more quantitative fashion, we calculated the Dice scores and found that none of them was higher than 0.96 across the four species (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). Specifically, the Dice scores between each junior rater and the ground truth provided by the senior raters are between 0.92 and 0.94 for the four species, and these are surpassed by BEN, with Dice scores of 0.96–0.97 (p&lt;0.01) (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). In addition to animal studies, we performed a parallel experiment to verify BEN’s predictions for human brain MRI. As shown in <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>, similar to the animal segmentation results, BEN produces attention and uncertainty maps to support interpretation of BEN’s robustness compared with conventional toolboxes.</p><p>We further examined the correlation between BEN’s uncertainty and segmentation quality. As shown in <xref ref-type="fig" rid="fig6">Figure 6G</xref>, the estimates of uncertainty at the volume level are negatively correlated with the Dice score (<italic>r</italic>=–0.75), suggesting that BEN’s uncertainty measure can be considered as an alternative metric for assessing the quality of segmentations at the time of inference, when the Dice score cannot be computed because the ground-truth segmentation is not available.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we introduce BEN, a generalized brain extraction tool for MRI data from rodents, NHPs, and humans. Our results demonstrate BEN’s transferability across different species, modalities, and platforms thanks to several key modules: an AdaBN module, a semi-supervised learning module, and an uncertainty estimation module (the contribution of each module refers to ablation study in <xref ref-type="table" rid="app1table4 app1table5">Appendix 1—tables 4 and 5</xref>). To facilitate BEN’s usage, we also provide interfaces to make BEN compatible with several popular toolboxes, including AFNI, FSL, and FreeSurfer, and therefore easy to integrate into conventional neuroimage processing pipelines.</p><sec id="s3-1"><title>Transferability</title><p>By virtue of its modules for domain adaptation and transfer learning, BEN is capable of handling the heterogeneity in MRI contrast associated with different subjects, MRI scanners, and imaging sequences. It is worth noting that the source domain for BEN can be freely chosen. In this study, we switched the source domain for BEN’s deployment from a mouse dataset to a human dataset (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Similar to the successful transfer of BEN from mouse data to human data (Dice scores of 0.96 with only 3 labeled scans, <xref ref-type="fig" rid="fig3">Figure 3</xref>), BEN can also achieve an impressive Dice score higher than 0.95 when only 1 labeled volume is used for transfer from human data to mouse data. This dual transferability between mouse and human data demonstrates that BEN can take advantage of the semantic features extracted from one domain and transfer the corresponding learned information to the other domain despite the enormous gap in brain morphometry between the two species. This also indicates that the source domain could be represented by any brain MRI dataset from any other species. On the project website, we have released trained model weights for five species (mouse, rat, marmoset, macaque, and human) to allow users to start domain transfer from the closest species to those of interest to them with minimal or even zero additional annotations.</p><p>To interpret how transfer learning affects network performance, we used uniform manifold approximation and projection (UMAP) (<xref ref-type="bibr" rid="bib29">McInnes et al., 2018</xref>) to visualize the feature distributions before and after domain transfer (details are described in the Materials and methods section). <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref> shows a representative cross-modality transfer task from Mouse-T2WI-11.7T to Mouse-EPI-11.7T, which shows substantial perceptual differences. Although the semantic features are separated by the network in the source domain (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4A</xref>), they are intermingled in the target domain without transfer learning (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4B</xref>). In contrast, only after BEN’s transfer modules are those features again separated in the target domain (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4C</xref>), demonstrating the importance of using the domain transfer strategy.</p></sec><sec id="s3-2"><title>Automatic quality assessment and ‘virtual’ annotation</title><p>Just as divergences always exist among human annotators, the quality of neural-network-generated segmentations may also vary greatly among diverse sample data due to the inherent data distribution or noise (<xref ref-type="bibr" rid="bib3">Baumgartner et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Hüllermeier and Waegeman, 2021</xref>; <xref ref-type="bibr" rid="bib47">Zeng et al., 2021</xref>). However, the uncertainty of a network’s decisions regarding its generated segmentations can be assessed via Monte Carlo sampling in many medical image analyses (<xref ref-type="bibr" rid="bib23">Jungo et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Mehrtash et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Wang et al., 2019a</xref>). This is also the basis of BEN’s uncertainty assessment module. Our results demonstrate that BEN’s uncertainty map represents the expected interrater variations and that the uncertainty linearly increases as BEN’s performance decreases. Based on the previously presented rater experiment (<xref ref-type="fig" rid="fig6">Figure 6</xref>), BEN generates segmentations that are comparable to the consensus segmentations of two senior experts with high consistency, indicating that BEN can apply consistent labeling rules alongside automatic quality assessments when constructing large datasets to prevent subjective bias in large-scale research.</p><p>In addition, BEN is able to produce uncertainty-guided pseudo-labels with high confidence by using a semi-supervised learning strategy. Note that on the ABCD human dataset, BEN achieves competitive or even superior performance with fewer than 5 labels, nearly perfectly delineating thousands of scans (n=3250; <xref ref-type="fig" rid="fig4">Figure 4E and J &amp; O</xref>); this further demonstrates that BEN can propagate annotations from limited labeled data to unlabeled data and produce a larger set of ‘virtual’ labels for training, suggesting that it can be easily applied to large-scale datasets with limited annotations for rapid automatic deployment. Nevertheless, one limitation of our model is that BEN’s quality assessment may result in deviations when the MRI images are of poor quality, such as motion artifacts and low signal-to-noise ratios (SNRs). In future work, this procedure may be updated by incorporating conditional random fields (<xref ref-type="bibr" rid="bib43">Xie et al., 2021</xref>) or heuristic algorithms.</p></sec><sec id="s3-3"><title>Compatibility with other toolboxes and deployment</title><p>BEN was developed in Python and is equipped with well-designed interfaces to support most types of software encountered in existing traditional pipelines (<xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>), including FSL, AFNI, FreeSurfer, Advanced Normalization Tools (ANTs), and statistical parametric mapping (SPM). We also provide tutorials and demo examples to flatten the learning curve and lower the entry barrier for researchers deploying our new algorithm. Moreover, BEN was built under the open-source paradigm from the beginning and has already been tested on various types of MRI data from many imaging centers. We have released packaged interfaces for BEN and source codes, including the implementations for domain transfer, semi-supervised training and quality control. Additionally, we have distributed the model weights trained on five species (mouse, rat, marmoset, macaque and human). Applications to other target domains can also benefit from our pretrained models, requiring the addition of zero or only a small number of labeled images to update the model.</p><p>For the introduction of a new dataset, we suggest that the user do as follows. (1) First, use BEN to load the pretrained weights for the closest corresponding species. (2) Run unsupervised domain adaptation on the user’s dataset. The customized weight will be updated and saved automatically, and the user will then use it to execute BEN on their data. (3) Consider labeling several scans to fine-tune the model if the self-configuring weights do not yield satisfactory performance, or the target species is beyond the scope of our datasets. The domain transfer procedures will be performed automatically, without human intervention. (4) If BEN is deployed to an external image domain/cohort, retraining the BEN might be required. In addition, there are two options to improve BEN’s generalizability. (1) The scans selected for retraining or fine-tuning BEN should be representative of target cohorts, taking into consideration field bias and brain injury (if any). (2) Pre- and post-processing steps are simple to execute yet effective, and these could be easily integrated into the BEN pipeline. For example, we have already provided largest connected region selection, orientation detection, conditional random field optimization, etc. as plug-and-play functions in our pipeline.</p><p>When dealing with external datasets, there could be a couple of reasons that cause suboptimal performance using a pretrained BEN. On the one hand, domain generalization is a challenging task for deep learning. Although BEN could adapt to new out-of-domain images without labels (zero-shot learning) when the domain shift is relatively small (e.g. successful transfer between modalities and scanners with different MR strengths), the domain gap existing in different cohorts might compromise the performance. In this case, additional labeled data and retraining are indeed necessary for BEN to perform few-shot learning. On the other hand, users can provide their own data and pretrained network as a new source domain to take advantage of BEN’s flexibility, which does not bind to a fixed source domain, therefore facilitating domain generalization by reducing the domain gap between the new source and target domains.</p><p>BEN is designed as an extensible toolbox and follows the open-access paradigm, allowing users to save their updated models and share their weights for use by the neuroimaging community. The accumulation of additional imaging data will further improve the performance and generalization of BEN and support the exploration of complex neuroimaging research.</p></sec><sec id="s3-4"><title>Conclusion</title><p>In summary, we have demonstrated the superiority of BEN in terms of accuracy, robustness and generalizability in a large-scale study involving eighteen different MRI datasets. BEN improves the robustness of atlas-to-target registration and brain volumetric quantification in neuroimaging. BEN is an open-source and modularly designed toolbox that provides uncertainty maps to quantify the confidence of the network’s decisions and to model interrater variability. We believe that BEN has great potential to enhance neuroimaging studies at high throughput for both preclinical and clinical applications.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Datasets</title><p>We conducted experiments to evaluate BEN on eighteen datasets covering five distinct species, namely, mouse, rat, marmoset, macaque, and human; four MRI modalities, namely, T1- and T2-weighted imaging (T1WI/T2WI), echo-planar imaging (EPI), susceptibility-weighted imaging (SWI) and arterial spin labeling (ASL); and six MRI platforms spanning a wide range of magnetic field strengths, namely, 1.5T, 3T, 4.7T, 7T, 9.4T, and 11.7T. Each dataset contains 14–3250 scans acquired from different research institutions/cohorts worldwide, including China, the United States, the United Kingdom, France, Canada, and the Netherlands. Partial rodent MRI data collection were approved by the Animal Care and Use Committee of Fudan University, China. The rest rodent data (Rat-T2WI-9.4T and Rat-EPI-9.4T datasets) are publicly available (CARMI: <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002870/versions/1.0.0">https://openneuro.org/datasets/ds002870/versions/1.0.0</ext-link>). Marmoset MRI data collection were approved by the Animal Care and Use Committee of the Institute of Neuroscience, Chinese Academy of Sciences, China. Macaque MRI data are publicly available from the nonhuman PRIMatE Data Exchange (PRIME-DE) (<ext-link ext-link-type="uri" xlink:href="https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html">https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html</ext-link>; <xref ref-type="bibr" rid="bib31">Milham et al., 2018</xref>). The Zhangjiang International Brain Biobank (ZIB) protocols were approved by the Ethics Committee of Fudan University (AF/SC-03/20200722) and written informed consents were obtained from all volunteers. UK Biobank (UKB) and Adolescent Brain Cognitive Development (ABCD) are publicly available. Detailed information on each dataset is shown in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> and summarized below:</p><list list-type="order"><list-item><p>Mouse: includes seven datasets consisting of 453 scans (14,358 slices), namely, Mouse-T2WI-11.7T (243 scans), Mouse-T2WI-9.4T (14 scans), Mouse-T2WI-7T (14 scans), Mouse-EPI-11.7T (54 scans), Mouse-EPI-9.4T (20 scans), Mouse-SWI-11.7T (50 scans), and Mouse-ASL-11.7T (58 scans).</p></list-item><list-item><p>Rat: includes four datasets consisting of 330 scans (11,264 slices), namely, Rat-T2WI-11.7T (132 scans), Rat-T2WI-9.4T (55 scans), Rat-T2WI-7T (88 scans) and Rat-EPI-9.4T (55 scans).</p></list-item><list-item><p>Marmoset: includes two datasets consisting of 112 scans (4,060 slices), namely, Marmoset-T2WI-9.4T (65 scans) and Marmoset-EPI-9.4T (50 scans).</p></list-item><list-item><p>Macaque: includes two datasets consisting of 134 scans (22,620 slices), namely, Macaque-T1WI (76 scans) and Macaque-EPI (58 scans).</p></list-item><list-item><p>Human: includes three datasets consisting of 4,601 scans (87,3453 slices): Human-ABCD (3,250 scans), Human-UKB (963 scans) and Human-ZIB (388 scans).</p></list-item></list></sec><sec id="s4-2"><title>Backbone network architecture</title><p>The backbone of BEN is based on a U-shaped architecture (<xref ref-type="bibr" rid="bib36">Ronneberger et al., 2015</xref>), as is commonly used for biomedical volumetric segmentation (<xref ref-type="bibr" rid="bib21">Isensee et al., 2021</xref>). We use 2D convolution kernels rather than 3D kernels here to accelerate the execution speed and reduce the demand for GPU memory. In addition, most rodent and NHP scans have an anisotropic spatial resolution, so 3D convolution is not appropriate. As shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, the backbone network consists of five symmetrical pairs of encoding and decoding blocks, which perform contraction and expansion operations, respectively, during the data feedforward process. Each encoding/decoding block is composed of two convolutional layers with 3*3 convolution kernels, followed by a BN layer, an exponential linear unit (ELU) function and a 2*2 max-pooling operation with stride 2. The first encoding block results in 16 feature channels, and every subsequent encoding block doubles the number of channels. Each decoding block is symmetric with respect to the encoding block at the corresponding level, except that the max pooling layer is replaced with transposed convolutional layers to upsample the feature map. In the last decoding block, we add a 1*1 convolutional layer with a 3*3 kernel size and a sigmoid activation function to map the 16 feature channels to a binary probability map that softly assigns each pixel to the brain and nonbrain tissue classes.</p><p>Additionally, we add a self-attention mechanism in the bottom layer of the U-Net to yield higher accuracy and smoother boundaries. A self-attention block (<xref ref-type="bibr" rid="bib39">Wang et al., 2017</xref>) is used to improve the model performance for various domain tasks. Concretely, we use a nonlocal attention layer in the bottleneck of the network to model long-range and global contextual dependencies over feature representations. This attention mechanism calculates a weighted saliency map at each position in the feature map that accounts for the global contextual response, thus better assisting the network in distinguishing brain and nonbrain tissue in specific domains with different contrast characteristics and distributions.</p><p>The model takes preprocessed MRI images as input (for details, refer to the subsection titled ‘Model input preprocessing’) and merges successive mask slices to reconstruct the original 3D volumes. Here, we mainly focus on the structure and components of the model as well as the training and evaluation procedures in the source domain. The pretrained model weights will be retained for the transfer learning and domain adaptation stages, which are discussed in the next section.</p></sec><sec id="s4-3"><title>Adaptive batch normalization (AdaBN)</title><p>To mitigate the domain shift problem encountered when DL models are applied to different domains, we adopt the AdaBN strategy (<xref ref-type="bibr" rid="bib25">Li et al., 2018</xref>). Standard BN layers use fixed statistical descriptors for multiple domains, thus damaging the generalization ability of the trained model. In contrast, in AdaBN, the statistical descriptors are updated in accordance with the specific target domain during deployment, resulting in better model generalization. Concretely, when deployed to target domains, we first freeze all network layers other than batch normalization layers, and then perform forward propagation using target domain data without labels being present. The differences in intensity distributions that exist between the source and target domains will be captured via batch normalization layers, and biased statistical descriptors (mean and variance) will be corrected. For our brain segmentation task, AdaBN can help the model to automatically adapt to the different MRI intensity distributions associated with different species, modalities and platforms without requiring modifications to the model architecture or model hyperparameters.</p></sec><sec id="s4-4"><title>Semi-supervised learning with pseudo-labels using Monte Carlo quality assessment (MCQA)</title><p>Although AdaBN facilitates domain transfer, it alone cannot completely solve the challenging problem of transfer learning between species, as the gap between different species lies not only in the MRI intensities but also in the brain structure and geometry. In this case, additional supervision is still necessary to guide the model to achieve highly accurate brain segmentation in the new species. Instead of common approaches such as training from scratch for fine-tuning, we propose a novel semi-supervised learning strategy for BEN. In addition to sparsely labeled data in the target domain, we make use of abundant unlabeled data by means of an iterative pseudo-labeling process (<xref ref-type="fig" rid="fig2">Figure 2D</xref>): we first evaluate the model on all unlabeled data and then select predictions that are associated with the lowest uncertainty (highest confidence) based on MCQA (detailed in subsequent sections). The predictions selected through quality screening are taken as pseudo-labels and are added to the training set in the next iteration. More specifically, we use a hybrid loss function:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents a labeled data point with the ground-truth annotation <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the current minibatch (total number N) and <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes a pseudo-labeled data point whose model prediction <inline-formula><mml:math id="inf4"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> from the previous iteration is below an uncertainty threshold <inline-formula><mml:math id="inf5"><mml:mi>ς</mml:mi></mml:math></inline-formula> based on MCQA (total number M). <inline-formula><mml:math id="inf6"><mml:mi>α</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is an important parameter that balances the relative contributions of true labels and pseudo-labels in the algorithm: we set this parameter to zero in the first several iterations, as our initial model is not good enough to make confident predictions, and then to a hyperparameter <inline-formula><mml:math id="inf7"><mml:mi>λ</mml:mi></mml:math></inline-formula> from iteration <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> onward. The whole semi-supervised training procedure can be summarized as follows: The total number of training epochs is denoted by <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , and the current epoch is denoted by <inline-formula><mml:math id="inf10"><mml:mi>t</mml:mi></mml:math></inline-formula>. During the first phase (<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), the network is trained directly on the target domain using labeled data. Then, in the next phase (<inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), unlabeled data with pseudo-labels additionally participate in the semi-supervised learning process by contributing to the loss function as determined by the balance coefficient <inline-formula><mml:math id="inf13"><mml:mi>α</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Empirically, we set <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="inf17"><mml:mi>ς</mml:mi></mml:math></inline-formula> is the minimal uncertainty value in each minibatch.</p></sec><sec id="s4-5"><title>Model training implementation</title><p>The model was trained with the Dice loss function using the adaptive moment estimation (Adam) optimizer for 50 epochs with a batch size of 16 and learning rates of 10<sup>-4</sup> and 10<sup>-5</sup> in the source domain and all target domains, respectively. For training and testing on each domain, a separate cross-validation process was performed to assess the final performance of our method, with progressively larger numbers of observations (n=1, 2, 3,…) assigned to the training set while the rest of the samples were assigned to the test set. The mechanisms for domain transfer and semi-supervised learning have been described in previous sections.</p></sec><sec id="s4-6"><title>Model input preprocessing</title><p>To lessen the variation across individual scans, we applied the following processes on all datasets. First, we applied the N4 algorithm (<xref ref-type="bibr" rid="bib37">Tustison et al., 2010</xref>) to remove bias fields in the MRI scans. Second, we normalized each image as follows (with <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denoting the normalized MRI volume and <inline-formula><mml:math id="inf19"><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>99</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> denoting the 1st and 99th percentiles, respectively, of the intensity values in the input volumes):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>99</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For all functional MRI data, our model can process each time-point image and produce a corresponding binary mask. For consistency with the structural modalities, we used the results computed for the 5th time point to calculate the performance metrics.</p><p>The processed volumes were then fed into the network as input, slice by slice, after being resized or cropped to a matrix with dimensions of 256*256 for compatibility with the network. In addition, data augmentation was employed to empower the network to learn invariant characteristics from data exhibiting variations among different subjects and protocols. For each input slice, the following transformations were implemented: random rotation (±10), random scaling (90–110%) and random translation (up to ± 10 pixels along each axis). For example, the rotation and translation operators can imitate conditions in which the subjects are represented in diverse coordinate systems and have different head orientations. For the case of inconsistent voxel sizes across different sites and even for the lifespan of one individual during brain maturation, scaling can simulate corresponding image data.</p></sec><sec id="s4-7"><title>Model output postprocessing for uncertainty estimation and quality assessment</title><p>To understand the intrarater divergences as represented in BEN and provide valuable insights into model interpretability for clinicians, we utilize dropout layers in the network to obtain uncertainty measures for the model-generated segmentations.</p><p>There are two principal types of prediction uncertainties for deep neural networks: aleatoric uncertainty and epistemic uncertainty (<xref ref-type="bibr" rid="bib20">Hüllermeier and Waegeman, 2021</xref>; <xref ref-type="bibr" rid="bib24">Kwon et al., 2020</xref>). Aleatoric uncertainty captures the potential inherent noise of the input test data, and therefore, this type of uncertainty is unlikely to be lessened by increasing the amount of training data. In contrast, epistemic uncertainty refers to the uncertainty in the model parameters, which represents the lack of knowledge of the best model. In DL networks, epistemic uncertainty can be caused by a lack of training data in certain areas of the input domain and will decrease with increased diversity of the training data distribution.</p><p>In our study, approximate Bayesian inference was used to estimate uncertainty. Specifically, keeping the dropout layers active in the inference phase, we collected N independent samples for each subject. The pixelwise uncertainties of each sample are given by the following formula:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>Uncertainty</mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mtext>aleatoric</mml:mtext></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mtext>epistemic</mml:mtext></mml:munder></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the sigmoid probability output in the final layer of the network in the inference stage and <inline-formula><mml:math id="inf22"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is the average prediction for all inferences. In this work, we chose <inline-formula><mml:math id="inf23"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula>, which offers a good trade-off between the reliability of uncertainty estimation and the time consumed for Monte Carlo sampling, except in the human expert disagreement study, for which we set <inline-formula><mml:math id="inf24"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:math></inline-formula> for consistency with the number of clinical raters.</p><p>Based on these estimated uncertainty observations, we can filter out relatively unreliable predictions (usually those with high uncertainty values) based on the guidance obtained through statistical analysis, thereby realizing quality assessment. Moreover, relatively convincing predictions can be further utilized as pseudo-labels to guide the semi-supervised learning process.</p></sec><sec id="s4-8"><title>Baseline methods and traditional toolbox settings</title><p>To demonstrate the utility of domain transfer, we additionally tested two baseline methods of training the BEN model: training from scratch and fine-tuning. These methods share the same network architecture, learning rate, batch size and number of training epochs. For training from scratch, the weights of the layers in the network are randomly initialized, and then all weights are adjusted based on the training set. In the second baseline method, the training process is initialized on the source domain, and the weights are then fine-tuned to the new task in accordance with the target-domain dataset.</p><p>We also compared BEN with four widely used tools FSL (<xref ref-type="bibr" rid="bib22">Jenkinson et al., 2012</xref>), AFNI (<xref ref-type="bibr" rid="bib9">Cox, 2012</xref>), FreeSurfer (<xref ref-type="bibr" rid="bib14">Fischl, 2012</xref>), and Sherm (<xref ref-type="bibr" rid="bib26">Liu et al., 2020</xref>) representing SOTA processing pipelines in their respective research fields. We adjusted the parameters provided by these traditional toolboxes to adapt to the different domain datasets to achieve better performance; however, for situations in which the demands were beyond the scope of the capability of the tools, the default parameters were used instead. The detail parameter settings for these four methods are shown in <xref ref-type="table" rid="app1table7">Appendix 1—table 7</xref>.</p></sec><sec id="s4-9"><title>Ground-truth generation</title><p>The ground-truth brain and nonbrain regions for each domain dataset were manually annotated by two experts in the field of brain anatomy. Discrepancies were addressed through consensus. In addition, the labels for the macaque dataset were provided by a previous study (<xref ref-type="bibr" rid="bib42">Wang et al., 2021</xref>), and we made no further modifications. The human brain masks of the ABCD, UKB, and ZIB datasets used in this work were obtained from efforts and contributions reported in previous literature (<xref ref-type="bibr" rid="bib35">Peng et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Alfaro-Almagro et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Casey et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Gong et al., 2021</xref>; <xref ref-type="bibr" rid="bib32">Miller et al., 2016</xref>). All of the labels have been visually inspected.</p></sec><sec id="s4-10"><title>Evaluation metrics</title><p>The following two metrics are used to quantitatively compare the segmentation performance. The Dice score measures the overlap between the segmentation results and the ground truth. The definition of this measure is formulated as follows:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mo>∩</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf25"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mi>G</mml:mi></mml:math></inline-formula> denote the predicted and ground-truth label maps, respectively. Higher scores indicate better consistency. The 95% Hausdorff distance (HD95) measures how far the segmentation results and ground truth for a metric space are from each other as given by the following equation:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>HD</mml:mtext><mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf27"><mml:mo>∂</mml:mo><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> represents the boundary of the set and <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mo>∂</mml:mo><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mo>∂</mml:mo><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> are the 95th percentiles of the Euclidean distances between pixels in <inline-formula><mml:math id="inf30"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:mi>G</mml:mi></mml:math></inline-formula>, respectively.</p></sec><sec id="s4-11"><title>Statistics</title><p>Statistical analyses were performed using SciPy (version 1.3.3) in Python (version 3.6.10).</p><p>The exact sample values and results of statistical tests, including the Mann–Whitney test and paired t-test, are provided in the figure captions or within the figure panels themselves. Differences are considered statistically significant when p&lt;0.05.</p></sec><sec id="s4-12"><title>Code availability</title><p>We release BEN and pretrained models via <ext-link ext-link-type="uri" xlink:href="https://github.com/yu02019/BEN">https://github.com/yu02019/BEN</ext-link> (<xref ref-type="bibr" rid="bib46">Yu, 2023</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf3"><p>is affiliated with Shanghai United Imaging Intelligence Co., Ltd. He has financial interests to declare</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Validation</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Investigation</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision</p></fn><fn fn-type="con" id="con5"><p>Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Software, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Resources, Supervision, Funding acquisition</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The Zhangjiang International Brain Biobank (ZIB) protocols were approved by the Ethics Committee of Fudan University (AF/SC-03/20200722) and written informed consents were obtained from all volunteers. UK Biobank (UKB) and Adolescent Brain Cognitive Development (ABCD) are publicly available.</p></fn><fn fn-type="other"><p>Partial rodent MRI data collection were approved by the Animal Care and Use Committee of Fudan University, China. The rest rodent data (Rat-T2WI-9.4T and Rat-EPI-9.4T datasets) are publicly available (CARMI: <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002870/versions/1.0.0">https://openneuro.org/datasets/ds002870/versions/1.0.0</ext-link>). Marmoset MRI data collection were approved by the Animal Care and Use Committee of the Institute of Neuroscience, Chinese Academy of Sciences, China. Macaque MRI data are publicly available from the nonhuman PRIMatE Data Exchange (PRIME-DE) (<ext-link ext-link-type="uri" xlink:href="https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html">https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html</ext-link>).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-81217-mdarchecklist1-v3.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>We release a longitudinal MRI dataset of young adult C57BL6J mouse via <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6844489">https://zenodo.org/record/6844489</ext-link>. The rest rodent data (Rat-T2WI-9.4T and Rat-EPI-9.4T datasets) are publicly available (CARMI: <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002870/versions/1.0.0">https://openneuro.org/datasets/ds002870/versions/1.0.0</ext-link>). Macaque MRI data are publicly available from the nonhuman PRIMatE Data Exchange (PRIME-DE) (<ext-link ext-link-type="uri" xlink:href="https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html">https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html</ext-link>; <xref ref-type="bibr" rid="bib31">Milham et al., 2018</xref>). UK Biobank (UKB) and Adolescent Brain Cognitive Development (ABCD) are publicly available. All code used in this work is released via <ext-link ext-link-type="uri" xlink:href="https://github.com/yu02019/BEN">https://github.com/yu02019/BEN</ext-link> (<xref ref-type="bibr" rid="bib46">Yu, 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Ziqi</given-names></name><name><surname>Xu</surname><given-names>W </given-names></name><name><surname>Zhang</surname><given-names>X-Y </given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>A longitudinal MRI dataset of young adult C57BL6J mouse brain</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.6844489</pub-id></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>L-M</given-names></name><name><surname>Ban</surname><given-names>W</given-names></name><name><surname>Chao</surname><given-names>T-H</given-names></name><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Cerri</surname><given-names>DH</given-names></name><name><surname>Walton</surname><given-names>L</given-names></name><name><surname>Broadwater</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>S-H</given-names></name><name><surname>Shih</surname><given-names>YI</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>CAMRI Rat Brain MRI Data</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds002870.v1.0.1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset3"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>L-M</given-names></name><name><surname>Ban</surname><given-names>W</given-names></name><name><surname>Chao</surname><given-names>T-H</given-names></name><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Cerri</surname><given-names>DH</given-names></name><name><surname>Walton</surname><given-names>L</given-names></name><name><surname>Broadwater</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>S-H</given-names></name><name><surname>Shih</surname><given-names>YI</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>CAMRI Mouse Brain MRI Data</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds002868.v1.0.1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Shanghai Municipal Science and Technology Major Project (No.2018SHZDZX01), ZJLab, Shanghai Center for Brain Science and Brain-Inspired Technology, National Natural Science Foundation of China (81873893, 82171903, 92043301), the Office of Global Partnerships (Key Projects Development Fund) at Fudan University.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bangerter</surname><given-names>NK</given-names></name><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Hernandez-Fernandez</surname><given-names>M</given-names></name><name><surname>Vallee</surname><given-names>E</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Webster</surname><given-names>M</given-names></name><name><surname>McCarthy</surname><given-names>P</given-names></name><name><surname>Rorden</surname><given-names>C</given-names></name><name><surname>Daducci</surname><given-names>A</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Dragonu</surname><given-names>I</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Image processing and quality control for the first 10,000 brain imaging datasets from UK biobank</article-title><source>NeuroImage</source><volume>166</volume><fpage>400</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.10.034</pub-id><pub-id pub-id-type="pmid">29079522</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrière</surname><given-names>DA</given-names></name><name><surname>Magalhães</surname><given-names>R</given-names></name><name><surname>Novais</surname><given-names>A</given-names></name><name><surname>Marques</surname><given-names>P</given-names></name><name><surname>Selingue</surname><given-names>E</given-names></name><name><surname>Geffroy</surname><given-names>F</given-names></name><name><surname>Marques</surname><given-names>F</given-names></name><name><surname>Cerqueira</surname><given-names>J</given-names></name><name><surname>Sousa</surname><given-names>JC</given-names></name><name><surname>Boumezbeur</surname><given-names>F</given-names></name><name><surname>Bottlaender</surname><given-names>M</given-names></name><name><surname>Jay</surname><given-names>TM</given-names></name><name><surname>Cachia</surname><given-names>A</given-names></name><name><surname>Sousa</surname><given-names>N</given-names></name><name><surname>Mériaux</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The sigma rat brain templates and atlases for multimodal MRI data analysis and visualization</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5699</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13575-7</pub-id><pub-id pub-id-type="pmid">31836716</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumgartner</surname><given-names>K</given-names></name><name><surname>Kübler</surname><given-names>J</given-names></name><name><surname>Bitzer</surname><given-names>M</given-names></name><name><surname>Bösmüller</surname><given-names>H</given-names></name><name><surname>Horger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Caroli’s syndrome</article-title><source>RoFo</source><volume>192</volume><fpage>119</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1055/a-1024-4526</pub-id><pub-id pub-id-type="pmid">31779027</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beare</surname><given-names>R</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Adamson</surname><given-names>CL</given-names></name><name><surname>Silk</surname><given-names>T</given-names></name><name><surname>Thompson</surname><given-names>DK</given-names></name><name><surname>Yang</surname><given-names>JYM</given-names></name><name><surname>Anderson</surname><given-names>VA</given-names></name><name><surname>Seal</surname><given-names>ML</given-names></name><name><surname>Wood</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain extraction using the watershed transform from markers</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>32</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00032</pub-id><pub-id pub-id-type="pmid">24367327</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calabrese</surname><given-names>E</given-names></name><name><surname>Badea</surname><given-names>A</given-names></name><name><surname>Watson</surname><given-names>C</given-names></name><name><surname>Johnson</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A quantitative magnetic resonance histology atlas of postnatal rat brain development with regional estimates of growth and variability</article-title><source>NeuroImage</source><volume>71</volume><fpage>196</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.01.017</pub-id><pub-id pub-id-type="pmid">23353030</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casey</surname><given-names>BJ</given-names></name><name><surname>Cannonier</surname><given-names>T</given-names></name><name><surname>Conley</surname><given-names>MI</given-names></name><name><surname>Cohen</surname><given-names>AO</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Heitzeg</surname><given-names>MM</given-names></name><name><surname>Soules</surname><given-names>ME</given-names></name><name><surname>Teslovich</surname><given-names>T</given-names></name><name><surname>Dellarco</surname><given-names>DV</given-names></name><name><surname>Garavan</surname><given-names>H</given-names></name><name><surname>Orr</surname><given-names>CA</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Banich</surname><given-names>MT</given-names></name><name><surname>Speer</surname><given-names>NK</given-names></name><name><surname>Sutherland</surname><given-names>MT</given-names></name><name><surname>Riedel</surname><given-names>MC</given-names></name><name><surname>Dick</surname><given-names>AS</given-names></name><name><surname>Bjork</surname><given-names>JM</given-names></name><name><surname>Thomas</surname><given-names>KM</given-names></name><name><surname>Chaarani</surname><given-names>B</given-names></name><name><surname>Mejia</surname><given-names>MH</given-names></name><name><surname>Hagler</surname><given-names>DJ</given-names></name><name><surname>Daniela Cornejo</surname><given-names>M</given-names></name><name><surname>Sicat</surname><given-names>CS</given-names></name><name><surname>Harms</surname><given-names>MP</given-names></name><name><surname>Dosenbach</surname><given-names>NUF</given-names></name><name><surname>Rosenberg</surname><given-names>M</given-names></name><name><surname>Earl</surname><given-names>E</given-names></name><name><surname>Bartsch</surname><given-names>H</given-names></name><name><surname>Watts</surname><given-names>R</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Kuperman</surname><given-names>JM</given-names></name><name><surname>Fair</surname><given-names>DA</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><collab>ABCD Imaging Acquisition Workgroup</collab></person-group><year iso-8601-date="2018">2018</year><article-title>The adolescent brain cognitive development (ABCD) study: imaging acquisition across 21 sites</article-title><source>Developmental Cognitive Neuroscience</source><volume>32</volume><fpage>43</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.dcn.2018.03.001</pub-id><pub-id pub-id-type="pmid">29567376</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>HH</given-names></name><name><surname>Yeh</surname><given-names>SJ</given-names></name><name><surname>Chiang</surname><given-names>MC</given-names></name><name><surname>Hsieh</surname><given-names>ST</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Automatic brain extraction and hemisphere segmentation in rat brain Mr images after stroke using deformable models</article-title><source>Medical Physics</source><volume>48</volume><fpage>6036</fpage><lpage>6050</lpage><pub-id pub-id-type="doi">10.1002/mp.15157</pub-id><pub-id pub-id-type="pmid">34388268</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>N</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Bai Bingren</surname><given-names>J</given-names></name><name><surname>Qiu</surname><given-names>A</given-names></name><name><surname>Chuang</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Robust automatic rodent brain extraction using 3-D pulse-coupled neural networks (PCNN)</article-title><source>IEEE Transactions on Image Processing</source><volume>20</volume><fpage>2554</fpage><lpage>2564</lpage><pub-id pub-id-type="doi">10.1109/TIP.2011.2126587</pub-id><pub-id pub-id-type="pmid">21411404</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>AFNI: what a long strange trip it’s been</article-title><source>NeuroImage</source><volume>62</volume><fpage>743</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.08.056</pub-id><pub-id pub-id-type="pmid">21889996</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Feo</surname><given-names>R</given-names></name><name><surname>Shatillo</surname><given-names>A</given-names></name><name><surname>Sierra</surname><given-names>A</given-names></name><name><surname>Valverde</surname><given-names>JM</given-names></name><name><surname>Gröhn</surname><given-names>O</given-names></name><name><surname>Giove</surname><given-names>F</given-names></name><name><surname>Tohka</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Automated joint skull-stripping and segmentation with multi-task U-net in large mouse brain MRI databases</article-title><source>NeuroImage</source><volume>229</volume><elocation-id>117734</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117734</pub-id><pub-id pub-id-type="pmid">33454412</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorr</surname><given-names>AE</given-names></name><name><surname>Lerch</surname><given-names>JP</given-names></name><name><surname>Spring</surname><given-names>S</given-names></name><name><surname>Kabani</surname><given-names>N</given-names></name><name><surname>Henkelman</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>High resolution three-dimensional brain atlas using an average magnetic resonance image of 40 adult C57BL/6J mice</article-title><source>NeuroImage</source><volume>42</volume><fpage>60</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.037</pub-id><pub-id pub-id-type="pmid">18502665</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>FMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name><name><surname>Mills</surname><given-names>SR</given-names></name><name><surname>Brown</surname><given-names>ED</given-names></name><name><surname>Kelly</surname><given-names>RL</given-names></name><name><surname>Peters</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>3D statistical neuroanatomical models from 305 MRI volumes</article-title><conf-name>1993 IEEE Conference Record Nuclear Science Symposium and Medical Imaging Conference</conf-name><conf-loc>San Francisco, CA, USA</conf-loc><pub-id pub-id-type="doi">10.1109/NSSMIC.1993.373602</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia-Saldivar</surname><given-names>P</given-names></name><name><surname>Garimella</surname><given-names>A</given-names></name><name><surname>Garza-Villarreal</surname><given-names>EA</given-names></name><name><surname>Mendez</surname><given-names>FA</given-names></name><name><surname>Concha</surname><given-names>L</given-names></name><name><surname>Merchant</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>PREEMACS: pipeline for preprocessing and extraction of the macaque brain surface</article-title><source>NeuroImage</source><volume>227</volume><elocation-id>117671</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117671</pub-id><pub-id pub-id-type="pmid">33359348</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>W</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Du</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>J</given-names></name><name><surname>Cheng</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Brain structure is linked to the association between family environment and behavioral problems in children in the ABCD study</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>3769</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-23994-0</pub-id><pub-id pub-id-type="pmid">34145259</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamezah</surname><given-names>HS</given-names></name><name><surname>Durani</surname><given-names>LW</given-names></name><name><surname>Ibrahim</surname><given-names>NF</given-names></name><name><surname>Yanagisawa</surname><given-names>D</given-names></name><name><surname>Kato</surname><given-names>T</given-names></name><name><surname>Shiino</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>S</given-names></name><name><surname>Damanhuri</surname><given-names>HA</given-names></name><name><surname>Ngah</surname><given-names>WZW</given-names></name><name><surname>Tooyama</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Volumetric changes in the aging rat brain and its impact on cognitive and locomotor functions</article-title><source>Experimental Gerontology</source><volume>99</volume><fpage>69</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.exger.2017.09.008</pub-id><pub-id pub-id-type="pmid">28918364</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Zhai</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Peng</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>XY</given-names></name></person-group><year iso-8601-date="2021">2021</year><chapter-title>Detecting extremely small lesions in mouse brain MRI with point annotations via multi-task learningmachine learning</chapter-title><person-group person-group-type="editor"><name><surname>Maier</surname><given-names>A</given-names></name><name><surname>Steidl</surname><given-names>S</given-names></name></person-group><source>Medical Imaging, Lecture Notes in Computer Science</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>498</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-87589-3</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>LM</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Ranadive</surname><given-names>P</given-names></name><name><surname>Ban</surname><given-names>W</given-names></name><name><surname>Chao</surname><given-names>THH</given-names></name><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Cerri</surname><given-names>DH</given-names></name><name><surname>Walton</surname><given-names>LR</given-names></name><name><surname>Broadwater</surname><given-names>MA</given-names></name><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Shen</surname><given-names>D</given-names></name><name><surname>Shih</surname><given-names>YYI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automatic skull stripping of rat and mouse brain MRI data using U-net</article-title><source>Frontiers in Neuroscience</source><volume>14</volume><elocation-id>568614</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2020.568614</pub-id><pub-id pub-id-type="pmid">33117118</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hüllermeier</surname><given-names>E</given-names></name><name><surname>Waegeman</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods</article-title><source>Machine Learning</source><volume>110</volume><fpage>457</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1007/s10994-021-05946-3</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isensee</surname><given-names>F</given-names></name><name><surname>Jaeger</surname><given-names>PF</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Petersen</surname><given-names>J</given-names></name><name><surname>Maier-Hein</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>NnU-net: a self-configuring method for deep learning-based biomedical image segmentation</article-title><source>Nature Methods</source><volume>18</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-01008-z</pub-id><pub-id pub-id-type="pmid">33288961</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fsl</article-title><source>NeuroImage</source><volume>62</volume><fpage>782</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id><pub-id pub-id-type="pmid">21979382</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jungo</surname><given-names>A</given-names></name><name><surname>Meier</surname><given-names>R</given-names></name><name><surname>Ermis</surname><given-names>E</given-names></name><name><surname>Blatti-Moreno</surname><given-names>M</given-names></name><name><surname>Herrmann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>On the effect of inter-observer variability for a reliable estimation of uncertainty of medical image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Frangi</surname><given-names>AF</given-names></name></person-group><source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, Lecture Notes in Computer Science</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>682</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-00928-1</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwon</surname><given-names>Y</given-names></name><name><surname>Won</surname><given-names>JH</given-names></name><name><surname>Kim</surname><given-names>BJ</given-names></name><name><surname>Paik</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Uncertainty quantification using bayesian neural networks in classification: application to biomedical image segmentation</article-title><source>Computational Statistics &amp; Data Analysis</source><volume>142</volume><elocation-id>106816</elocation-id><pub-id pub-id-type="doi">10.1016/j.csda.2019.106816</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>N</given-names></name><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Hou</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Adaptive batch normalization for practical domain adaptation</article-title><source>Pattern Recognition</source><volume>80</volume><fpage>109</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2018.03.005</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Unsal</surname><given-names>HS</given-names></name><name><surname>Tao</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automatic brain extraction for rodent MRI images</article-title><source>Neuroinformatics</source><volume>18</volume><fpage>395</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1007/s12021-020-09453-z</pub-id><pub-id pub-id-type="pmid">31989442</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lohmeier</surname><given-names>J</given-names></name><name><surname>Kaneko</surname><given-names>T</given-names></name><name><surname>Hamm</surname><given-names>B</given-names></name><name><surname>Makowski</surname><given-names>MR</given-names></name><name><surname>Okano</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>AtlasBREX: automated template-derived brain extraction in animal MRI</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>12219</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-48489-3</pub-id><pub-id pub-id-type="pmid">31434923</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>An</surname><given-names>X</given-names></name><name><surname>Ge</surname><given-names>C</given-names></name><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Dong</surname><given-names>G</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>Z</given-names></name><name><surname>Cao</surname><given-names>T</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Nie</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Toward data-efficient learning: a benchmark for COVID-19 CT lung and infection segmentation</article-title><source>Medical Physics</source><volume>48</volume><fpage>1197</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1002/mp.14676</pub-id><pub-id pub-id-type="pmid">33354790</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Saul</surname><given-names>N</given-names></name><name><surname>Großberger</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>UMAP: uniform manifold approximation and projection</article-title><source>Journal of Open Source Software</source><volume>3</volume><elocation-id>861</elocation-id><pub-id pub-id-type="doi">10.21105/joss.00861</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrtash</surname><given-names>A</given-names></name><name><surname>Wells</surname><given-names>WM</given-names></name><name><surname>Tempany</surname><given-names>CM</given-names></name><name><surname>Abolmaesumi</surname><given-names>P</given-names></name><name><surname>Kapur</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Confidence calibration and predictive uncertainty estimation for deep medical image segmentation</article-title><source>IEEE Transactions on Medical Imaging</source><volume>39</volume><fpage>3868</fpage><lpage>3878</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.3006437</pub-id><pub-id pub-id-type="pmid">32746129</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milham</surname><given-names>MP</given-names></name><name><surname>Ai</surname><given-names>L</given-names></name><name><surname>Koo</surname><given-names>B</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name><name><surname>Amiez</surname><given-names>C</given-names></name><name><surname>Balezeau</surname><given-names>F</given-names></name><name><surname>Baxter</surname><given-names>MG</given-names></name><name><surname>Blezer</surname><given-names>ELA</given-names></name><name><surname>Brochier</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Croxson</surname><given-names>PL</given-names></name><name><surname>Damatac</surname><given-names>CG</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name><name><surname>Fair</surname><given-names>DA</given-names></name><name><surname>Fleysher</surname><given-names>L</given-names></name><name><surname>Freiwald</surname><given-names>W</given-names></name><name><surname>Froudist-Walsh</surname><given-names>S</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Guedj</surname><given-names>C</given-names></name><name><surname>Hadj-Bouziane</surname><given-names>F</given-names></name><name><surname>Ben Hamed</surname><given-names>S</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Hiba</surname><given-names>B</given-names></name><name><surname>Jarraya</surname><given-names>B</given-names></name><name><surname>Jung</surname><given-names>B</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Kwok</surname><given-names>SC</given-names></name><name><surname>Laland</surname><given-names>KN</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Lindenfors</surname><given-names>P</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Messinger</surname><given-names>A</given-names></name><name><surname>Meunier</surname><given-names>M</given-names></name><name><surname>Mok</surname><given-names>K</given-names></name><name><surname>Morrison</surname><given-names>JH</given-names></name><name><surname>Nacef</surname><given-names>J</given-names></name><name><surname>Nagy</surname><given-names>J</given-names></name><name><surname>Rios</surname><given-names>MO</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Pinsk</surname><given-names>M</given-names></name><name><surname>Poirier</surname><given-names>C</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Rajimehr</surname><given-names>R</given-names></name><name><surname>Reader</surname><given-names>SM</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Rudko</surname><given-names>DA</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Russ</surname><given-names>BE</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Schmid</surname><given-names>MC</given-names></name><name><surname>Schwiedrzik</surname><given-names>CM</given-names></name><name><surname>Seidlitz</surname><given-names>J</given-names></name><name><surname>Sein</surname><given-names>J</given-names></name><name><surname>Shmuel</surname><given-names>A</given-names></name><name><surname>Sullivan</surname><given-names>EL</given-names></name><name><surname>Ungerleider</surname><given-names>L</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Todorov</surname><given-names>OS</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wilson</surname><given-names>CRE</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ye</surname><given-names>FQ</given-names></name><name><surname>Zarco</surname><given-names>W</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An open resource for non-human primate imaging</article-title><source>Neuron</source><volume>100</volume><fpage>61</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.039</pub-id><pub-id pub-id-type="pmid">30269990</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Bangerter</surname><given-names>NK</given-names></name><name><surname>Thomas</surname><given-names>DL</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Bartsch</surname><given-names>AJ</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Okell</surname><given-names>TW</given-names></name><name><surname>Weale</surname><given-names>P</given-names></name><name><surname>Dragonu</surname><given-names>I</given-names></name><name><surname>Garratt</surname><given-names>S</given-names></name><name><surname>Hudson</surname><given-names>S</given-names></name><name><surname>Collins</surname><given-names>R</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multimodal population brain imaging in the UK Biobank prospective epidemiological study</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1523</fpage><lpage>1536</lpage><pub-id pub-id-type="doi">10.1038/nn.4393</pub-id><pub-id pub-id-type="pmid">27643430</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nie</surname><given-names>J</given-names></name><name><surname>Shen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Automated segmentation of mouse brain images using multi-atlas multi-ROI deformation and label fusion</article-title><source>Neuroinformatics</source><volume>11</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1007/s12021-012-9163-0</pub-id><pub-id pub-id-type="pmid">23055043</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oguz</surname><given-names>I</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Rumple</surname><given-names>A</given-names></name><name><surname>Sonka</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rats: rapid automatic tissue segmentation in rodent brain MRI</article-title><source>Journal of Neuroscience Methods</source><volume>221</volume><fpage>175</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.09.021</pub-id><pub-id pub-id-type="pmid">24140478</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H</given-names></name><name><surname>Gong</surname><given-names>W</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Accurate brain age prediction with lightweight deep neural networks</article-title><source>Medical Image Analysis</source><volume>68</volume><elocation-id>101871</elocation-id><pub-id pub-id-type="doi">10.1016/j.media.2020.101871</pub-id><pub-id pub-id-type="pmid">33197716</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>U-net: convolutional networks for biomedical image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Zaynidinov</surname><given-names>H</given-names></name></person-group><source>Lecture Notes in Computer Science</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: improved N3 bias correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><pub-id pub-id-type="pmid">20378467</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valverde</surname><given-names>JM</given-names></name><name><surname>Shatillo</surname><given-names>A</given-names></name><name><surname>De Feo</surname><given-names>R</given-names></name><name><surname>Gröhn</surname><given-names>O</given-names></name><name><surname>Sierra</surname><given-names>A</given-names></name><name><surname>Tohka</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>RatLesNetv2: a fully convolutional network for rodent brain lesion segmentation</article-title><source>Frontiers in Neuroscience</source><volume>14</volume><elocation-id>610239</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2020.610239</pub-id><pub-id pub-id-type="pmid">33414703</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>He</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Non-local Neural Networks</article-title><conf-name>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Salt Lake City, UT, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2018.00813</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Aertsen</surname><given-names>M</given-names></name><name><surname>Deprest</surname><given-names>J</given-names></name><name><surname>Ourselin</surname><given-names>S</given-names></name><name><surname>Vercauteren</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</article-title><source>Neurocomputing</source><volume>335</volume><fpage>34</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2019.01.103</pub-id><pub-id pub-id-type="pmid">31595105</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Zheng</surname><given-names>VW</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Miao</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>A survey of zero-shot learning</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>10</volume><fpage>1</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1145/3293318</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>XH</given-names></name><name><surname>Cho</surname><given-names>JW</given-names></name><name><surname>Russ</surname><given-names>BE</given-names></name><name><surname>Rajamani</surname><given-names>N</given-names></name><name><surname>Omelchenko</surname><given-names>A</given-names></name><name><surname>Ai</surname><given-names>L</given-names></name><name><surname>Korchmaros</surname><given-names>A</given-names></name><name><surname>Sawiak</surname><given-names>S</given-names></name><name><surname>Benn</surname><given-names>RA</given-names></name><name><surname>Garcia-Saldivar</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Kalin</surname><given-names>NH</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Craddock</surname><given-names>RC</given-names></name><name><surname>Fox</surname><given-names>AS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>Messinger</surname><given-names>A</given-names></name><name><surname>Milham</surname><given-names>MP</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>U-net model for brain extraction: trained on humans for transfer to non-human primates</article-title><source>NeuroImage</source><volume>235</volume><elocation-id>118001</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118001</pub-id><pub-id pub-id-type="pmid">33789137</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Niu</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A survey on incorporating domain knowledge into deep learning for medical image analysis</article-title><source>Medical Image Analysis</source><volume>69</volume><elocation-id>101985</elocation-id><pub-id pub-id-type="doi">10.1016/j.media.2021.101985</pub-id><pub-id pub-id-type="pmid">33588117</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>CG</given-names></name><name><surname>Wang</surname><given-names>XD</given-names></name><name><surname>Zuo</surname><given-names>XN</given-names></name><name><surname>Zang</surname><given-names>YF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>DPABI: data processing &amp; analysis for (resting-state) brain imaging</article-title><source>Neuroinformatics</source><volume>14</volume><fpage>339</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1007/s12021-016-9299-4</pub-id><pub-id pub-id-type="pmid">27075850</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Zhai</surname><given-names>Y</given-names></name><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Peng</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>XY</given-names></name></person-group><year iso-8601-date="2021">2021</year><chapter-title>MouseGAN: GAN-based multiple MRI modalities synthesis and segmentation for mouse brain structures</chapter-title><person-group person-group-type="editor"><name><surname>de Bruijne</surname><given-names>M</given-names></name><name><surname>Cattin</surname><given-names>PC</given-names></name></person-group><source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2021</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>442</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-87193-2_42</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>BEN</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/yu02019/BEN">https://github.com/yu02019/BEN</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Ding</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Xie</surname><given-names>Q</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Fei</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>M</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Segmentation with multiple acceptable annotations: A case study of myocardial segmentation in contrast echocardiography</article-title><source>Lecture Notes in Computer Science</source><volume>12729</volume><elocation-id>37</elocation-id><pub-id pub-id-type="doi">10.1007/978-3-030-78191-0_37</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Oler</surname><given-names>JA</given-names></name><name><surname>Meyerand</surname><given-names>ME</given-names></name><name><surname>Kalin</surname><given-names>NH</given-names></name><name><surname>Birn</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian convolutional neural network based MRI brain extraction on nonhuman primates</article-title><source>NeuroImage</source><volume>175</volume><fpage>32</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.03.065</pub-id><pub-id pub-id-type="pmid">29604454</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>T</given-names></name><name><surname>Zhao</surname><given-names>F</given-names></name><name><surname>Pei</surname><given-names>Y</given-names></name><name><surname>Ning</surname><given-names>Z</given-names></name><name><surname>Liao</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Niu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Shen</surname><given-names>D</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>DIKA-nets: domain-invariant knowledge-guided attention networks for brain skull stripping of early developing macaques</article-title><source>NeuroImage</source><volume>227</volume><elocation-id>117649</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117649</pub-id><pub-id pub-id-type="pmid">33338616</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>MRI scan information of the fifteen animal datasets and three human datasets.</title><p>FDU: Fudan University. UCAS: University of Chinese Academy of Sciences; UNC: University of North Carolina at Chapel Hill; ABCD: Adolescent Brain Cognitive Developmental study; ZIB: Zhangjiang International Brain BioBank at Fudan University.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Species</th><th align="left" valign="top">Modality</th><th align="left" valign="top">Magnetic Field(T)</th><th align="left" valign="top">Scans</th><th align="left" valign="top">Slices</th><th align="left" valign="top">In-plane Resolution(mm)</th><th align="left" valign="top">Thickness(mm)</th><th align="left" valign="top">Manufacturer</th><th align="left" valign="top">Institution</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="7">Mouse</td><td align="left" valign="top" rowspan="3">T2WI</td><td align="char" char="." valign="top">11.7</td><td align="char" char="." valign="top">243</td><td align="char" char="." valign="top">9,030</td><td align="char" char="." valign="top">0.10<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.10</td><td align="char" char="." valign="top">0.4</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">FDU</td></tr><tr><td align="char" char="." valign="top">9.4</td><td align="char" char="." valign="top">14</td><td align="char" char="." valign="top">448</td><td align="char" char="." valign="top">0.06<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.06</td><td align="char" char="." valign="top">0.4</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UCAS</td></tr><tr><td align="char" char="." valign="top">7</td><td align="char" char="." valign="top">14</td><td align="char" char="." valign="top">126</td><td align="char" char="." valign="top">0.08<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.08</td><td align="char" char="." valign="top">0.8</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UCAS</td></tr><tr><td align="left" valign="top" rowspan="2">EPI</td><td align="char" char="." valign="top">11.7</td><td align="char" char="." valign="top">54</td><td align="char" char="." valign="top">2,198</td><td align="char" char="." valign="top">0.2<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.2</td><td align="char" char="." valign="top">0.4</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">FDU</td></tr><tr><td align="char" char="." valign="top">9.4</td><td align="char" char="." valign="top">20</td><td align="char" char="." valign="top">360</td><td align="char" char="." valign="top">0.15<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.15</td><td align="char" char="." valign="top">0.5</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UCAS</td></tr><tr><td align="left" valign="top">SWI</td><td align="char" char="." valign="top">11.7</td><td align="char" char="." valign="top">50</td><td align="char" char="." valign="top">1,500</td><td align="char" char="." valign="top">0.06<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.06</td><td align="char" char="." valign="top">0.5</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">FDU</td></tr><tr><td align="left" valign="top">ASL</td><td align="char" char="." valign="top">11.7</td><td align="char" char="." valign="top">58</td><td align="char" char="." valign="top">696</td><td align="char" char="." valign="top">0.167<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.167</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">FDU</td></tr><tr><td align="left" valign="top" rowspan="4">Rat</td><td align="left" valign="top" rowspan="3">T2WI</td><td align="char" char="." valign="top">11.7</td><td align="char" char="." valign="top">132</td><td align="char" char="." valign="top">5,544</td><td align="char" char="." valign="top">0.14<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.14</td><td align="char" char="." valign="top">0.6</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">FDU</td></tr><tr><td align="char" char="." valign="top">9.4</td><td align="char" char="." valign="top">55</td><td align="char" char="." valign="top">660</td><td align="char" char="." valign="top">0.1<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.1</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UNC <xref ref-type="table-fn" rid="app1table1fn2">†</xref></td></tr><tr><td align="char" char="." valign="top">7</td><td align="char" char="." valign="top">88</td><td align="char" char="." valign="top">4,400</td><td align="char" char="." valign="top">0.09<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.09</td><td align="char" char="." valign="top">0.4</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UCAS</td></tr><tr><td align="left" valign="top">EPI</td><td align="char" char="." valign="top">9.4</td><td align="char" char="." valign="top">55</td><td align="char" char="." valign="top">660</td><td align="char" char="." valign="top">0.32<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.32</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UNC <xref ref-type="table-fn" rid="app1table1fn2">†</xref></td></tr><tr><td align="left" valign="top"><bold>Sum of rodent</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="char" char="." valign="top"><bold>783</bold></td><td align="char" char="." valign="top"><bold>25,622</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top" rowspan="2">Marmoset</td><td align="left" valign="top">T2WI</td><td align="char" char="." valign="top">9.4</td><td align="char" char="." valign="top">62</td><td align="char" char="." valign="top">2,480</td><td align="char" char="." valign="top">0.2<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.2</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UCAS</td></tr><tr><td align="left" valign="top">EPI</td><td align="char" char="." valign="top">9.4</td><td align="char" char="." valign="top">50</td><td align="char" char="." valign="top">1,580</td><td align="char" char="." valign="top">0.5<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.5</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">Bruker</td><td align="left" valign="top">UCAS</td></tr><tr><td align="left" valign="top" rowspan="2">Macaque <xref ref-type="table-fn" rid="app1table1fn1">*</xref></td><td align="left" valign="top">T1WI</td><td align="char" char="." valign="top">4.7<break/>3<break/>1.5</td><td align="char" char="." valign="top">76</td><td align="char" char="." valign="top">20,063</td><td align="left" valign="top">0.3<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.3~0.6*0.6</td><td align="left" valign="top">0.3~0.75</td><td align="left" valign="top">Siemens,<break/>Bruker,<break/>Philips</td><td align="left" valign="top">Multicenter</td></tr><tr><td align="left" valign="top">EPI</td><td align="char" char="." valign="top">1.5</td><td align="char" char="." valign="top">58</td><td align="char" char="." valign="top">2,557</td><td align="left" valign="top">0.7<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.7~2.0*2.0</td><td align="left" valign="top">1.0~3.0</td><td align="left" valign="top">Siemens,<break/>Bruker,<break/>Philips</td><td align="left" valign="top">Multicenter</td></tr><tr><td align="left" valign="top"><bold>Sum of nonhuman primate</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="char" char="." valign="top"><bold>246</bold></td><td align="char" char="." valign="top"><bold>26,680</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Human<break/>(ABCD)</td><td align="left" valign="top">T1WI</td><td align="char" char="." valign="top">3</td><td align="char" char="." valign="top">3,250</td><td align="char" char="." valign="top">552,500</td><td align="char" char="." valign="top">1.0<xref ref-type="table-fn" rid="app1table1fn1">*</xref>1.0</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">GE<break/>Siemens<break/>Philips</td><td align="left" valign="top">Multicenter</td></tr><tr><td align="left" valign="top">Human<break/>(UK Biobank)</td><td align="left" valign="top">T1WI</td><td align="char" char="." valign="top">3</td><td align="char" char="." valign="top">963</td><td align="char" char="." valign="top">196,793</td><td align="char" char="." valign="top">1.0<xref ref-type="table-fn" rid="app1table1fn1">*</xref>1.0</td><td align="char" char="." valign="top">1</td><td align="left" valign="top">Siemens</td><td align="left" valign="top">Multicenter</td></tr><tr><td align="left" valign="top">Human<break/>(ZIB)</td><td align="left" valign="top">T1WI</td><td align="char" char="." valign="top">3</td><td align="char" char="." valign="top">388</td><td align="char" char="." valign="top">124,160</td><td align="char" char="." valign="top">0.8<xref ref-type="table-fn" rid="app1table1fn1">*</xref>0.8</td><td align="char" char="." valign="top">0.8</td><td align="left" valign="top">Siemens</td><td align="left" valign="top">FDU</td></tr><tr><td align="left" valign="top">Sum of human</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="char" char="." valign="top">4,601</td><td align="char" char="." valign="top">873,453</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>In total</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="char" char="." valign="top"><bold>5,630</bold></td><td align="char" char="." valign="top"><bold>925,755</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr></tbody></table><table-wrap-foot><fn id="app1table1fn1"><label>*</label><p><ext-link ext-link-type="uri" xlink:href="https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html">https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html</ext-link>.</p></fn><fn id="app1table1fn2"><label>†</label><p><ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002870/versions/1.0.0">https://openneuro.org/datasets/ds002870/versions/1.0.0</ext-link>.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Performance comparison of BEN with SOTA methods on the source domain (Mouse-T2WI-11.7T).</title><p>Dice: Dice score; SEN: sensitivity; SPE: specificity; ASD: Average Surface Distance; HD95: the 95-th percentile of Hausdorff distance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Method</th><th align="left" valign="top">Dice</th><th align="left" valign="top">SEN</th><th align="left" valign="top">SPE</th><th align="left" valign="top">ASD</th><th align="left" valign="top">HD95</th></tr></thead><tbody><tr><td align="left" valign="top">Sherm</td><td align="char" char="." valign="top">0.9605</td><td align="char" char="." valign="top">0.9391</td><td align="char" char="." valign="top"><bold>0.9982</bold></td><td align="char" char="." valign="top">0.6748</td><td align="char" char="." valign="top">0.4040</td></tr><tr><td align="left" valign="top">AFNI</td><td align="char" char="." valign="top">0.9093</td><td align="char" char="." valign="top">0.9162</td><td align="char" char="." valign="top">0.9894</td><td align="char" char="." valign="top">1.9346</td><td align="char" char="." valign="top">0.9674</td></tr><tr><td align="left" valign="top">FSL</td><td align="char" char="." valign="top">0.3948</td><td align="char" char="." valign="top"><bold>1.0000</bold></td><td align="char" char="." valign="top">0.6704</td><td align="char" char="." valign="top">20.4724</td><td align="char" char="." valign="top">5.5975</td></tr><tr><td align="left" valign="top">BEN</td><td align="char" char="." valign="top"><bold>0.9859</bold></td><td align="char" char="." valign="top">0.9889</td><td align="char" char="." valign="top"><bold>0.9982</bold></td><td align="char" char="." valign="top"><bold>0.3260</bold></td><td align="char" char="." valign="top"><bold>0.1436</bold></td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Performance comparison of BEN with SOTA methods on two public datasets.</title><p>Dice: Dice score; Jaccard: Jaccard Similarity; SEN: sensitivity; HD: Hausdorff distance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" colspan="2">CARMI dataset <xref ref-type="table-fn" rid="app1table3fn1">*</xref> (Rodent)</th><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">T2WI</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Methods</td><td align="left" valign="top">Dice</td><td align="left" valign="top">Jaccard</td><td align="left" valign="top">SEN</td><td align="left" valign="top">HD (voxels)</td></tr><tr><td align="left" valign="top">RATS (<xref ref-type="bibr" rid="bib34">Oguz et al., 2014</xref>)</td><td align="char" char="." valign="top">0.91</td><td align="char" char="." valign="top">0.83</td><td align="char" char="." valign="top">0.85</td><td align="char" char="." valign="top">8.76</td></tr><tr><td align="left" valign="top">PCNN (<xref ref-type="bibr" rid="bib8">Chou et al., 2011</xref>)</td><td align="char" char="." valign="top">0.89</td><td align="char" char="." valign="top">0.80</td><td align="char" char="." valign="top">0.90</td><td align="char" char="." valign="top">7.00</td></tr><tr><td align="left" valign="top">SHERM (<xref ref-type="bibr" rid="bib26">Liu et al., 2020</xref>)</td><td align="char" char="." valign="top">0.88</td><td align="char" char="." valign="top">0.79</td><td align="char" char="." valign="top">0.86</td><td align="char" char="." valign="top">6.72</td></tr><tr><td align="left" valign="top">U-Net (<xref ref-type="bibr" rid="bib19">Hsu et al., 2020</xref>)</td><td align="char" char="." valign="top">0.97</td><td align="char" char="." valign="top">0.94</td><td align="char" char="." valign="top">0.96</td><td align="char" char="." valign="top">4.27</td></tr><tr><td align="left" valign="top">BEN</td><td align="char" char="." valign="top"><bold>0.98</bold></td><td align="char" char="." valign="top"><bold>0.95</bold></td><td align="char" char="." valign="top"><bold>0.98</bold></td><td align="char" char="." valign="top"><bold>2.72</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">EPI</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Methods</td><td align="left" valign="top">Dice</td><td align="left" valign="top">Jaccard</td><td align="left" valign="top">SEN</td><td align="left" valign="top">HD (voxels)</td></tr><tr><td align="left" valign="top">RATS (<xref ref-type="bibr" rid="bib34">Oguz et al., 2014</xref>)</td><td align="char" char="." valign="top">0.86</td><td align="char" char="." valign="top">0.75</td><td align="char" char="." valign="top">0.75</td><td align="char" char="." valign="top">7.68</td></tr><tr><td align="left" valign="top">PCNN (<xref ref-type="bibr" rid="bib8">Chou et al., 2011</xref>)</td><td align="char" char="." valign="top">0.85</td><td align="char" char="." valign="top">0.74</td><td align="char" char="." valign="top">0.93</td><td align="char" char="." valign="top">8.25</td></tr><tr><td align="left" valign="top">SHERM (<xref ref-type="bibr" rid="bib26">Liu et al., 2020</xref>)</td><td align="char" char="." valign="top">0.80</td><td align="char" char="." valign="top">0.67</td><td align="char" char="." valign="top">0.78</td><td align="char" char="." valign="top">7.14</td></tr><tr><td align="left" valign="top">U-Net (<xref ref-type="bibr" rid="bib19">Hsu et al., 2020</xref>)</td><td align="char" char="." valign="top">0.96</td><td align="char" char="." valign="top">0.93</td><td align="char" char="." valign="top">0.96</td><td align="char" char="." valign="top">4.60</td></tr><tr><td align="left" valign="top">BEN</td><td align="char" char="." valign="top"><bold>0.97</bold></td><td align="char" char="." valign="top"><bold>0.94</bold></td><td align="char" char="." valign="top"><bold>0.98</bold></td><td align="char" char="." valign="top"><bold>4.20</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top" colspan="2">PRIME-DE <xref ref-type="table-fn" rid="app1table3fn2">†</xref> (Macaque)</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">T1WI</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Methods</td><td align="left" valign="top">Dice</td><td align="left" valign="top">Jaccard</td><td align="left" valign="top">SEN</td><td align="left" valign="top">HD (voxels)</td></tr><tr><td align="left" valign="top">FSL</td><td align="char" char="." valign="top">0.81</td><td align="char" char="." valign="top">0.71</td><td align="char" char="." valign="top">0.96</td><td align="char" char="." valign="top">32.38</td></tr><tr><td align="left" valign="top">FreeSurfer</td><td align="char" char="." valign="top">0.56</td><td align="char" char="." valign="top">0.39</td><td align="char" char="." valign="top"><bold>0.99</bold></td><td align="char" char="." valign="top">42.18</td></tr><tr><td align="left" valign="top">AFNI</td><td align="char" char="." valign="top">0.86</td><td align="char" char="." valign="top">0.79</td><td align="char" char="." valign="top">0.82</td><td align="char" char="." valign="top">25.46</td></tr><tr><td align="left" valign="top">U-Net (<xref ref-type="bibr" rid="bib42">Wang et al., 2021</xref>)</td><td align="char" char="." valign="top">0.98</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td></tr><tr><td align="left" valign="top">BEN</td><td align="char" char="." valign="top"><bold>0.98</bold></td><td align="char" char="." valign="top">0.94</td><td align="char" char="." valign="top">0.98</td><td align="char" char="." valign="top"><bold>13.21</bold></td></tr></tbody></table><table-wrap-foot><fn id="app1table3fn1"><label>*</label><p><ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002870/versions/1.0.0">https://openneuro.org/datasets/ds002870/versions/1.0.0</ext-link>.</p></fn><fn id="app1table3fn2"><label>†</label><p><ext-link ext-link-type="uri" xlink:href="https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html">https://fcon_1000.projects.nitrc.org/indi/indiPRIME.html</ext-link>.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Ablation study of each module of BEN in the source domain.</title><p>(a) training with all labeled data using U-Net. The backbone of BEN is non-local U-Net (NL-U-Net). (b) training with 5% labeled data. (c) training with 5% labeled data using BEN’s semi-supervised learning module (SSL). The remaining 95% of the unlabeled data is also used for the training. Since this ablation study is performed on the source domain, the adaptive batch normalization (AdaBN) module is not used. Dice: Dice score; SEN: sensitivity; SPE: specificity; HD95: the 95-th percentile of Hausdorff distance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2">Method</th><th align="left" valign="top" colspan="2">Scans used</th><th align="left" valign="top" colspan="4">Metrics</th></tr><tr><th align="left" valign="top">Labeled</th><th align="left" valign="top">Unlabeled</th><th align="left" valign="top">Dice</th><th align="left" valign="top">SEN</th><th align="left" valign="top">SPE</th><th align="left" valign="top">HD95</th></tr></thead><tbody><tr><td align="left" valign="top"><sup>a</sup>U-Net</td><td align="char" char="." valign="top">243</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.9773</td><td align="char" char="." valign="top">0.9696</td><td align="char" char="." valign="top"><bold>0.9984</bold></td><td align="char" char="." valign="top">0.2132</td></tr><tr><td align="left" valign="top"><sup>a</sup>Backbone</td><td align="char" char="." valign="top">243</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top"><bold>0.9844</bold></td><td align="char" char="." valign="top"><bold>0.9830</bold></td><td align="char" char="." valign="top"><bold>0.9984</bold></td><td align="char" char="." valign="top"><bold>0.0958</bold></td></tr><tr><td align="left" valign="top"><sup>b</sup>U-Net</td><td align="char" char="." valign="top">12</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.9588</td><td align="char" char="." valign="top">0.9546</td><td align="char" char="." valign="top">0.9945</td><td align="char" char="." valign="top">1.1388</td></tr><tr><td align="left" valign="top"><sup>b</sup>Backbone</td><td align="char" char="." valign="top">12</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.9614</td><td align="char" char="." valign="top">0.9679</td><td align="char" char="." valign="top"><bold>0.9970</bold></td><td align="char" char="." valign="top">0.7468</td></tr><tr><td align="left" valign="top"><sup>c</sup>Backbone +SSL</td><td align="char" char="." valign="top">12</td><td align="char" char="." valign="top">231</td><td align="char" char="." valign="top"><bold>0.9728</bold></td><td align="char" char="." valign="top"><bold>0.9875</bold></td><td align="char" char="." valign="top">0.9952</td><td align="char" char="." valign="top"><bold>0.2937</bold></td></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Ablation study of each module of BEN in the target domain.</title><p>(a) training from scratch with all labeled data. The backbone of BEN is non-local U-Net (NL-U-Net). (b) training from scratch with 5% labeled data. (c) fine-tuning (using pretrained weights) with 5% labeled data. (d) fine-tuning with 5% labeled data using BEN’s SSL and AdaBN modules. The remaining 95% of the unlabeled data is also used for the training stage. Dice: Dice score; SEN: sensitivity; SPE: specificity; HD95: the 95-th percentile of Hausdorff distance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2">Method</th><th align="left" valign="top" rowspan="2">Pretrained</th><th align="left" valign="top" colspan="2">Scans used</th><th align="left" valign="top" colspan="4">Metrics</th></tr><tr><th align="left" valign="top">Labeled</th><th align="left" valign="top">Unlabeled</th><th align="left" valign="top">Dice</th><th align="left" valign="top">SEN</th><th align="left" valign="top">SPE</th><th align="left" valign="top">HD95</th></tr></thead><tbody><tr><td align="left" valign="top"><sup>a</sup>Backbone<break/>(from scratch)</td><td align="left" valign="top"/><td align="char" char="." valign="top">132</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top"><bold>0.9827</bold></td><td align="char" char="." valign="top">0.9841</td><td align="char" char="." valign="top">0.9987</td><td align="char" char="." valign="top"><bold>0.1881</bold></td></tr><tr><td align="left" valign="top"><sup>b</sup>Backbone<break/>(from scratch)</td><td align="left" valign="top"/><td align="char" char="." valign="top">7</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.8990</td><td align="char" char="." valign="top">0.8654</td><td align="char" char="." valign="top">0.9960</td><td align="char" char="." valign="top">4.6241</td></tr><tr><td align="left" valign="top"><sup>c</sup>Backbone</td><td align="left" valign="top">✓</td><td align="char" char="." valign="top">7</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.9483</td><td align="char" char="." valign="top">0.9063</td><td align="char" char="." valign="top"><bold>0.9997</bold></td><td align="char" char="." valign="top">0.6563</td></tr><tr><td align="left" valign="top"><sup>c</sup>Backbone +AdaBN</td><td align="left" valign="top">✓</td><td align="char" char="." valign="top">7</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.9728</td><td align="char" char="." valign="top"><bold>0.9875</bold></td><td align="char" char="." valign="top">0.9952</td><td align="char" char="." valign="top">0.2937</td></tr><tr><td align="left" valign="top"><sup>d</sup>Backbone +SSL</td><td align="left" valign="top">✓</td><td align="char" char="." valign="top">7</td><td align="char" char="." valign="top">125</td><td align="char" char="." valign="top">0.9614</td><td align="char" char="." valign="top">0.9679</td><td align="char" char="." valign="top">0.9970</td><td align="char" char="." valign="top">0.7468</td></tr><tr><td align="left" valign="top"><sup>d</sup>Backbone +AdaBN + SSL</td><td align="left" valign="top">✓</td><td align="char" char="." valign="top">7</td><td align="char" char="." valign="top">125</td><td align="char" char="." valign="top"><bold>0.9779</bold></td><td align="char" char="." valign="top">0.9763</td><td align="char" char="." valign="top">0.9986</td><td align="char" char="." valign="top"><bold>0.2912</bold></td></tr></tbody></table></table-wrap><table-wrap id="app1table6" position="float"><label>Appendix 1—table 6.</label><caption><title>BEN provides interfaces for the following conventional neuroimaging software.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Name</th><th align="left" valign="bottom">Link</th></tr></thead><tbody><tr><td align="left" valign="bottom">AFNI</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://afni.nimh.nih.gov/">https://afni.nimh.nih.gov/</ext-link></td></tr><tr><td align="left" valign="bottom">ANTs</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://stnava.github.io/ANTs/">http://stnava.github.io/ANTs/</ext-link></td></tr><tr><td align="left" valign="bottom">FSL</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki</ext-link></td></tr><tr><td align="left" valign="bottom">FreeSurfer</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://freesurfer.net/">https://freesurfer.net/</ext-link></td></tr><tr><td align="left" valign="bottom">SPM</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link></td></tr><tr><td align="left" valign="bottom">Nipype</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/nipype/">https://pypi.org/project/nipype/</ext-link></td></tr></tbody></table></table-wrap><table-wrap id="app1table7" position="float"><label>Appendix 1—table 7.</label><caption><title>Protocols and parameters used for conventional neuroimaging toolboxes.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" colspan="2">Method</th><th align="left" valign="top">Command</th><th align="left" valign="top">Parameter</th><th align="left" valign="top">Description</th><th align="left" valign="top">Range</th><th align="left" valign="top">Chosen value</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="3">AFNI</td><td align="left" valign="top" rowspan="3" colspan="2">3dSkullStrip</td><td align="char" char="." valign="top">-marmoset</td><td align="left" valign="top">Brain of a marmoset</td><td align="left" valign="top">on/off</td><td align="left" valign="top">on for marmoset</td></tr><tr><td align="char" char="." valign="top">-rat</td><td align="left" valign="top">Brain of a rat</td><td align="left" valign="top">on/off</td><td align="left" valign="top">on for rodent</td></tr><tr><td align="char" char="." valign="top">-monkey</td><td align="left" valign="top">Brain of a monkey</td><td align="left" valign="top">on/off</td><td align="left" valign="top">on for macaque</td></tr><tr><td align="left" valign="top" rowspan="4">FreeSurfer</td><td align="left" valign="top" rowspan="4" colspan="2">mri_watershed</td><td align="char" char="." valign="top">-T1</td><td align="left" valign="top">Specify T1 input volume</td><td align="left" valign="top">on/off</td><td align="left" valign="top">on</td></tr><tr><td align="char" char="." valign="top">-r</td><td align="left" valign="top">Specify the radius of the brain (in voxel unit)</td><td align="left" valign="top">positive number</td><td align="char" char="." valign="top">60</td></tr><tr><td align="char" char="." valign="top">-less</td><td align="left" valign="top">Shrink the surface</td><td align="left" valign="top">on/off</td><td align="left" valign="top">off</td></tr><tr><td align="char" char="." valign="top">-more</td><td align="left" valign="top">Expand the surface</td><td align="left" valign="top">on/off</td><td align="left" valign="top">off</td></tr><tr><td align="left" valign="top" rowspan="3">FSL</td><td align="left" valign="top" rowspan="3" colspan="2">bet2</td><td align="char" char="." valign="top">-f</td><td align="left" valign="top">Fractional intensity threshold</td><td align="left" valign="top">0.1~0.9</td><td align="char" char="." valign="top">0.5</td></tr><tr><td align="char" char="." valign="top">-m</td><td align="left" valign="top">Generate binary brain mask</td><td align="left" valign="top">on/off</td><td align="left" valign="top">on</td></tr><tr><td align="char" char="." valign="top">-n</td><td align="left" valign="top">Don't generate the default brain image output</td><td align="left" valign="top">on/off</td><td align="left" valign="top">on</td></tr><tr><td align="left" valign="top" rowspan="2">Sherm</td><td align="left" valign="top" rowspan="2" colspan="2">sherm</td><td align="char" char="." valign="top">-animal</td><td align="left" valign="top">Species of the task</td><td align="left" valign="top">'rat' or 'mouse'</td><td align="left" valign="top">according to the task</td></tr><tr><td align="char" char="." valign="top">-isotropic</td><td align="left" valign="top">Characteristics of voxels</td><td align="char" char="." valign="top">0/1</td><td align="char" char="." valign="top">0</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81217.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jbabdi</surname><given-names>Saad</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.05.25.492956" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.05.25.492956"/></front-stub><body><p>This article is an important contribution to the field of neuroimaging. The paper proposes a deep neural network for brain extraction and an approach to training the network that generalises across domains, including species, scanners, and MRI sequences. The authors provide convincing evidence that their approach works for a varied set of data, protocols, and species.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81217.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Jbabdi</surname><given-names>Saad</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Robinson</surname><given-names>Emma C</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220mzb33</institution-id><institution>King's College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Lerch</surname><given-names>Jason P</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.05.25.492956">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.05.25.492956v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A generalizable brain extraction net (BEN) for multimodal MRI data from rodents, nonhuman primates, and humans&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Floris de Lange as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Emma C. Robinson, PhD (Reviewer #1); Jason P Lerch (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The one major point raised by reviewer 2 appears to me to be the most important to properly address, as it appears the method did not work well on the reviewer's own data, casting doubt on the generalisability of the approach- the main selling point of the paper.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I recommend that the paper is largely ready for publication in its current form.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The major point I'd like to see the authors discuss is when BEN needs to be retrained on different input data and discuss techniques to improve generalizability. In the examples given and weights provided they suggest that, for example, 7T and 9.4T T2w mouse data needs different networks. This is somewhat surprising to me and suggests that the networks might be overfitting to their input data. My own tests (as described in the public review) also suggest that even subtle changes to out-of-sample data quickly degrade performance.</p><p>Secondly, I find that the narrative in places overstates the importance of their work, primarily since in my opinion the community has created multiple brain masking algorithms in different species that work well. Three examples include:</p><p>1) Line 17: the claim that brain extraction in animals is not fully automated; the relatively simpler brains, especially in rodents, means that image registration-based approaches to segmenting brains is quite successful and has been implemented in multiple toolkits. Similarly, the claim that the performance of registration-based methods is limited is at odds with the data.</p><p>2) It is not clear to me why the authors would expect FSL or FreeSurfer to work on rodents out of the box, given that the algorithms were never tuned for non-human brains (as far as I am aware). Their inclusion for animal brain segmentation tasks thus appears to be a bit of a straw man.</p><p>3) I also found Figure 7 and the related arguments about why BEN is necessary a bit odd; any decent registration/segmentation pipeline would incorporate brain masking, so the comparison of with and without masking is also a false contrast. There are lots of interesting ideas in this manuscript that it does not need these types of strawman arguments, so I would suggest removing this section entirely or alternately comparing the inclusion of BEN for masking as compared to alternate pipelines with masking included as well.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81217.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The one major point raised by reviewer 2 appears to me to be the most important to properly address, as it appears the method did not work well on the reviewer's own data, casting doubt on the generalisability of the approach- the main selling point of the paper.</p></disp-quote><p>We thank the Editors very much. These comments are very encouraging, valuable, and constructive. We have carefully revised our manuscript based on the comments of the reviewers. Please refer to our response to the reviewers for details.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The major point I'd like to see the authors discuss is when BEN needs to be retrained on different input data and discuss techniques to improve generalizability. In the examples given and weights provided they suggest that, for example, 7T and 9.4T T2w mouse data needs different networks. This is somewhat surprising to me and suggests that the networks might be overfitting to their input data. My own tests (as described in the public review) also suggest that even subtle changes to out-of-sample data quickly degrade performance.</p></disp-quote><p>We thank the reviewer for the helpful comment and raise important aspects we have addressed in our paper and Github codes. The adjustments are listed as follows:</p><p>1. We have discussed briefly when BEN needs to be retrained in the second paragraph in “Discussion – Compatibility with other toolboxes and deployment”. In the revised version, we have updated the descriptions to make it clear, and also provide several video tutorials (using public ex vivo data to demonstrate). Since the initial version of BEN is intended to reproduce our results in paper, some corner cases and complex cases were not well taken into consideration. These concerns have now been well addressed in updated codes (https://github.com/yu02019/BEN).</p><p>2. The techniques for increasing generalizability have also been added to “Discussion” and BEN pipeline, e.g., orientation detection, post-processing, etc.</p><p>3. As for experiments across 7T and 9.4T T2w, BEN could adapt to these out-of-domain images without labels (zero-shot learning) , as the domain shift between 7T and 9.4T is relatively small. The quantitative results for these cross-MR scanners with various magnetic field strengths are presented in Figure 3—figure supplement 2. Without additional labels (zero-shot), BEN presents satisfactory performance on two of the three tasks, while the other two baseline methods all fail on all three tasks. In fact, we deem this to be essentially a cross-center task that disables the performances of many deep learning-based methods; BEN addresses and alleviates this problem using the &quot;domain adaptation module&quot; and packages it well for the general user without coding skills. Alternatively, one can train a “super-network” with big training data across different species, modalities and magnetic strengths so the network can be used in an ‘out-of-the-box' fashion. But this is difficult in practice as the animal MR experiments are very diverse in nature and there is always some “out-of-sample” testing data.</p><p>4. Besides, the original intent for cross-field strength experiments was to demonstrate BEN could provide a fast and label-efficient domain adaptation training method as a scalable tool. We can certainly provide joint-training weights that could easily meet the reviewer’s requirement. However, when deploying BEN or other toolboxes into customized data/cohorts, it is inevitable to address domain adaptation issues, suggesting the importance of our model design.</p><p>5. How to deal with out-of-sample data is a challenge for many deep learning methods. To the best of our knowledge, it is very difficult for almost all existing methods to handle the images with low quality (artifacts, field inhomogeneity, high noise, or low SNR). To solve this issue, in our model, we suggest adding several exemplary MR scans to retrain BEN; on the other hand, it could be addressed partly by post-processing or the “Monte Carlo quality assessment module” in the BEN pipeline. We will give some examples in our tutorials.</p><disp-quote content-type="editor-comment"><p>Secondly, I find that the narrative in places overstates the importance of their work, primarily since in my opinion the community has created multiple brain masking algorithms in different species that work well. Three examples include:</p><p>1) Line 17: the claim that brain extraction in animals is not fully automated; the relatively simpler brains, especially in rodents, means that image registration-based approaches to segmenting brains is quite successful and has been implemented in multiple toolkits. Similarly, the claim that the performance of registration-based methods is limited is at odds with the data.</p></disp-quote><p>We thank the reviewer for raising this issue. We think the success of automatic registration-based approaches depends on the quality and the number of atlases (e.g. multi-atlas registration is usually better than single-atlas one), and registration is not an easy task as the heterogeneous contrasts existing in different image spaces (e.g., native space and atlas space) might not provide enough guidance for intensity-based registration metrics. Due to the scarcity of publicly available MRI data and multi-atlases, it is generally more difficult in animal MR studies than human ones where multi-atlas registration is well established.</p><p>Alternatively, some semi-automatic registration-based approaches using one dataset-specific template atlas which have to be manually labeled (then it has similar or identical experimental conditions, MR parameters, and image properties) present better and more stable performance on the current experimental cohort than fully automated methods using public atlas from different cohorts or imaging centers. There still remain limitations to semi-automatic registration-based methods: (1) If registration-based methods fail, it’s hard to adjust, thus requiring laborious manual corrections. (2) The dataset-specific template mask is usually manually annotated, which is another time-consuming step in addition to the registration. (3) Based on our results (<xref ref-type="table" rid="sa2table1">Author response table 1</xref>), registration-based methods performed unsatisfactory on fMRI data and scans with thick slice thickness.</p><table-wrap id="sa2table1" position="float"><label>Author response table 1.</label><caption><title>Comparison of brain extraction performance of different methods on different datasets.</title><p>SkullStrip: semi-automatic registration-based method. (ASD: average surface distance. HD95: 95% Hausdorff distance).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Species</th><th align="left" valign="top">Field</th><th align="left" valign="top">Modality</th><th align="left" valign="top">Method</th><th align="left" valign="top">Dice</th><th align="left" valign="top">Sensitivity</th><th align="left" valign="top">Specificity</th><th align="left" valign="top">ASD</th><th align="left" valign="top">HD95</th></tr></thead><tbody><tr><td align="left" valign="top">Mouse</td><td align="left" valign="top">11.7T</td><td align="left" valign="top">T2WI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9859</bold></td><td align="left" valign="top">0.9889</td><td align="left" valign="top"><bold>0.9982</bold></td><td align="left" valign="top"><bold>0.3260</bold></td><td align="left" valign="top"><bold>0.1436</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.9605</td><td align="left" valign="top">0.9391</td><td align="left" valign="top">0.9982</td><td align="left" valign="top">0.6748</td><td align="left" valign="top">0.4040</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.9093</td><td align="left" valign="top">0.9162</td><td align="left" valign="top">0.9894</td><td align="left" valign="top">1.9346</td><td align="left" valign="top">0.9674</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9697</td><td align="left" valign="top">0.9783</td><td align="left" valign="top">0.9957</td><td align="left" valign="top">0.4729</td><td align="left" valign="top">0.2829</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">FSL</td><td align="left" valign="top">0.3948</td><td align="left" valign="top"><bold>1.0000</bold></td><td align="left" valign="top">0.6704</td><td align="left" valign="top">20.4724</td><td align="left" valign="top">5.5975</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">EPI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9791</bold></td><td align="left" valign="top"><bold>0.9912</bold></td><td align="left" valign="top">0.9970</td><td align="left" valign="top">0.5945</td><td align="left" valign="top">0.4946</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.9440</td><td align="left" valign="top">0.9206</td><td align="left" valign="top"><bold>0.9971</bold></td><td align="left" valign="top"><bold>0.4827</bold></td><td align="left" valign="top"><bold>0.4896</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.9139</td><td align="left" valign="top">0.9365</td><td align="left" valign="top">0.9890</td><td align="left" valign="top">0.7835</td><td align="left" valign="top">0.7315</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9237</td><td align="left" valign="top">0.9502</td><td align="left" valign="top">0.9899</td><td align="left" valign="top">0.8570</td><td align="left" valign="top">0.5673</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SWI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9879</bold></td><td align="left" valign="top"><bold>0.9912</bold></td><td align="left" valign="top">0.9979</td><td align="left" valign="top"><bold>0.4548</bold></td><td align="left" valign="top"><bold>0.2420</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.9586</td><td align="left" valign="top">0.9342</td><td align="left" valign="top"><bold>0.9983</bold></td><td align="left" valign="top">0.4633</td><td align="left" valign="top">0.4019</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.9060</td><td align="left" valign="top">0.8631</td><td align="left" valign="top">0.9950</td><td align="left" valign="top">0.8632</td><td align="left" valign="top">0.5522</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9600</td><td align="left" valign="top">0.9590</td><td align="left" valign="top">0.9954</td><td align="left" valign="top">0.4777</td><td align="left" valign="top">0.3797</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">ASL</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top">0.<bold>9807</bold></td><td align="left" valign="top"><bold>0.9804</bold></td><td align="left" valign="top"><bold>0.9966</bold></td><td align="left" valign="top"><bold>0.1766</bold></td><td align="left" valign="top"><bold>0.2679</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0.9938</td><td align="left" valign="top">3.5669</td><td align="left" valign="top">10.6817</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.7584</td><td align="left" valign="top">0.9379</td><td align="left" valign="top">0.9324</td><td align="left" valign="top">4.3252</td><td align="left" valign="top">2.3752</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.8893</td><td align="left" valign="top">0.9177</td><td align="left" valign="top">0.9737</td><td align="left" valign="top">0.7351</td><td align="left" valign="top">0.7029</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top">9.4T</td><td align="left" valign="top">T2WI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9830</bold></td><td align="left" valign="top"><bold>0.9903</bold></td><td align="left" valign="top"><bold>0.9954</bold></td><td align="left" valign="top"><bold>0.5129</bold></td><td align="left" valign="top"><bold>0.3284</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.8675</td><td align="left" valign="top">0.7893</td><td align="left" valign="top">0.9951</td><td align="left" valign="top">1.7973</td><td align="left" valign="top">1.4372</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.8699</td><td align="left" valign="top">0.9532</td><td align="left" valign="top">0.9548</td><td align="left" valign="top">2.2979</td><td align="left" valign="top">1.2606</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9298</td><td align="left" valign="top">0.9683</td><td align="left" valign="top">0.9750</td><td align="left" valign="top">1.5024</td><td align="left" valign="top">0.8822</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">EPI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9535</bold></td><td align="left" valign="top">0.9567</td><td align="left" valign="top"><bold>0.9920</bold></td><td align="left" valign="top"><bold>0.5686</bold></td><td align="left" valign="top">0.5021</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.9368</td><td align="left" valign="top">0.9394</td><td align="left" valign="top">0.9901</td><td align="left" valign="top">0.5956</td><td align="left" valign="top"><bold>0.4370</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.8855</td><td align="left" valign="top"><bold>0.9585</bold></td><td align="left" valign="top">0.9650</td><td align="left" valign="top">0.9960</td><td align="left" valign="top">0.6349</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9249</td><td align="left" valign="top">0.9613</td><td align="left" valign="top">0.9810</td><td align="left" valign="top">0.7440</td><td align="left" valign="top">0.6718</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top">7T</td><td align="left" valign="top">T2WI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9815</bold></td><td align="left" valign="top"><bold>0.9682</bold></td><td align="left" valign="top"><bold>0.9951</bold></td><td align="left" valign="top"><bold>0.4272</bold></td><td align="left" valign="top"><bold>0.2595</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.8436</td><td align="left" valign="top">0.9423</td><td align="left" valign="top">0.9596</td><td align="left" valign="top">4.3723</td><td align="left" valign="top">1.6113</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.9123</td><td align="left" valign="top">0.9423</td><td align="left" valign="top">0.9910</td><td align="left" valign="top">2.4293</td><td align="left" valign="top">0.8190</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.8599</td><td align="left" valign="top">0.9423</td><td align="left" valign="top">0.9703</td><td align="left" valign="top">1.5989</td><td align="left" valign="top">0.9821</td></tr><tr><td align="left" valign="top">Rat</td><td align="left" valign="top">7T</td><td align="left" valign="top">T2WI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9854</bold></td><td align="left" valign="top"><bold>0.9878</bold></td><td align="left" valign="top">0.9966</td><td align="left" valign="top"><bold>0.4688</bold></td><td align="left" valign="top"><bold>0.1790</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.9532</td><td align="left" valign="top">0.9314</td><td align="left" valign="top">0.9956</td><td align="left" valign="top">0.8428</td><td align="left" valign="top">1.0463</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.8285</td><td align="left" valign="top">0.7232</td><td align="left" valign="top"><bold>0.9969</bold></td><td align="left" valign="top">2.8672</td><td align="left" valign="top">2.7561</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">FSL</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9754</td><td align="left" valign="top">0.9813</td><td align="left" valign="top">0.9939</td><td align="left" valign="top">0.5015</td><td align="left" valign="top">0.4158</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">EPI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9705</bold></td><td align="left" valign="top">0.9415</td><td align="left" valign="top">0.9966</td><td align="left" valign="top"><bold>0.2858</bold></td><td align="left" valign="top"><bold>0.7081</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">0.6339</td><td align="left" valign="top">0.4645</td><td align="left" valign="top"><bold>0.9998</bold></td><td align="left" valign="top">1.2283</td><td align="left" valign="top">3.4795</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.8080</td><td align="left" valign="top">0.7151</td><td align="left" valign="top">0.9896</td><td align="left" valign="top">0.9018</td><td align="left" valign="top">4.2521</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9394</td><td align="left" valign="top"><bold>0.9500</bold></td><td align="left" valign="top">0.9869</td><td align="left" valign="top">0.4161</td><td align="left" valign="top">0.7991</td></tr><tr><td align="left" valign="top">Marmoset</td><td align="left" valign="top">9.4T</td><td align="left" valign="top">T2WI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9804</bold></td><td align="left" valign="top">0.9788</td><td align="left" valign="top"><bold>0.9957</bold></td><td align="left" valign="top">0.5568</td><td align="left" valign="top">5.7393</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.9311</td><td align="left" valign="top">0.9526</td><td align="left" valign="top">0.9803</td><td align="left" valign="top">1.1744</td><td align="left" valign="top">9.7897</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9755</td><td align="left" valign="top"><bold>0.9789</bold></td><td align="left" valign="top">0.9943</td><td align="left" valign="top"><bold>0.4005</bold></td><td align="left" valign="top"><bold>3.8443</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">FSL</td><td align="left" valign="top">0.7689</td><td align="left" valign="top">0.8126</td><td align="left" valign="top">0.9552</td><td align="left" valign="top">2.0939</td><td align="left" valign="top">5.1353</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">EPI</td><td align="left" valign="top"><bold>BEN</bold></td><td align="left" valign="top"><bold>0.9774</bold></td><td align="left" valign="top"><bold>0.9816</bold></td><td align="left" valign="top">0.9949</td><td align="left" valign="top">0.9126</td><td align="left" valign="top">3.500</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">Sherm</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td><td align="left" valign="top">-</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">AFNI</td><td align="left" valign="top">0.9159</td><td align="left" valign="top">0.9142</td><td align="left" valign="top">0.9847</td><td align="left" valign="top">0.8314</td><td align="left" valign="top">5.1391</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">SkullStrip</td><td align="left" valign="top">0.9533</td><td align="left" valign="top">0.9286</td><td align="left" valign="top"><bold>0.9965</bold></td><td align="left" valign="top"><bold>0.4489</bold></td><td align="left" valign="top"><bold>3.0342</bold></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">FSL</td><td align="left" valign="top">0.9326</td><td align="left" valign="top">0.9748</td><td align="left" valign="top">0.9757</td><td align="left" valign="top">1.0846</td><td align="left" valign="top">8.7247</td></tr></tbody></table></table-wrap><p>We have conducted comparisons using such semi-automatic registration-based approaches, “SkullStrip” [1] (based on NiftyReg). The results are presented in <xref ref-type="table" rid="sa2table1">Author response table 1</xref> as follows. As a conclusion, though [1] could perform well in some cases, it seems that its performance suffers from functional MR data and scans with thick slice thickness (e.g., Mouse-ASL-11.7T and Mouse-T2WI-9.4T images, etc.). Besides, BEN’s results show a much lower HD95 value, which means higher boundary agreement with the ground truth. One more point we want to emphasize is that the computational speed for such registration-based methods (which take hours for one small cohort) is much slower than BEN (which only takes several minutes).</p><p>Moreover, unlike registration-based methods (it’s hard to adjust when dealing with failed segmentations), BEN could improve its output by updating annotations of failed cases, which is like a “human-in-the-loop” manner. In addition, BEN‘s clear running logs and interactive training procedures would further provide more information for researchers.</p><p>[1] Delora A, Gonzales A, Medina C S, et al. A simple rapid process for semi-automated brain extraction from magnetic resonance images of the whole mouse head[J]. Journal of neuroscience methods, 2016, 257: 185-193.</p><disp-quote content-type="editor-comment"><p>2) It is not clear to me why the authors would expect FSL or FreeSurfer to work on rodents out of the box, given that the algorithms were never tuned for non-human brains (as far as I am aware). Their inclusion for animal brain segmentation tasks thus appears to be a bit of a straw man.</p></disp-quote><p>We thank the reviewer for this comment. We know that FSL/FreeSurfer are designed for human beings, and are not designed for rodents. Here we include these tools just for parallel comparison (otherwise we do not have enough comparison), not criticizing these well-established tools. Since such publicly available animal neuroimaging tools are scarce, we include these tools in our consideration of the influence and seminal role of these tools, and it’s impractical to find a more suitable substitute for each species and modality. Indeed, we focus on the comparison of BEN and FSL/FreeSurfer in human brain performance (Figure 4 E, J and O and Figure 5—figure supplement 3). As BEN could achieve competitive performance on human brains compared with FSL/FreeSurfer and the transferability of BEN made it not bound to specific species or modality, readers can easily deploy BEN on their customized dataset, without the requirement of complex programming skills or mathematics knowledge.</p><disp-quote content-type="editor-comment"><p>3) I also found Figure 7 and the related arguments about why BEN is necessary a bit odd; any decent registration/segmentation pipeline would incorporate brain masking, so the comparison of with and without masking is also a false contrast. There are lots of interesting ideas in this manuscript that it does not need these types of strawman arguments, so I would suggest removing this section entirely or alternately comparing the inclusion of BEN for masking as compared to alternate pipelines with masking included as well.</p></disp-quote><p>We thank the reviewer for raising this valuable point. We have moved Figure 7 into supporting materials (Figure 5—figure supplement 1) and revised it (BEN vs AFNI-rats) in the revised version. The reason why we did not remove this figure is that we think this figure is an important example application to show the impact of BEN on downstream analysis. We agree that for human studies, brain masking is already integrated in the standard MR brain image processing pipeline, e.g. FSL or Freesurfer. Yet in animal studies, there are no standardized pipelines for decent animal brain segmentation/registration. Therefore, it is essential to show the strength of BEN in improving downstream analysis and its potential to be incorporated into a recommended animal MR brain processing pipeline.</p></body></sub-article></article>