<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">74500</article-id><article-id pub-id-type="doi">10.7554/eLife.74500</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Monkey plays Pac-Man with compositional strategies and hierarchical decision-making</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-166762"><name><surname>Yang</surname><given-names>Qianli</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4226-2319</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-84203"><name><surname>Lin</surname><given-names>Zhongqiao</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-242333"><name><surname>Zhang</surname><given-names>Wenyi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-256074"><name><surname>Li</surname><given-names>Jianshu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-256075"><name><surname>Chen</surname><given-names>Xiyuan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-256076"><name><surname>Zhang</surname><given-names>Jiaqi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1649-3378</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-79734"><name><surname>Yang</surname><given-names>Tianming</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6976-9246</contrib-id><email>tyang@ion.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vpwhm04</institution-id><institution>Institute of Neuroscience, Key Laboratory of Primate Neurobiology, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0551a0y31</institution-id><institution>Shanghai Center for Brain Science and Brain-Inspired Intelligence Technology</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>03</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e74500</elocation-id><history><date date-type="received" iso-8601-date="2021-10-07"><day>07</day><month>10</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-03-13"><day>13</day><month>03</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-10-04"><day>04</day><month>10</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.10.02.462713"/></event></pub-history><permissions><copyright-statement>© 2022, Yang et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Yang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-74500-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-74500-figures-v2.pdf"/><abstract><p>Humans can often handle daunting tasks with ease by developing a set of strategies to reduce decision-making into simpler problems. The ability to use heuristic strategies demands an advanced level of intelligence and has not been demonstrated in animals. Here, we trained macaque monkeys to play the classic video game Pac-Man. The monkeys’ decision-making may be described with a strategy-based hierarchical decision-making model with over 90% accuracy. The model reveals that the monkeys adopted the take-the-best heuristic by using one dominating strategy for their decision-making at a time and formed compound strategies by assembling the basis strategies to handle particular game situations. With the model, the computationally complex but fully quantifiable Pac-Man behavior paradigm provides a new approach to understanding animals’ advanced cognition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cognition</kwd><kwd>decision making</kwd><kwd>behavior modeling</kwd><kwd>strategy</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>National Science and Technology Innovation 2030 Major Program</institution></institution-wrap></funding-source><award-id>2021ZD0203701</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Tianming</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002367</institution-id><institution>Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>XDB32070100</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Tianming</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Shanghai Municipal Science and Technology Major Project</institution></institution-wrap></funding-source><award-id>2018SHZDZX05</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Tianming</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32100832</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Qianli</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Macaque monkeys play Pac-Man with strategy-based hierarchical decision-making, a cognitive capacity hitherto unknown in them.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Our lives are full of ambitious goals to achieve. Often, the goals we set out to accomplish are complex. For it to be acquiring a life with financial stability or winning the heart of your love of life, these ambitious goals are often beyond the reach of any straightforward decision-making tactics. They, however, may be approached with a specific and elaborate set of basis strategies. With each strategy, individuals may prioritize their gains and risks according to the current situation and solve the decision-making within a smaller scope. As we live in a dynamic world that presents us with unexpected disturbances, it is also crucial to have the flexibility to alter our course of strategies accordingly. Additionally, the basis strategies can be pieced together and combined into compound strategies to reach grander goals.</p><p>For animals living in nature, the ability to flexibly formulate strategies for a complex goal is equally, if not more, crucial in their lives. Many have shown that animals exhibit complex strategy-like behaviors (<xref ref-type="bibr" rid="bib1">Beran et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Bird and Emery, 2009</xref>; <xref ref-type="bibr" rid="bib6">Brotcorne et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Gruber et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Leca et al., 2021</xref>; <xref ref-type="bibr" rid="bib20">Loukola et al., 2017</xref>; <xref ref-type="bibr" rid="bib27">Reinhold et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Sabbatini et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Sanz et al., 2010</xref>), but quantitative studies are lacking. Moreover, despite the continuing effort in studying complex behavior in animals and the underlying neural mechanisms (<xref ref-type="bibr" rid="bib13">Haroush and Williams, 2015</xref>; <xref ref-type="bibr" rid="bib16">Kira et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Ong et al., 2021</xref>; <xref ref-type="bibr" rid="bib41">Yoo et al., 2020</xref>), the level of complexity of the existing animal behavioral paradigms is insufficient for studying how animals manage strategies to simplify a sophisticated task. A sufficiently complex behavior task should allow the animal to approach an overall objective with a variety of strategies in which both the objective, its associated rewards and cost, and the behaviors can be measured and quantified. Establishing such a behavior paradigm would not only help us to understand advanced cognitive functions in animals but also lay the foundation for a thorough investigation of the underlying neural mechanism.</p><p>Here, we adapted the popular arcade game Pac-Man. The game was tweaked slightly for the macaque monkeys. Just as in the original game, the monkeys learned to use a joystick to control the movement of Pac-Man to collect all the pellets inside an enclosed maze while avoiding ghosts. The monkeys received fruit juice as a reward instead of earning points. The animals were able to learn how each element of the game led to different reward outcomes and made continuous decisions accordingly. While the game is highly dynamic and complex, it is essentially a foraging task, which may be the key to the successful training. More importantly, both the game states and the monkeys’ behavior were well-defined and could be measured and recorded, providing us opportunities for quantitative analyses and modeling.</p><p>The game has a clear objective, but an optimal solution is computationally difficult. However, a set of intuitive strategies would allow players to achieve reasonable performance. To find out whether the monkeys’ behavior can be decomposed into a set of strategies, we fit their gameplay with a dynamic compositional strategy model, which is inspired by recent advances in the artificial intelligence field in developing AI algorithms that solve the game with a multiagent approach (<xref ref-type="bibr" rid="bib10">Foderaro et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Rohlfshagen et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Sutton et al., 1999</xref>; <xref ref-type="bibr" rid="bib38">Van Seijen et al., 2017</xref>). The model consists of a set of simple strategies, each considering a specific aspect of the game to form decisions on how to move Pac-Man. By fitting the model to the behavior of the animals, we were able to deduce the strategy weights. The model was able to achieve over 90% accuracy for explaining the decision-making of the monkeys. More importantly, the strategy weights revealed that the monkeys adopted a take-the-best (TTB) heuristic by using a dominant strategy and only focusing on a subset of game aspects at a time. In addition, the monkeys were able to use the strategies as building blocks to form compound strategies to handle particular game situations. Our results demonstrated that animals are capable of managing a set of compositional strategies and employing hierarchical decision-making to solve a complex task.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The Pac-Man game</title><p>We trained two monkeys to play an adapted Pac-Man (Namco) game (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the game, the monkeys navigated a character known as Pac-Man in a maze and their objective is to traverse through the maze to eat all the pellets and energizers. The game presented the obstacles of having two ghosts named Blinky and Clyde, who behaved as predators. As in the original game, each ghost followed a unique deterministic algorithm based on Pac-Man’s location and their own locations with Blinky chasing Pac-Man more aggressively. If Pac-Man was caught, the monkeys would receive a time-out penalty. Afterward, both Pac-Man and the ghosts were reset to their starting locations, and the monkeys could continue to clear the maze. If Pac-Man ate an energizer, a special kind of pellet, the ghosts would be cast into a temporary scared mode. Pac-Man could eat the scared ghosts to gain extra rewards. All the game elements that yield points in the original game provided monkeys juice rewards instead (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right). After successfully clearing the maze, the monkeys would also receive additional juice as a reward for completing a game. The fewer attempts the animals made to complete a game, the more rewards they would be given.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The Pac-Man game and the performance of the monkeys.</title><p>(<bold>A</bold>) The monkeys used a joystick to navigate Pac-Man in the maze and collect pellets for juice rewards. Also in the maze, there were two ghosts, Blinky and Clyde. The maze was fixed, but the pellets and the energizers were placed randomly initially in each game. Eating energizers turned the ghosts into the scared mode for 14 s, during which they were edible. There were also fruits randomly placed in the maze. The juice rewards corresponding to each game element are shown on the right. (<bold>B</bold>) The monkeys were more likely to move toward the direction with more local rewards. The abscissa is the reward difference between the most and the second most rewarding direction. Different grayscale shades indicate path types with different numbers of moving directions. Means and standard errors are plotted with lines and shades. See <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for the analysis for individual monkeys. (<bold>C</bold>) The monkeys escaped from normal ghosts and chased scared ghosts. The abscissa is the Dijkstra distance between Pac-Man and the ghosts. Dijkstra distance measures the distance of the shortest path between two positions on the map. Means and standard errors are denoted with lines and shades. See <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for the analysis for individual monkeys.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>The performance of Monkey O (left) and Monkey P (right).</title><p>(<bold>A, C</bold>) Probability of monkeys moving toward the largest reward. The monkeys were more likely to move toward the direction with more local rewards. The abscissa is the reward difference between the most and the second most rewarding direction. The colors indicate the path types in which different numbers of moving directions are possible. Means and standard errors are plotted with lines and shades. (<bold>B</bold>, <bold>D</bold>) Probability of monkeys moving toward ghosts. The monkeys escaped from the normal ghosts and chased the scared ghosts. The abscissa is the distance between Pac-Man and the ghosts. The colors and the line types indicate the ghosts and their modes. Means and standard errors are plotted with lines and shades.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig1-figsupp1-v2.tif"/></fig></fig-group><p>The game was essentially a foraging task for the monkeys. The maze required navigation, and to gain rewards, the animals had to collect pallets with the risk of encountering predators. Therefore, the game was intuitive for the monkeys, which was crucial for the training’s success. The training started with simple mazes with no ghosts, and more elaborated game elements were introduced one by one throughout the training process (see Materials and methods and <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> for detailed training procedures).</p><p>The behavior analyses include data from 74 testing sessions after all the game elements were introduced to the monkeys and their performance reached a level that was subjectively determined reasonable. We recorded the joystick movements, eye movements, and pupil sizes of the animals during the game. On average, the animals completed 33 ± 9 (mean ± standard error [SE]) games in each session and each game took them 4.9 ± 1.8 attempts (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>).</p><p>Optimal gameplay requires the monkeys to consider a large number of factors, and many of them vary throughout the game either according to the game rules or as a result of the monkeys’ own actions. Finding the optimal strategy poses a computational challenge not only for monkeys but also for human and AI agents alike. The monkeys learned the task and played the game well, as one can see from the example games (<xref ref-type="video" rid="app1video1">Appendix 1—video 1</xref>, <xref ref-type="video" rid="app1video2">Appendix 1—video 2</xref>, <xref ref-type="video" rid="app1video3">Appendix 1—video 3</xref> for Monkey O; <xref ref-type="video" rid="app1video4">Appendix 1—video 4</xref>, <xref ref-type="video" rid="app1video5">Appendix 1—video 5</xref> for Monkey P). As a starting point to understand how the monkeys solved the task, we first studied if they understood the basic game elements, namely, the pellets and the ghosts.</p><p>First, we analyzed the monkeys’ decision-making concerning the local rewards, which included the pellets, the energizers, and the fruits, within five tiles from Pac-Man for each direction. The monkeys tended to choose the direction with the largest local reward (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A and C</xref>). The probability of choosing the most rewarding direction decreased with the growing number of available directions, suggesting a negative effect of option numbers on the decision-making optimality.</p><p>The monkeys also understood how to react to the ghosts in different modes. The likelihood of Pac-Man moving toward or away from the ghosts in different modes is plotted in <xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B and D</xref>. As expected, the monkeys tended to avoid the ghosts in the normal mode and chase them when they were scared. Interestingly, the monkeys picked up the subtle difference in the ghosts’ ‘personalities.’ By design, Blinky aggressively chases Pac-Man, but Clyde avoids Pac-Man when they get close (see Materials and methods for details). Accordingly, both monkeys were more likely to run away from Blinky but ignored or even followed Clyde when it was close by. Because the ghosts do not reverse their directions, it was actually safe for the monkeys to follow Clyde when they were near each other. On the other hand, the ghosts were treated the same by the monkeys when in scared mode. The monkeys went after the scared ghosts when they were near Pac-Man. This model-based behavior with respect to the ghosts’ ‘personalities’ and modes suggests sophisticated decision-making of the monkeys.</p><p>These analyses suggest that the monkeys understood the basic elements of the game. While they revealed some likely strategies of the monkeys, collecting local pellets and escaping or eating the ghosts, they did not fully capture the monkeys’ decision-making. Many other factors as well as the interaction between them affected the monkeys’ decisions. More sophisticated behavior was required for optimal performance, and to this end, the dynamic compositional strategy model was developed to understand the monkeys’ behavior.</p></sec><sec id="s2-2"><title>Basis strategies</title><p>While the overall goal of the game is to clear the maze, the monkeys may adopt different strategies for smaller objectives in different circumstances. We use the term ‘strategy’ to refer to the solution for these sub-goals, and each strategy involves a smaller set of game variables with easier computation for decisions that form actions.</p><p>We consider six intuitive and computationally simple strategies as the basis strategies. The <italic>local</italic> strategy moves Pac-Man toward the direction with the largest reward within 10 tiles. The <italic>global</italic> strategy moves Pac-Man toward the direction with the largest overall reward in the maze. The <italic>energizer</italic> strategy moves Pac-Man toward the nearest energizer. The two <italic>evade</italic> strategies move Pac-Man away from Blinky and Clyde in the normal mode, respectively. Finally, the <italic>approach</italic> strategy moves Pac-Man toward the nearest ghost. At any time during the game, monkeys could adopt one or a mixture of multiple strategies for decision-making. These basis strategies, although not necessarily orthogonal to each other (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>), can be linearly combined to explain the monkeys’ behavior.</p><p>At any time during the game, monkeys could adopt one or a mixture of multiple strategies for decision-making. We assumed that the final decision for Pac-Man’s moving direction was based on a linear combination of the basis strategies, and the relative strategy weights were stable for a certain period. We adopted a softmax policy to linearly combine utility values under each basis strategy, with the strategy weights as model parameters. To avoid potential overfitting, we designed a two-pass fitting procedure to divide each game trial into segments and performed maximum likelihood estimation (MLE) to estimate the model parameters with the monkeys’ behavior within each time segment (see Materials and methods for details). When tested with simulated data, this fitting procedure recovers the ground-truth weights used to generate the data (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>).</p></sec><sec id="s2-3"><title>Monkeys adopted different strategies at different game stages</title><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows the normalized strategy weights in an example game segment (<xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>). In this example, the monkey started with the <italic>local</italic> strategy and grazed pellets. With the ghosts getting close, it initiated the <italic>energizer</italic> strategy and went for a nearby energizer. Once eating the energizer, the monkey switched to the <italic>approach</italic> strategy to hunt the scared ghosts. Afterward, the monkey resumed the <italic>local</italic> strategy and then used the <italic>global</italic> strategy to navigate toward another patch when the local rewards were depleted. The dynamic compositional strategy model (see Materials and methods for details) faithfully captures the monkey’s behavior by explaining Pac-Man’s movement with an accuracy of 0.943 in this example.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Fitting behavior with basis strategies.</title><p>(<bold>A</bold>) The normalized strategy weights in an example game segment. The horizontal axis is the time step. Each time step is 417 ms, which is the time that it takes Pac-Man to move across a tile. The color bar indicates the dominant strategies across the segment. The monkey’s actual choice and the corresponding model prediction at each time step are shown below, with red indicating a mismatch. The prediction accuracy for this segment is 0.943. Also, see <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>. (<bold>B</bold>) Comparison of prediction accuracy across four models in four game contexts. Four game contexts were defined according to the criteria listed in <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>. Vertical bars denote standard deviations. Horizontal dashed lines denote the chance-level prediction accuracies. See <xref ref-type="table" rid="app1table4 app1table5 app1table6">Appendix 1—tables 4–6</xref> for detailed prediction accuracy comparisons. (<bold>C</bold>) The distribution of the three dominating strategies’ weights. The most dominating strategy’s weights (0.907 ± 0.117) were significantly larger than the secondary strategy (0.273 ± 0.233) and tertiary strategy (0.087 ± 0.137) by far. Horizontal white bars denote means, and the vertical black bars denote standard errors. (<bold>D</bold>) The distribution of the weight difference between the most and the second dominating strategies. The distribution is heavily skewed toward 1. In over 90% of the time, the weight difference was larger than 0.1, and more than 33% of the time the difference was over 0.9. (<bold>E</bold>) The ratios of labeled dominating strategies across four game contexts. In the early game, the <italic>local</italic> strategy was the dominating strategy. In comparison, in the late game, both the <italic>local</italic> and the <italic>global</italic> strategies had large weights. The weight of the <italic>approach</italic> strategy was largest when the ghosts were in the scared mode. See <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for the analysis for individual monkeys.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Fitting behavior with strategy labels for Monkey O (left) and Monkey P (right).</title><p>(<bold>A</bold>, <bold>E</bold>) Comparison of prediction accuracy across three models in four game contexts. See <xref ref-type="table" rid="app1table5 app1table6">Appendix 1—tables 5 and 6</xref> for details. Vertical bars denote standard deviations. Horizontal dashed lines denote the chance-level prediction accuracies. (<bold>B</bold>, <bold>F</bold>) The histograms of the three dominating strategies’ weights. (<bold>B</bold>) The most dominating strategy’s weights (0.907 ± 0.115) were much larger than the secondary strategy (0.276 ± 0.233) and tertiary strategy (0.088 ± 0.136). (<bold>F</bold>) The most dominating strategy’s weights (0.907 ± 0.119), the secondary strategy (0.270 ± 0.233), and tertiary strategy (0.07 ± 0.138). Horizontal white bars denote means, and the vertical black bars denote standard errors. (<bold>C</bold>, <bold>G</bold>) The histogram of the weight difference between the most and the second dominating strategies. The distribution is heavily skewed toward 1. In about 90% of the time, the weight difference was larger than 0.1, and more than 30% of the time the difference was over 0.9. Horizontal white bars denote means, and the vertical black bars denote standard errors. (<bold>D</bold>, <bold>H</bold>) The ratios of labeled dominating strategies across four game contexts. In the early game, the <italic>local</italic> strategy was the dominating strategy. In comparison, in the late game, both the <italic>local</italic> and the <italic>global</italic> strategies had large weights. The weight of the <italic>approach</italic> strategy was largest when the ghosts were in the scared mode.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig2-figsupp1-v2.tif"/></fig><media id="fig2video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-fig2-video1.mp4"><label>Figure 2—video 1.</label><caption><title>Example game segment.</title><p>Monkey’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in this example game segment. Monkey’s real-time saccade position is plotted as a moving white dot. In this example, the monkey started with the <italic>local</italic> strategy and grazed pellets. With the ghosts getting close, it initiated the <italic>energizer</italic> strategy and went for a nearby energizer. Once eating the energizer, the monkey switched to the <italic>approach</italic> strategy to hunt the scared ghosts. Afterward, the monkey resumed the <italic>local</italic> strategy and then used the <italic>global</italic> strategy to navigate toward another patch when the local rewards were depleted.</p></caption></media></fig-group><p>Overall, the dynamic compositional strategy model explains the monkeys’ behavior well. The model’s prediction accuracy is 0.907 ± 0.008 for Monkey O and 0.900 ± 0.009 for Monkey P. In comparison, a static strategy model, which uses the fixed strategy weights, achieves an overall accuracy of 0.806 ± 0.014 and 0.825 ± 0.012 for monkeys O and P, respectively (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The static strategy model’s accuracy is still high, reflecting the fact that the monkeys were occupied with collecting pellets most of the time in the game. Thus, a combination of local and global strategies was often sufficient for explaining the monkeys’ choice. However, the average accuracy measurement alone and the fixed model could not reveal the monkeys’ adaptive behavior. The strategy dynamics are evident when we look at different game situations (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). During the early game, defined as when there were more than 90% remaining pellets in the maze, the <italic>local</italic> strategy dominated all other strategies. In comparison, during the late game, defined as when there were fewer than 10% remaining pellets, both the <italic>local</italic> and the <italic>global</italic> strategies had large weights. The <italic>approach</italic> strategy came online when one or both scared ghosts were within 10 tiles around Pac-Man. The model’s prediction accuracies for the early game, the late game, and the scared-ghosts situations were 0.886 ± 0.0016, 0.898 ± 0.011, and 0.958 ± 0.010, which were significantly higher than the static strategy model’s accuracies (early: 0.804 ± 0.025, p&lt; <inline-formula><mml:math id="inf1"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; late: 0.805 ± 0.019, p&lt; <inline-formula><mml:math id="inf2"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>35</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; scared ghosts: 0.728 ± 0.031, p&lt; <inline-formula><mml:math id="inf3"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; two-sample <italic>t</italic>-test).</p><p>The dynamic compositional strategy decision-making model is hierarchical. A strategy is first chosen, and the primitive actions (i.e., joystick movements) are then determined under the selected strategy with a narrowed down set of features (<xref ref-type="bibr" rid="bib4">Botvinick et al., 2009</xref>; <xref ref-type="bibr" rid="bib5">Botvinick and Weinstein, 2014</xref>; <xref ref-type="bibr" rid="bib9">Dezfouli and Balleine, 2013</xref>; <xref ref-type="bibr" rid="bib26">Ostlund et al., 2009</xref>; <xref ref-type="bibr" rid="bib32">Sutton et al., 1999</xref>). In contrast, in a flat model, decisions are computed directly for the primitive actions based on all relevant game features. Hierarchical models can learn and compute a sufficiently good solution much more efficiently due to its natural additive decomposition of the overall strategy utility.</p><p>To illustrate the efficiency of the hierarchical model, we tested two representative flat models. First, we considered a linear approximate reinforcement learning (LARL) model (<xref ref-type="bibr" rid="bib31">Sutton, 1988</xref>; <xref ref-type="bibr" rid="bib35">Tsitsiklis and Van Roy, 1997</xref>). The LARL model shared the same structure with a standard Q-learning algorithm but used the monkeys’ actual joystick movements as the fitting target. To highlight the flatness of this baseline model, we adopted a common assumption that the parameterization of the utility function is linear (<xref ref-type="bibr" rid="bib33">Sutton and Barto, 2018</xref>) with respect to seven game features (see Materials and methods for details). Second, we trained a perceptron network as an alternative flat model. The perceptron had one layer of 64 units for Monkey P and 16 units for Monkey O (the number of units was determined by the highest fitting accuracy with fivefold cross-validation, see Materials and methods for details). The inputs were the same features used in our six strategies, and the outputs were the joystick movements. Compared to our hierarchical models, neither flat model performed as well. The LARL model achieved 0.669 ± 0.011 overall prediction accuracy (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, light gray bars) and performed worse than the hierarchical models under each game situation (early: 0.775 ± 0.021, p&lt; <inline-formula><mml:math id="inf4"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; late: 0.621 ± 0.018, p&lt; <inline-formula><mml:math id="inf5"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>17</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; scared ghosts: 0.672 ± 0.025, p&lt; <inline-formula><mml:math id="inf6"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; two-sample <italic>t</italic>-test). The perceptron model was even worse, both overall (0.624 ± 0.010, <xref ref-type="fig" rid="fig2">Figure 2B</xref>, white bars) and under each game situation (early: 0.582 ± 0.026, p&lt; <inline-formula><mml:math id="inf7"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; late: 0.599 ± 0.019, p&lt; <inline-formula><mml:math id="inf8"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; scared ghosts: 0.455 ± 0.030, p&lt; <inline-formula><mml:math id="inf9"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> ; two-sample <italic>t</italic>-test). The results were similar when we tested the models with individual monkeys separately (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A and E</xref>). Admittedly, one may design better and more complex flat models than the two tested here. Yet, even our relatively simple LARL model was more computationally complex than our hierarchical model but performed much worse, illustrating the efficiency of hierarchical models.</p></sec><sec id="s2-4"><title>Monkeys adopted TTB heuristic</title><p>Neither our model nor the fitting procedure limits the number of strategies that may simultaneously contribute to the monkeys’ choices at any time, yet the fitting results show that a single strategy often dominated the monkeys’ behavior. In the example (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>), the monkey switched between different strategies with one dominating strategy at each time point. This was a general pattern. We ranked the strategies according to their weights at each time point. The histograms of the three dominating strategies’ weights from all time points show that the most dominating strategy’s weights (0.907 ± 0.117) were significantly larger than those of the secondary strategy (0.273 ± 0.233) and tertiary strategy (0.087 ± 0.137) by a significant margin (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The weight difference between the first and the second most dominating strategies was heavily skewed toward one (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Individual monkey analysis results were consistent (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B, C, F and G</xref>). Taken together, these results indicate that the monkeys adopted a TTB heuristics in which action decisions were formed with a single strategy heuristically and dynamically chosen.</p><p>Therefore, we labeled the monkeys’ strategy at each time point with the dominating strategy. When the weight difference between the dominating and secondary strategies was smaller than 0.1, the strategy was labeled as <italic>vague</italic>. It may reflect a failure of the model to identify the correct strategy, a period of strategy transition during which a dominating strategy is being formed to replace the existing one, or a period during which the monkeys were indeed using multiple strategies. No matter which is the case, they are only a small percentage of data and not representative.</p><p>The <italic>local</italic> and the <italic>global</italic> strategy were most frequently used overall. The <italic>local</italic> strategy was particularly prevalent during the early game when the local pellets were abundant, while the <italic>global</italic> strategy contributed significantly during the late game when the local pellets were scarce (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Similar strategy dynamics were observed in the two monkeys (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D and H</xref>).</p></sec><sec id="s2-5"><title>Strategy manifested in behavior</title><p>The strategy fitting procedure is indifferent to how the monkeys chose between the strategies, but the fitting results provide us with some hints. The probability of the monkeys adopting the <italic>local</italic> or the <italic>global</italic> strategy correlated with the availability of local rewards: abundant local rewards lead to the <italic>local</italic> strategy (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, individual monkeys: <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A and E</xref>). On the other hand, when the ghosts were scared, the decision between chasing the ghosts and going on collecting the pellets depended on the distance between Pac-Man and the scared ghosts (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, individual monkeys: <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B and F</xref>). In addition, during the <italic>global</italic> strategy, the monkeys often moved Pac-Man to reach a patch of pellets far away from its current location. They chose the shortest path (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, individual monkeys: <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C and G</xref>) and made the fewest turns to do so (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, individual monkeys: <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D and H</xref>), demonstrating their goal-directed path-planning behavior under the particular strategy.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Monkeys’ behavior under different strategies.</title><p>(<bold>A</bold>) The probabilities of the monkeys adopting the <italic>local</italic> or <italic>global</italic> strategy correlate with the number of local pellets. Solid lines denote means, and shades denote standard errors. (<bold>B</bold>) The probabilities of the monkeys adopting the <italic>local</italic> or <italic>approach</italic> strategy correlate with the distance between Pac-Man and the ghosts. Solid lines denote means, and shades denote standard errors. (<bold>C</bold>) When adopting the <italic>global</italic> strategy to reach a far-away patch of pellets, the monkeys’ actual trajectory length was close to the shortest. The column denotes the actual length, and the row denotes the optimal number. The percentages of the cases with the corresponding actual lengths are presented in each cell. High percentages in the diagonal cells indicate close to optimal behavior. (<bold>D</bold>) When adopting the <italic>global</italic> strategy to reach a far-away patch of pellets, the monkeys’ number of turns was close to the fewest possible turns. The column denotes the actual turns, and the row denotes the optimal number. The percentages of the cases with the corresponding optimal numbers are presented in each cell. High percentages in the diagonal cells indicate close to optimal behavior. (<bold>E</bold>) Average fixation ratios of ghosts, energizers, and pellets when the monkeys used different strategies. (<bold>F</bold>) The monkeys’ pupil diameter increases around the strategy transition (solid line). Such increase was absent if the strategy transition went through the <italic>vague</italic> strategy (dashed line). Shades denote standard errors. Black bar at the bottom denotes p&lt;0.01, two-sample <italic>t</italic>-test. See <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for the analysis for individual monkeys.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Monkey’s behavior under different strategies for Monkey O (left) and Monkey P (right).</title><p>(<bold>A</bold>, <bold>E</bold>) The probabilities of the monkeys adopting the <italic>local</italic> or <italic>global</italic> strategy correlate with the number of <italic>local</italic> pellets. Solid lines denote means, and shades denote standard errors. (<bold>B</bold>, <bold>F</bold>) The probabilities of the monkeys adopting the <italic>local</italic> or <italic>approach</italic> strategy correlate with the distance between Pac-Man and the ghosts. Solid lines denote means, and shades denote standard errors. (<bold>C</bold>, <bold>G</bold>) When adopting the <italic>global</italic> strategy to reach a far-away patch of pellets, the monkeys’ actual trajectory length was close to the shortest. The column denotes the actual length, and the row denotes the optimal number. The percentages of the cases with the corresponding actual lengths are presented in each cell. High percentages in the diagonal cells indicate close to optimal behavior. (<bold>D</bold>, <bold>H</bold>) When adopting the <italic>global</italic> strategy to reach a far-away patch of pellets, the monkeys’ number of turns was close to the fewest possible turns. The column denotes the actual turns, and the row denotes the optimal number. The percentages of the cases with the corresponding optimal numbers are presented in each cell. High percentages in the diagonal cells indicate close to optimal behavior.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Monkey’s eye movement patterns under different strategies for Monkey O (left) and Monkey P (right).</title><p>(<bold>A</bold>, <bold>E</bold>) Average fixation ratios of ghosts, energizers, and pellets when the monkeys used different strategies. (<bold>B</bold>, <bold>F</bold>) The monkeys’ pupil diameter increases around the strategy transition (solid line). Such increase was absent if the strategy transition went through the <italic>vague</italic> strategy (dashed line). Shades denote standard errors across trials. Black bars at the bottom denote p&lt;0.01, two-sample <italic>t</italic>-test. (<bold>C</bold>, <bold>G</bold>) The monkeys’ pupil diameter increase was evident in transitions from <italic>local</italic> to <italic>global</italic>. Shades denote standard errors across trials. Black bars at the bottom denote p&lt;0.01, two-sample <italic>t</italic>-test. (<bold>D</bold>, <bold>H</bold>) The monkeys’ pupil diameter increase was evident in transitions from <italic>global</italic> to <italic>local</italic> in Monkey P. It was not obvious in Monkey O as the pupil size increased in general. Shades denote standard errors across trials. Black bars at the bottom denote p&lt;0.01, two-sample <italic>t</italic>-test.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig3-figsupp2-v2.tif"/></fig></fig-group><p>The fitting results can be further corroborated from the monkeys’ eye movements and pupil dilation. Because different game aspects were used in different strategies, the monkeys should be looking at different things when using different strategies. We classified monkeys’ fixation locations into four categories: ghosts, energizers, pellets, and others (see Materials and methods for details). <xref ref-type="fig" rid="fig3">Figure 3E</xref> (individual monkeys: <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A–E</xref>) shows the fixation ratio of these game objects under different strategies. Although a large number of fixations were directed at the pellets in all situations, they were particularly frequent under the <italic>local</italic> and <italic>energizer</italic> strategies. Fixations directed to the energizers were scarce, unless when the monkeys adopted the <italic>energizer</italic> strategy. On the other hand, monkeys looked at the ghosts most often when the monkeys were employing the <italic>approach</italic> strategy to chase the ghosts (p&lt;0.001, two-sample <italic>t</italic>-test). Interestingly, the monkeys also looked at the ghosts more often under the <italic>energizer</italic> strategy than under the <italic>local</italic> strategy (p&lt;0.001, two-sample <italic>t</italic>-test), which suggests that the monkeys were also keeping track of the ghosts when going for the energizer.</p><p>While the fixation patterns revealed that the monkeys paid attention to different game elements in different strategies, we also identified a physiological marker that reflected the strategy switches in general but was not associated with any particular strategy. Previous studies revealed that non-luminance-mediated changes in pupil diameter can be used as markers of arousal, surprise, value, and other factors during decision-making (<xref ref-type="bibr" rid="bib15">Joshi and Gold, 2020</xref>). Here, we analyzed the monkeys’ pupil dilation during strategy transitions. When averaged across all types of transitions, the pupil diameter exhibited a significant but transient increase around strategy transitions (p&lt;0.01, two-sample <italic>t</italic>-test, <xref ref-type="fig" rid="fig3">Figure 3F</xref>, also individual monkeys: <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B–F</xref>). Such an increase was absent when the strategy transition went through a <italic>vague</italic> period. This increase was evident in the transitions in both directions, for example, from <italic>local</italic> to <italic>global</italic> (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C–G</xref>) and from <italic>global</italic> to <italic>local</italic> (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2D–H</xref>). Therefore, it cannot be explained by any particular changes in the game state, such as the number of local pellets. Instead, it reflected a computation state of the brain associated with strategy switches (<xref ref-type="bibr" rid="bib24">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">Urai et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Wang et al., 2021</xref>).</p></sec><sec id="s2-6"><title>Compound strategies</title><p>The compositional strategy model divides the monkeys’ decision-making into different hierarchies (<xref ref-type="fig" rid="fig4">Figure 4</xref>). At the lowest level, decisions are made for actions, the joystick movements of up, down, left, and right, using one of the basis strategies. At the middle level, decisions are made between these basis strategies, most likely with a heuristic for the monkeys to reduce the complexity of the decision-making. The pupil dilation change reflected the decision-making at this level. In certain situations, the basis strategies may be pieced together and form compound strategies as a higher level of decision-making. These compound strategies are not simple impromptu strategy assemblies. Instead, they may reflect more advanced planning. Here, building on the strategy analyses, we describe two scenarios in which compound strategies were used by the monkeys.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Monkeys’ decision-making in different hierarchies.</title><p>At the lowest level, decisions are made for the joystick movements: up, down, left, or right. At the middle level, choices are made between the basis strategies. At a higher level, simple strategies may be pieced together for more sophisticated compound strategies. Monkeys may adopt one of the compound strategies or just a basis strategy depending on the game situation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig4-v2.tif"/></fig><p>The first scenario involves the energizers, which is an interesting feature of the game. They not only provide an immediate reward but also lead to potential future rewards from eating ghosts. With the knowledge that the effect of the energizers was only transient, the monkeys could plan accordingly to maximize their gain from the energizers. In some trials, the monkeys immediately switched to the <italic>approach</italic> strategy after eating an energizer and actively hunted nearby ghosts (<xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>). In contrast, sometimes the monkey appeared to treat an energizer just as a more rewarding pellet. They continued collecting pellets with the <italic>local</italic> strategy after eating the energizer, and catching a ghost seemed to be accidental and unplanned (<xref ref-type="video" rid="fig5video2">Figure 5—video 2</xref>). Accordingly, we distinguished these two behaviors using the strategy labels after the energizer consumption and named the former as <italic>planned attack</italic> and the latter as <italic>accidental consumption</italic> (see Materials and methods for details). With this criterion, we extracted 493 (Monkey O) and 463 (Monkey P) <italic>planned attack</italic> plays, and 1970 (Monkey O) and 1295 (Monkey P) <italic>accidental consumption</italic> plays in our dataset.</p><p>The strategy weight dynamics around the energizer consumption showed distinct patterns when the animals adopted the compound strategy <italic>planned attack</italic> (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, individual monkeys: <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A–E</xref>). When the monkeys carried out <italic>planned attacks</italic>, they started to approach the ghosts well before the energizer consumption, which is revealed by the larger weights of the <italic>approach</italic> strategy than that in the <italic>accidental consumption</italic>. The monkeys also cared less for the local pellets in <italic>planned attacks</italic> before the energizer consumption. The weight dynamics suggest that the decision of switching to the <italic>approach</italic> strategy was not an afterthought but planned well ahead. The monkeys strung the <italic>energizer</italic>/<italic>local</italic> strategy with the <italic>approach</italic> strategy together into the compound strategy to eat an energizer and then hunt the ghosts. Such a compound strategy should only be employed when Pac-Man, ghosts, and an energizer are in close range. Indeed, the average of distance between Pac-Man, the energizer, and the ghosts was significantly smaller in <italic>planned attack</italic> than in <italic>accidental consumption</italic> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, p&lt;0.001, two-sample <italic>t</italic>-test, individual monkeys: <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B–F</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Compound strategies: <italic>planned attack</italic>.</title><p>(<bold>A</bold>) Average strategy weight dynamics in <italic>planned attacks</italic> (left) and <italic>accidental consumptions</italic> (right). Solid lines denote means, and shades denote standard errors. (<bold>B</bold>) The average distance between Pac-Man, the energizer, and the ghosts in <italic>planned attacks</italic> and <italic>accidental consumptions</italic>. Vertical dashed lines denote means. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>C</bold>) Ratios of fixations on the ghosts, the energizer, and Pac-Man. Vertical bars denote standard errors. ***p&lt;0.001, **p&lt;0.01, two-sample <italic>t</italic>-test. (<bold>D</bold>) The pupil size aligned to the ghost consumption. The black bar near the abscissa denotes data points where the two traces are significantly different (p&lt;0.01, two-sample <italic>t</italic>-test). Shades denote standard errors at every time point. See <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for the analysis for individual monkeys.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title><italic>Planned attacks</italic> in Monkey O (upper) and Monkey P (lower).</title><p>(<bold>A</bold>, <bold>E</bold>) Average strategy weight dynamics in <italic>planned attacks</italic> (left) and <italic>accidental consumptions</italic> (right). Solid lines denote means, and shades denote standard errors. (<bold>B</bold>, <bold>F</bold>) The average distance between Pac-Man, the energizer, and the ghosts in <italic>planned attacks</italic> and <italic>accidental consumptions</italic>. Vertical dashed lines denote means. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>C</bold>, <bold>G</bold>) Ratios of fixations on the ghosts, the energizer, and Pac-Man. Vertical bars denote standard errors. ***p&lt;0.001, ** p&lt;0.01, two-sample <italic>t</italic>-test. (<bold>D</bold>, <bold>H</bold>) The pupil size aligned to the ghost consumption. The black bar near the abscissa denotes data points where the two traces are significantly different (p&lt;0.01, two-sample <italic>t</italic>-test). Shades denote standard errors at every time point.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig5-figsupp1-v2.tif"/></fig><media id="fig5video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-fig5-video1.mp4"><label>Figure 5—video 1.</label><caption><title><italic>Planned attack</italic> game segment.</title><p>Monkey’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in this <italic>planned attack</italic> game segment. Monkey’s real-time saccade position is plotted as a moving white dot. In this trial, the monkey immediately switched to the <italic>approach</italic> strategy after eating an energizer.</p></caption></media><media id="fig5video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-fig5-video2.mp4"><label>Figure 5—video 2.</label><caption><title><italic>Accidental consumption</italic> game segment.</title><p>Monkey’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in this <italic>accidental consumption</italic> game segment. Monkey’s real-time saccade position is plotted as a moving white dot. In this trial, the monkey treated an energizer just as a more rewarding pellet and continued collecting pellets with the <italic>local</italic> strategy after eating the energizer.</p></caption></media></fig-group><p>Again, the <italic>planned attacks</italic> were also associated with distinct eye movement and pupil size dynamics. The monkeys fixated on the ghosts, the energizers, and Pac-Man more frequently before the energizer consumption in <italic>planned attacks</italic> than in <italic>accidental consumption</italic> (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, p&lt;0.001, two-sample <italic>t</italic>-test, individual monkeys: <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C–G</xref>), reflecting more active planning under the way. In addition, the monkeys’ pupil sizes were smaller before they caught a ghost in <italic>planned attacks</italic> than in <italic>accidental consumption</italic> (p&lt;0.01, two-sample <italic>t</italic>-test), which may reflect a lack of surprise under <italic>planned attacks</italic> (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, individual monkeys: <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D–H</xref>). The difference was absent after the ghost was caught.</p><p>The second scenario involves a counterintuitive move in which the monkeys moved Pac-Man toward a normal ghost to die on purpose in some situations. Although the move appeared to be suboptimal, it was beneficial in a certain context. The death of Pac-Man resets the game and returns Pac-Man and the ghosts to their starting positions in the maze. As the only punishment in the monkey version of the game is a time-out, it is advantageous to reset the game by committing such suicide when local pellets are scarce, and the remaining pellets are far away.</p><p>To analyze this behavior, we defined the compound strategy <italic>suicide</italic> using strategy labels. We computed the distances between Pac-Man and the closest pellets before and after its death. In <italic>suicides</italic>, Pac-Man’s death significantly reduced this distance (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, upper histogram, <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref>, individual monkeys: <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A–E</xref>). This was not true when the monkeys were adopting the <italic>evade</italic> strategy but failed to escape from the ghosts (<italic>failed evasions</italic>, <xref ref-type="fig" rid="fig6">Figure 6A</xref>, bottom histogram, <xref ref-type="video" rid="fig6video2">Figure 6—video 2</xref>, individual monkeys: <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A–E</xref>). In addition, the distance between Pac-Man and the ghosts was greater in <italic>suicides</italic> (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, p&lt;0.001, two-sample <italic>t</italic>-test, individual monkeys: <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B and F</xref>). Therefore, these suicides were a proactive decision. Consistent with the idea, the monkeys tended to saccade toward the ghosts and pellets more often in <italic>suicides</italic> than in <italic>failed evasions</italic> (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, p&lt;0.001, two-sample <italic>t</italic>-test, individual monkeys: <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C and G</xref>). Their pupil size decreased even before Pac-Man’s death in <italic>suicides</italic>, which was significantly smaller than in <italic>failed evasions</italic>, suggesting that the death was anticipated (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, p&lt;0.01, two-sample <italic>t</italic>-test, individual monkeys: <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1D and H</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Compound strategies: <italic>suicide</italic>.</title><p>(<bold>A</bold>) Distance difference between Pac-Man and closest pellet before and after the death is smaller in <italic>suicides</italic> than in <italic>failed evasions</italic>. Vertical dashed lines denote means. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>B</bold>) Average distance between Pac-Man and the ghosts was greater in <italic>suicides</italic> than in <italic>failed evasions</italic>. Vertical dashed lines denote means. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>C</bold>) The monkeys fixated more frequently on the ghosts and the pellets in <italic>suicides</italic> than in <italic>failed evasions</italic>. Vertical bars denote standard errors. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>D</bold>) The monkeys’ pupil size decreased before Pac-Man’s death in <italic>suicides</italic>. The black bar near the abscissa denotes data points where the two traces are significantly different (p&lt;0.01, two-sample <italic>t</italic>-test). Shades denote standard errors at every time point. See <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> for the analysis for individual monkeys.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title><italic>Suicides</italic> in Monkey O (left) and Monkey P (right).</title><p>(<bold>A</bold>, <bold>E</bold>) The distance difference between Pac-Man and closest pellet before and after the death is smaller in <italic>suicides</italic> than in <italic>failed evasions</italic>. Vertical dashed lines denote means. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>B</bold>, <bold>F</bold>) The average distance between Pac-Man and the ghosts was greater in <italic>suicides</italic> than in <italic>failed evasions</italic>. Vertical dashed lines denote means. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>C</bold>, <bold>G</bold>) The monkeys fixated more frequently on the ghosts and the pellets in <italic>suicides</italic> than in <italic>failed evasions</italic>. Vertical bars denote standard errors. ***p&lt;0.001, two-sample <italic>t</italic>-test. (<bold>D</bold>, <bold>H</bold>) The monkeys’ pupil size decreased before Pac-Man’s death in <italic>suicides</italic>. The black bars near the abscissa denote data points where the two traces are significantly different (p&lt;0.01, two-sample <italic>t</italic>-test). Shades denote standard errors at every time point.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-fig6-figsupp1-v2.tif"/></fig><media id="fig6video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-fig6-video1.mp4"><label>Figure 6—video 1.</label><caption><title><italic>Suicide</italic> game segment.</title><p>Monkey’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in this <italic>suicide</italic> game segment. Monkey’s real-time saccade position is plotted as a moving white dot. In this trial, the monkey moved Pac-Man toward a normal ghost to be eaten on purpose. The local pellets were scarce, and the remaining pellets were far away. Thus, the death of Pac-Man resets the game and returns Pac-Man and the ghosts to their starting positions, making <italic>suicide</italic> a more advantageous compound strategy.</p></caption></media><media id="fig6video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-fig6-video2.mp4"><label>Figure 6—video 2.</label><caption><title><italic>Failed evasion</italic> game segment.</title><p>Monkey’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in this <italic>failed evasion</italic> game segment. Monkey’s real-time saccade position is plotted as a moving white dot. In this trial, the monkeys were adopting the <italic>evade</italic> strategy but failed to escape from the ghosts.</p></caption></media></fig-group><p>Together, these two examples demonstrated how monkeys’ advanced gameplay can be understood with concatenated basis strategies. The compositional strategy model not only provides a good fit for the monkeys’ behavior but also offers insights into the monkeys’ gameplay. The compound strategies demonstrate that the monkeys learned to actively change the game into desirable states that can be solved with planned strategies. Such intelligent behavior cannot be explained with a passive foraging strategy.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Just as one cannot gain a full understanding of the visual system by studying it with bars and dots, pursuing a deeper insight into the cognitive capability of the brain demands sophisticated behavior paradigms in which an ensemble of perception, attention, valuation, executive control, decision-making, motor planning, and other cognitive processes need to work together continuously across time. Naturally, quantifying and modeling these behaviors in such paradigms is challenging, but here we demonstrated that the behavior of monkeys during a complex game can be understood and described with a set of basis strategies that decompose the decision-making into different hierarchies.</p><p>Our hierarchical model explains the monkeys’ joystick movements well (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Importantly, the strategies derived from the model can be verified with independent behavior measurements. The monkeys' fixation pattern, a measure of their attention, reflected the features associated with the current strategy (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Moreover, an increase in pupil dilation (<xref ref-type="fig" rid="fig3">Figure 3F</xref>), which was not associated with any particular changes of game states, was found at the deduced strategy switches. This is consistent with the prediction from the hierarchical model that there should be crucial decision-making of strategies around the strategy transitions.</p><p>In contrast to hierarchical models in which the decision-maker divides decision-making into multiple levels and at each level focuses on an increasingly refined smaller set of game features (<xref ref-type="bibr" rid="bib4">Botvinick et al., 2009</xref>; <xref ref-type="bibr" rid="bib5">Botvinick and Weinstein, 2014</xref>; <xref ref-type="bibr" rid="bib9">Dezfouli and Balleine, 2013</xref>; <xref ref-type="bibr" rid="bib26">Ostlund et al., 2009</xref>; <xref ref-type="bibr" rid="bib32">Sutton et al., 1999</xref>), a flat model’s decisions are directly computed at the primitive action level, and each action choice is evaluated with all game features. Although in theory a flat model may achieve equal or even greater performance than a hierarchical model, flat models are much more computationally costly. Especially when working memory has a limited capacity, as in the case of the real brain, hierarchical models can achieve a faster and more accurate performance (<xref ref-type="bibr" rid="bib5">Botvinick and Weinstein, 2014</xref>). Our Pac-Man task contains an extensive feature space while requiring real-time decision-making that composes limitations on the cognitive resources. Even for a complex flat model such as Deep Q-Network, which evaluates primitive actions directly with a deep learning network structure without any temporally extended higher-level decisions (<xref ref-type="bibr" rid="bib22">Mnih et al., 2015</xref>), the game performance is much worse than a hierarchical model (<xref ref-type="bibr" rid="bib38">Van Seijen et al., 2017</xref>). In fact, the most successful AI player to date uses a multiagent solution, which is hierarchical in nature (<xref ref-type="bibr" rid="bib38">Van Seijen et al., 2017</xref>). Our study shows that the monkeys also adopted a hierarchical solution for the Pac-Man game.</p><p>Although the particular set of basis strategies in the model is hand-crafted, we have good reasons to believe that they reflect the decision-making of the monkeys. Our model fitting procedure is agnostic to how one should choose between the strategies, yet the resulting strategies can be corroborated both from monkeys’ route planning, eye movements, and pupil dilation patterns. This is evidence for both the validity of the model and the rationality behind monkeys’ behavior. The correlation between the results of strategy fitting and the fixation patterns of the monkeys indicates that the animals learned to selectively attend to the features that were relevant for their current strategy while ignoring others to reduce the cognitive load for different states. Similar behaviors have also been observed in human studies (<xref ref-type="bibr" rid="bib19">Leong et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Wilson and Niv, 2011</xref>). In particular, the pupil dilation at the time of strategy transitions indicated the extra cognitive processing carried out in the brain to handle the strategy transitions. Lastly, the model, without being specified so, revealed that a single strategy dominates monkeys’ decision-making during most of the game. This is consistent with the idea that strategy-using is a method that the brain uses to simplify decision-making by ignoring irrelevant game aspects to solve complex tasks (<xref ref-type="bibr" rid="bib2">Binz et al., 2022</xref>; <xref ref-type="bibr" rid="bib23">Moreno-Bote et al., 2020</xref>).</p><p>In some previous animal studies, strategies were equated to decision rules (<xref ref-type="bibr" rid="bib8">Bunge and Wallis, 2007</xref>; <xref ref-type="bibr" rid="bib11">Genovesio and Wise, 2007</xref>; <xref ref-type="bibr" rid="bib14">Hoshi et al., 2000</xref>; <xref ref-type="bibr" rid="bib21">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Tsujimoto et al., 2011</xref>). The rules were typically mutually exclusive, and the appropriate rule was either specified with explicit sensory cues or the trial-block structure. The rules in these studies can be boiled down to simple associations, even in cases when the association may be abstract (<xref ref-type="bibr" rid="bib11">Genovesio and Wise, 2007</xref>). In this study, however, we defined a set of strategies as a heuristic that reduces a complex computation into a set of smaller and more manageable problems or computations. There were no explicit cues or trial-block structures to instruct animals on which strategies to choose. Nevertheless, the same prefrontal network, including the dorsolateral prefrontal cortex, orbitofrontal cortex, and polar cortex that are suggested to engage in rule use and rule switching (<xref ref-type="bibr" rid="bib8">Bunge and Wallis, 2007</xref>; <xref ref-type="bibr" rid="bib11">Genovesio and Wise, 2007</xref>; <xref ref-type="bibr" rid="bib14">Hoshi et al., 2000</xref>; <xref ref-type="bibr" rid="bib21">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Tsujimoto et al., 2011</xref>), may also play important roles in strategy-based decision-making, too.</p><p>Our Pac-Man paradigm elicits monkeys’ more complex and natural cognitive ability. First, the game contains an extensive state space. This requires monkeys to simplify the task by developing temporally extended strategies to accomplish sub-goals. Second, there exists nonexclusive solutions or strategies to solve the Pac-Man task appropriately. Instead of spoon-feeding monkeys the exact solution in simple tasks, we trained them with all relevant game elements during the training phases and allowed them to proactively coordinate and select strategies freely. Therefore, our Pac-Man paradigm does not restrict monkeys’ behavior with a small number of particular rules and allows the brain and its neural circuitry to be studied in a more natural setting (<xref ref-type="bibr" rid="bib17">Krakauer et al., 2017</xref>).</p><p>In summary, our model distilled a complex task into different levels of decision-making centered around a set of compositional strategies, which paved the way for future experiments that will provide key insights into the neural mechanisms underlying sophisticated cognitive behavior that go beyond what most of the field currently studies.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects and materials</title><p>Two male rhesus monkeys (<italic>Macaca mulatta</italic>) were used in the study (O and P). They weighed on average 6–7 kg during the experiments. All procedures followed the protocol approved by the Animal Care Committee of Shanghai Institutes for Biological Sciences, Chinese Academy of Sciences (CEBSIT-2021004).</p></sec><sec id="s4-2"><title>Training procedure</title><p>To help monkeys understand the Pac-Man game and develop their decision-making strategies, we divided the training procedures into the following three stages. In each stage, we gradually increased game depth based on their conceptual and implementational complexity.</p><sec id="s4-2-1"><title>Stage 1: Reward</title><p>In the first stage, the monkeys were trained to use the joystick to control Pac-Man to navigate in simple mazes for collecting pellets (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). Training began with the horizontal and the vertical linear mazes. In each maze, Pac-Man started from the center where pellets were at one end and a static ghost was at the other end. Monkeys earned two drops of juice (one drop = 0.5 mL) immediately when consuming a pellet. Monkeys could earn an extra-large amount of juice by clearing all pellets. Running toward the static ghost would lead to the end of the trial with a time-out penalty (5 s). When the monkeys completed more than 100 correct trials with above 80% accuracy, we introduced two slightly more complex mazes, the T and the upside-down T maze. After the monkeys completed more than 50 correct trials in the T-mazes with above 80% accuracy, we introduced the H-maze. Stage 1 training included 58 sessions for Monkey O and 84 sessions for Monkey P.</p></sec><sec id="s4-2-2"><title>Stage 2: Ghost</title><p>In the second stage, the monkeys were trained to deal with the ghosts (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>). In addition, the mazes used in this stage were closed and had loops. A ghost would block one of the routes leading to the pellets, forcing the monkeys to take alternative routes. In the first phase, the ghost was stationary in a square maze. Pac-Man started from one of the four corners, and pellets were distributed in the two adjacent arms. The ghost was placed at the corner where the two arms joined, forcing Pac-Man to retreat after clearing the pellets in one arm. In the second phase, the ghost moved within the arm. In the third phase, the ghost would chase Pac-Man. Stage 2 training included 86 sessions for Monkey O and 74 sessions for Monkey P.</p></sec><sec id="s4-2-3"><title>Stage 3: Energizer</title><p>In this stage, the monkeys were trained to understand the energizer (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>). In the first phase, the monkeys were trained to understand the distinction between normal and scared ghosts. We used the square maze with a normal or a scared ghost randomly placed across trials. Blinky in the normal mode would chase Pac-Man, while in scared mode move in random directions at half of Pac-Man’s speed. Monkeys earned eight drops of juice after eating a scared ghost. In the second phase, the monkeys were trained with the maze that the scared mode could only be triggered by an energizer. Two energizers were randomly placed in each maze. Monkeys earned four drops of juice when eating an energizer and turned ghosts into the scared mode immediately. The scared mode lasted 14 s. As a reminder, ghosts in the scared mode flashed for 2 s before turning back into the normal mode. In the third phase, we adopted the maze used in our final gameplay recording (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The detailed game rules can be found in the following ‘Task paradigm’ session. Stage 3 training included 248 sessions for Monkey O and 254 sessions for Monkey P.</p></sec></sec><sec id="s4-3"><title>Task paradigm</title><p>The Pac-Man game in this study was adapted from the original game by Namco. All key concepts of the game are included. In the game, the monkey navigates a character named Pac-Man through a maze with a four-way joystick to collect pellets and energizers. The maze is sized at 700 × 900 pixels, displayed at the resolution of 1920 × 1080 on a 27-inch monitor placed at 68 cm away from the monkey. The maze can be divided into square tiles of 25 × 25 pixel.(<xref ref-type="bibr" rid="bib2">Binz et al., 2022</xref>). The pellets and energizers are placed at the center of a tile, and they are consumed when Pac-Man moves into the tile. In the recording sessions, there are 88 or 73 pellets in the maze, each worth two drops of juice, and three or four energizers, each worth four drops of juice. We divided the maze into four quarters, and the pellets and energizers are randomly placed in three of them, with one randomly chosen quarter empty. In addition, just as in the original game, there are five different kinds of fruits: cherry, strawberry, orange, apple, and melon. They yield 3, 5, 8, 12, and 17 drops of juice, respectively. In each game, one randomly chosen fruit is placed at a random location at each game. As in the original game, the maze also contains two tunnels that teleport Pac-Man to the opposite side of the maze.</p><p>There are two ghosts in the game, Blinky and Clyde. They are released from the Ghost Home, which is the center box of the maze, at the beginning of each game. Blinky is red and is more aggressive. It chases Pac-Man all the time. Clyde is orange. It moves toward Pac-Man when it is more than eight tiles away from Pac-Man. Otherwise, it moves toward the lower-left corner of the maze. The eyes of the ghosts indicate the direction they are traveling. The ghosts cannot abruptly reverse their direction in the normal mode. Scared ghosts move slowly to the ghost pen located at the center of the maze. The scared state lasts 14 s, and the ghosts flash as a warning during the last 2 s of the scared mode. Monkeys get eight drops of juice if they eat a ghost. Dead ghosts move back to the ghost pen and then respawn. The ghosts can also move through the tunnels, but their speed is reduced when inside the tunnel. For more explanations on the ghost behavior, please refer to <ext-link ext-link-type="uri" xlink:href="https://gameinternals.com/understanding-pac-man-ghost-behavior">https://gameinternals.com/understanding-pac-man-ghost-behavior</ext-link>.</p><p>When Pac-Man is caught by a ghost, it and the ghosts return to the starting location. The game is restarted after a time-out penalty. When all the pellets and energizers are collected, the monkey receives a reward based on the number of rounds that takes the monkey to complete the game: 20 drops if the round number is from 1 to 3; 10 drops if the round number is from 4 to 5; 5 drops if the round number is larger than 5.</p></sec><sec id="s4-4"><title>Behavioral data recording and preprocessing</title><p>We monitored the monkeys’ joystick movements, eye positions, and pupil sizes during the game. The joystick movements were sampled at 60 Hz. We used Eyelink 1000 Plus to record two monkeys’ eye positions and pupil sizes. The sampling rate was 500 Hz or 1000 Hz.</p><p>The data we presented here are based on the sessions after the monkeys went through all the training stages and were able to play the game consistently. On average, monkeys completed 33 ± 9 games in each session and each game took them 4.86 ± 1.75 attempts. The dataset includes 3217 games, 15,772 rounds, and 899,381 joystick movements. The monkeys’ detailed game statistics are shown in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>.</p></sec><sec id="s4-5"><title>Basic performance analysis</title><p>In <xref ref-type="fig" rid="fig1">Figure 1B</xref>, we compute the rewards for each available moving direction at each location by summing up the rewards from the pellets (one unit) and the energizers (two units) within five steps from Pac-Man’s location. Locations are categorized into four path types defined in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>. For each path type, we calculate the probability that the monkey moved in the direction with the largest rewards conditioned on the reward difference between the most (<inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and the second most rewarding directions (<inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). In <xref ref-type="fig" rid="fig1">Figure 1C</xref>, we compute the likelihood of Pac-Man moving toward or away from the ghosts in different modes with different Dijkstra distance between Pac-Man and the ghosts. We classified Pac-Man’s moving action into two types, toward and away, according to whether the action decreased or increased the Dijkstra distance between Pac-Man and the ghosts. Dijkstra distance is defined as the distance of the shortest path between two positions in the maze.</p></sec><sec id="s4-6"><title>Basis strategies</title><p>We include six basis strategies in the hierarchical strategy model. In each basis strategy, we compute the utility values for all directions (<inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>={left, right, up, down}), expressed as a vector of length 4. Notice that not all directions are available, utility values for unavailable directions are set to be negative infinity. The moving direction is computed according to the largest average utility value for each strategy.</p><p>We determine the utility associated with each direction and its possible trajectories. Specifically, let <inline-formula><mml:math id="inf13"><mml:mi>p</mml:mi></mml:math></inline-formula> represents Pac-Man’s position and <inline-formula><mml:math id="inf14"><mml:mi>τ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> represent a path starting from <italic>p</italic> with the length of 10. We define <inline-formula><mml:math id="inf15"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to be the position of two ghosts, Blinky and Clyde, and <inline-formula><mml:math id="inf16"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to be the positions of pellets, energizers, and fruits, respectively. We compute the utility of each path <inline-formula><mml:math id="inf17"><mml:mi>τ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> as follows (without specific noting, <inline-formula><mml:math id="inf18"><mml:mi>τ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is denoted as <inline-formula><mml:math id="inf19"><mml:mi>τ</mml:mi></mml:math></inline-formula> for simplicity).</p><p>We use the <italic>local</italic> strategy to describe the local graze behavior within a short distance with the utility function defined as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>τ</mml:mi><mml:mo>∩</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf20"><mml:mi>τ</mml:mi><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the pellets/energizers/fruits on the path. Specific parameters for awarded and penalized utilities of each game element in the model can be found in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>.</p><p><italic>Evade</italic> strategy focuses on dodging close-by ghosts. Specifically, we create two evade strategies (<italic>evade</italic> Blinky and <italic>evade</italic> Clyde) that react to the respective ghost, with the utility function defined as<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="fraktur">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf21"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. Here, <inline-formula><mml:math id="inf23"><mml:mi mathvariant="script">I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is an indication function, where <inline-formula><mml:math id="inf24"><mml:mi mathvariant="script">I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> when statement <inline-formula><mml:math id="inf25"><mml:mi>s</mml:mi></mml:math></inline-formula> is true, otherwise <inline-formula><mml:math id="inf26"><mml:mi mathvariant="script">I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p><p><italic>Energizer</italic> strategy moves Pac-Man toward the closest energizer. In this case, the rewards set <inline-formula><mml:math id="inf27"><mml:mi>r</mml:mi></mml:math></inline-formula> only contains the positions of energizers (i.e., <inline-formula><mml:math id="inf28"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>τ</mml:mi><mml:mo>∩</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><italic>Approach</italic> strategy moves Pac-Man toward the ghosts, regardless of the ghosts’ mode. Its utility function is<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>τ</mml:mi><mml:mo>∩</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><italic>Global</italic> strategy does not use a decision tree. It counts the total number of pellets in the whole maze in each direction without considering any trajectories. For example, the utility for the down direction is the total number of pellets that sit vertically below Pac-Man’s location.</p><p>We construct the utility of each agent as a vector <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi> </mml:mi><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi> </mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> of the four directions. For each direction <inline-formula><mml:math id="inf30"><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula>, its utility is obtained by averaging utilities <inline-formula><mml:math id="inf31"><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">τ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> on all the path sets <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in that direction:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-7"><title>Models and model fitting</title><p>We adopted a softmax policy to linearly combine the utility values under each basis strategy and used MLE to estimate the model parameters with the monkeys’ behavior.</p><sec id="s4-7-1"><title>Utility preprocessing</title><p>To combine the strategies and produce a decision, we first preprocess the utility data computed from the decision trees with two steps. First, because two <italic>evade</italic> strategies have negative utility values, we calculate their difference to the worst-case scenario within a trial and use the difference, which is a positive value, as the utility for the two <italic>evade</italic> strategies:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>C</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Second, because the scale of utility value varies in different strategies, we normalize the utilities within each strategy:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-7-2"><title>Softmax policy</title><p>With the adjusted and normalized utility values, each strategy <italic>a</italic> is associated with a set of utility values <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for four directions <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We compute the utility for each direction <italic>d</italic> by simply combining them linearly with strategy weights <inline-formula><mml:math id="inf36"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> :<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The final decision is based on a softmax policy:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf37"><mml:mi>π</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> describes the probability of choosing <italic>d</italic> given weights <bold><italic>w</italic></bold>.</p></sec><sec id="s4-7-3"><title>Maximum likelihood estimate</title><p>We use the MLE approach to estimate monkeys’ strategy weights <inline-formula><mml:math id="inf38"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula> in a time window <inline-formula><mml:math id="inf39"><mml:mi>δ</mml:mi></mml:math></inline-formula>. Based on Pac-Man’s actual moving directions <inline-formula><mml:math id="inf40"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> , we compute the likelihood as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The strategy weights within a time window can be estimated by maximizing the log-likelihood:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:munder><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-7-4"><title>Dynamic compositional strategy model</title><p>The dynamic compositional strategy model estimates the strategy weights using time windows of flexible length. We assume that the relative strategy weights are stable for a period. The weights can be estimated from the monkeys’ choices during this period. We design a two-pass fitting procedure to divide each trial into segments of stable strategies and extract the strategy weights for each segment, avoiding potential overfitting caused by segmentations too fine with too many weight parameters while still capturing the strategy dynamics. The procedure is as follows:</p><list list-type="order"><list-item><p>We first formulate fine-grained time windows <inline-formula><mml:math id="inf41"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> according to the following events: Pac-Man direction changes, ghost consumptions, and energizer consumptions. The assumption is that the strategy changes only occurred at those events.</p></list-item><list-item><p>The first-pass fitting is done by using the fine-grained time windows to get the maximum likelihood estimates of the strategy weights as a time series of <inline-formula><mml:math id="inf42"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">δ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic"> </mml:mi><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> .</p></list-item><list-item><p>We then use a change-point detection algorithm to detect any changes in the strategy weights in the time series of <inline-formula><mml:math id="inf43"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">δ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic"> </mml:mi><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> . Specifically, we select a changing-points number <inline-formula><mml:math id="inf44"><mml:mi>K</mml:mi></mml:math></inline-formula> and used a forward dynamic programming algorithm (<xref ref-type="bibr" rid="bib34">Truong et al., 2020</xref>) to divide the series into <inline-formula><mml:math id="inf45"><mml:mi>K</mml:mi></mml:math></inline-formula> segments <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by minimizing the quadratic loss <inline-formula><mml:math id="inf47"><mml:mi>c</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> . Here, <inline-formula><mml:math id="inf48"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> is the empirical mean of these fine-grained weights corresponding to segment sets, <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . With <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , we construct the coarse-grained time windows by combining the fine-grained time windows.</p></list-item><list-item><p>The second-pass fitting is then done using the coarse-grained time windows <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with MLE, and the sum of log-likelihood <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mtext> </mml:mtext><mml:mo>∈</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the loss function.</p></list-item><list-item><p>We repeat the steps 3 and 4 with hyperparameter <inline-formula><mml:math id="inf53"><mml:mi>K</mml:mi></mml:math></inline-formula> traversing through 2, 3, ..., 20 to find out <inline-formula><mml:math id="inf54"><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> . The final fitting results are based on the normalized fitted weights with <inline-formula><mml:math id="inf55"><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> coarse-grained time windows <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> .</p></list-item></list><p>To ensure that the fitted weights are unique (<xref ref-type="bibr" rid="bib7">Buja et al., 1989</xref>) in each time window, we combine utilities of any strategies that give exactly the same action sequence and reduce multiple strategy terms (e.g., local and energizer) to one hybrid strategy (e.g., local + energizer). After MLE fitting, we divide the fitted weight for this hybrid strategy equally among the strategies that give the same actions in the time segments.</p></sec><sec id="s4-7-5"><title>Static strategy model</title><p>The static strategy model uses all data to estimate a single set of strategy weights.</p></sec><sec id="s4-7-6"><title>LARL model</title><p>The model shares the same structure with a standard Q-learning algorithm but uses the monkeys' actual joystick movements as the fitting target. To highlight the flatness of the model, we adopt a common assumption that the parameterization of the utility function is linear (<xref ref-type="bibr" rid="bib33">Sutton and Barto, 2018</xref>) with respect to the seven game features: <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>∙</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> . These features include the local pellet number within five steps in four directions <inline-formula><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the Dijkstra distance to the closest pellet <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the Dijkstra distance to the closest energizer <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the global pellet number (distance larger than five steps) weighted by their inverse Dijkstra distances to Pac-Man <inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the Dijkstra distance to Blinky <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the Dijkstra distances to Clyde <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and the Dijkstra distance to the closest scared ghost <inline-formula><mml:math id="inf64"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . If a feature is not available in some game context (e.g., <inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is not available when ghosts are in the normal mode or the dead mode), we denote it to be null. The update rule follows the standard temporal-difference learning rule:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf66"><mml:mi>α</mml:mi></mml:math></inline-formula> is the learning rate, <inline-formula><mml:math id="inf67"><mml:mi>γ</mml:mi></mml:math></inline-formula> is the discount factor, and <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the reward that the agent received from state <inline-formula><mml:math id="inf69"><mml:mi mathvariant="bold-italic">s</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf70"><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">’</mml:mi></mml:math></inline-formula> via action <inline-formula><mml:math id="inf71"><mml:mi>d</mml:mi></mml:math></inline-formula>. All the reward values are the actual rewards that the monkeys received in the game. Compared to a typical TD update rule, the max operation in the utility-to-go term <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is replaced with the utility under the monkeys’ actual joystick movement <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> . Feature weights are randomly initialized, and we use the monkey behavioral data to update these weights. There are two model hyper-parameters:  learning rate <inline-formula><mml:math id="inf74"><mml:mi mathvariant="normal">α</mml:mi></mml:math></inline-formula> and discount factor <inline-formula><mml:math id="inf75"><mml:mi mathvariant="normal">γ</mml:mi></mml:math></inline-formula>. They are selected through threefold cross-validation. The best hyperparameters are <inline-formula><mml:math id="inf76"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf77"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula> for Monkey O and <inline-formula><mml:math id="inf78"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.025</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf79"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula> for Monkey P. The trained feature weights <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are shown in <xref ref-type="table" rid="app1table7">Appendix 1—table 7</xref>.</p></sec><sec id="s4-7-7"><title>Linear perceptron model</title><p>We build a linear perceptron as another representative flat descriptive model (without calculating utilities and strategies) to describe monkeys’ decision-making based on the same 20 features included in the other models. These features include the modes of Blinky and Clyde (two features), Dijkstra distances between Blinky and Pac-Man in four directions (four features), Dijkstra distances between Clyde and Pac-Man in four directions (four features), Dijkstra distances between the closest energizer and Pac-Man in four directions (four features), distances between fruits and Pac-Man in four directions (four features), the number of pellets within 10 steps of Pac-Man, and the number of pellets left in the maze. For unavailable directions, the corresponding feature value is filled with a none value. We trained a three-layer perceptron with monkeys’ choice behavior: an input layer for 20 features, a hidden layer, and an output layer for four directions. We used scikit-learn (<ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/">https://scikit-learn.org/</ext-link>). The size of the hidden layer was selected from <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:math></inline-formula>{16, 32, 64, 128, 256} with the largest average prediction accuracy on all data. For Monkey O, the best hidden unit number is 64, and for Monkey P, the best hidden unit number is 128. Each model uses Adam for optimization, training batch size = 128, learning rate = 0.001, regularization parameter = 0.0001, and the activation function <inline-formula><mml:math id="inf82"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> for the hidden layer is an identity function.</p></sec><sec id="s4-7-8"><title>Model comparison</title><p>We compare four models (static strategy model, dynamic strategy model, LARL, and linear perceptron model) in four game contexts shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A and E</xref>, and <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>. Fivefold cross-validations are used to evaluate the fitting performance of these models with each monkeys’ behavior data.</p></sec><sec id="s4-7-9"><title>Strategy heuristic analysis</title><p>We label the behavior strategy as <italic>vague</italic> when the weight difference between the largest and the second largest strategies is less than 0.1 (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Otherwise, the labels are based on the strategy with the largest weight.</p><p>In <xref ref-type="fig" rid="fig3">Figure 3A and B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A, B, E, and F</xref>, we evaluate the strategy probability dynamics with respect to two features: local pellet density (the number of pellets within 10 steps from Pac-Man) and scared ghost distance. We group the behavior data based on these two features and calculate the frequency of the relevant strategies in each. Means and standard deviations are computed by bootstrapping 10 times with a sample size of 100 for each data point (<xref ref-type="fig" rid="fig3">Figure 3A and B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A, B, E and F</xref>).</p><p>In <xref ref-type="fig" rid="fig3">Figure 3C and D</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C, D, G and H</xref>, monkeys’ moving trajectories with at least four consecutive steps labeled as <italic>global</italic> strategy are selected. We use Dijkstra’s algorithm to compute the shortest path from the starting position when the monkey switched to <italic>global</italic> strategy to the ending position when the monkey first reached a pellet. The trajectory with the fewest turns is determined by sorting all possible paths between the starting and the ending position.</p></sec></sec><sec id="s4-8"><title>Eye movement analysis</title><p>We label monkeys’ fixation targets based on the distance between the eye position and the relevant game objects: Pac-Man, ghosts, pellets, and energizer. When the distances are within one tile (25 pixel), we add the corresponding target to the label. There can be multiple fixation labels because these objects may be close to each other.</p><p>In <xref ref-type="fig" rid="fig3">Figure 3E</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A and E</xref>, we select strategy periods with more than 10 consecutive steps and compute the fixation ratio by dividing the time that the monkeys spent looking at an object within each period by the period length. As pellets and energizers do not move but Pac-Man and ghosts do, we do not differentiate between fixations and smooth pursuits when measuring where the monkeys looked at. We compute the average fixation ratio across the periods with the same strategies.</p><p>In the pupil dilation analyses in <xref ref-type="fig" rid="fig3">Figure 3F</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B, C, D, F, G, and H</xref>, we z-score the pupil sizes in each game round. Data points that are three standard deviations away from the mean are excluded. We align the pupil size to strategy transitions and calculate the mean and the standard error (<xref ref-type="fig" rid="fig3">Figure 3F</xref>, solid line and shades). As the control, we select strategy transitions that go through the <italic>vague</italic> strategy and align the data to the center of the vague period to calculate the average pupil size and the standard error (<xref ref-type="fig" rid="fig3">Figure 3F</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B and F</xref>, dashed line and shades). <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C, D, G and H</xref> are plotted in a similar way but only specific strategy transitions.</p></sec><sec id="s4-9"><title>Compound strategy analysis</title><sec id="s4-9-1"><title>Planned attack</title><p>We define <italic>planned attack</italic> and <italic>accidental consumption</italic> trials according to the strategy labels after the energizer consumption: when at least 8 out of the 10 time steps after the energizer consumption are labeled as the <italic>approach</italic> strategy, the trial is defined as <italic>planned attack</italic>; otherwise, this trial is defined as <italic>accidental consumption</italic>. There are 478 (Monkey O) and 459 (Monkey P) <italic>planned attack</italic> trials and 1984 (Monkey O) and 1257 (Monkey P) <italic>accidental consumption</italic> trials. These trials are aligned to the time of energizer consumption in <xref ref-type="fig" rid="fig5">Figure 5A</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A and E</xref>.</p><p>In <xref ref-type="fig" rid="fig5">Figure 5B</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B and F</xref>, the average of Pac-Man-energizer distance, energizer-ghost distance, and Pac-Man-ghost distance is computed at the beginning of the <italic>planned attack</italic> and <italic>accidental consumption</italic> trials. The beginning of each trial is defined as the position where Pac-Man started to take the direct shortest route toward the energizer. The average fixation ratios in <xref ref-type="fig" rid="fig5">Figure 5C</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C and G</xref> are computed from the beginning of each <italic>planned attack</italic> or <italic>accidental consumption</italic> till when the energizer is eaten.</p><p>In some of the <italic>accidental consumption</italic> trials (Monkey O: 625/31.5%; Monkey P: 477/37.9%), Pac-Man caught a ghost although the monkeys did not pursue the ghosts immediately after the energizer consumption. In contrast, all <italic>planned attack</italic> trials resulted in Pac-Man catching the ghosts successfully. These trials are aligned to the ghost consumption in <xref ref-type="fig" rid="fig5">Figure 5D</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D and H</xref>.</p></sec><sec id="s4-9-2"><title>Suicide</title><p>We define <italic>suicide</italic> and <italic>failed evasion</italic> trials based on the strategy labels in the last ten steps before Pac-Man’s death: a trial is defined as <italic>suicide</italic> when all 10 steps are labeled as <italic>approach</italic> and as <italic>failed evasion</italic> when all steps are labeled as <italic>evade</italic>.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6A</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A and E</xref>, the distance between Pac-Man and the closest pellet and the distance between Pac-Man reset location and the ghost are computed at the time point when the monkeys switched to the <italic>approach</italic> (<italic>suicide</italic>) or <italic>evade</italic> (<italic>failed evasion</italic>) strategy. Also, in <xref ref-type="fig" rid="fig6">Figure 6B</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B and F</xref>, the average distance between Pac-Man and two ghosts is computed in the same condition. The average fixation ratios in <xref ref-type="fig" rid="fig6">Figure 6C</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C and G</xref> are computed from that time point until Pac-Man’s death. In <xref ref-type="fig" rid="fig6">Figure 6D</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1D and H</xref>, the relative pupil sizes are aligned to Pac-Man’s death.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Investigation, Methodology, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con4"><p>Data curation, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con5"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con6"><p>Formal analysis</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All procedures followed the protocol approved by the Animal Care Committee of Shanghai Institutes for Biological Sciences, Chinese Academy of Sciences (CEBSIT-2021004).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-74500-transrepform1-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data and codes that support the findings of this study are provided at: <ext-link ext-link-type="uri" xlink:href="https://github.com/superr90/Monkey_PacMan">https://github.com/superr90/Monkey_PacMan</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e2d1526384a505c18c7f7a47a6effdebf5bffb29;origin=https://github.com/superr90/Monkey_PacMan;visit=swh:1:snp:32114cb66a1d13416bc0fc1fd95626879b0e4215;anchor=swh:1:rev:6f74eef3b321718ac2f8d4d4f5f1d904b10d2a85">swh:1:rev:6f74eef3b321718ac2f8d4d4f5f1d904b10d2a85</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Hierarchical Decision-Making Analysis for Behavior Data</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/superr90/Monkey_PacMan">GitHub</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Wei Kong, Lu Yu, Ruixin Su, Yunxian Bai, Zhewei Zhang, Yang Xie, Tian Qiu, Yiwen Xu, Ce Ma, Zhihua Zhu, and Yue Hao for their help in all phases of the study, and Liping Wang and Xaq Pitkow for providing comments and advice.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beran</surname><given-names>MJ</given-names></name><name><surname>Parrish</surname><given-names>AE</given-names></name><name><surname>Futch</surname><given-names>SE</given-names></name><name><surname>Evans</surname><given-names>TA</given-names></name><name><surname>Perdue</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Looking ahead? Computerized maze task performance by chimpanzees (Pan troglodytes), rhesus monkeys (Macaca mulatta), capuchin monkeys (Cebus apella), and human children (<italic>Homo sapiens</italic>)</article-title><source>Journal of Comparative Psychology</source><volume>129</volume><fpage>160</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1037/a0038936</pub-id><pub-id pub-id-type="pmid">25798793</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binz</surname><given-names>M</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Endres</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Heuristics From Bounded Meta-Learned Inference</article-title><source>Psychological Review</source><volume>1</volume><elocation-id>330</elocation-id><pub-id pub-id-type="doi">10.1037/rev0000330</pub-id><pub-id pub-id-type="pmid">34990160</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bird</surname><given-names>CD</given-names></name><name><surname>Emery</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rooks Use Stones to Raise the Water Level to Reach a Floating Worm</article-title><source>Current Biology</source><volume>19</volume><fpage>1410</fpage><lpage>1414</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.07.033</pub-id><pub-id pub-id-type="pmid">19664926</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective</article-title><source>Cognition</source><volume>113</volume><fpage>262</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2008.08.011</pub-id><pub-id pub-id-type="pmid">18926527</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Weinstein</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Model-based hierarchical reinforcement learning and human action control</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>369</volume><elocation-id>20130480</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0480</pub-id><pub-id pub-id-type="pmid">25267822</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brotcorne</surname><given-names>F</given-names></name><name><surname>Giraud</surname><given-names>G</given-names></name><name><surname>Gunst</surname><given-names>N</given-names></name><name><surname>Fuentes</surname><given-names>A</given-names></name><name><surname>Wandia</surname><given-names>IN</given-names></name><name><surname>Beudels-Jamar</surname><given-names>RC</given-names></name><name><surname>Poncin</surname><given-names>P</given-names></name><name><surname>Huynen</surname><given-names>MC</given-names></name><name><surname>Leca</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Intergroup variation in robbing and bartering by long-tailed macaques at Uluwatu Temple (Bali, Indonesia)</article-title><source>Primates; Journal of Primatology</source><volume>58</volume><fpage>505</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1007/s10329-017-0611-1</pub-id><pub-id pub-id-type="pmid">28516338</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buja</surname><given-names>A</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Linear smoothers and additive models</article-title><source>The Annals of Statistics</source><volume>17</volume><fpage>453</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1214/aos/1176347115</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bunge</surname><given-names>SA</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Neuroscience of rule-guided behavior</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195314274.001.0001</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dezfouli</surname><given-names>A</given-names></name><name><surname>Balleine</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Actions, Action Sequences and Habits: Evidence That Goal-Directed and Habitual Action Control Are Hierarchically Organized</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003364</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003364</pub-id><pub-id pub-id-type="pmid">24339762</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foderaro</surname><given-names>G</given-names></name><name><surname>Swingler</surname><given-names>A</given-names></name><name><surname>Ferrari</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A model-based approach to optimizing Ms Pac-Man game strategies in real time</article-title><source>IEEE Transactions on Computational Intelligence and AI in Games</source><volume>9</volume><fpage>153</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1109/TCIAIG.2016.2523508</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Genovesio</surname><given-names>A</given-names></name><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Neuroscience of Rule-Guided Behavior</source><publisher-loc>Oxford</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195314274.003.0006</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruber</surname><given-names>R</given-names></name><name><surname>Schiestl</surname><given-names>M</given-names></name><name><surname>Boeckle</surname><given-names>M</given-names></name><name><surname>Frohnwieser</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>R</given-names></name><name><surname>Gray</surname><given-names>RD</given-names></name><name><surname>Clayton</surname><given-names>NS</given-names></name><name><surname>Taylor</surname><given-names>AH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>New Caledonian Crows Use Mental Representations to Solve Metatool Problems</article-title><source>Current Biology</source><volume>29</volume><fpage>686</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.01.008</pub-id><pub-id pub-id-type="pmid">30744978</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haroush</surname><given-names>K</given-names></name><name><surname>Williams</surname><given-names>ZM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal prediction of opponent’s behavior during cooperative social interchange in primates</article-title><source>Cell</source><volume>160</volume><fpage>1233</fpage><lpage>1245</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.01.045</pub-id><pub-id pub-id-type="pmid">25728667</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoshi</surname><given-names>E</given-names></name><name><surname>Shima</surname><given-names>K</given-names></name><name><surname>Tanji</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Neuronal activity in the primate prefrontal cortex in the process of motor selection based on two behavioral rules</article-title><source>Journal of Neurophysiology</source><volume>83</volume><fpage>2355</fpage><lpage>2373</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.83.4.2355</pub-id><pub-id pub-id-type="pmid">10758139</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pupil Size as a Window on Neural Substrates of Cognition</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>466</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.03.005</pub-id><pub-id pub-id-type="pmid">32331857</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kira</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Shadlen</surname><given-names>MNN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural implementation of Wald’s sequential probability ratio test</article-title><source>Neuron</source><volume>85</volume><fpage>861</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.01.007</pub-id><pub-id pub-id-type="pmid">25661183</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience Needs Behavior: Correcting a Reductionist Bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leca</surname><given-names>JB</given-names></name><name><surname>Gunst</surname><given-names>N</given-names></name><name><surname>Gardiner</surname><given-names>M</given-names></name><name><surname>Wandia</surname><given-names>IN</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Acquisition of object-robbing and object/food-bartering behaviours: a culturally maintained token economy in free-ranging long-tailed macaques</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>376</volume><elocation-id>20190677</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0677</pub-id><pub-id pub-id-type="pmid">33423623</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Radulescu</surname><given-names>A</given-names></name><name><surname>Daniel</surname><given-names>R</given-names></name><name><surname>DeWoskin</surname><given-names>V</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic Interaction between Reinforcement Learning and Attention in Multidimensional Environments</article-title><source>Neuron</source><volume>93</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.040</pub-id><pub-id pub-id-type="pmid">28103483</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loukola</surname><given-names>OJ</given-names></name><name><surname>Solvi</surname><given-names>C</given-names></name><name><surname>Coscos</surname><given-names>L</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bumblebees show cognitive flexibility by improving on an observed complex behavior</article-title><source>Science (New York, N.Y.)</source><volume>355</volume><fpage>833</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1126/science.aag2360</pub-id><pub-id pub-id-type="pmid">28232576</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Riedmiller</surname><given-names>M</given-names></name><name><surname>Fidjeland</surname><given-names>AK</given-names></name><name><surname>Ostrovski</surname><given-names>G</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Beattie</surname><given-names>C</given-names></name><name><surname>Sadik</surname><given-names>A</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>King</surname><given-names>H</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Legg</surname><given-names>S</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/nature14236</pub-id><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Ramírez-Ruiz</surname><given-names>J</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Heuristics and optimal solutions to the breadth:depth dilemma</article-title><source>PNAS</source><volume>117</volume><fpage>19799</fpage><lpage>19808</lpage><pub-id pub-id-type="doi">10.1073/pnas.2004929117</pub-id><pub-id pub-id-type="pmid">32759219</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Rumsey</surname><given-names>KM</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Parikh</surname><given-names>K</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1040</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/nn.3130</pub-id><pub-id pub-id-type="pmid">22660479</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ong</surname><given-names>WS</given-names></name><name><surname>Madlon-Kay</surname><given-names>S</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuronal correlates of strategic cooperation in monkeys</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>116</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00746-9</pub-id><pub-id pub-id-type="pmid">33230321</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostlund</surname><given-names>SB</given-names></name><name><surname>Winterbauer</surname><given-names>NE</given-names></name><name><surname>Balleine</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Evidence of action sequence chunking in goal-directed instrumental conditioning and its dependence on the dorsomedial prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>8280</fpage><lpage>8287</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1176-09.2009</pub-id><pub-id pub-id-type="pmid">19553467</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinhold</surname><given-names>AS</given-names></name><name><surname>Sanguinetti-Scheck</surname><given-names>JI</given-names></name><name><surname>Hartmann</surname><given-names>K</given-names></name><name><surname>Brecht</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Behavioral and neural correlates of hide-and-seek in rats</article-title><source>Science (New York, N.Y.)</source><volume>365</volume><fpage>1180</fpage><lpage>1183</lpage><pub-id pub-id-type="doi">10.1126/science.aax4705</pub-id><pub-id pub-id-type="pmid">31515395</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohlfshagen</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Perez-Liebana</surname><given-names>D</given-names></name><name><surname>Lucas</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pac-Man conquers academia: Two decades of research using a classic arcade game</article-title><source>IEEE Transactions on Games</source><volume>10</volume><fpage>233</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1109/TG.2017.2737145</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabbatini</surname><given-names>G</given-names></name><name><surname>Manrique</surname><given-names>HM</given-names></name><name><surname>Trapanese</surname><given-names>C</given-names></name><name><surname>De Bortoli Vizioli</surname><given-names>A</given-names></name><name><surname>Call</surname><given-names>J</given-names></name><name><surname>Visalberghi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sequential use of rigid and pliable tools in tufted capuchin monkeys (Sapajus spp</article-title><source>Animal Behaviour</source><volume>87</volume><fpage>213</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2013.10.033</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sanz</surname><given-names>CM</given-names></name><name><surname>Call</surname><given-names>J</given-names></name><name><surname>Boesch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Tool Use in Animals Cognition and Ecology</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511894800</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Learning to predict by the methods of temporal differences</article-title><source>Machine Learning</source><volume>3</volume><fpage>9</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1007/BF00115009</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Precup</surname><given-names>D</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</article-title><source>Artificial Intelligence</source><volume>112</volume><fpage>181</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/S0004-3702(99)00052-1</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning, Second Edition: An Introduction</source><publisher-loc>Massachusetts, United States</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truong</surname><given-names>C</given-names></name><name><surname>Oudre</surname><given-names>L</given-names></name><name><surname>Vayatis</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Selective review of offline change point detection methods</article-title><source>Signal Processing</source><volume>167</volume><elocation-id>107299</elocation-id><pub-id pub-id-type="doi">10.1016/j.sigpro.2019.107299</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsitsiklis</surname><given-names>JN</given-names></name><name><surname>Van Roy</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>An analysis of temporal-difference learning with function approximation</article-title><source>IEEE Transactions on Automatic Control</source><volume>42</volume><fpage>674</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1109/9.580874</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsujimoto</surname><given-names>S</given-names></name><name><surname>Genovesio</surname><given-names>A</given-names></name><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Comparison of strategy signals in the dorsolateral and orbital prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>4583</fpage><lpage>4592</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5816-10.2011</pub-id><pub-id pub-id-type="pmid">21430158</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Braun</surname><given-names>A</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pupil-linked arousal is driven by decision uncertainty and alters serial choice bias</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14637</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14637</pub-id><pub-id pub-id-type="pmid">28256514</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Van Seijen</surname><given-names>H</given-names></name><name><surname>Fatemi</surname><given-names>M</given-names></name><name><surname>Romoff</surname><given-names>J</given-names></name><name><surname>Laroche</surname><given-names>R</given-names></name><name><surname>Barnes</surname><given-names>T</given-names></name><name><surname>Tsang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hybrid reward architecture for reinforcement learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.04208">https://arxiv.org/abs/1706.04208</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Ortega</surname><given-names>HK</given-names></name><name><surname>Atilgan</surname><given-names>H</given-names></name><name><surname>Murphy</surname><given-names>CE</given-names></name><name><surname>Kwan</surname><given-names>AC</given-names></name><name><surname>Zaluchu</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pupil correlates of decision variables in mice playing a competitive mixed-strategy game</article-title><source>Neuroscience</source><volume>2</volume><elocation-id>5292</elocation-id><pub-id pub-id-type="doi">10.1101/2021.08.05.455292</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inferring relevance in a changing world</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2011.00189</pub-id><pub-id pub-id-type="pmid">22291631</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>SBM</given-names></name><name><surname>Tu</surname><given-names>JC</given-names></name><name><surname>Piantadosi</surname><given-names>ST</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Bum</surname><given-names>S</given-names></name><name><surname>Yoo</surname><given-names>M</given-names></name><name><surname>Tu</surname><given-names>JC</given-names></name><name><surname>Piantadosi</surname><given-names>ST</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The neural basis of predictive pursuit</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>252</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0561-6</pub-id><pub-id pub-id-type="pmid">31907436</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Training procedure.</title><p>(<bold>A</bold>) Stage 1 training mazes. From left to right: (1) Vertical maze. Pac-Man started from the middle position, with several pellets in one direction and a static ghost in the other. The monkeys learned to move the joystick upward and downward. (2) Horizontal maze. The monkeys learned to move the joystick toward left and right. (3) T-maze. Pac-Man started from the vertical arm, and the monkeys learned to move out of it by turning left or right. Pellets were placed in one arm and a static ghost in the other. (4) H-maze. Pac-Man started from the middle of the maze. There were pellets placed on the way leading to one of the three arms, and a static ghost was placed at the crossroad on the opposite side. (<bold>B</bold>) Stage 2 training mazes. From left to right: (1) Square maze with a static ghost. Pac-Man started from one of the four corners, and pellets were placed in two adjacent sides with a static ghost placed at the corner connecting the two. (2) Square maze with a moving ghost. Pac-Man started from the middle of one of the four sides, and pellets were placed on the opposite side. A ghost moved from one end of the pellet side and stopped at the other end. (3) Eight-shaped maze with a moving ghost. Pac-Man stated from one of the four corners. The pellets were placed in the middle tunnel. A ghost started from a corner and moved toward the pellets. (<bold>C</bold>) Stage 3 training mazes. From left to right: (1) Square maze with Blinky. Pac-Man started from the middle of the bottom side with pellets placed on both sides. Blinky in normal mode started from its home. (2) Square maze with a ghost in a permanent scared mode. The scared ghost started from its home. Once caught by Pac-Man, the ghost went back to its home. (3) Maze with an energizer and Blinky. An energizer was randomly placed in the maze. Once the energizer was eaten, the ghost would be turned into the scared mode. The scared mode lasted until the ghost was eaten by Pac-Man. Once the ghost was eaten, it returned to its home immediately and came out again in the normal mode. After the monkeys were able to perform the task, we limited the scared mode to 14 s.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-app1-fig1-v2.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Basic game statistics of Monkey O (left) and Monkey P (right).</title><p>(<bold>A</bold>, <bold>G</bold>) The number of rounds to clear all pellets in each game. Vertical dashed lines denote means. (<bold>B, H</bold>) The number of games accomplished on each day. Vertical dashed lines denote means. (<bold>C, I</bold>) The average number of rounds to clear a maze plotted against the number of games in a session. Vertical lines denote standard deviations. Playing more games in each session can slightly improve the monkey’s game performance. (<bold>D, J</bold>) The average number of rounds during the training. (<bold>E, K</bold>) The time needed to clear a maze. Vertical dashed lines denote means. (<bold>E</bold>, <bold>K</bold>) The time needed to clear a maze. Vertical dashed lines denote means.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-app1-fig2-v2.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Strategy basis correlation matrix.</title><p>We computed the Pearson correlations between the action sequences chosen with each basis strategy within each coarse-grained segment determined by the two-pass fitting procedure. As a control, we computed the correlation between each basis strategy and a random strategy, which generates action randomly, as a baseline. Most strategy pairs' correlation was lower than the random baseline.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-app1-fig3-v2.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Recovering the strategy labels of an artificial agent with the dynamic compositional strategy model based on simulated gameplay data.</title><p>The confusion matrix between the fitted strategy labels and the ground-truth strategy labels from an artificial agent is shown. The artificial agent used time-varying strategy weights to combine six strategies illustrated in the method. Strategy weights were selected based on two monkeys’ choices at the same game context determined by the location and state of Pac-Man, the pellets, the energizers, and the ghosts. We used the dynamic compositional strategy model to estimate the strategy labels from 2050 rounds of simulated data and produced the confusion matrix. In most cases, the model was able to recover the correct strategy (diagonal boxes).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-app1-fig4-v2.tif"/></fig><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Four path types in the maze.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Path type</th><th align="left" valign="bottom">Selection criteria</th></tr></thead><tbody><tr><td align="left" valign="bottom">Straight</td><td align="left" valign="bottom">Contains two opposite moving directions</td></tr><tr><td align="left" valign="bottom">L-shape</td><td align="left" valign="bottom">Contains two orthogonal moving directions</td></tr><tr><td align="left" valign="bottom">Fork</td><td align="left" valign="bottom">Contains three moving directions</td></tr><tr><td align="left" valign="bottom">Cross</td><td align="left" valign="bottom">Contains four moving directions</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Awarded and penalized utilities for each game element in the model.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>(1–5)</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">3, 5, 8, 12, 17</td><td align="char" char="." valign="bottom">8</td><td align="char" char="." valign="bottom">-8</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Special game contexts and corresponding selection criteria.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Context</th><th align="left" valign="bottom">Selection criteria</th></tr></thead><tbody><tr><td align="left" valign="bottom">All stage</td><td align="left" valign="bottom">n.a.</td></tr><tr><td align="left" valign="bottom">Early game</td><td align="left" valign="bottom">Remaining number of pellets <inline-formula><mml:math id="inf88"><mml:mo>≥</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>90</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math></inline-formula> of total (80 for O and 65 for P)</td></tr><tr><td align="left" valign="bottom">Late game</td><td align="left" valign="bottom">Remaining number of pellets <inline-formula><mml:math id="inf89"><mml:mo>≤</mml:mo><mml:mn>10</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math></inline-formula> of total (10 for O and 7 for P)</td></tr><tr><td align="left" valign="bottom">Scared ghost</td><td align="left" valign="bottom">Any scared ghosts within 10 steps away from Pac-Man</td></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Comparison of prediction accuracy (± SE) across four models in four game contexts for the two monkeys.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Strategy</th><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom"/></tr><tr><th align="left" valign="bottom">Context</th><th align="left" valign="bottom">Dynamic</th><th align="left" valign="bottom">Static</th><th align="left" valign="bottom">LARL</th><th align="left" valign="bottom">Perceptron</th></tr></thead><tbody><tr><td align="left" valign="bottom">Overall</td><td align="char" char="." valign="bottom">0.904 ± 0.006</td><td align="char" char="." valign="bottom">0.816 ± 0.010</td><td align="char" char="." valign="bottom">0.669 ± 0.011</td><td align="char" char="." valign="bottom">0.624 ± 0.010</td></tr><tr><td align="left" valign="bottom">Early game</td><td align="char" char="." valign="bottom">0.886 ± 0.0016</td><td align="char" char="." valign="bottom">0.804 ± 0.025</td><td align="char" char="." valign="bottom">0.775 ± 0.021</td><td align="char" char="." valign="bottom">0.582 ± 0.026</td></tr><tr><td align="left" valign="bottom">Late game</td><td align="char" char="." valign="bottom">0.898 ± 0.011</td><td align="char" char="." valign="bottom">0.805 ± 0.019</td><td align="char" char="." valign="bottom">0.621 ± 0.018</td><td align="char" char="." valign="bottom">0.599 ± 0.019</td></tr><tr><td align="left" valign="bottom">Scared ghosts</td><td align="char" char="." valign="bottom">0.958 ± 0.010</td><td align="char" char="." valign="bottom">0.728 ± 0.031</td><td align="char" char="." valign="bottom">0.672 ± 0.025</td><td align="char" char="." valign="bottom">0.455 ± 0.030</td></tr></tbody></table><table-wrap-foot><fn><p>LARL: linear approximate reinforcement learning.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Comparison of prediction accuracy (± SE) across four models in four game contexts for Monkey O.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">Strategy</th><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/></tr><tr><th align="left" valign="top">Context</th><th align="left" valign="top">Dynamic</th><th align="left" valign="top">Static</th><th align="left" valign="top">LARL</th><th align="left" valign="top">Perceptron</th></tr></thead><tbody><tr><td align="left" valign="top">Overall</td><td align="char" char="." valign="top">0.907 ± 0.008</td><td align="char" char="." valign="top">0.806 ± 0.014</td><td align="char" char="." valign="top">0.659 ± 0.016</td><td align="char" char="." valign="top">0.632 ± 0.013</td></tr><tr><td align="left" valign="top">Early game</td><td align="char" char="." valign="top">0.868 ± 0.0132</td><td align="char" char="." valign="top">0.772 ± 0.054</td><td align="char" char="." valign="top">0.765 ± 0.042</td><td align="char" char="." valign="top">0.548 ± 0.037</td></tr><tr><td align="left" valign="top">Late game</td><td align="char" char="." valign="top">0.901 ± 0.016</td><td align="char" char="." valign="top">0.786 ± 0.027</td><td align="char" char="." valign="top">0.595 ± 0.026</td><td align="char" char="." valign="top">0.598 ± 0.026</td></tr><tr><td align="left" valign="top">Scared ghosts</td><td align="char" char="." valign="top">0.952 ± 0.019</td><td align="char" char="." valign="top">0.729 ± 0.047</td><td align="char" char="." valign="top">0.658 ± 0.046</td><td align="char" char="." valign="top">0.545 ± 0.050</td></tr></tbody></table><table-wrap-foot><fn><p>LARL: linear approximate reinforcement learning.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table6" position="float"><label>Appendix 1—table 6.</label><caption><title>Comparison of prediction accuracy (± SE) across four models in four game contexts for Monkey P.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">Strategy</th><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/></tr><tr><th align="left" valign="top">Context</th><th align="left" valign="top">Dynamic</th><th align="left" valign="top">Static</th><th align="left" valign="top">LARL</th><th align="left" valign="top">Perceptron</th></tr></thead><tbody><tr><td align="left" valign="top">Overall</td><td align="char" char="." valign="top">0.900 ± 0.009</td><td align="char" char="." valign="top">0.825 ± 0.012</td><td align="char" char="." valign="top">0.679 ± 0.015</td><td align="char" char="." valign="top">0.615 ± 0.014</td></tr><tr><td align="left" valign="top">Early game</td><td align="char" char="." valign="top">0.898 ± 0.017</td><td align="char" char="." valign="top">0.824 ± 0.021</td><td align="char" char="." valign="top">0.781 ± 0.022</td><td align="char" char="." valign="top">0.605 ± 0.035</td></tr><tr><td align="left" valign="top">Late game</td><td align="char" char="." valign="top">0.894 ± 0.016</td><td align="char" char="." valign="top">0.826 ± 0.026</td><td align="char" char="." valign="top">0.653 ± 0.025</td><td align="char" char="." valign="top">0.599 ± 0.028</td></tr><tr><td align="left" valign="top">Scared ghosts</td><td align="char" char="." valign="top">0.962 ± 0.011</td><td align="char" char="." valign="top">0.723 ± 0.041</td><td align="char" char="." valign="top">0.682 ± 0.027</td><td align="char" char="." valign="top">0.456 ± 0.037</td></tr></tbody></table><table-wrap-foot><fn><p>LARL: linear approximate reinforcement learning.</p></fn></table-wrap-foot></table-wrap><table-wrap id="app1table7" position="float"><label>Appendix 1—table 7.</label><caption><title>Trained feature weights in the LARL model for Monkey O and Monkey P.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">Feature</th><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/></tr><tr><th align="left" valign="top">Monkey</th><th align="left" valign="top"><inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf91"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf95"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf96"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="top">O</td><td align="char" char="." valign="top">0.3192</td><td align="char" char="." valign="top">0.0049</td><td align="char" char="." valign="top">–0.0758</td><td align="char" char="." valign="top">2.4743</td><td align="char" char="." valign="top">–0.7799</td><td align="char" char="." valign="top">1.0287</td><td align="char" char="." valign="top">–0.9717</td></tr><tr><td align="left" valign="top">P</td><td align="char" char="." valign="top">0.5584</td><td align="char" char="." valign="top">0.0063</td><td align="char" char="." valign="top">–0.0563</td><td align="char" char="." valign="top">2.5802</td><td align="char" char="." valign="top">–0.6324</td><td align="char" char="." valign="top">1.6068</td><td align="char" char="." valign="top">–1.1994</td></tr></tbody></table><table-wrap-foot><fn><p>LARL: linear approximate reinforcement learning.</p></fn></table-wrap-foot></table-wrap><media id="app1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-video1.mp4"><label>Appendix 1—video 1.</label><caption><title>Example game trials.</title><p>Monkey O’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in these example game trials.</p></caption></media><media id="app1video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-video2.mp4"><label>Appendix 1—video 2.</label><caption><title>Example game trials.</title><p>Monkey O’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in these example game trials.</p></caption></media><media id="app1video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-video3.mp4"><label>Appendix 1—video 3.</label><caption><title>Example game trials.</title><p>Monkey O’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in these example game trials.</p></caption></media><media id="app1video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-video4.mp4"><label>Appendix 1—video 4.</label><caption><title>Example game trials.</title><p>Monkey P’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in this example game segment.</p></caption></media><media id="app1video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-74500-video5.mp4"><label>Appendix 1—video 5.</label><caption><title>Example game trials.</title><p>Monkey P’s moving trajectory, actual and predicted actions, and labeled strategies are plotted in this example game segment.</p></caption></media></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74500.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.10.02.462713" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.02.462713"/></front-stub><body><p>Dr. Yang and colleagues trained nonhuman primates (rhesus monkeys) to play a semi-controlled version of the video game Pac-Man. This novel experimental paradigm allowed the authors to analyze and model the kinds of heuristic behavioral strategies monkeys use to solve relatively complex problems. The results provide insight into higher cognition in primates.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74500.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Averbeck</surname><given-names>Bruno B</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>NIH/NIMH</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.02.462713">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.10.02.462713v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Monkey Plays Pac-Man with Compositional Strategies and Hierarchical Decision-making&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Bruno B Averbeck (Reviewer #1).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The authors report on a complex behavior paradigm together with sophisticated modeling to study higher cognition in primates. The key claims of the manuscript are mostly supported by the data, and the approaches are rigorous. The reviewers were overall enthusiastic about this work, yet identified three key ways in which the findings should be clarified and the report improved:</p><p>1) An identified weakness was a lack of comparison with other models and a need for better justification of the modeling strategy. This is especially important since the dataset is very complex. Rather than solely a quantitative comparison of model performance, which may dilute the argument, there could be some discussion of an account for non-hierarchical possibilities. For example, a large neural network (i.e., a non-hierarchical model) could be trained on these trials and produce equal performance or better prediction. The authors should in some way provide more of a rationale for their approach. This could be in discussion, presentation of converging evidence, and/or additional model comparison.</p><p>2) Also related to #1, the reviewers suggested that perhaps a way to better justify the choice of model would be to do a selective comparison within the data (i.e. one or two better-chosen baselines) to support the hierarchical claim.</p><p>3) The authors should acknowledge that monkeys do not always perform this task in the optimal way, and that monkeys perhaps use a more passive strategy, or use multiple strategies at the same time. The monkeys could obtain almost all the rewards by the end of the game as there is nothing to pressure or force them to optimize their choices. Authors should more fully account for this feature in the experimental design when interpreting their data.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. What do you mean by saying &quot;a reasonable level&quot; (line 114)? You may want to show the criteria.</p><p>2. Although Clyde tends to keep a certain distance from Pac-Man, why does Pac-Man also chase it when they are close to each other? (figure 1C)</p><p>3. Essential references are needed when explaining the behavior or mentioning previous literature, for example, in lines 316 and 485.</p><p>4. More details about how you tweaked the task are needed, such as how the monkeys operated the joystick? Holding it to move the Pac-Man, I guess.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– I think there are potential problems with identifiability in the model as detailed. If, for instance, the authors used a model with known time-varying weights to play the game and then fit their model to these data, do they correctly recover the weights?</p><p>– I am unfamiliar with the sense of &quot;hierarchical&quot; behavioral models as used here. By contrast, hierarchical models in RL involve goals, subgoals, etc., with models nested inside one another. If the authors are borrowing this less common sense of the term from somewhere, it would be helpful to cite it. I am not sure in what sense a perceptron is a &quot;flat&quot; model.</p><p>– Figure 1B: I find it hard to distinguish the grayscale lines and match them with their corresponding choice types. Also, I don't see standard errors here, as indicated in the caption.</p><p>– Figure 2: I found the boxes around the figure panels visually unappealing. This occurs in a couple of other figures.</p><p>– Figure 2C-D: If I follow, these are violin plots, not histograms, as stated in the caption.</p><p>– Figure 2E: The authors might consider a radar chart as an alternative way of representing these proportion data. Such a chart would allow them to plot each column here as one line and might facilitate more direct comparisons across strategy proportions.</p><p>– ll. 206-210: Before having read the methods, this text does not make it clear to me what the authors have done. That is, I don't know what it means here to linearly combine basis strategies or to &quot;pool predictions.&quot; More specific language would be helpful, even if just a few words.</p><p>– Figure 3C-D: I don't think the dotted magenta lines are necessary, and they make the numbers hard to read.</p><p>– I was not sure what to take away from Figure 4. As a conceptual description, perhaps it belongs earlier in the text? Perhaps with some schematic as to how models are combined?</p><p>– ll 748-749: Shouldn't utilities for impossible actions be -∞? That is, in any probabilistic action selection model, there will be a probability of selecting any action with a <italic>finite</italic> utility difference, so the difference would need to be infinite to rule out an action.</p><p>– ll. 918-920: Three standard deviations seems like a pretty drastic cutoff for outliers. Why not five or at least four? Does this make a difference?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.74500.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The authors report on a complex behavior paradigm together with sophisticated modeling to study higher cognition in primates. The key claims of the manuscript are mostly supported by the data, and the approaches are rigorous. The reviewers were overall enthusiastic about this work, yet identified three key ways in which the findings should be clarified and the report improved:</p><p>1) An identified weakness was a lack of comparison with other models and a need for better justification of the modeling strategy. This is especially important since the dataset is very complex. Rather than solely a quantitative comparison of model performance, which may dilute the argument, there could be some discussion of an account for non-hierarchical possibilities. For example, a large neural network (i.e., a non-hierarchical model) could be trained on these trials and produce equal performance or better prediction. The authors should in some way provide more of a rationale for their approach. This could be in discussion, presentation of converging evidence, and/or additional model comparison.</p><p>2) Also related to #1, the reviewers suggested that perhaps a way to better justify the choice of model would be to do a selective comparison within the data (i.e. one or two better-chosen baselines) to support the hierarchical claim.</p></disp-quote><p>In the frontier of Machine Learning and Reinforcement Learning, hierarchical and flat models are differentiated by the action levels they are modeling (Sutton and Barto, 2018). Flat models find the policy based on the optimal primitive actions (e.g., joystick movements) at each step. For example, a typical flat model may use a standard Q-learning algorithm. In contrast, hierarchical models make goal-directed and temporally extended high-level decisions based on which primitive decisions are made. Optimal policies are searched over these high-level decisions. These high-level decisions are the <italic>strategies</italic> in our paper but may have different names in the literature, such as <italic>skills</italic>, <italic>operators</italic>, <italic>macro-operators</italic>, <italic>macro-actions</italic>, or <italic>options</italic> (M. M. Botvinick et al., 2009; M. Botvinick and Weinstein, 2014; Sutton et al., 1999).</p><p>Based on the distinction between hierarchical and flat models, we did the following in the new revision to argue that the monkeys used a hierarchical model instead of a flat model to play the Pac-Man game.</p><p>Firstly, in Results, we include a linear approximate reinforcement learning model (LARL) (Sutton, 1988; Tsitsiklis and Van Roy, 1997). The LARL model shared the same structure with a standard Q-learning algorithm but used the monkeys’ actual joystick movements as the fitting target. To highlight the flatness of this baseline model, we adopted a common assumption that the parameterization of the utility function is linear (Sutton and Barto, 2018) with respect to 7 game features. In updated Figure 2B, Figure 2—figure supplement 1AE, and Appendix Table 4-6, we compare the prediction accuracy of this model in four different game contexts. The LARL model only achieves 0.669±0.011 overall prediction accuracy (Figure 2B, light gray bars) and performs worse under each game situation (early: 0.775±0.021, p&lt;<inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, late: 0.621±0.018, p&lt;<inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>17</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, scared ghosts: 0.672±0.025, p&lt;<inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>; two-sample t-test).</p><p>Secondly, we have revised Discussion to discuss the evidence supporting the hypothesis that monkeys used a hierarchical model.</p><p>“Our hierarchical model explains the monkeys’ joystick movements well (Figure 2B). Importantly, the strategies derived from the model can be verified with independent behavior measurements. The monkeys' fixation pattern, a measure of their attention, reflected the features associated with the current strategy (Figure 3E). Moreover, an increase of pupil dilation (Figure 3F), which was not associated with any particular changes of game states, was found at the deduced strategy switches. This is consistent with the prediction from the hierarchical model that there should be crucial decision-making of strategies around the strategy transitions.”</p><p>Finally, we have included a new paragraph in Discussion to point out that the cognitive constraint and planning complexity required in our Pac-Man task also suggest that the hierarchical model is more biologically plausible.</p><p>“In contrast to hierarchical models in which the decision-maker divides decision making into multiple levels and at each level focuses on an increasingly refined smaller set of game features (M. M. Botvinick et al., 2009; M. Botvinick and Weinstein, 2014; Dezfouli and Balleine, 2013; Ostlund et al., 2009; Sutton et al., 1999), a flat model’s decisions are directly computed at the primitive action level, and each action choice is evaluated with all game features. Although in theory a flat model may achieve equal or even greater performance than a hierarchical model, flat models are much more computationally costly. Especially when working memory has a limited capacity, as in the case of the real brain, hierarchical models can achieve a faster and more accurate performance (M. Botvinick and Weinstein, 2014). Our Pac-Man task contains an extensive feature space while requiring real-time decision-making that composes limitations on the cognitive resources. Even for a complex flat model such as Deep Q-Network, which evaluates primitive actions directly with a deep learning network structure without any temporally-extended higher-level decisions (Mnih et al., 2015), the game performance is much worse than a hierarchical model (Van Seijen et al., 2017). In fact, the most successful AI player to date uses a multi-agent solution, which is hierarchical in nature (Van Seijen et al., 2017). Our study shows that the monkeys also adopted a hierarchical solution for the Pac-Man game.”</p><disp-quote content-type="editor-comment"><p>3) The authors should acknowledge that monkeys do not always perform this task in the optimal way, and that monkeys perhaps use a more passive strategy, or use multiple strategies at the same time. The monkeys could obtain almost all the rewards by the end of the game as there is nothing to pressure or force them to optimize their choices. Authors should more fully account for this feature in the experimental design when interpreting their data.</p></disp-quote><p>1. The monkeys certainly did not always perform the task in the optimal way. Some sources of non-optimal behaviors are rather trivial. The joystick was not a natural instrument for the monkeys, and motor errors significantly contributed to their suboptimal behavior. Lapses of concentration were another factor. The monkeys might not have always played the game as attentively as we would like. The other sources of non-optimality, however, can be rather complicated. First, it is difficult to define optimality when we do not know what exactly the monkeys were optimizing. A lot of potential factors may have affected the monkeys’ decision making, including reward rate, temporal discounting, cost of effort (both mentally and physically), etc. What appears suboptimal may actually be optimal for the monkeys. The <italic>suicide</italic> behavior is a good example. Moreover, the computation of decisions in this task is complicated and limited by the monkeys’ cognitive resource. With an extensive state space, action sequences grow exponentially with the planning horizon, and the value of actions depends on the entire sequence of past actions, observations, and states (M. M. Botvinick et al., 2009; Kaelbling et al., 1998). The time constraint imposed by the task exacerbates this scaling problem and forces the monkeys to make decisions based on suboptimal heuristics.</p><p>2. The monkeys could not solve the task <italic>passively</italic>. The game is designed to force the players to make decisions quickly to clear the pellets, otherwise the ghosts would catch Pac-Man and end the game. Even in the monkey version of the game where the monkeys always get another chance, Pac-Man deaths lead to prolonged delays with no rewards. In addition, we provided additional rewards when a maze was cleared in fewer rounds (20 drops if in 1 to 3 rounds; 10 drops if in 4 to 5 rounds; and 5 drops if in more than 5 rounds), which added motivation for the monkeys to complete a game quickly.</p><p>There are additional clues from the monkeys’ behavior suggesting that the monkeys were actively making decisions to play the game. For example, both monkeys treated the two ghosts differently. They would sometimes follow Clyde when it was close (Figure 1C, solid yellow line, Pac-Man's moving tendency towards ghost Clyde was larger than 50%). As Clyde was programmed to move to the left corner of the map when Pac-Man was within 10 steps away, following Clyde was actually safe. Another good example is how the monkeys dealt with energizers and Pac-Man death. Our analyses of the <italic>planned attack</italic> and <italic>suicide</italic> behavior clearly demonstrated that the monkeys actively made plans to change the game into more desirable states. Such behavior cannot be explained with a passive foraging strategy.</p><p>In light of this discussion, we have revised the manuscript in several places to reflect the points above.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. What do you mean by saying &quot;a reasonable level&quot; (line 114)? You may want to show the criteria.</p></disp-quote><p>The criterium was determined subjectively. As shown in Appendix Figure 2D and 2J, monkeys’ behavior was stable within the period. We have collected more behavior data since the manuscript was written and submitted for review. But to keep consistency, the new data were not included in the revision. The new data are qualitatively very similar.</p><disp-quote content-type="editor-comment"><p>2. Although Clyde tends to keep a certain distance from Pac-Man, why does Pac-Man also chase it when they are close to each other? (figure 1C)</p></disp-quote><p>Just as in the original Pac-Man game, Clyde is programmed to move to the left corner of the map when Pac-Man is within 10 steps away. In addition, the ghosts do not move in the reversed direction. Therefore, it is actually safe for Pac-Man to follow Clyde when they are near each other.</p><disp-quote content-type="editor-comment"><p>3. Essential references are needed when explaining the behavior or mentioning previous literature, for example, in lines 316 and 485.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>4. More details about how you tweaked the task are needed, such as how the monkeys operated the joystick? Holding it to move the Pac-Man, I guess.</p></disp-quote><p>Just as in the original Pac-Man game, the monkeys only need to flip the joystick toward a direction, and holding the joystick is not necessary. If the turn associated with the joystick movement is currently not possible, Pac-Man continues to move in the current direction until the turn becomes valid. Therefore, the monkey may move the joystick before the Pac-Man reaches a crossing to make a turn at that crossing.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>– I think there are potential problems with identifiability in the model as detailed. If, for instance, the authors used a model with known time-varying weights to play the game and then fit their model to these data, do they correctly recover the weights?</p></disp-quote><p>We have tested our model with simulated data generated with an artificial agent playing the game with time-varying strategy weights. The model could recover the ground-truth strategy label faithfully. The details of these analyses are included in the Supplementary (Appendix Figure 4).</p><disp-quote content-type="editor-comment"><p>– I am unfamiliar with the sense of &quot;hierarchical&quot; behavioral models as used here. By contrast, hierarchical models in RL involve goals, subgoals, etc., with models nested inside one another. If the authors are borrowing this less common sense of the term from somewhere, it would be helpful to cite it. I am not sure in what sense a perceptron is a &quot;flat&quot; model.</p></disp-quote><p>In the new revision, we define the flat model in Results as follows:</p><p>“… In contrast, in a flat model, decisions are computed directly for the primitive actions based on all relevant game features…”</p><p>In Discussion, we further point out:</p><p>“In contrast to hierarchical models in which the decision-maker divides decision making into multiple levels and at each level focuses on an increasingly refined smaller set of game features (M. M. Botvinick et al., 2009; M. Botvinick and Weinstein, 2014; Dezfouli and Balleine, 2013; Ostlund et al., 2009; Sutton et al., 1999), a flat model’s decisions are directly computed at the primitive action level, and each action choice is evaluated with all game features…”</p><p>We further revised the manuscript in multiple places to discuss how our results support the hierarchical decision-making by the monkeys.</p><disp-quote content-type="editor-comment"><p>– Figure 1B: I find it hard to distinguish the grayscale lines and match them with their corresponding choice types. Also, I don't see standard errors here, as indicated in the caption.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>– Figure 2: I found the boxes around the figure panels visually unappealing. This occurs in a couple of other figures.</p></disp-quote><p>We have fixed the problem of pdf conversion and hopefully the figure quality issue has been solved.</p><disp-quote content-type="editor-comment"><p>– Figure 2C-D: If I follow, these are violin plots, not histograms, as stated in the caption.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>– Figure 2E: The authors might consider a radar chart as an alternative way of representing these proportion data. Such a chart would allow them to plot each column here as one line and might facilitate more direct comparisons across strategy proportions.</p></disp-quote><p>We tried Radar chart (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>), but there are too many overlaps between the four game contexts, making the figure hard to read. Therefore, we have kept the bar chart.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Radar chart of the labeled dominating strategy ratios across four game contexts.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-sa2-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>– ll. 206-210: Before having read the methods, this text does not make it clear to me what the authors have done. That is, I don't know what it means here to linearly combine basis strategies or to &quot;pool predictions.&quot; More specific language would be helpful, even if just a few words.</p></disp-quote><p>We have revised the corresponding section to clarify the fitting process:</p><p>“At any time during the game, monkeys could adopt one or a mixture of multiple strategies for decision making. We assumed that the final decision for Pac-Man’s moving direction was based on a linear combination of the basis strategies, and the relative strategy weights were stable for a certain period. We adopted a softmax policy to linearly combine utility values under each basis strategy, with the strategy weights as model parameters. To avoid potential overfitting, we designed a two-pass fitting procedure to divide each game trial into segments and performed maximum likelihood estimation (MLE) to estimate the model parameters with the monkeys’ behavior within each time segment (See Methods for details). When tested with simulated data, the fitting procedure recovers the ground-truth weights used to generate the data (Appendix 1—figure 4).”</p><disp-quote content-type="editor-comment"><p>– Figure 3C-D: I don't think the dotted magenta lines are necessary, and they make the numbers hard to read.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>– I was not sure what to take away from Figure 4. As a conceptual description, perhaps it belongs earlier in the text? Perhaps with some schematic as to how models are combined?</p></disp-quote><p>Figure 4 introduces the compound strategies and leads to our analyses of compound strategies. With Figure 4, we would like to wrap up all crucial concepts developed in the above texts and emphasize the relationships between task, compound strategy, basis strategy, and primitive action selections. We played with the idea of having a schematic illustration earlier in the text but were worried that readers might not sufficiently appreciate the illustration without the relevant analyses. We certainly are still open to adjusting the writing flow should the reviewers and the editors favor the alternative.</p><disp-quote content-type="editor-comment"><p>– ll 748-749: Shouldn't utilities for impossible actions be -∞? That is, in any probabilistic action selection model, there will be a probability of selecting any action with a finite utility difference, so the difference would need to be infinite to rule out an action.</p></disp-quote><p>Thanks for the suggestion. We have now adopted this suggestion, re-done our analyses, and updated all figures under this modification. The results were qualitatively similar to the original.</p><disp-quote content-type="editor-comment"><p>– ll. 918-920: Three standard deviations seems like a pretty drastic cutoff for outliers. Why not five or at least four? Does this make a difference?</p></disp-quote><p>We tried five and four as alternative cutoffs. The results are very much the same. So we did not change the original plot.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>The monkeys’ pupil diameters around the strategy transition under different cutoff conditions.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-74500-sa2-fig2-v2.tif"/></fig></body></sub-article></article>