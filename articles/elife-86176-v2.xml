<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86176</article-id><article-id pub-id-type="doi">10.7554/eLife.86176</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>How honey bees make fast and accurate decisions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-302432"><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7612-6465</contrib-id><email>maboudi@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-304159"><name><surname>Marshall</surname><given-names>James AR</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1506-167X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-304160"><name><surname>Dearden</surname><given-names>Neville</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109171"><name><surname>Barron</surname><given-names>Andrew B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8135-6628</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Department of Computer Science, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Sheffield Neuroscience Institute, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sf06y89</institution-id><institution>School of Natural Sciences, Macquarie University</institution></institution-wrap><addr-line><named-content content-type="city">North Ryde</named-content></addr-line><country>Australia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Grunwald Kadow</surname><given-names>Ilona C</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041nas322</institution-id><institution>University of Bonn</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Opteran Technologies, Sheffield Innovation Centre, Sheffield, United Kingdom</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>06</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e86176</elocation-id><history><date date-type="received" iso-8601-date="2023-01-14"><day>14</day><month>01</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2023-05-24"><day>24</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2023-01-03"><day>03</day><month>01</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.02.522517"/></event></pub-history><permissions><copyright-statement>© 2023, MaBouDi et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>MaBouDi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86176-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86176-figures-v2.pdf"/><abstract><p>Honey bee ecology demands they make both rapid and accurate assessments of which flowers are most likely to offer them nectar or pollen. To understand the mechanisms of honey bee decision-making, we examined their speed and accuracy of both flower acceptance and rejection decisions. We used a controlled flight arena that varied both the likelihood of a stimulus offering reward and punishment and the quality of evidence for stimuli. We found that the sophistication of honey bee decision-making rivalled that reported for primates. Their decisions were sensitive to both the quality and reliability of evidence. Acceptance responses had higher accuracy than rejection responses and were more sensitive to changes in available evidence and reward likelihood. Fast acceptances were more likely to be correct than slower acceptances; a phenomenon also seen in primates and indicative that the evidence threshold for a decision changes dynamically with sampling time. To investigate the minimally sufficient circuitry required for these decision-making capacities, we developed a novel model of decision-making. Our model can be mapped to known pathways in the insect brain and is neurobiologically plausible. Our model proposes a system for robust autonomous decision-making with potential application in robotics.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>In the natural world, decision-making processes are often intricate and challenging. Animals frequently encounter situations where they have limited information on which to rely to guide them, yet even simple choices can have far-reaching impact on survival.</p><p>Each time a bee sets out to collect nectar, for example, it must use tiny variations in colour or odour to decide which flower it should land on and explore. Each ‘mistake’ is costly, wasting energy and exposing the insect to potential dangers. To learn how to refine their choices through trial-and-error, bees only have at their disposal a brain the size of a sesame seed, which contains fewer than a million neurons. And yet, they excel at this task, being both quick and accurate. The underlying mechanisms which drive these remarkable decision-making capabilities remain unclear.</p><p>In response, MaBouDi et al. aimed to explore which strategies honeybees adopt to forage so effectively, and the neural systems that may underlie them. To do so, they released the insects in a ‘field’ containing artificial flowers in five different colours. The bees were trained to link each colour with a certain likelihood of receiving either a sugary liquid (reward) or bitter quinine (punishment); they were then tested on this knowledge.</p><p>Next, MaBouDi et al. recorded how the bees would navigate a ‘reduced evidence’ test, where the colour of the flowers were ambiguous and consisted in various blends of the originally rewarded or punished colours; and a ‘reduced reward likelihood’ test, where the sweet recompense was offered less often than before.</p><p>Response times and accuracy rates revealed a complex pattern of decision-making processes. How quickly the insects made a choice, and the types of mistakes they made (such as deciding to explore a non-rewarded flower, or to ignore a rewarded one) were dependent on both the quality of the evidence and the certainty of the reward. Such sophistication and subtlety in decision-making is comparable to that of primates.</p><p>Next, MaBouDi et al. developed a computational model which could faithfully replicate the pattern of decisions exhibited by the bees, while also being plausible biologically. This approach offered insights into how a small brain could execute such complex choices ‘on the fly’, and the type of neural circuits that would be required. Going forward, this knowledge could be harnessed to design more efficient decision-making algorithms for artificial systems, and in particular for autonomous robotics.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>action selection</kwd><kwd>decision-making</kwd><kwd>foraging</kwd><kwd>mushroom bodies</kwd><kwd>sequential sampling model</kwd><kwd>protocerebrum</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Apis mellifera</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/P006094/1</award-id><principal-award-recipient><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><name><surname>Marshall</surname><given-names>James AR</given-names></name><name><surname>Dearden</surname><given-names>Neville</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>FT140100452</award-id><principal-award-recipient><name><surname>Barron</surname><given-names>Andrew B</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>VP1-2017-026</award-id><principal-award-recipient><name><surname>Barron</surname><given-names>Andrew B</given-names></name><name><surname>Marshall</surname><given-names>James AR</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100011730</institution-id><institution>Templeton World Charity Foundation</institution></institution-wrap></funding-source><award-id>TWCF-2020-20539</award-id><principal-award-recipient><name><surname>Barron</surname><given-names>Andrew B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A fast and accurate decision-making strategy observed in bees, and reproduced in a neurally-grounded model, suggests a robust, risk-averse decision strategy suitable for when sampling and errors are both costly.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Decision-making is at the core of cognition. A decision can be considered as the result of an evaluation of possible outcomes (<xref ref-type="bibr" rid="bib51">Mobbs et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Stevens, 2011</xref>), and animal lives are full of decisions. What we might consider to be a simple choice, for example choosing the best option from two alternatives, is rarely simple in an ecological setting (<xref ref-type="bibr" rid="bib51">Mobbs et al., 2018</xref>). Consider the decisions a foraging bee makes. A bee, moment by moment, must decide whether a flower should be explored for pollen and nectar or whether it is not worth landing on. We could suppose that decision to be influenced by what the bee can sense about the flower, her past experiences with that flower type, the context (is a predator nearby?), the state of the bee (does she already carry a full load of nectar and pollen?) and the state of her colony (what does the colony need?) (<xref ref-type="bibr" rid="bib10">Chittka, 2022</xref>; <xref ref-type="bibr" rid="bib12">Conradt and Roper, 2005</xref>; <xref ref-type="bibr" rid="bib69">Stephens, 2008</xref>). Even this simple decision is a whole-brain activity involving sensory systems, memory systems, motor systems, and the bee’s subjective state. Here, we studied honey bee foraging decisions in controlled conditions to establish their decision-making capacities. We then developed a simple model with the same capacities for decision-making as a bee to assist in hypothesising the necessary neural mechanisms supporting bees’ foraging decisions.</p><p>Abstract theories and models of decision-making are well-developed, and these provide frameworks for evaluating animals’ decision-making capacity (<xref ref-type="bibr" rid="bib23">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib51">Mobbs et al., 2018</xref>; <xref ref-type="bibr" rid="bib56">O’Connell et al., 2018</xref>). Here, we apply signal detection theory to understand how bees make a decision (<xref ref-type="bibr" rid="bib24">Green and Swets, 1966</xref>; <xref ref-type="bibr" rid="bib24">Green and Swets, 1966</xref>; <xref ref-type="bibr" rid="bib74">Sumner and Sumner, 2020</xref>; <xref ref-type="bibr" rid="bib85">Wickens, 2001</xref>). Signal detection theory helps us think formally about the processes of signal discrimination, which is essential for making decisions (<xref ref-type="bibr" rid="bib85">Wickens, 2001</xref>). It provides an abstract model and simple logic for how animals should respond given the signal they have received and their prior knowledge. Typically signal detection theory assumes that an individual must choose between two possible actions (acceptance or rejection) after detecting a signal. In such a scenario, there are four possible outcomes, which include two correct actions. These are: 1, correct acceptance when the subject accepts the correct stimulus (‘hit’), 2, correct rejection when the subject rejects the incorrect stimulus (correct rejection), 3, incorrect acceptance when the subject wrongly accepts the incorrect stimulus (‘false positive’, Type I error), 4, incorrect rejection when the subject rejects the correct stimulus (‘false negative’, Type II error). The optimal decision is calculated by considering the expected payoffs of all four outcomes together. Both errors are integral parts of the decision-making process. In an ecological context, both errors typically differ in costs to an animal (<xref ref-type="bibr" rid="bib74">Sumner and Sumner, 2020</xref>). For example, wrongly rejecting a food item might see an animal missing a meal, but wrongly accepting a food item could see an animal ingesting poison. Signal detection theory emphasises that both acceptance and rejection choices have to be assessed if decision-making is to be understood, but typically in studies of animal behaviour rejection behaviour is ignored (<xref ref-type="bibr" rid="bib34">Ings and Chittka, 2008</xref>; <xref ref-type="bibr" rid="bib74">Sumner and Sumner, 2020</xref>; <xref ref-type="bibr" rid="bib78">Trimmer et al., 2017</xref>).</p><p>Decision-making processes are most often modelled with sequential sampling models, of which there are many variations (<xref ref-type="bibr" rid="bib55">O’Connell and Hofmann, 2012</xref>; <xref ref-type="bibr" rid="bib56">O’Connell et al., 2018</xref>). Sequential sampling models are built on the biologically realistic assumptions that sensory information on available options is noisy, but evidence for different options accumulates over time through sequential sampling (<xref ref-type="bibr" rid="bib23">Gold and Shadlen, 2007</xref>). A decision is made when the cumulant reaches a threshold. Variations in sequential sampling models differ in the nature of the threshold for the decision. For example, in the race model (<xref ref-type="bibr" rid="bib81">Vickers, 1970</xref>) a decision is made when evidence for one alternative reaches an upper threshold. Leaky competing accumulator (LCA) models set the evidence for different options in competition such that as evidence for one option accumulates it inhibits evidence for the alternative and a decision is made when the difference in evidence for the two alternatives reaches a threshold (<xref ref-type="bibr" rid="bib2">Barron et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Bogacz et al., 2006</xref>). Sequential sampling models have proved very influential in neuroscience, psychology, and computer science. While they are highly abstract, they capture many features of biological decision-making, particularly a speed/accuracy trade-off (<xref ref-type="bibr" rid="bib2">Barron et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib23">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib61">Pirrone et al., 2014</xref>).</p><p>Investigation of the neural mechanisms of choice in primates has revealed interacting neural systems for the evaluation of different options and the selection of a choice that involve the frontal cortex, the basal ganglia, and the frontal and parietal cortices (<xref ref-type="bibr" rid="bib2">Barron et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Gurney et al., 2001</xref>; <xref ref-type="bibr" rid="bib65">Seed et al., 2011</xref>; <xref ref-type="bibr" rid="bib66">Shadlen and Kiani, 2013</xref>; <xref ref-type="bibr" rid="bib82">Wang, 2012</xref>). This is a system of extreme complexity, involving billions of neurons. Most animal brains are orders of magnitude smaller than this. How might smaller brains make effective decisions? To this end, we explored honey bee foraging decisions. We measured bees’ acceptance and rejection of different options under controlled conditions that manipulated the quality of available evidence and the probability of a rewarding outcome. To understand the properties of bee decision-making, we explored our data with signal detection theory and also examined how accuracy varied with decision speed. Having identified the key properties of bee decision-making we then constructed the simplest sequential sampling model capable of the same decision-making capacities as the bee. Finally, we related this abstract model to the known systems of the bee brain to propose a hypothetical brain mechanism for autonomous decision-making in insects.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We individually trained 20 honey bees (<italic>Apis mellifera</italic>) on a colour discrimination task in which they learned to associate five distinct colours each with their visit history of reward and punishment. Over 18 training trials, each colour offered bees a different likelihood of reward and punishment (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>). The five colours offered the reward in 100%, 66%, 50%, 33%, and 0% of training trials (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) and were otherwise punished. The colour rewarded in 100% of training trials was never punished while the colour rewarded in 0% of training trials was always punished. Each trial offered bees just one pair of colours with one colour in the pair rewarded more often than the other during training (See Materials and methods, <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A,B</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). Following training, bees were given three tests. In the <italic>easy discrimination test</italic>, each honey bee was tested with the two colours rewarded at 100% and 0% in training. In the <italic>reduced evidence test,</italic> bees were tested with two novel colours that were different blends of blue and green (the 100% and 0% rewarded colours) to determine how behaviour changed when the available evidence was degraded. One blend was closer to blue and one closer to green. In the <italic>reduced reward likelihood test</italic> bees were presented with the 66% and 33% rewarded colours to assess how bees’ behaviour changed when the likelihood of reward offered by a choice was less than 100%. In the easy discrimination and reduced evidence tests, correct choices were considered as acceptance of the more rewarded colour, and rejection of the less rewarded colour. Bee’s acceptance and rejection responses were analysed from videos recorded during the training and tests (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, see Materials and methods section). We employed the Matthew Correlation Coefficient (MCC) (<xref ref-type="bibr" rid="bib40">MaBouDi et al., 2020a</xref>) to measure the performance of the bees in each test. This considered all types of responses (i.e. hit, correct rejection, false positive, and false negative) to calculate decision accuracy such that a positive correlation (with a maximum value of +1) indicates perfect performance accuracy while a value of zero indicates chance-level performance. Values between 0 and +1 demonstrate varying degrees of decision accuracy (see Materials and methods section).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Bees’ behaviour in a colour discrimination task.</title><p>(<bold>A &amp; B</bold>) Each bee was given 18 training trials in which she could choose between two different colours: one rewarded and the other punished. The bee was free to select each colour and return to the hive when satiated marking the end of a trial. Stimuli positions in the arena were changed in each trial in a pseudo-random manner. Stimuli were 2 cm diameter-coloured disks on a small platform (5 cm tall). On the top of each colour was placed either 10 μl reward (50% sucrose) or punishment (quinine) in training, or distilled water in tests. Two different colours, four disks of each colour, were presented in each training trial and test. Five different colours were used in the training. The colours differed in the proportion of training bouts in which they offered reward and punishment (rewarded at 100, 66, 50, 33, and 0% of training trials). Two groups of bees were trained with different likelihoods of reward and punishment from each colour (see Materials and methods section and <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>). (<bold>C</bold>) Following training, the bee was given three unreinforced tests where the positive or negative reinforcements were replaced with distilled water. Bees’ responses were analysed from video recordings of the first 120 s in the flight arena. In the easy colour discrimination test, bees were presented with three pairs of the 100% and 0% rewarded colours (blue and green). In the reduced reward likelihood test, bees were examined with 66% and 33% rewarded colours (yellow and orange). In the reduced evidence test. bees were given two colours intermediate between green and blue (<bold>D &amp; E</bold>) Examples of flight paths showing the inspection activity of a bee during the easy discrimination test in accepting blue (<bold>D</bold>) and rejecting green (<bold>E</bold>). Each black line on the flight path corresponds to the bee’s body orientation in a single video frame with 4ms intervals between frames. Line colour: flight speed 0.0–0.5 m/s (See <xref ref-type="video" rid="video1">Video 1</xref>).</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Bees’ choices during the training trials.</title><p>Tables show the number of bees’ correct and incorrect choices to the high and low rewarded stimuli during two different sequences of training trials were used (A: Protocol 1; B: Protocol 2). The blue cells indicate the number of reward bees received, whereas the red cells indicate the number of punishment bees received.</p></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-86176-fig1-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig1-v2.tif"/></fig><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Two different sequences of training trials were used.</title><p>10 bees were trained with the protocol P1 and 10 with the protocol P2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" colspan="2">Protocol P1</th><th align="center" valign="middle" colspan="2">Protocol P2</th></tr><tr><th align="left" valign="bottom">#trials</th><th align="left" valign="bottom">colours at each trial</th><th align="left" valign="bottom">#trials</th><th align="left" valign="bottom">colours at each trial</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">S100% vs S66%</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">S50% vs S0%</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">S50% vs S0%</td><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">S100% vs S66%</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">S100% vs S33%</td><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">S100% vs S33%</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">S66% vs S0%</td><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">S66% vs S0%</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">S50% vs S33%</td><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">S100% vs S50%</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">S100% vs S50%</td><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">S50% vs S33%</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">S33% vs S0%</td><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">S100% vs S0%</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">S66% vs S50%</td><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">S33% vs S0%</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">S100% vs S0%</td><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">S66% vs S50%</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">S100% vs S66%</td><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">S100% vs S0%</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom">S50% vs S0%</td><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom">S50% vs S33%</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">S100% vs S33%</td><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">S66% vs S50%</td></tr><tr><td align="char" char="." valign="bottom">13</td><td align="left" valign="bottom">S66% vs S0%</td><td align="char" char="." valign="bottom">13</td><td align="left" valign="bottom">S33% vs S0%</td></tr><tr><td align="char" char="." valign="bottom">14</td><td align="left" valign="bottom">S50% vs S33%</td><td align="char" char="." valign="bottom">14</td><td align="left" valign="bottom">S100% vs S50%</td></tr><tr><td align="char" char="." valign="bottom">15</td><td align="left" valign="bottom">S100% vs S50%</td><td align="char" char="." valign="bottom">15</td><td align="left" valign="bottom">S66% vs S0%</td></tr><tr><td align="char" char="." valign="bottom">16</td><td align="left" valign="bottom">S33% vs S0%</td><td align="char" char="." valign="bottom">16</td><td align="left" valign="bottom">S100% vs S33%</td></tr><tr><td align="char" char="." valign="bottom">17</td><td align="left" valign="bottom">S66% vs S50%</td><td align="char" char="." valign="bottom">17</td><td align="left" valign="bottom">S100% vs S66%</td></tr><tr><td align="char" char="." valign="bottom">18</td><td align="left" valign="bottom">S100% vs S0%</td><td align="char" char="." valign="bottom">18</td><td align="left" valign="bottom">S50% vs S0%</td></tr></tbody></table></table-wrap><p>In our free-flight choice assay bees learned to prefer the 100% rewarded colour from the 0% rewarded colour (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; Wilcoxon signed rank test: z=3.62, n=20, p=2.93e-4; see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref> for power analysis). Bees’ performance in the reduced evidence test was lower but was still higher than chance (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; Wilcoxon signed rank test: z=2.10, n=18, p=0.03). In the reduced reward likelihood test, bees selected the 66% reward colour more frequently than chance (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Characteristics of bee decision making.</title><p>(<bold>A</bold>) Matthew correlation coefficients (MCC) (mean ± SEM) for the easy discrimination and reduced evidence tests. In the both easy discrimination and reduced evidence tests, this correlation is computed with respect to choosing the high-rewarded colours for each bee. A positive correlation (max at +1) indicates perfect correct performance while zero indicates chance level performance. Correlation coefficients were significantly greater than zero for both tests. (<bold>B</bold>) Average time to the first choices for three tests and the first training trial. Bees naive to the stimuli made their first choice faster than bees trained on the stimuli (p=1.55e-3). (<bold>C</bold>) Scatter plot showing a negative correlation between the MCC and the time to first acceptance in the easy discrimination test. A rapid first choice correlated with higher performance. Values for each individual bee are shown by small circles. n=20, **p&lt;0.005 and *p&lt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Bees’ performance during the training.</title><p>(<bold>A, B</bold>) Bar graphs show the proportion of bees’ responses to high rewarded stimuli at each training trials (A: bees in Protocol 1; B: bees in Protocol 2). Below exhibit the stimuli presented at each training trails. (<bold>C</bold>) Results of power analyses for both easy discrimination and reduced evidence tests, assuming the mean and variance in MCC seen in this study (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). It indicates that our sample size of 20 bee provided us with a power level that exceeded the commonly accepted threshold of 80%.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Bees’ performance on the reduced reward likelihood test.</title><p>Bar shows mean proportion of choosing high-rewarded colour. Dashed line indicates performance expected at random. n=20.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig2-figsupp2-v2.tif"/></fig></fig-group><p>Bees spent longer in flight before their first landing in the tests than in the first training trial (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; Kruskal-Wallis test, chi-sq=13, df = 7, p=4.60e-3). This shows that during training bees developed a behaviour of assessing the available stimuli in the arena for longer before landing. There was a significant negative correlation between bees’ performance in the easy discrimination test and their time to first landing (assessed by the MCC: Spearman correlation, rho = –0.55, n=20, p=0.02). Poor performance in the test was associated with a longer time before a first choice (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><sec id="s2-1"><title>Investigation of bee decision-making using classical signal detection theory</title><p>Signal detection theory provides a framework for understanding and predicting how animals make decisions under uncertainty by modelling the relationship between the sensory information they received and their ability to accurately discriminate between stimuli. Hence, the probability of a stimulus being correctly identified is assumed to be a function of the sensory information received. If we have two different stimuli (in our case the high and low rewarded colours) we can model how the probability of identifying them changes as perceived colour information is sampled from two overlapping normal distributions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For each colour, it could be identified correctly or incorrectly. For a trained bee we would recognise this as four types of behavioural response. For the highly rewarded colour, these would be correct acceptance or incorrect rejection. For the low rewarded colour these would be correct rejection or incorrect acceptance (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Discriminability (d′) is the difference in the sensory information between the maximal probability of the two different stimuli (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). From our data, we could calculate discriminability following <xref ref-type="bibr" rid="bib74">Sumner and Sumner, 2020</xref> by modelling total accept and reject responses as cumulative distribution functions and considering the hit rate (correct acceptance / total acceptance) and the false positive rate (incorrect rejections/ total rejections; <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, Materials and methods).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>An investigation by classical signal detection theory.</title><p>(<bold>A</bold>) Probability of responding to the high (blue) and low (green) rewarded stimuli at different levels of sensory input. For a trained bee we recognise a threshold (decision criterion, d.c.) at which their behaviour shifts from rejection to acceptance. As a result, we have four types of behavioural responses. d’ is the discriminability of the two stimuli. (<bold>B</bold>) Discriminability was greatest in the easy discrimination task and was reduced in both reduced evidence and reduced reward likelihood tests. (<bold>C</bold>) The decision criterion was negative for the easy discrimination and reduced evidence tests indicating fewer incorrect acceptances than incorrect rejections in these tests. The decision criterion was closer to zero in the reduced reward likelihood test indicating similar accuracy of acceptance and rejection in this test. (<bold>D</bold>) Plotting the ratio of correct to incorrect acceptances and rejections (crosses show the mean and SEM) for the three tests show that generally, bees were more accurate in acceptance than rejection responses. Acceptance accuracy fell in the reduced evidence and reduced reward likelihood tests. n = 20, **p&lt;0.005 and *p&lt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig3-v2.tif"/></fig><p>When considering contrasting responses to two different stimuli using signal detection theory we can identify a threshold sensory signal at which behaviour should shift from acceptance to rejection. This is the decision criterion (<italic>d.c</italic>., <xref ref-type="fig" rid="fig3">Figure 3A</xref>). From our experimental data we can estimate the relative location of the <italic>d.c</italic>. by considering both the hit rate and the false positive rate (<xref ref-type="bibr" rid="bib85">Wickens, 2001</xref>, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> in the Materials and methods section). A value of 0 for the <italic>d.c</italic>. indicates that there were as many incorrect rejections as there were incorrect acceptances, or that the acceptance and rejection responses were equally accurate. A negative value for the decision criterion (<italic>d.c</italic>.) would move the decision criterion to the left in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. This would result in more correct acceptances (i.e. the area under the probability of responding to the high rewarded stimuli (blue) is increased) but fewer correct rejections (i.e. the area under the probability of responding to the low rewarded stimuli [green] is decreased). It would indicate acceptance responses are more precise than rejections.</p><p>The reduced evidence test significantly decreased the discriminability of more and less rewarded stimuli (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; Wilcoxon rank sum test: z=1.81, n=20, p=0.03). Discriminability was also reduced in the reduced evidence test in which the two stimuli were closer in their likelihood of being rewarded (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; Wilcoxon rank sum test: z=3.94, n=20, p=8.01e-5). This shows that for bees’ discriminability is influenced by both available evidence and reward likelihood.</p><p>When the likelihood of reward for the two stimuli was more similar the decision criterion was closer to zero (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; Wilcoxon signed rank test: z=–2.21, n=20, p=8.4e-3) indicating that the accuracy of acceptance and rejection were more similar when the reward outcomes for the two stimuli were more similar. Otherwise, in both the easy discrimination and reduced evidence tests (in which one stimulus was always rewarded and one punished) acceptance was more accurate than rejection (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; Wilcoxon signed rank test: z=–3.62, n=20, p=2.93e-4 for easy discrimination test, z=–2.91, n=18, p=3.5e-3 for reduced evidence test). Finally, the comparison of the ratio of correct and incorrect acceptance and rejection in the three tests (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) revealed that the acceptance accuracy in both reduced evidence and reduced likelihood tests decreased compared to the easy discrimination test, indicating that acceptance accuracy was sensitive to both evidence and reward likelihood. Overall rejection accuracy was lower than acceptance accuracy. Rejection accuracy was lowest in the reduced reward likelihood test than in the reduced evidence test, indicating the rejection accuracy was more influenced by reward likelihood than available evidence (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). This indicates that the evidence thresholds for accept and reject decisions were distinct, as discussed further in the Discussion section.</p></sec><sec id="s2-2"><title>How quality of evidence and reward likelihood influence decision accuracy and decision speed</title><p>In the easy discrimination test, there were more rejections than acceptances (<xref ref-type="fig" rid="fig4">Figure 4B</xref>; Wilcoxon signed rank test: z=–3.62, n=20, p=2.9e-4) and bees’ accuracy (the difference between the number of correct and incorrect choices) of acceptance was higher than rejection (<xref ref-type="fig" rid="fig4">Figure 4B</xref>; Wilcoxon signed rank test: z=3.42, n=20, p=6.1e-4). Also, bees’ accuracy of acceptance in the easy discrimination test was higher than bees’ responses in the reduced evidence test (<xref ref-type="fig" rid="fig4">Figure 4B and C</xref>; Wilcoxon signed rank test: z=3.77, n=18, p=1.57e-4). While the number of correct rejections is higher than the number of incorrect rejection responses in the easy discrimination test (<xref ref-type="fig" rid="fig4">Figure 4B</xref>; Wilcoxon signed rank test: z=1.94, n=20, p=0.43), in the reduced evidence test there was no difference in the number of correct and incorrect rejection responses (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; Wilcoxon signed rank test: z=–0.66, n=20, p=0.50). Hence, we propose that acceptance responses are more accurate than rejection responses, but reducing the available evidence reduced the capacity of bees to distinguish the correct and incorrect options.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Response times of bee decisions.</title><p>(<bold>A</bold>) Bees inspected the coloured stimuli prior to accepting or rejecting a colour. (<bold>B</bold>) The number of rejections was higher than the number of acceptances in the easy discrimination test. The difference between the correct and incorrect acceptances was larger than the difference between correct and incorrect rejections. (<bold>C</bold>) In the easy discrimination test bees accepted correct colours faster than incorrect colours, but there was no difference in the response time for correct and incorrect rejections. (<bold>D</bold>) In the reduced evidence test there were still more correct acceptances than incorrect acceptances, but the number of correct acceptances decreased. (<bold>E</bold>) Acceptance times for the correct colour were increased in the reduced evidence test. Bees took longer to accept stimuli with reduced evidence comparing to rejection responses, for both correct or incorrect choices. (<bold>F</bold>) Conditional Accuracy Function (CAF) plot for acceptance responses in the reduced evidence and easy discrimination tests. Lines show the best fit of piece-wise logistic regressions to the bee’s response time. Acceptance accuracy declined with increasing response time. The vertical and horizontal lines at each cross indicate the standard deviation of the proportion of correct acceptance and accept time, respectively. (<bold>G</bold>) CAF curve for rejections in both easy discrimination and reduced evidence tests. The accuracy of rejection did not change significantly with response time. The vertical and horizontal lines at each cross indicate the standard deviation of the proportion of correct rejection and rejection time, respectively. n=20, **p&lt;0.005, *p&lt;0.05 and n.s., p&gt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig4-v2.tif"/></fig><p>Classical signal detection theory does not consider how signals might be influenced by sampling time, but in our data, we noticed bees differed in the time they spent inspecting stimuli. To explore this, we analysed how bees’ response times influenced their choices.</p><p>Prior to bees accepting or rejecting stimuli, we noticed the bees hovered close to and facing the stimulus (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We hypothesise bees were sampling information about the stimulus. In the easy discrimination test bees accepted the correct colour faster than the incorrect one (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; Wilcoxon signed rank test: z=–2.62, n=20, p=8.8e-3), but rejection times did not differ for correct and incorrect colours (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; Wilcoxon signed rank test: z=–0.40, n=20, p=0.68). This shows the acceptance response was more accurate than the rejection, indicating a higher level of discrimination. In the reduced evidence test there was little difference between correct and incorrect response times (<xref ref-type="fig" rid="fig4">Figure 4E</xref>; Wilcoxon signed rank test: z=–0.25, n=20, p=0.79 for acceptances; z=–1.28, n=18, p=0.19 for rejections), and longer acceptance times overall (<xref ref-type="fig" rid="fig4">Figure 4E</xref>; Wilcoxon signed rank test: z=1.98, n=18, p=0.046), suggesting bees struggled to distinguish the correct and incorrect options in the reduced evidence test.</p><p>We calculated the Conditional Accuracy Functions (CAF) for acceptance and rejection responses, which is the subject’s accuracy as a function of the decision time (<xref ref-type="fig" rid="fig4">Figure 4F &amp; G</xref>; <xref ref-type="bibr" rid="bib52">Murphy et al., 2016</xref>). For each bee, we assessed the response time for all acceptance responses (both correct and incorrect) in the reduced evidence and easy discrimination tests. Response times were divided into 0.5 s bins and, for each bin, we calculated the proportion of correct acceptances as the number of correct acceptances / total acceptances in that response time bin. The negative slope of the CAF curves for acceptance indicates that bees made correct acceptances faster than incorrect acceptances (<xref ref-type="fig" rid="fig4">Figure 4F</xref>; Spearman correlation, rho = –0.43, n=20, p=3.0e-3). However, the CAF for the reduced evidence test was lower than the CAF for the easy discrimination test for almost the entire range of the response time (<xref ref-type="fig" rid="fig4">Figure 4F</xref>; Spearman correlation, rho = –0.25, n=18, p=6.5e-2). The gradient of the CAF curve was decreased by reducing the available evidence. This shows that decisions based on reduced evidence are slower and less accurate, and accuracy varied less with decision time. The CAF for the rejection response showed that rejection time did not vary with accuracy (<xref ref-type="fig" rid="fig4">Figure 4G</xref>; Spearman correlation, rho = 0.07, n=20, p=0.87 for easy discrimination test; rho = 0.02, n=18, p=0.81 for reduced evidence test). Collectively our analyses show that acceptance behaviour is very accurate and therefore very sensitive to available evidence, whereas rejection behaviour is less accurate, and hence is less sensitive to changes in evidence (See Discussion section).</p></sec><sec id="s2-3"><title>Bees' choice strategy is sensitive to the history of reward</title><p>In the reduced reward likelihood test bees were more likely to reject than accept stimuli (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; Wilcoxon signed rank test: z=–3.46, n=20, p=5.35e-4). In the reduced reward likelihood test bees had experienced both stimuli as rewarded and punished (33% and 66% punished) during training. We observed acceptance and rejection responses to both stimuli, most likely because bees were displaying the strategy of matching their choices to the probability each stimulus was rewarded in training (<xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref>). In the reduced reward likelihood test, there was no difference in times to accept and reject (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; Wilcoxon signed rank test: z=–0.51, n=20, p=0.60 for acceptances; z=–1.15, n=20, p=0.24). Comparing the acceptance time of the easy discrimination, reduced evidence and reduced likelihood reward tests showed that fast acceptance is associated with more reliable evidence and certainty of outcome, and slower acceptance times are associated with less reliable evidence or less certainty of reward (comparing <xref ref-type="fig" rid="fig4">Figures 4C</xref> and <xref ref-type="fig" rid="fig5">5B</xref>). No negative slope of CAF curves was observed for either acceptance or rejection behaviour in the reduced likelihood reward test (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Acceptance time decreased with increasing reward expectation (<xref ref-type="fig" rid="fig5">Figure 5D</xref>; Spearman correlation, rho = 0.04, n=20, p=0.78 for acceptances; rho = –0.11, n=20, p=0.39 for rejections). Generally, our results show that bees were more likely to reject when either the available evidence or the reward likelihood was reduced.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Bees’ performance in the reduced reward likelihood test.</title><p>(<bold>A</bold>) In the reduced reward likelihood test bees made more rejection than acceptance responses. Bees accepted the highly-rewarded colour more than the low-rewarded colour, but there was no difference in rejections of the two colours. (<bold>B</bold>) Response times did not differ for either colour or response. (<bold>C</bold>) CAF curves for acceptance and rejection response. The accuracy of acceptance or rejection responses did not change with response time in the reduced reward likelihood test (see <xref ref-type="fig" rid="fig4">Figure 4F&amp;G</xref>). (<bold>D</bold>) Comparing acceptance times in the easy discrimination and reduced evidence tests allowed us to compare acceptance times for stimuli with different likelihoods of reward in training. Bees accepted the stimuli with higher reward likelihood faster. n=20, *p&lt;0.05 and n.s., p&gt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig5-v2.tif"/></fig></sec><sec id="s2-4"><title>A minimal model for honey bee decision-making capacity</title><p>We assessed various computational sequential sampling models to explore what kinds of computation are necessary for these capacities of decision-making. We used well-established abstract models of decision-making (<xref ref-type="bibr" rid="bib3">Bogacz et al., 2006</xref>). Our first model had separate accumulators for acceptance or rejection responses (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). Both accumulators receive sensory input and they provide inputs to acceptance (<italic>A</italic>) and rejection (<italic>R</italic>) command cells, respectively (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). A decision is made either when one of the command cells reaches a predetermined threshold, or when a maximal decision time is exceeded. In this case, the command cell (<inline-formula><mml:math id="inf2"><mml:mi>A</mml:mi><mml:mo>∨</mml:mo><mml:mi>R</mml:mi></mml:math></inline-formula>) with the highest activity determines the decision (see Materials and Methods section). It is more common in sequential sampling models to assume accumulators for specific stimuli, with each stimulus channel activating a different specific response. This structure is not biologically feasible as it would demand separate accumulators for every possible visible stimulus. Hence, we modelled accumulators for response (accept and reject) and provided both with sensory input. Simulations showed this model could neither correctly accept nor reject stimuli at above chance levels.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Models of decision-making.</title><p>(<bold>A</bold>) A simple model with independent accumulators and command cells for acceptance and rejection was not able to reproduce the features of bee decisions. Correct and incorrect choices were made at equal frequency. (<bold>B</bold>) When cross-inhibitory feedback from the command cells was added to the model, the model was still not able to discriminate between the correct and incorrect choices, despite the number of rejections now being higher than acceptances. (<bold>C</bold>) A model with parallel pathways and learning cells that inhibit the accumulators with different values (i.e. <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>∧</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) had the ability to discriminate between stimuli, but the proportion of accepting correct colours and rejecting the incorrect colours are equal.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig6-v2.tif"/></fig><p>We then added to the model cross-inhibitory feedback signals from command cells back to the accumulators, which are constantly active during accumulating evidence at each accumulator (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). In this model, as evidence accumulates in one command cell, it dampens the accumulation of evidence in the other accumulator. To build a model with a higher threshold for acceptance than the rejection response we set a stronger inhibitory connection between the reject command cell and the accept accumulator (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). This difference between the strength of cross-inhibitory feedback signals makes the model more likely to reject a stimulus whenever the evidence is insufficient. This model did indeed reject stimuli more often than accept (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), but it still made an equal number of correct and incorrect choices and therefore could not discriminate between correct and incorrect decisions (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><p>To improve the accuracy of the model in acceptance responses we added learning cells and (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) to the model (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) that receive input from the sensory cells on the identity of the colours and send different inhibitory outputs to the accumulator cells (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Following a model approach by <xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref> <inline-formula><mml:math id="inf6"><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:math></inline-formula> is activated when the low rewarded colours were presented to the model. <inline-formula><mml:math id="inf7"><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:math></inline-formula> is activated by the high rewarded colour. The two accumulators receive different levels of inhibition from the learning cells based on the reward likelihood of the presented colour. If a highly-rewarded colour is presented to the model, <inline-formula><mml:math id="inf8"><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:math></inline-formula> is activated and inhibits the reject accumulator more than the accept accumulator. This lowers evidence accumulation in the rejection accumulator. Conversely, a low rewarded colour activates <inline-formula><mml:math id="inf9"><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:math></inline-formula> which inhibits the accept accumulator. The model with learning cells could discriminate between the high-rewarded and low-rewarded colours but in simulations, it made equal numbers of correct acceptance and correct rejection responses (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). This differed from the behaviour of bees (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In summary, none of the classical sequential sampling models in <xref ref-type="fig" rid="fig6">Figure 6</xref> were able to reproduce the experimental data.</p><p>Our final model included parallel accumulators for accept and reject, learning cells and the cross-inhibitory feedback signals from the command cells (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). This model could reproduce the features of bee choice behaviour (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>): (1) In this model there was a higher threshold for acceptance than rejection, and acceptance was more accurate than rejection (<xref ref-type="fig" rid="fig7">Figure 7C</xref>); (2) When the available evidence was reduced, the model showed reduced discriminability (<xref ref-type="fig" rid="fig7">Figure 7D</xref>); (3) The model was sensitive to reward likelihood (<xref ref-type="fig" rid="fig7">Figure 7E</xref>); (4) Finally, changing evidence and reward likelihood influenced acceptance and rejection response times. By comparing the model outputs with observed bee behaviours, it becomes evident that our final model can appropriately capture the dynamic features of bee decision-making. Comparing the outputs of the different models indicates that a parallel pathway for accept and reject accumulators is crucial in modelling bee decision-making, where both accumulators' evidence is subject to modification through learning and feedback from command cells (see decision letter section).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Neurobiologically-plausible model for honey bee foraging choices.</title><p>(<bold>A</bold>) The model shows the connectivity of the components of the minimum circuitry of bee decision-making, including sensory cells, two parallel accumulators, learning cells and motor commands (See Materials and Methods). Synaptic connection classes are represented at the left-hand side. (<bold>B</bold>) The diagram shows a part of the insect brain involved in the decision-making process. The photoreceptors provide input from the eye to the lamina, which then sends its projections to the medulla. The medulla connects to the protocerebrum and, in parallel, to a third-order visual processing centre, the lobula, which then sends inputs via several tracts into the protocerebrum. In parallel, neurons in the optic lobe region (medulla and lobula) branch in the mushroom body. The anterior portions of the protocerebrum receive outputs from mushroom body output neurons (MBONs), supporting learning and memory. The output from the protocerebrum are premotor neurons. MB: mushroom bodies; AL: antennal lobe; la &amp; me: lamina and medulla neuropils; lo: lobula; pro: protocerebrum. Our model reproduces the bees’ responses to easy discrimination (<bold>C</bold>), reduced evidence (<bold>D</bold>), and reduced reward likelihood tests (<bold>E</bold>). The average percentage of correct choices (acceptance or rejection) made by the model bees within blocks of 25 trials. All non-overlapping SEM error bars are significantly different (p&lt;0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86176-fig7-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our study has shown both sophistication and subtlety in honey bee decision-making. Honey bee choice behaviour is sensitive to the quality of the available evidence and the certainty of the outcome (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). Acceptance and rejection behaviours each had different relationships with reward quality and the likelihood of reward or punishment as an outcome (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Acceptance had a higher evidence threshold than rejection, and the response time to accept was longer than the time to reject (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). As a consequence, acceptance was more accurate. We observed a large number of erroneous rejections but far fewer erroneous acceptances (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Acceptance behaviour was more sensitive to reductions in reward quality and reductions in the certainty of a rewarding outcome than rejection. Correct acceptance responses were faster than incorrect acceptances (<xref ref-type="fig" rid="fig4">Figure 4</xref>) which seems counter to the well-known psychophysical speed/accuracy trade-off (<xref ref-type="bibr" rid="bib6">Chittka et al., 2003</xref>; <xref ref-type="bibr" rid="bib29">Hanks et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Heitz, 2014</xref>). The complexity of honey bee decision-making only became apparent because we scored both acceptance and rejection behaviour. Signal detection theory has always highlighted the importance of considering both acceptance and rejection responses to understand choices but typically in animal behaviour studies rejection behaviour is usually ignored (<xref ref-type="fig" rid="fig3">Figure 3</xref>; <xref ref-type="bibr" rid="bib78">Trimmer et al., 2017</xref>; <xref ref-type="bibr" rid="bib85">Wickens, 2001</xref>).</p><p>How animal decision-making is influenced by sampling time has been studied in species from insects to humans (<xref ref-type="bibr" rid="bib8">Chittka and Niven, 2009</xref>; <xref ref-type="bibr" rid="bib55">O’Connell and Hofmann, 2012</xref>; <xref ref-type="bibr" rid="bib56">O’Connell et al., 2018</xref>). The sophistication of honey bee decision-making has features in common with primates. For example, for honey bees correct acceptance decisions were faster than incorrect acceptance decisions (<xref ref-type="fig" rid="fig4">Figure 4</xref>). A similar phenomenon has been reported for primates <xref ref-type="bibr" rid="bib11">Churchland et al., 2008</xref>; <xref ref-type="bibr" rid="bib29">Hanks et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib76">Thura and Cisek, 2016</xref> found that for humans in a situation requiring an urgent decision, decision accuracy decreased with increasing response times.</p><p>Primates and honey bees then appear to be behaving opposite to the expectation of the well-known speed-accuracy trade-off which predicts greater accuracy for slower decisions (<xref ref-type="bibr" rid="bib6">Chittka et al., 2003</xref>; <xref ref-type="bibr" rid="bib31">Heitz and Schall, 2012</xref>; <xref ref-type="bibr" rid="bib45">Marshall et al., 2006</xref>; <xref ref-type="bibr" rid="bib84">Wickelgren, 1977</xref>). How can this be? The speed-accuracy trade-off is considered a general psychophysical property of decision-making. It is assumed that if a signal is noisy (for any reason) evidence of the identity of the signal will build up with time. As a consequence of this decision, accuracy should increase with increasing sampling time (<xref ref-type="bibr" rid="bib9">Chittka et al., 2009</xref>; <xref ref-type="bibr" rid="bib32">Heitz, 2014</xref>). This psychophysical approach to animal decision-making assumes that the threshold of evidence for making a decision is fixed and does not change with the amount of time spent sampling. Ecologically that is rarely the case because sampling time incurs costs; be they energetic costs of sampling, risk of predation or opportunity costs (<xref ref-type="bibr" rid="bib49">McNamara and Houston, 1985</xref>; <xref ref-type="bibr" rid="bib50">McNamara and Trimmer, 2019</xref>; <xref ref-type="bibr" rid="bib51">Mobbs et al., 2018</xref>). If sampling is costly and the consequences of an error are severe then a better strategy is to vary the evidence threshold for making a decision with sampling time (<xref ref-type="bibr" rid="bib16">Drugowitsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib19">Frazier and Yu, 2007</xref>; <xref ref-type="bibr" rid="bib44">Malhotra et al., 2018</xref>; <xref ref-type="bibr" rid="bib75">Thura et al., 2012</xref>). One strategy under these conditions is to restrict sampling time, only to accept options for which there is very high confidence in a short sampling interval, and to reject everything else (<xref ref-type="bibr" rid="bib7">Chittka and Osorio, 2007</xref>; <xref ref-type="bibr" rid="bib18">Fawcett et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Ings and Chittka, 2008</xref>; <xref ref-type="bibr" rid="bib51">Mobbs et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib77">Trimmer et al., 2008</xref>). A consequence of this strategy is that a very high proportion of acceptances made quickly will be correct (because the evidence threshold is high for rapid acceptance). For slower acceptances, the proportion of correct choices will be lower because the evidence threshold is lower for slower decisions. This gives an appearance of a reversed speed/accuracy relationship, but it is a consequence of the dynamic variation of the evidence threshold with increasing sampling time. The strategy of asymmetric errors that bees have taken in their decision is also predictable from the well-known optimal weighting rule from decision theory (<xref ref-type="bibr" rid="bib20">Freund and Schapire, 1997</xref>; <xref ref-type="bibr" rid="bib25">Grofman et al., 1983</xref>), the drift-diffusion model (<xref ref-type="bibr" rid="bib46">Marshall et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Ratcliff, 1978</xref>) and reported neural data (<xref ref-type="bibr" rid="bib36">Kiani and Shadlen, 2009</xref>; <xref ref-type="bibr" rid="bib66">Shadlen and Kiani, 2013</xref>). With this strategy, the number of rejections should be high overall, the number of erroneous rejections should be high and rejection accuracy less time dependent. These were all features we observed in honey bee decision-making. Hence, we propose in this study bees were following a similar time-dependent decision-making strategy (<xref ref-type="bibr" rid="bib36">Kiani and Shadlen, 2009</xref>; <xref ref-type="bibr" rid="bib44">Malhotra et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Marshall et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">O’Connell et al., 2018</xref>).</p><p><xref ref-type="bibr" rid="bib6">Chittka et al., 2003</xref> reported that for bumblebees, accuracy was positively correlated with choice time (<xref ref-type="bibr" rid="bib6">Chittka et al., 2003</xref>). In this study choice time was the flight time between flowers, and they did not report an actual response time of each decision. Rejections were not reported at all. In our study recording times for all responses (acceptance and rejection) gave a more nuanced interpretation of the honey bee decision-making strategy. This emphasises the importance of recording rejections as well as acceptances.</p><p>Acceptance and rejection are fundamental aspects of animal decision-making. While rejection can be complementary to acceptance when an animal has to choose between two simultaneous choices, typically acceptance and rejection are distinct types of choices in nature. For instance, bees scan each flower independently and decide whether to land on it or reject it based on the evidence sampled from the flower, prior knowledge and other factors. Our results emphasize that acceptance and rejection are distinct features of bees’ decision-making. Because rejection behaviour has a lower evidence threshold for the response it operates rather like a ‘default’ response to a stimulus and acceptance of a stimulus is more considered. This could be considered adaptive since accepting a flower is more risky for a bee than rejecting a flower. Rejection is performed in flight and honey bees in flight have high manoeuvrability and are only exposed to aerial predators. Accepting and landing exposes bees to far greater predation risk. Many bee predators, particularly mantids and spiders, have evolved as flower mimics and/or hide in vegetation close to flowers (<xref ref-type="bibr" rid="bib54">Nieh, 1993</xref>; <xref ref-type="bibr" rid="bib57">O’Hanlon et al., 2014</xref>). A foraging bee feeding on a flower is therefore exposed to greater risks than a bee in flight. Ecologically accept and reject behaviours carry different costs and benefits, and it is beneficial for bees to have separate evidence thresholds and sensitivities to evidence for acceptance and rejection.</p><p>The properties of acceptance behaviour were not fixed and were sensitive to the history of reinforcement experienced at a stimulus. Previously we have shown that in response to variable rewards bees match their choice behaviour to the probability a stimulus offers a reward (<xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref>). Such a probability matching strategy is the most likely ecologically rational strategy, and the best option in circumstances where the rewards offered by different options are unknown and liable to change. Here, we showed that even individual choices were influenced by the history of reinforcement (<xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref>). Faced with stimuli that offered both reward and punishment in training, bees' acceptance time increased, indicating the threshold for acceptance increased when there was a chance of a negative outcome from the stimulus. This shows that bees adjust how they respond to specific stimuli according to the totality of their prior experience with that stimulus.</p><sec id="s3-1"><title>A neurobiological model for honey bee decision-making</title><p>Our exploration of race and LCA modelling (<xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7A</xref>) showed that the simplest forms of the race model were not sufficient to capture the dynamic features of bee decision-making. Modelling all the properties of bee decisions required two channels for processing stimulus information, one of which was modifiable by learning (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). These channels interacted with populations of neurons that accumulated evidence for different available options, with feedback from the command cells into the accumulator populations. Our identified model was the simplest found capable of reproducing all the qualitative features of bee decision-making (<xref ref-type="fig" rid="fig7">Figure 7C, D and E</xref>). There was a striking similarity between the features of this minimal model and our understanding of the sensory-motor transformation in the insect brain (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><p>In the bee brain, visual input is processed by the lamina and medulla in the optic lobes (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The medulla projects to the protocerebrum directly, and also indirectly via a third-order visual processing centre, the lobula (<xref ref-type="bibr" rid="bib33">Hertel and Maronde, 1987</xref>; <xref ref-type="bibr" rid="bib59">Paulk et al., 2009</xref>; <xref ref-type="bibr" rid="bib71">Strausfeld, 1976</xref>; <xref ref-type="bibr" rid="bib73">Strausfeld and Okamura, 2007</xref>). In parallel, the medulla and lobula project to the mushroom bodies (<xref ref-type="bibr" rid="bib71">Strausfeld, 1976</xref>). The mushroom bodies are considered the cognitive centres of the brain. They receive multimodal input and support learning and classification (<xref ref-type="bibr" rid="bib4">Bräcker et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Giurfa and Sandoz, 2012</xref>; <xref ref-type="bibr" rid="bib30">Heisenberg, 2003</xref>; <xref ref-type="bibr" rid="bib38">Li et al., 2017</xref>). The protocerebrum is a complex region that is not completely characterised in honey bees, but in <italic>Drosophila</italic> the protocerebrum is thought to establish the valence of stimuli, whether attractive or repellent (<xref ref-type="bibr" rid="bib13">Das Chakraborty and Sachse, 2021</xref>; <xref ref-type="bibr" rid="bib39">MaBouDi et al., 2017</xref>; <xref ref-type="bibr" rid="bib58">Parnas et al., 2013</xref>). The protocerebral regions have been hypothesised to contain ‘action channels’ that help to organise different kinds of behavioural output (<xref ref-type="bibr" rid="bib21">Galizia, 2014</xref>). We believe the protocerebrum could feasibly contain neural populations acting like accumulators for accept or reject responses (<xref ref-type="bibr" rid="bib1">Aso et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Dolan et al., 2019</xref>).</p><p>That valence can be modified by learning via the outputs of the mushroom body (<xref ref-type="bibr" rid="bib15">Dolan et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Eschbach et al., 2020</xref>; <xref ref-type="bibr" rid="bib37">Lewis et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Sayin et al., 2019</xref>). These are inhibitory projections to the protocerebrum (<xref ref-type="bibr" rid="bib48">Mauelshagen, 1993</xref>; <xref ref-type="bibr" rid="bib63">Rybak and Menzel, 1993</xref>; <xref ref-type="bibr" rid="bib72">Strausfeld, 2002</xref>). Finally, protocerebrum interneurons connect with premotor regions such as the lateral accessory lobes and central complex which generate output commands for turning and hence have the capacity to transform an accept or reject signal into an approach or avoid manoeuvre (<xref ref-type="bibr" rid="bib5">Cheong et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Guo and Ritzmann, 2013</xref>; <xref ref-type="bibr" rid="bib53">Namiki et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Steinbeck et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Varela et al., 2019</xref>).</p><p>From these features of the insect brain, we can identify the functional elements needed for our minimal decision model and propose how sophisticated decisions might be possible in the insect brain (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Recent evidence from <italic>Drosophila</italic> has highlighted the role of the fly mushroom body in decision-making (<xref ref-type="bibr" rid="bib26">Groschner et al., 2018</xref>). In a simple binary choice task, the fly mushroom body accumulated evidence on different available options using separate pools of Kenyon cells that were connected to each other by reciprocal inhibition. These experimental findings lend support to how we have mapped our model against the insect brain, but our results suggest that the fly story may be incomplete. The fly experiments did not score rejection responses, nor did they explore if the properties of the decision were sensitive to evidence quality or reward likelihood, hence the bioassay might not have exposed all the decision-making capabilities of the insect. For bees at least the mushroom body pathway cannot be the only system contributing to the decision, as dual interacting pathways were necessary (<xref ref-type="bibr" rid="bib2">Barron et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Cheong et al., 2020</xref>). Further electrophysiological or neurogenetic work is needed to test whether our dual pathway model is an appropriate abstraction of the insect decision system. Our model proposes a simple decision architecture that is capable of responding adaptively to the kinds of variable evidence and circumstances encountered in real-world situations. This type of model could prove of value in autonomous robotics applications (<xref ref-type="bibr" rid="bib14">de Croon et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Kelly and Barron, 2022</xref>; <xref ref-type="bibr" rid="bib67">Stankiewicz and Webb, 2021</xref>; <xref ref-type="bibr" rid="bib83">Webb, 2020</xref>).</p><p>Our study unveils the remarkable sophistication and subtlety of honey bee decision-making while emphasizing the significance of considering both acceptance and rejection responses in animal behaviour research, an aspect often overlooked in such studies. We provide compelling evidence that honey bee decision-making is influenced by the quality of available evidence and the probability of receiving a reward as an outcome. Notably, acceptance and rejection behaviours exhibit distinct characteristics, with acceptance displaying higher accuracy albeit with greater risk. Interestingly, correct acceptances were found to be faster than incorrect acceptances, contrary to the commonly observed speed/accuracy trade-off in psychophysics. Furthermore, our study, for the first time, introduces a novel and straightforward model that elucidates parallel pathways in decision-making in honey bees. This model aligns with known pathways in the insect brain and holds neurobiological plausibility. By shedding light on the neural mechanisms underlying decision-making, our findings not only provide valuable insights into honey bee behaviour but also propose a potential framework for the development of robust autonomous decision-making systems with applications in the field of robotics.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Bees and flight arena</title><p>Experiments were conducted at the Sheffield University Research Apiary with four standard commercial hives of honey bees (<italic>Apis mellifera</italic>). To source honey bees for our experiments, we provide them with a feeder containing 20% sucrose solution (w/w). Some bees visiting the feeder were given individually distinctive marks with coloured paints on their abdomen and/or thorax using coloured Posca marking pens (Uni-Ball, Japan). Experiments were performed in a (100x80 x 80 cm) flight arena made from expanded PVC foam boards with a roof of UV-transparent Plexiglas. To create a natural foraging environment for the bees, we set up the flight arena 5 m away from the gravity feeder and an additional 15 meters away from the hives. A transparent Perspex corridor (20x4 x 4 cm) provided access to the flight arena for bees. The interior walls and floor of the arena were covered with a pink random dot pattern, which created a contrast between the bees' colour and the background. This pattern was specifically designed to aid video analysis in tracking bees (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). All bees visiting the arena were forager bees motivated to gather sucrose for their colony. In this way, the behavioural state of bees participating in the study was standardised. The flight arena was not connected to the hives, rather for each trial bees visited the flight arena under their own volition when motivated to perform a foraging flight. Forager bees forage for their colony not for themselves and they feed in the colony prior to beginning a foraging flight. Thus, bees visiting the flight arena should have been in similar physiological and motivational states. The typical inter-visit interval by a bee was 5–10 min.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86176-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Sample video of honeybee in the test.</title><p>The video was captured from an overhead perspective, providing a clear view of the bees' movements within the flight arena, showcasing their reactions to various stimuli. The black lines depict the orientation of the bee’s body at each frame of the video, offering further observations of their positioning and behaviour during the experiment.</p></caption></media></sec><sec id="s4-2"><title>Training and testing stimuli</title><p>Bees were trained to visit coloured stimuli inside the arena. Stimuli were disks (2.5 cm in diameter) of coloured paper covered with transparent laminate (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) placed on small inverted transparent plastic cups (5 cm in height). Two additional colours intermediate between green and blue were designed for the reduced evidence test (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). All colours were distinguishable for bees (<xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref>).</p></sec><sec id="s4-3"><title>Training protocol</title><p>During the pre-training phase, marked bees were attracted to the entrance of the arena from the gravity feeder using a cotton bud soaked in a 50% sucrose solution (w/w). Once at the gravity feeder, the bees were given more 50% sucrose solution and were gently moved to the entrance of the corridor. This process was repeated until the bee was able to fly independently to the entrance of the corridor. The bees were then trained to fly into the arena via the entrance corridor to locate drops of 50% sucrose placed on transparent disks of laminate top plastic cups. The roof of the arena was lifted to release the bees from the arena every time they were satiated. Only those bees that flew independently into the arena to feed were selected for the training phase.</p><p>Each bee was separately trained with five different coloured stimuli in a colour discrimination task for 18 bouts of training. In each trial a bee was presented with a pair of colours selected from the training stimuli (<xref ref-type="fig" rid="fig1">Figure 1B</xref>); one rewarded colour and the other punished (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). During each trial, the bees were presented with four stimuli of each colour of the pair and given multiple opportunities to choose each colour until they reached satiation. Stimuli were placed randomly in the arena. Based on the reward or punishment assigned to each colour listed in the Protocol 1 and 2 during 18 trials (<xref ref-type="table" rid="table1">Table 1</xref>), the five different colours were each assigned a different likelihood of reward during the training trials: 100%, %66, %50, %33, and %0 of training trials (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Colour pairs were organised such that in every trial one colour was rewarded and one punished (<xref ref-type="table" rid="table1">Table 1</xref>). For example, bees were rewarded with stimulus S%66 in four trials (#4, #8, #13, #17 in the protocol P1; #4, #9, #12, #15 in protocol P2) and punished in two trials (#1, #10 in protocol P1; #2, #17 in the protocol P2) (<xref ref-type="table" rid="table1">Table 1</xref>). Thus, the likelihood of receiving a reward for stimulus S%66 during the training trials was 4/6=66%. Stimuli were rewarded with 10 <italic>μl</italic> sucrose solution 50% (w/w) or punished with 10 <italic>μl</italic> of saturated quinine hemisulphate solution.</p><p>To evaluate any effect of the innate colour preference of bees on their decision, bees were randomly assigned to one of two groups: A and B. For group A, colours were ordered as: blue = 100%, yellow = 66%, pink = 50%, orange = 33%, green = 0%. For group B, the colours were ordered as: green = 100%, orange = 66%, white = 50%, yellow = 33%, blue = 0%. Specific details on the reflectance spectrum of each colour are given in <xref ref-type="bibr" rid="bib40">MaBouDi et al., 2020a</xref>.</p><p>Over 18 training trials, bees experienced all combinations of the five colours twice, with the exception that bees in training never experienced %66 rewarded paired with %33 rewarded colours. This pairing was excluded from training so that in the post-training, <italic>reduced reward likelihood test,</italic> we could examine how trained bees evaluate a colour pair based on the reward likelihood of colours. To control the effect of the training sequence on bees’ colour preferences, bees were randomly assigned to one colour group (A or B) and one of two different sequences of training bouts (protocols P1 and P2; <xref ref-type="table" rid="table1">Table 1</xref>). In each training bout, bees were able to freely choose and feed from rewarded stimuli. 10 μL drops of 50% sucrose solution were replaced on depleted rewarded stimuli until the bee had fed to satiation and left the arena via the roof. Between trials, all stimuli and the arena were cleaned with soap water and then 70% ethanol and water to remove any possible pheromonal cues left by the bee. They finally were air-dried before reuse.</p></sec><sec id="s4-4"><title>Testing</title><p>Each bee was given three tests. Each test was video recorded for 120 s. In all tests, all stimuli provided 10 μl water. The <italic>easy colour discrimination test</italic> presented bees with the colours that had been rewarded in 100% and 0% of training trials. The <italic>reduced reward likelihood test</italic> presented bees with 66% and 33% rewarded colours – a combination they never experienced in training. In the <italic>reduced evidence test</italic> bees were given two novel colours that were similar to but intermediate to the 100% and 0% rewarded colours. The sequence of the three tests was pseudo-randomised for each bee. To maintain the bees’ motivation to visit the arena, one or two refreshment trials were given between tests. In a refreshment trial, the bees were allowed to feed from 10 μL sucrose drops placed on eight disks of transparent laminate positioned in the arena. As in training, stimuli and the arena were cleaned between each test.</p></sec><sec id="s4-5"><title>Automatic bee tracking algorithm</title><p>The flight arena was equipped with an iPhone 6 camera placed at the top of the arena, 1 meter distance from the floor, facing down that captured the full base of the flight arena in the field of view (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The camera was configured to record at 30 FPS (at a resolution of 1080 pixels) in the training phase, and 240 FPS at 720 pixels in the testing phase. The first 120 s of the test and the first bout of the training phase were used to analyse bees’ flights. Examples of a recorded flight path are shown in <xref ref-type="video" rid="video1">Video 1</xref>.</p><p>A bee’s flight path was determined frame by frame extracting the x, y coordinates of the bee’s body and its body orientation. From each frame, the background was subtracted using the average of the previous 50 frames. By modifying MATLAB’s blob detection function with a threshold set close to the size of the bee very few candidate positions for the bee were found in each frame. We associated each pixel in each frame of the video with either a bee or the background. The bee’s position at each frame obtained from the algorithm became a single point in the trajectory over time. The obtained trajectory represents the position of the bee as a function of time. An elliptic filter was applied to the frame at the position of the detected bee to evaluate the bee’s body orientation. The smoothing function, ‘smoothdata’, was used to exclude outlier locations from the trajectory.</p><p>The flight path began when the bee entered the arena. Hovering time prior to accepting or rejecting a stimulus was assessed as the total time the bee’s body was within a 5 cm radius of the centre of the stimulus (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). We assumed that bees did not attend to the stimuli when flying over them at high speeds (above the height of the cups) as they did when flying between the stimuli at a similar speed, opposed to when bees were approaching the stimuli at the same height as the plastic cups. Thus, 7% of all paths with length &lt;0.2 s close to the edge of the focal area were excluded from analyses. A bee accepted a colour when it made contact with the colour (antennae at least contacting the platform; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). This translated to an automatically count bees’ landings algorithm. This algorithm counts bee’s landing and utilises a threshold flight speed classifier based on the k-means algorithm that was applied to flight paths that crossed over the stimuli (<xref ref-type="bibr" rid="bib42">MaBouDi et al., 2021</xref>). In this dynamic threshold determination, the speed of bees within the border of the colours was clustered into two groups: acceptance (very low-speed paths) and reject (high-speed paths). The boundary between the two groups obtained by the K-means algorithm was set as a defined rule to determine whether the bee chose or did not choose the colour.</p></sec><sec id="s4-6"><title>Flight analysis and statistics</title><p>In each test, we evaluated bees’ performance from their choices during their first 120 s in the arena. Choices were scored as accepting (made a contact with colour) or rejecting a stimulus (flying away without landing). If the bee accepted the colour more likely to be rewarded in training, we considered this a correct choice. If the bee rejected the colour more likely to be punished in training, we also considered this a correct choice. Hence the bees’ decision was classified into four distinct responses: (1) <italic>correct acceptance</italic> (CA), landing on the more rewarded colour (2) <italic>incorrect acceptance</italic> (IA), landing on the less rewarded colour (3) <italic>correct rejection</italic> (CR), rejecting the less rewarded colour and (4) <italic>incorrect rejection</italic> (IR), rejecting the more rewarded colour. To summarise the bees’ performance in the tests, the Matthew correlation coefficient (MCC) was used as follows <xref ref-type="bibr" rid="bib40">MaBouDi et al., 2020a</xref>; <xref ref-type="bibr" rid="bib47">Matthews, 1975</xref>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represent the number of CA, CR, IA and IRs for a bee in a test. The MCC has a scale from –1 to +1. High positive values indicate mostly correct acceptance and rejection choices. Negative values correspond to bees making mostly incorrect choices. Zero indicates bees choose colours randomly. A Wilcoxon signed rank test was applied to the MCC values to compare bees’ performance. Finally, the relationship between bees’ MCC and their scanning behaviours in the tests was evaluated by the Spearman’s correlation tests. All statistical tests were performed in MATLAB 2019 (MathWorks, Natick, MA, USA). Also, to ensure the validity of our conclusions, we conducted a power analysis on the bees' performance in the experimental tests, which helped us to confirm that our sample size was sufficient (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). This approach allowed us to have greater confidence in the statistical significance of our findings and to draw more accurate conclusions from our data.</p></sec><sec id="s4-7"><title>Signal detection theory</title><p>Signal detection theory (<xref ref-type="bibr" rid="bib85">Wickens, 2001</xref>) was used to analyse bee decisions. Signal detection theory proposes that bees evaluate a signal (stimulus with strength x) as either rewarded or punished. We assume that the probability of either accepting or rejecting a perceived signal can be described by two distributions that are normal in shape with equal variance (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We also assume a decision criterion (<italic>d.c</italic>.) of the perceived signal at which the response changes from accept to reject (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). From the positions of the distributions and the location of the criterion, we can estimate the expected probabilities of correct acceptances (hits) correct rejections, incorrect acceptance (false negative), and incorrect rejections (false positive; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). The location of <italic>d.c</italic>. can be influenced by training and the experience of each signal as either punished or rewarded as well as the consequences of correct and incorrect acceptance and rejection choices (<xref ref-type="bibr" rid="bib85">Wickens, 2001</xref>). Discriminability (d’) is the difference in signal between the maximum likelihood of acceptance and rejection responses (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). If d’ is low the acceptance and rejection distributions overlap. Hence more errors are made.</p><p>Discriminability (<inline-formula><mml:math id="inf14"><mml:mi>d</mml:mi><mml:mi>`</mml:mi></mml:math></inline-formula>) and the decision criteria (<inline-formula><mml:math id="inf15"><mml:mi>d</mml:mi><mml:mo>.</mml:mo><mml:mi>c</mml:mi><mml:mo>.</mml:mo></mml:math></inline-formula>) can be calculated from the empirical measurements of hit and false positive rates as follows<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>Z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>and<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi>d</mml:mi><mml:mo>.</mml:mo><mml:mi>c</mml:mi><mml:mo>.</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>Z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where the function <inline-formula><mml:math id="inf16"><mml:mi>Z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> is the inverse of the standard normal cumulative distribution function (CDF). The hit rate is the ratio of correct acceptance to all acceptances (<inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>) and the false positive rate is the ratio of incorrect rejections to all rejections (<inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-8"><title>Modelling honey bee decision-making</title><p>We started with the simple and well-defined sequential sampling model (<xref ref-type="bibr" rid="bib3">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib60">Pike, 1966</xref>; <xref ref-type="bibr" rid="bib81">Vickers, 1970</xref>) which we adjusted to provide a better fit to experimental data for both accuracy and reaction times (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>). Our adjustments to the sequential sampling model were constrained by the types of processing considered plausible to derive both acceptance and rejection responses through two parallel pathways.</p><p>In the model, evidence favouring each alternative (<inline-formula><mml:math id="inf19"><mml:mi>I</mml:mi></mml:math></inline-formula>) accumulated in separate accept (<inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) or reject (<inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) accumulators over time (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Biologically plausible leaky accumulators (with decay rate, <inline-formula><mml:math id="inf22"><mml:mi>k</mml:mi></mml:math></inline-formula>) were used to model the decision time which represent the duration that bees spend accumulating evidence in favour of or against a stimulus. At each time step, accept and reject accumulators send signals to the accept (<inline-formula><mml:math id="inf23"><mml:mi>A</mml:mi></mml:math></inline-formula>) and rejection (<inline-formula><mml:math id="inf24"><mml:mi>R</mml:mi></mml:math></inline-formula>) command cells, respectively. The output of command cells of accept and rejection was calculated by <inline-formula><mml:math id="inf25"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> with the baseline activity at 0.1. A decision was made either when one of the command cells reached a predetermined threshold, or when a decision was forced by exceeding a maximal assessment time in which case the decision associated with the command cell with the highest activity was chosen. The accumulation of evidence in the model is governed according to the following stochastic ordinary differential equations:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>At time zero, the evidence accumulated <inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are set to zero; <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. Brownian random motions <inline-formula><mml:math id="inf30"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are added to represent noise in input and model the random walk behaviour.</p><p>To add inhibitory feedback signals from the command cells into the accumulators (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), both accept and reject accumulators actively received feedback inhibitory signals from the opposite command cells while simultaneously receiving inputs from their respective accumulators as:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>and<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the fraction of command outputs that inhibit the alternative accumulator.</p><p>In a previous studies (<xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref>; <xref ref-type="bibr" rid="bib80">Vasas et al., 2019</xref>), we developed a model for the five-armed bandit task, which showed that plasticity in both the input (calyx) and output (lobes) of the mushroom body can effectively learn the history of reinforcement for different colours. This implies that the mushroom body output neurons can provide distinct inhibitory signals to the accumulator cells based on the reinforcement history of each colour. In the current study, we utilized the abstract version of learning cells from our previous work, which underwent 18 training trials for the five different colours in the five-armed bandit task, identical to what the bees experienced in this study. Building upon the model proposed in <xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref>, we incorporated two types of learning cells (<inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) into the model and presented the modified version in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. Both learning cells received the sensory input and sent different inhibitory outputs to the accumulators based on the reward likelihood of the colours. <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the value of inhibitory signals that the accept and reject accumulators received from the learning cells (<inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) such that <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> . The model activates the first learning cell, <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula>, if the high rewarded colour is presented to the model, and activates the second learning cell, <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula>, if the low rewarded colour is presented to the model. <inline-formula><mml:math id="inf44"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>α</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> represent the rate of the learning cells activity based on the input signal (<inline-formula><mml:math id="inf45"><mml:mi>I</mml:mi></mml:math></inline-formula>). The behaviour of learning cells and the value of the alpha were assumed and inspired by the model presented in our previous research (<xref ref-type="bibr" rid="bib41">MaBouDi et al., 2020b</xref>), that demonstrated how the reinforcement neurons modulates the strengths of the synaptic connectivity in mushroom bodies in response to both reward and punishment. Synaptic weights <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were updated for each presented stimulus during training such that the accumulation of evidence in the model proceed according to the following equations:<disp-formula id="equ8">,<label>(8)</label><mml:math id="m8"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> represent the activity of learning cells <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , respectively. Our final model, (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) accumulated evidence following <xref ref-type="disp-formula" rid="equ8 equ9">Equations 8 and 9</xref>. The accumulators received cross-inhibitory signals from the command cells according to <xref ref-type="disp-formula" rid="equ6 equ7">Equations 6 and 7</xref>.</p></sec><sec id="s4-9"><title>Model evaluation</title><p>The models are presented with 25 trials in which high-rewarded and low-rewarded stimuli were randomly presented. Each model responded after each trial by accepting or rejecting the presented stimulus. The performance of the model was evaluated by counting the number of correct and incorrect acceptances or rejections and their corresponding response times. In addition, we normalised the time response of the model to the maximum time response of all model bees, which allowed us to make meaningful comparisons between the relative time responses of different experimental conditions and the observed time responses. This approach helped us to identify significant differences in the bees' responses to different stimuli and to gain a deeper understanding of the factors that influence their behaviour. Twenty different model bees with different random factors were examined and reported in this study. The final model could be simplified to emphasise the effect of the contributions of learning and feedback from command cells. In this way, the final model (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) was also examined with learning cells inactive (<inline-formula><mml:math id="inf52"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) or without the contribution of command cells by synaptic weights <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> set to zero. We assumed the accept and reject pathways process the input interdependently (i.e. no interaction between pathways) if <inline-formula><mml:math id="inf55"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Validation, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="data-availability" id="s6"><title>Data availability</title><p>Collected data have been deposited in <ext-link ext-link-type="uri" xlink:href="https://figshare.com/">figshare</ext-link> via link <ext-link ext-link-type="uri" xlink:href="https://github.com/hadiimaboudi/data_bee_decision_making">GitHub</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib43">MaBouDi, 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Dearden</surname><given-names>N</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data for: How foraging honeybees make decisions</data-title><source>figshare</source><pub-id pub-id-type="accession" xlink:href="https://figshare.com/s/b7b9495beda2a0bb6db0">b7b9495beda2a0bb6db0</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Michael Port from Sheffield Robotics for assistance in building the testing arena. We thank Amy Bullivant for her assistance in analysing the video. HM, ND and JARM were supported by the Engineering and Physical Sciences Research Council (grant no EP/P006094/1). ABB is supported by funding by a Future Fellowship from the Australian Research Council (FT140100452), a Leverhulme Visiting Fellowship from the Leverhulme Trust and the Templeton World Charity Foundation (grant no. TWCF-2020–20539).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aso</surname><given-names>Y</given-names></name><name><surname>Sitaraman</surname><given-names>D</given-names></name><name><surname>Ichinose</surname><given-names>T</given-names></name><name><surname>Kaun</surname><given-names>KR</given-names></name><name><surname>Vogt</surname><given-names>K</given-names></name><name><surname>Belliart-Guérin</surname><given-names>G</given-names></name><name><surname>Plaçais</surname><given-names>PY</given-names></name><name><surname>Robie</surname><given-names>AA</given-names></name><name><surname>Yamagata</surname><given-names>N</given-names></name><name><surname>Schnaitmann</surname><given-names>C</given-names></name><name><surname>Rowell</surname><given-names>WJ</given-names></name><name><surname>Johnston</surname><given-names>RM</given-names></name><name><surname>Ngo</surname><given-names>TTB</given-names></name><name><surname>Chen</surname><given-names>N</given-names></name><name><surname>Korff</surname><given-names>W</given-names></name><name><surname>Nitabach</surname><given-names>MN</given-names></name><name><surname>Heberlein</surname><given-names>U</given-names></name><name><surname>Preat</surname><given-names>T</given-names></name><name><surname>Branson</surname><given-names>KM</given-names></name><name><surname>Tanimoto</surname><given-names>H</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mushroom body output neurons Encode Valence and guide memory-based action selection in <italic>Drosophila</italic></article-title><source>eLife</source><volume>3</volume><elocation-id>e04580</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04580</pub-id><pub-id pub-id-type="pmid">25535794</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>AB</given-names></name><name><surname>Gurney</surname><given-names>KN</given-names></name><name><surname>Meah</surname><given-names>LFS</given-names></name><name><surname>Vasilaki</surname><given-names>E</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Decision-making and action selection in insects: inspiration from vertebrate-based theories</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>216</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00216</pub-id><pub-id pub-id-type="pmid">26347627</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>E</given-names></name><name><surname>Moehlis</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bräcker</surname><given-names>LB</given-names></name><name><surname>Siju</surname><given-names>KP</given-names></name><name><surname>Varela</surname><given-names>N</given-names></name><name><surname>Aso</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Hein</surname><given-names>I</given-names></name><name><surname>Vasconcelos</surname><given-names>ML</given-names></name><name><surname>Grunwald Kadow</surname><given-names>IC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Essential role of the mushroom body in context-dependent Co2 avoidance in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>23</volume><fpage>1228</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.029</pub-id><pub-id pub-id-type="pmid">23770186</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheong</surname><given-names>HS</given-names></name><name><surname>Siwanowicz</surname><given-names>I</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multi-regional circuits underlying visually guided decision-making in <italic>Drosophila</italic></article-title><source>Current Opinion in Neurobiology</source><volume>65</volume><fpage>77</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2020.10.010</pub-id><pub-id pub-id-type="pmid">33217639</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Dyer</surname><given-names>AG</given-names></name><name><surname>Bock</surname><given-names>F</given-names></name><name><surname>Dornhaus</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Psychophysics: bees trade off foraging speed for accuracy</article-title><source>Nature</source><volume>424</volume><elocation-id>388</elocation-id><pub-id pub-id-type="doi">10.1038/424388a</pub-id><pub-id pub-id-type="pmid">12879057</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cognitive dimensions of predator responses to imperfect Mimicry</article-title><source>PLOS Biology</source><volume>5</volume><elocation-id>e339</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0050339</pub-id><pub-id pub-id-type="pmid">18162048</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Niven</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Are bigger brains better?</article-title><source>Current Biology</source><volume>19</volume><fpage>R995</fpage><lpage>R1008</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.08.023</pub-id><pub-id pub-id-type="pmid">19922859</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Raine</surname><given-names>NE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Speed–accuracy Tradeoffs in animal decision making</article-title><source>Trends in Ecology &amp; Evolution</source><volume>24</volume><fpage>400</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2009.02.010</pub-id><pub-id pub-id-type="pmid">19409649</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>The Mind of a Bee</source><publisher-name>Princeton University Press</publisher-name><pub-id pub-id-type="doi">10.1515/9780691236247</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision-making with multiple alternatives</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>693</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nn.2123</pub-id><pub-id pub-id-type="pmid">18488024</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conradt</surname><given-names>L</given-names></name><name><surname>Roper</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Consensus decision making in animals</article-title><source>Trends in Ecology &amp; Evolution</source><volume>20</volume><fpage>449</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2005.05.008</pub-id><pub-id pub-id-type="pmid">16701416</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das Chakraborty</surname><given-names>S</given-names></name><name><surname>Sachse</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Olfactory processing in the lateral Horn of <italic>Drosophila</italic></article-title><source>Cell and Tissue Research</source><volume>383</volume><fpage>113</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1007/s00441-020-03392-6</pub-id><pub-id pub-id-type="pmid">33475851</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Croon</surname><given-names>GCHE</given-names></name><name><surname>Dupeyroux</surname><given-names>JJG</given-names></name><name><surname>Fuller</surname><given-names>SB</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Insect-inspired AI for autonomous robots</article-title><source>Science Robotics</source><volume>7</volume><elocation-id>eabl6334</elocation-id><pub-id pub-id-type="doi">10.1126/scirobotics.abl6334</pub-id><pub-id pub-id-type="pmid">35704608</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>MJ</given-names></name><name><surname>Frechter</surname><given-names>S</given-names></name><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Dan</surname><given-names>C</given-names></name><name><surname>Huoviala</surname><given-names>P</given-names></name><name><surname>Roberts</surname><given-names>RJ</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Dhawan</surname><given-names>S</given-names></name><name><surname>Tabano</surname><given-names>R</given-names></name><name><surname>Dionne</surname><given-names>H</given-names></name><name><surname>Christoforou</surname><given-names>C</given-names></name><name><surname>Close</surname><given-names>K</given-names></name><name><surname>Sutcliffe</surname><given-names>B</given-names></name><name><surname>Giuliani</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Ihrke</surname><given-names>G</given-names></name><name><surname>Meissner</surname><given-names>GW</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Aso</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Jefferis</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neurogenetic dissection of the <italic>Drosophila</italic> lateral Horn reveals major outputs, diverse behavioural functions, and interactions with the mushroom body</article-title><source>eLife</source><volume>8</volume><elocation-id>e43079</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43079</pub-id><pub-id pub-id-type="pmid">31112130</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The cost of accumulating evidence in perceptual decision making</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3612</fpage><lpage>3628</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4010-11.2012</pub-id><pub-id pub-id-type="pmid">22423085</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eschbach</surname><given-names>C</given-names></name><name><surname>Fushiki</surname><given-names>A</given-names></name><name><surname>Winding</surname><given-names>M</given-names></name><name><surname>Schneider-Mizell</surname><given-names>CM</given-names></name><name><surname>Shao</surname><given-names>M</given-names></name><name><surname>Arruda</surname><given-names>R</given-names></name><name><surname>Eichler</surname><given-names>K</given-names></name><name><surname>Valdes-Aleman</surname><given-names>J</given-names></name><name><surname>Ohyama</surname><given-names>T</given-names></name><name><surname>Thum</surname><given-names>AS</given-names></name><name><surname>Gerber</surname><given-names>B</given-names></name><name><surname>Fetter</surname><given-names>RD</given-names></name><name><surname>Truman</surname><given-names>JW</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name><name><surname>Zlatic</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recurrent architecture for adaptive regulation of learning in the insect brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>544</fpage><lpage>555</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0607-9</pub-id><pub-id pub-id-type="pmid">32203499</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fawcett</surname><given-names>TW</given-names></name><name><surname>Fallenstein</surname><given-names>B</given-names></name><name><surname>Higginson</surname><given-names>AD</given-names></name><name><surname>Houston</surname><given-names>AI</given-names></name><name><surname>Mallpress</surname><given-names>DEW</given-names></name><name><surname>Trimmer</surname><given-names>PC</given-names></name><name><surname>McNamara</surname><given-names>JM</given-names></name><collab>Modelling Animal Decisions Group</collab></person-group><year iso-8601-date="2014">2014</year><article-title>The evolution of decision rules in complex environments</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>153</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.12.012</pub-id><pub-id pub-id-type="pmid">24467913</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Frazier</surname><given-names>P</given-names></name><name><surname>Yu</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sequential hypothesis testing under stochastic deadlines</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>Y</given-names></name><name><surname>Schapire</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A decision-Theoretic generalization of on-line learning and an application to boosting</article-title><source>Journal of Computer and System Sciences</source><volume>55</volume><fpage>119</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1006/jcss.1997.1504</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galizia</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Olfactory coding in the insect brain: data and conjectures</article-title><source>The European Journal of Neuroscience</source><volume>39</volume><fpage>1784</fpage><lpage>1795</lpage><pub-id pub-id-type="doi">10.1111/ejn.12558</pub-id><pub-id pub-id-type="pmid">24698302</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Sandoz</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Invertebrate learning and memory: fifty years of olfactory conditioning of the Proboscis extension response in honeybees</article-title><source>Learning &amp; Memory</source><volume>19</volume><fpage>54</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1101/lm.024711.111</pub-id><pub-id pub-id-type="pmid">22251890</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Green</surname><given-names>DM</given-names></name><name><surname>Swets</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Signal Detection Theory and Psychophysics</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grofman</surname><given-names>B</given-names></name><name><surname>Owen</surname><given-names>G</given-names></name><name><surname>Feld</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Thirteen Theorems in search of the truth</article-title><source>Theory and Decision</source><volume>15</volume><fpage>261</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1007/BF00125672</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groschner</surname><given-names>LN</given-names></name><name><surname>Chan Wah Hak</surname><given-names>L</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>DasGupta</surname><given-names>S</given-names></name><name><surname>Miesenböck</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dendritic integration of sensory evidence in perceptual decision-making</article-title><source>Cell</source><volume>173</volume><fpage>894</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.03.075</pub-id><pub-id pub-id-type="pmid">29706545</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>P</given-names></name><name><surname>Ritzmann</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural activity in the central complex of the Cockroach brain is linked to turning behaviors</article-title><source>The Journal of Experimental Biology</source><volume>216</volume><fpage>992</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1242/jeb.080473</pub-id><pub-id pub-id-type="pmid">23197098</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurney</surname><given-names>K</given-names></name><name><surname>Prescott</surname><given-names>TJ</given-names></name><name><surname>Redgrave</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A computational model of action selection in the basal ganglia. I. A new functional anatomy</article-title><source>Biological Cybernetics</source><volume>84</volume><fpage>401</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1007/PL00007984</pub-id><pub-id pub-id-type="pmid">11417052</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>T</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A neural mechanism of speed-accuracy Tradeoff in Macaque area LIP</article-title><source>eLife</source><volume>3</volume><elocation-id>e02260</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.02260</pub-id><pub-id pub-id-type="pmid">24867216</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heisenberg</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Mushroom body Memoir: from maps to models</article-title><source>Nature Reviews. Neuroscience</source><volume>4</volume><fpage>266</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1038/nrn1074</pub-id><pub-id pub-id-type="pmid">12671643</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heitz</surname><given-names>RP</given-names></name><name><surname>Schall</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural mechanisms of speed-accuracy Tradeoff</article-title><source>Neuron</source><volume>76</volume><fpage>616</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.030</pub-id><pub-id pub-id-type="pmid">23141072</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heitz</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The speed-accuracy Tradeoff: history, physiology, methodology, and behavior</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>150</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00150</pub-id><pub-id pub-id-type="pmid">24966810</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertel</surname><given-names>H</given-names></name><name><surname>Maronde</surname><given-names>U</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The physiology and morphology of centrally projecting visual Interneurones in the honeybee brain</article-title><source>Journal of Experimental Biology</source><volume>133</volume><fpage>301</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1242/jeb.133.1.301</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ings</surname><given-names>TC</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Speed-accuracy Tradeoffs and false alarms in bee responses to cryptic predators</article-title><source>Current Biology</source><volume>18</volume><fpage>1520</fpage><lpage>1524</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.07.074</pub-id><pub-id pub-id-type="pmid">18771920</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>M</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The best of both worlds: dual systems of reasoning in animals and AI</article-title><source>Cognition</source><volume>225</volume><elocation-id>105118</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105118</pub-id><pub-id pub-id-type="pmid">35453083</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of confidence associated with a decision by neurons in the Parietal cortex</article-title><source>Science</source><volume>324</volume><fpage>759</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1126/science.1169405</pub-id><pub-id pub-id-type="pmid">19423820</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>LPC</given-names></name><name><surname>Siju</surname><given-names>KP</given-names></name><name><surname>Aso</surname><given-names>Y</given-names></name><name><surname>Friedrich</surname><given-names>AB</given-names></name><name><surname>Bulteel</surname><given-names>AJB</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Grunwald Kadow</surname><given-names>IC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A higher brain circuit for immediate integration of conflicting sensory information in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>25</volume><fpage>2203</fpage><lpage>2214</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.07.015</pub-id><pub-id pub-id-type="pmid">26299514</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Egertová</surname><given-names>M</given-names></name><name><surname>Elphick</surname><given-names>MR</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Perry</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A possible structural correlate of learning performance on A colour discrimination task in the brain of the bumblebee</article-title><source>Proceedings of the Royal Society B</source><volume>284</volume><elocation-id>20171323</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2017.1323</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Shimazaki</surname><given-names>H</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Olfactory learning without the mushroom bodies: Spiking neural network models of the honeybee lateral Antennal lobe tract reveal its capacities in odour memory tasks of varied complexities</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005551</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005551</pub-id><pub-id pub-id-type="pmid">28640825</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Galpayage Dona</surname><given-names>HS</given-names></name><name><surname>Gatto</surname><given-names>E</given-names></name><name><surname>Loukola</surname><given-names>OJ</given-names></name><name><surname>Buckley</surname><given-names>E</given-names></name><name><surname>Onoufriou</surname><given-names>PD</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Bumblebees use sequential scanning of Countable items in visual patterns to solve Numerosity tasks</article-title><source>Integrative and Comparative Biology</source><volume>60</volume><fpage>929</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1093/icb/icaa025</pub-id><pub-id pub-id-type="pmid">32369562</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Honeybees solve a multi-comparison ranking task by probability matching</article-title><source>Proceedings. Biological Sciences</source><volume>287</volume><elocation-id>20201525</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2020.1525</pub-id><pub-id pub-id-type="pmid">32873200</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Guiraud</surname><given-names>M</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Automated Video Tracking and Flight Analysis Show How Bumblebees Solve a Pattern Discrimination Task Using Active Vision</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.09.434580</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Data_Bee_Decision_Making</data-title><version designator="swh:1:rev:3ed511a1f1e8c6a3625a95070092df4a1c0c6d8b">swh:1:rev:3ed511a1f1e8c6a3625a95070092df4a1c0c6d8b</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3f49c0db144f4280e3a021ccb4f4ab9a663d51a8;origin=https://github.com/hadiimaboudi/data_bee_decision_making;visit=swh:1:snp:46f53d29067ef23e2ed1b85074faa6c16af643a4;anchor=swh:1:rev:3ed511a1f1e8c6a3625a95070092df4a1c0c6d8b">https://archive.softwareheritage.org/swh:1:dir:3f49c0db144f4280e3a021ccb4f4ab9a663d51a8;origin=https://github.com/hadiimaboudi/data_bee_decision_making;visit=swh:1:snp:46f53d29067ef23e2ed1b85074faa6c16af643a4;anchor=swh:1:rev:3ed511a1f1e8c6a3625a95070092df4a1c0c6d8b</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>Leslie</surname><given-names>DS</given-names></name><name><surname>Ludwig</surname><given-names>CJH</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Time-varying decision boundaries: insights from optimality analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>971</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1340-6</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Dornhaus</surname><given-names>A</given-names></name><name><surname>Franks</surname><given-names>NR</given-names></name><name><surname>Kovacs</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Noise, cost and speed-accuracy trade-offs: decision-making in a decentralized system</article-title><source>Journal of the Royal Society, Interface</source><volume>3</volume><fpage>243</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1098/rsif.2005.0075</pub-id><pub-id pub-id-type="pmid">16849234</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Brown</surname><given-names>G</given-names></name><name><surname>Radford</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Individual confidence-weighting and group decision-making</article-title><source>Trends in Ecology &amp; Evolution</source><volume>32</volume><fpage>636</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2017.06.004</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthews</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Comparison of the predicted and observed secondary structure of T4 Phage lysozyme</article-title><source>Biochimica et Biophysica Acta</source><volume>405</volume><fpage>442</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1016/0005-2795(75)90109-9</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mauelshagen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Neural correlates of olfactory learning paradigms in an identified neuron in the honeybee brain</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>609</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.2.609</pub-id><pub-id pub-id-type="pmid">8459289</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamara</surname><given-names>JM</given-names></name><name><surname>Houston</surname><given-names>AI</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Optimal foraging and learning</article-title><source>Journal of Theoretical Biology</source><volume>117</volume><fpage>231</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/S0022-5193(85)80219-8</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamara</surname><given-names>JM</given-names></name><name><surname>Trimmer</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sequential choices using signal detection theory can reverse classical predictions</article-title><source>Behavioral Ecology</source><volume>30</volume><fpage>16</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1093/beheco/ary132</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mobbs</surname><given-names>D</given-names></name><name><surname>Trimmer</surname><given-names>PC</given-names></name><name><surname>Blumstein</surname><given-names>DT</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Foraging for foundations in decision Neuroscience: insights from Ethology</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>419</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0010-7</pub-id><pub-id pub-id-type="pmid">29752468</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>PR</given-names></name><name><surname>Boonstra</surname><given-names>E</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Global gain modulation generates time-dependent urgency during perceptual choice in humans</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13526</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13526</pub-id><pub-id pub-id-type="pmid">27882927</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Namiki</surname><given-names>S</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name><name><surname>Wong</surname><given-names>AM</given-names></name><name><surname>Korff</surname><given-names>W</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The functional organization of descending sensory-motor pathways in <italic>Drosophila</italic></article-title><source>eLife</source><volume>7</volume><elocation-id>e34272</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34272</pub-id><pub-id pub-id-type="pmid">29943730</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieh</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The stop signal of honey bees: reconsidering its message</article-title><source>Behavioral Ecology and Sociobiology</source><volume>33</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1007/BF00164346</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connell</surname><given-names>LA</given-names></name><name><surname>Hofmann</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Evolution of a vertebrate social decision-making network</article-title><source>Science</source><volume>336</volume><fpage>1154</fpage><lpage>1157</lpage><pub-id pub-id-type="doi">10.1126/science.1218889</pub-id><pub-id pub-id-type="pmid">22654056</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connell</surname><given-names>RG</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Wong-Lin</surname><given-names>K</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bridging neural and computational viewpoints on perceptual decision-making</article-title><source>Trends in Neurosciences</source><volume>41</volume><fpage>838</fpage><lpage>852</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.06.005</pub-id><pub-id pub-id-type="pmid">30007746</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Hanlon</surname><given-names>JC</given-names></name><name><surname>Holwell</surname><given-names>GI</given-names></name><name><surname>Herberstein</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pollinator deception in the Orchid Mantis</article-title><source>The American Naturalist</source><volume>183</volume><fpage>126</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1086/673858</pub-id><pub-id pub-id-type="pmid">24334741</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parnas</surname><given-names>M</given-names></name><name><surname>Lin</surname><given-names>AC</given-names></name><name><surname>Huetteroth</surname><given-names>W</given-names></name><name><surname>Miesenböck</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Odor discrimination in <italic>Drosophila</italic>: from neural population codes to behavior</article-title><source>Neuron</source><volume>79</volume><fpage>932</fpage><lpage>944</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.006</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Dacks</surname><given-names>AM</given-names></name><name><surname>Phillips-Portillo</surname><given-names>J</given-names></name><name><surname>Fellous</surname><given-names>JM</given-names></name><name><surname>Gronenberg</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual processing in the central bee brain</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>9987</fpage><lpage>9999</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1325-09.2009</pub-id><pub-id pub-id-type="pmid">19675233</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pike</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Stochastic models of choice behaviour: response probabilities and latencies of finite Markov chain Systems1</article-title><source>The British Journal of Mathematical and Statistical Psychology</source><volume>19</volume><fpage>15</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8317.1966.tb00351.x</pub-id><pub-id pub-id-type="pmid">5939142</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pirrone</surname><given-names>A</given-names></name><name><surname>Stafford</surname><given-names>T</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>When natural selection should optimize speed-accuracy trade-offs</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>73</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00073</pub-id><pub-id pub-id-type="pmid">24782703</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rybak</surname><given-names>J</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Anatomy of the mushroom bodies in the honey bee brain: the neuronal connections of the alpha-lobe</article-title><source>The Journal of Comparative Neurology</source><volume>334</volume><fpage>444</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1002/cne.903340309</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sayin</surname><given-names>S</given-names></name><name><surname>De Backer</surname><given-names>J-F</given-names></name><name><surname>Siju</surname><given-names>KP</given-names></name><name><surname>Wosniack</surname><given-names>ME</given-names></name><name><surname>Lewis</surname><given-names>LP</given-names></name><name><surname>Frisch</surname><given-names>L-M</given-names></name><name><surname>Gansen</surname><given-names>B</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Edmondson-Stait</surname><given-names>A</given-names></name><name><surname>Sharifi</surname><given-names>N</given-names></name><name><surname>Fisher</surname><given-names>CB</given-names></name><name><surname>Calle-Schuler</surname><given-names>SA</given-names></name><name><surname>Lauritzen</surname><given-names>JS</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Jefferis</surname><given-names>GSXE</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name><name><surname>Grunwald Kadow</surname><given-names>IC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A neural circuit arbitrates between persistence and withdrawal in hungry <italic>Drosophila</italic></article-title><source>Neuron</source><volume>104</volume><fpage>544</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.07.028</pub-id><pub-id pub-id-type="pmid">31471123</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seed</surname><given-names>A</given-names></name><name><surname>Clayton</surname><given-names>N</given-names></name><name><surname>Carruthers</surname><given-names>P</given-names></name><name><surname>Dickinson</surname><given-names>A</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Güntürkün</surname><given-names>O</given-names></name><name><surname>Hampton</surname><given-names>RR</given-names></name><name><surname>Kacelnik</surname><given-names>A</given-names></name><name><surname>Shanahan</surname><given-names>M</given-names></name><name><surname>Stevens</surname><given-names>JR</given-names></name><name><surname>Tebbich</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Planning, Memory, and Decision MakingAnimal Thinking</source><publisher-name>The MIT Press</publisher-name><fpage>121</fpage><lpage>147</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Decision making as a window on cognition</article-title><source>Neuron</source><volume>80</volume><fpage>791</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.047</pub-id><pub-id pub-id-type="pmid">24183028</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stankiewicz</surname><given-names>J</given-names></name><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Looking down: a model for visual route following in flying insects</article-title><source>Bioinspiration &amp; Biomimetics</source><volume>16</volume><elocation-id>055007</elocation-id><pub-id pub-id-type="doi">10.1088/1748-3190/ac1307</pub-id><pub-id pub-id-type="pmid">34243169</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinbeck</surname><given-names>F</given-names></name><name><surname>Adden</surname><given-names>A</given-names></name><name><surname>Graham</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Connecting brain to behaviour: a role for general purpose steering circuits in insect orientation</article-title><source>The Journal of Experimental Biology</source><volume>223</volume><elocation-id>jeb212332</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.212332</pub-id><pub-id pub-id-type="pmid">32161054</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision Ecology: foraging and the Ecology of animal decision making</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>8</volume><fpage>475</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.3758/CABN.8.4.475</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Mechanisms for Decisions about the Future</source><publisher-name>MIT press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9780262016636.001.0001</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strausfeld</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1976">1976</year><source>Mosaic Organizations, Layers, and Visual Pathways in the Insect BrainNeural Principles in Vision</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strausfeld</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Organization of the honey bee mushroom body: representation of the Calyx within the vertical and gamma lobes</article-title><source>The Journal of Comparative Neurology</source><volume>450</volume><fpage>4</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1002/cne.10285</pub-id><pub-id pub-id-type="pmid">12124764</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strausfeld</surname><given-names>NJ</given-names></name><name><surname>Okamura</surname><given-names>JY</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual system of Calliphorid flies: organization of optic glomeruli and their Lobula complex Efferents</article-title><source>The Journal of Comparative Neurology</source><volume>500</volume><fpage>166</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1002/cne.21196</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sumner</surname><given-names>CJ</given-names></name><name><surname>Sumner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Signal detection: applying analysis methods from psychology to animal behaviour</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>375</volume><elocation-id>20190480</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0480</pub-id><pub-id pub-id-type="pmid">32420861</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Beauregard-Racine</surname><given-names>J</given-names></name><name><surname>Fradet</surname><given-names>CW</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decision making by urgency gating: theory and experimental support</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>2912</fpage><lpage>2930</lpage><pub-id pub-id-type="doi">10.1152/jn.01071.2011</pub-id><pub-id pub-id-type="pmid">22993260</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Modulation of Premotor and primary motor cortical activity during Volitional adjustments of speed-accuracy trade-offs</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>938</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2230-15.2016</pub-id><pub-id pub-id-type="pmid">26791222</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trimmer</surname><given-names>PC</given-names></name><name><surname>Houston</surname><given-names>AI</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Paul</surname><given-names>ES</given-names></name><name><surname>Mendl</surname><given-names>MT</given-names></name><name><surname>McNamara</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mammalian choices: combining fast-but-inaccurate and slow-but-accurate decision-making systems</article-title><source>Proceedings. Biological Sciences</source><volume>275</volume><fpage>2353</fpage><lpage>2361</lpage><pub-id pub-id-type="doi">10.1098/rspb.2008.0417</pub-id><pub-id pub-id-type="pmid">18611852</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trimmer</surname><given-names>PC</given-names></name><name><surname>Ehlman</surname><given-names>SM</given-names></name><name><surname>McNamara</surname><given-names>JM</given-names></name><name><surname>Sih</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The erroneous signals of detection theory</article-title><source>Proceedings. Biological Sciences</source><volume>284</volume><elocation-id>20171852</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2017.1852</pub-id><pub-id pub-id-type="pmid">29046382</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varela</surname><given-names>N</given-names></name><name><surname>Gaspar</surname><given-names>M</given-names></name><name><surname>Dias</surname><given-names>S</given-names></name><name><surname>Vasconcelos</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Avoidance response to Co2 in the lateral Horn</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e2006749</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2006749</pub-id><pub-id pub-id-type="pmid">30653496</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasas</surname><given-names>V</given-names></name><name><surname>Peng</surname><given-names>F</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Randomly weighted receptor inputs can explain the large diversity of colour-coding neurons in the bee visual system</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>8330</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-44375-0</pub-id><pub-id pub-id-type="pmid">31171814</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vickers</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Evidence for an Accumulator model of Psychophysical discrimination</article-title><source>Ergonomics</source><volume>13</volume><fpage>37</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1080/00140137008931117</pub-id><pub-id pub-id-type="pmid">5416868</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural Dynamics and circuit mechanisms of decision-making</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>1039</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.08.006</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Robots with insect brains</article-title><source>Science</source><volume>368</volume><fpage>244</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1126/science.aaz6869</pub-id><pub-id pub-id-type="pmid">32299938</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickelgren</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Speed-accuracy Tradeoff and information processing Dynamics</article-title><source>Acta Psychologica</source><volume>41</volume><fpage>67</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(77)90012-9</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickens</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Elementary Signal Detection Theory</source><publisher-name>Oxford university press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195092509.001.0001</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86176.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Grunwald Kadow</surname><given-names>Ilona C</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041nas322</institution-id><institution>University of Bonn</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2023.01.02.522517" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.02.522517"/></front-stub><body><p>This valuable study elucidates the honeybee's behavioral strategy to associate sensory cues with rewards of different values. Based on solid experimental evidence the study demonstrates how sensory evidence and reward likelihood quantitatively affect the decision-making process and the bees' response time. The behavioral paradigm and the proposed model could provide interesting insights for scientists studying decision-making in higher animal species.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86176.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Grunwald Kadow</surname><given-names>Ilona C</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041nas322</institution-id><institution>University of Bonn</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Riabinina</surname><given-names>Olena</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01v29qb04</institution-id><institution>Durham Univeristy</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Fujiwara</surname><given-names>Terufumi</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03g001n57</institution-id><institution>Champalimaud Foundation</institution></institution-wrap><country>Portugal</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.02.522517">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2023.01.02.522517v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;How honey bees make fast and accurate decisions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Olena Riabinina (Reviewer #1); Terufumi Fujiwara (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The two reviewers found your study of interest and the results novel and exciting. However, they've raised several substantial concerns regarding the clarity of presentation and description of the methodology. In addition, we would like to ask you to either justify the number of bees and the statistical methods used or, if and where appropriate, to include additional analysis and possibly more animals. Please pay specific attention to the following points:</p><p>i) Please improve the presentation of the relevance and novelty of the data in the abstract, text, and model.</p><p>ii) Please explain the methodology of behavioral testing as well as analysis and modelling better. This point is of particular importance as it was a concern to the editors when reaching their decision to proceed to peer review.</p><p>iii) Please justify the, relatively, low number of animals used per experiment and consider applying additional statistical tests.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) We recommend to the authors make a clearer statement about the relevance and novelty of the model they developed.</p><p>2) The Methods need to be clearly explained, especially those concerning the implementations of the formulae and models.</p><p>3) We are not convinced that 20 bees (3 tests each) can give sufficient data to support the conclusions. Please justify your choice of numbers.</p><p>4) The manuscript is well-written and very nicely formatted, and we love the figures in the text – thank you!</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Line 112-113: It would be nice to have a detailed explanation of how rejection response was defined. I think it is important to clarify in the Main Text how you separated &quot;active&quot; rejection from just &quot;passively&quot; passing through the flower. Relating to this, it might be better to bring Figure 4A and the corresponding explanation earlier.</p><p>Maybe relating to this, I would like to know more about the close relationship between behaviour and the model. What does the evidence accumulation look like on behaviour? Do bees go back and forth between a pair of flowers or accumulate evidence over approaching a flower?</p><p>I did not get how the model associates 5 different flour colors with different reward probabilities. How many Learning Cells does the entire model have? Only two or more?</p><p>It would be nice to have some discussion of how the reward and punishment (i.e., sugar and quinine) signals contribute to the model. The model has input signals (flower color signals) but doesn't have reinforcement signals. Would it be possible to compare the proposed model with an RL model?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86176.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>1) We recommend to the authors make a clearer statement about the relevance and novelty of the model they developed.</p></disp-quote><p>Thank you – we have added your suggestions to the discussion as requested on lines 420-432 which now reads: “Our study reveals the sophistication and subtlety of honey bee decision-making and highlights the importance of considering both acceptance and rejection responses in animal behaviour studies, which is typically ignored in animal behaviour studies. We demonstrate that honey bee decision-making is sensitive to the quality of available evidence and the likelihood of reward as an outcome. Acceptance and rejection behaviours are distinct, with acceptance being more accurate but riskier. Surprisingly, correct acceptances were faster than incorrect acceptances, counter to the psychophysical speed/accuracy trade-off. Additionally, for the first time, we propose a simple model of parallel decision-making in honey bees that can be mapped to known pathways in the insect brain and is neurobiologically plausible. Our study provides insights into the neural mechanisms of decision-making and proposes a potential system for robust autonomous decision-making with applications in robotics.”</p><disp-quote content-type="editor-comment"><p>2) The Methods need to be clearly explained, especially those concerning the implementations of the formulae and models.</p></disp-quote><p>We are sorry for not being clearer on the method. The method section now is updated with more details of the model proposed to describe the features of bees decision-making.</p><disp-quote content-type="editor-comment"><p>3) We are not convinced that 20 bees (3 tests each) can give sufficient data to support the conclusions. Please justify your choice of numbers.</p></disp-quote><p>Thank you for raising the issue of sample size in our study. We agree that this is an important consideration in scientific research, and we appreciate your feedback.</p><p>In order to address this concern, we conducted a power analysis on the performance of the bees in the experimental tests, which helped us to confirm that our sample size of 20 bees was sufficient to support our interpretation and conclusions. We have provided the results of this analysis in Figure 2—figure supplement 1C of our paper. This is also mentioned in the paper on lines 547-551.</p><p>While there are currently no established guidelines for the care and use of bees in research, we strive to follow the 3Rs principles (Replacement, Reduction, and Refinement) in designing our experiments. Specifically, we made every effort to utilize the minimum number of bees necessary to obtain acceptable conclusions. Recent studies have suggested that bees may experience emotion-like and pain experiences (Perry et al., 2016; Gibbons and Chittka, 2022; Gibbons et al., 2022), further highlighting the importance of minimizing any potential harm or discomfort during experimental procedures.</p><p>Furthermore, in our review of the literature on bee cognition, we found that many studies have utilized a sample size of 20 model bees in their experiments, even when conducting multiple experimental tests. While we recognize that sample size may vary depending on the specific research question and experimental design, we chose to follow this common practice in order to ensure that our results are comparable to previous studies and can be generalized to the broader population of bees.</p><p>Overall, while we acknowledge that increasing the sample size could potentially strengthen the statistical power of our study, we believe that our conclusions would not be significantly impacted by doing so. We have taken care to ensure that our methodology is sound, and our results are robust, and we are confident in the validity of our findings.</p><disp-quote content-type="editor-comment"><p>4) The manuscript is well-written and very nicely formatted, and we love the figures in the text – thank you!</p></disp-quote><p>Thank you very much.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Line 112-113: It would be nice to have a detailed explanation of how rejection response was defined. I think it is important to clarify in the Main Text how you separated &quot;active&quot; rejection from just &quot;passively&quot; passing through the flower. Relating to this, it might be better to bring Figure 4A and the corresponding explanation earlier.</p></disp-quote><p>Thank you for your suggestion. We’re sorry for not being clearer on this topic. We have revised the section on lines 519-531 and included more details and references on how we identified active rejections through video analysis. We hope that this clarification will improve the comprehensibility of our study.</p><disp-quote content-type="editor-comment"><p>Maybe relating to this, I would like to know more about the close relationship between behaviour and the model. What does the evidence accumulation look like on behaviour? Do bees go back and forth between a pair of flowers or accumulate evidence over approaching a flower?</p></disp-quote><p>We apologise for the lack of clarity about the model. we have added more detailed explanation of the proposed model in the method section.</p><p>This study aimed to investigate the decision-making process of bees by designing a multi-options task that allowed the bees to explore each flower individually, which closely mimics their natural response in nature. The duration of time that the bees spent accumulating evidence for or against a stimulus was measured as their decision time. To model this process, we used biologically plausible leaky accumulators in both accumulators P<sub>a</sub> and P<sub>r</sub> as the acceptance and reception accumulators (See Figure 7A). Additionally, inputs from learning cells and command cells were added to modulate the accumulation process based on the learned stimuli and the bees' decision-making strategy. However, we attempted to present an abstract model of how bees inspect flowers and make decisions to land or reject stimuli. In the method section on lines 581- 627, variable &quot;I&quot; represented the value of the input signal received by the accumulators, which had two versions, one directly from the sensory cells and another modified version from learning cells. The response time of the bees in choosing or rejecting stimuli was assumed to be the time until one of the command cells reached the threshold. The number of accept or reject command cells above the threshold was considered the choice number of acceptance or rejection.</p><disp-quote content-type="editor-comment"><p>I did not get how the model associates 5 different flour colors with different reward probabilities. How many Learning Cells does the entire model have? Only two or more?</p><p>It would be nice to have some discussion of how the reward and punishment (i.e., sugar and quinine) signals contribute to the model. The model has input signals (flower color signals) but doesn't have reinforcement signals. Would it be possible to compare the proposed model with an RL model?</p></disp-quote><p>We again apologise for not being clear about the model. We have now updated the method section with more details about the model.</p><p>In our previous research (MaBouDi et al., 2020b), we developed a model to simulate bee learning in the five-armed bandit task, showing how the learning cells in mushroom bodies can be modified by reinforcement signals to provide distinct outputs based on the history of reinforcements for different colours. Additionally, we discussed the contribution of rewards and punishments in the plasticity of mushroom bodies. In the current study, our focus was on the decision-making mechanisms, so we only presented an abstract model of the learning cells based on our previous research. However, a combined model that incorporates both learning and decision-making processes would be valuable for understanding bees' behaviour in inspecting and deciding on a set of flowers with varying reinforcement values. We plan to pursue this research question in the future.</p></body></sub-article></article>