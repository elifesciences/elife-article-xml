<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79854</article-id><article-id pub-id-type="doi">10.7554/eLife.79854</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Generative power of a protein language model trained on multiple sequence alignments</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-279501"><name><surname>Sgarbossa</surname><given-names>Damiano</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7878-6061</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-279500"><name><surname>Lupo</surname><given-names>Umberto</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6767-493X</contrib-id><email>umberto.lupo@epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-12070"><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1020-494X</contrib-id><email>anne-florence.bitbol@epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Institute of Bioengineering, School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL)</institution></institution-wrap><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002n09z45</institution-id><institution>SIB Swiss Institute of Bioinformatics</institution></institution-wrap><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Colwell</surname><given-names>Lucy J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Cambridge University</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>03</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e79854</elocation-id><history><date date-type="received" iso-8601-date="2022-04-28"><day>28</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-02-02"><day>02</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-04-15"><day>15</day><month>04</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.04.14.488405"/></event></pub-history><permissions><copyright-statement>© 2023, Sgarbossa et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Sgarbossa et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79854-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-79854-figures-v2.pdf"/><abstract><p>Computational models starting from large ensembles of evolutionarily related protein sequences capture a representation of protein families and learn constraints associated to protein structure and function. They thus open the possibility for generating novel sequences belonging to protein families. Protein language models trained on multiple sequence alignments, such as MSA Transformer, are highly attractive candidates to this end. We propose and test an iterative method that directly employs the masked language modeling objective to generate sequences using MSA Transformer. We demonstrate that the resulting sequences score as well as natural sequences, for homology, coevolution, and structure-based measures. For large protein families, our synthetic sequences have similar or better properties compared to sequences generated by Potts models, including experimentally validated ones. Moreover, for small protein families, our generation method based on MSA Transformer outperforms Potts models. Our method also more accurately reproduces the higher-order statistics and the distribution of sequences in sequence space of natural data than Potts models. MSA Transformer is thus a strong candidate for protein sequence generation and protein design.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>protein sequences</kwd><kwd>protein families</kwd><kwd>protein language models</kwd><kwd>deep learning</kwd><kwd>protein sequence generation</kwd><kwd>protein design</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>851173</award-id><principal-award-recipient><name><surname>Sgarbossa</surname><given-names>Damiano</given-names></name><name><surname>Lupo</surname><given-names>Umberto</given-names></name><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An iterative procedure using language models allows the generation of sequences from protein families, which score similarly to natural and experimentally validated sequences, with particular promise for small families.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Designing new proteins with specific structure and function is a highly important goal of bioengineering. Indeed, it can allow to tune their stability or their biochemical properties, including their enzymatic activities, enabling important medical applications. The search for novel proteins is difficult due to the huge size of protein sequence space: for instance, there are 20<sup>100</sup> different possible sequences for a short protein domain with 100 amino acids. Furthermore, only a small fraction of this space comprises sequences that do fold, as demonstrated by experiments studying random sequences (<xref ref-type="bibr" rid="bib77">Socolich et al., 2005</xref>), and by theoretical arguments based on the physics of disordered systems (<xref ref-type="bibr" rid="bib10">Bialek, 2012</xref>). De novo or rational protein design, which starts with target three-dimensional structures and physico-chemical potentials, can generate proteins which are not in a known protein family (<xref ref-type="bibr" rid="bib21">Dahiyat and Mayo, 1997</xref>; <xref ref-type="bibr" rid="bib44">Kuhlman et al., 2003</xref>; <xref ref-type="bibr" rid="bib47">Liang et al., 2009</xref>), but is generally restricted to small proteins (<xref ref-type="bibr" rid="bib70">Rocklin et al., 2017</xref>). Conversely, directed evolution allows to perform a local search of sequence space, but generally remains limited to the vicinity of a natural sequence (<xref ref-type="bibr" rid="bib3">Arnold, 2018</xref>).</p><p>Generative computational models that build on the breadth of available natural protein sequence data, and capture a representation of protein families, now offer great alternatives that can allow to sample novel sequences belonging to protein families. In particular, Potts models, or DCA models (<xref ref-type="bibr" rid="bib86">Weigt et al., 2009</xref>; <xref ref-type="bibr" rid="bib62">Morcos et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Marks et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Ekeberg et al., 2013</xref>), which are pairwise maximum entropy models trained to reproduce the one- and two-body statistics of the sequences of a family, allow direct sampling from a probability distribution modeling this family (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>), and have been used successfully for protein design (<xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>). Variational autoencoders are deep learning models which also allow sampling, and they have been shown to successfully produce functional proteins (<xref ref-type="bibr" rid="bib38">Hawkins-Hooker et al., 2021a</xref>), although their statistical properties appear to have a lower quality than with Potts models (<xref ref-type="bibr" rid="bib55">McGee et al., 2021</xref>).</p><p>Protein language models are deep learning models based on natural language processing methods, especially attention (<xref ref-type="bibr" rid="bib5">Bahdanau et al., 2015</xref>) and transformers (<xref ref-type="bibr" rid="bib82">Vaswani et al., 2017</xref>). They are trained on large ensembles of protein sequences, and capture long-range dependencies within a protein sequence (<xref ref-type="bibr" rid="bib1">Alley et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Elnaggar et al., 2021</xref>; <xref ref-type="bibr" rid="bib69">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib68">Rao et al., 2021b</xref>; <xref ref-type="bibr" rid="bib83">Vig et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Madani et al., 2020</xref>; <xref ref-type="bibr" rid="bib50">Madani et al., 2021</xref>; <xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>). These pre-trained models are able to predict structure in an unsupervised way (<xref ref-type="bibr" rid="bib68">Rao et al., 2021b</xref>), either taking as input a single sequence (<xref ref-type="bibr" rid="bib69">Rives et al., 2021</xref>) or a multiple sequence alignment (MSA) (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>), potentially by transferring knowledge from their large training set (<xref ref-type="bibr" rid="bib7">Bhattacharya et al., 2020</xref>; <xref ref-type="bibr" rid="bib8">Bhattacharya et al., 2022</xref>). The great success of supervised protein structure prediction by AlphaFold (<xref ref-type="bibr" rid="bib43">Jumper et al., 2021</xref>) is partly based on the use of transformers. It is therefore of strong interest to assess the generative ability of protein language models, and recent works show that this has high potential (<xref ref-type="bibr" rid="bib50">Madani et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Johnson et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Hawkins-Hooker et al., 2021b</xref>; <xref ref-type="bibr" rid="bib29">Ferruz et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">Hie et al., 2022</xref>).</p><p>Correlations in amino-acid usage that can be observed between the columns of MSAs of homologous proteins (<xref ref-type="bibr" rid="bib13">Casari et al., 1995</xref>; <xref ref-type="bibr" rid="bib45">Lapedes et al., 1999</xref>; <xref ref-type="bibr" rid="bib24">Dunn et al., 2008</xref>) were experimentally demonstrated to be highly important to generate functional synthetic proteins (<xref ref-type="bibr" rid="bib77">Socolich et al., 2005</xref>; <xref ref-type="bibr" rid="bib9">Bialek and Ranganathan, 2007</xref>). The importance of pairwise coevolution signals was then corroborated by the success of Potts models at predicting structural contacts (<xref ref-type="bibr" rid="bib86">Weigt et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Marks et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Morcos et al., 2011</xref>; <xref ref-type="bibr" rid="bib79">Sułkowska et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Ekeberg et al., 2013</xref>), analyzing mutational effects (<xref ref-type="bibr" rid="bib25">Dwyer et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Cheng et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Cheng et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">Figliuzzi et al., 2016</xref>), protein evolution (<xref ref-type="bibr" rid="bib22">de la Paz et al., 2020</xref>) and conformational changes (<xref ref-type="bibr" rid="bib63">Morcos et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Malinverni et al., 2015</xref>), designing proteins (<xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>), and predicting protein–protein interaction partners (<xref ref-type="bibr" rid="bib12">Bitbol et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Gueudré et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Cong et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Green et al., 2021</xref>). Protein language models that take MSAs as input (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>; <xref ref-type="bibr" rid="bib43">Jumper et al., 2021</xref>) are able to directly exploit this covariation signal, and are thus particularly interesting candidates for protein design. Thus motivated, we focus on MSA Transformer (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>), a protein language model which was trained on MSAs using the masked language modeling (MLM) objective, without additional supervised training – by contrast to AlphaFold (<xref ref-type="bibr" rid="bib43">Jumper et al., 2021</xref>). We ask how the generative properties of MSA Transformer compare to those of Boltzmann machine DCA (bmDCA) (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>; <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>), a state-of-the-art generative Potts model.</p><p>We propose and test a generating method that directly uses the MLM objective in an iterative way to generate sequences using MSA Transformer. Using homology, coevolution, and structural scores, we demonstrate that the sequences generated by this method score as well as natural sequences. We further show that this good performance is not restricted to synthetic sequences that are very similar to natural sequences. For large protein families, our synthetic sequences have homology and structure-based scores as good as or better than sequences generated by bmDCA, and have similar properties to experimentally validated bmDCA-generated sequences. Moreover, for small protein families, our generation method based on MSA Transformer outperforms bmDCA, by providing synthetic sequences that score well without being extremely similar to natural ones. However, we find that bmDCA better reproduces the one- and two-body statistics of the natural MSAs than MSA Transformer when used with default parameters, consistently with its training objective. Interestingly, the opposite generally holds for higher-order statistics. MSA-Transformer–generated sequences also better reproduce the distribution of sequences in sequence space than bmDCA-generated ones. Our conclusion is that MSA Transformer is a strong candidate for protein sequence generation and protein design.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>An iterative masking procedure allows MSA Transformer to generate novel sequences with high scores</title><p>Can the protein language model MSA Transformer (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>) be used to generate sequences that are credible members of protein families? How do its generative abilities compare to Potts models inferred by bmDCA (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>), a state-of-the-art generative DCA method which has been experimentally shown to generate functional proteins (<xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>)? To address these questions, we developed and employed an iterative masking procedure to generate synthetic MSAs from natural MSAs of 14 different large Pfam protein families (see <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>) and 7 small ones (see <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>) with MSA Transformer, as described in ‘Using MSA Transformer to generate sequences via an iterative masking procedure’. We also generated synthetic sequences by Markov Chain Monte Carlo (MCMC) sampling from Potts models inferred from these MSAs by bmDCA, using two variants that differ by sampling temperature <inline-formula><mml:math id="inf1"><mml:mi>T</mml:mi></mml:math></inline-formula> and regularization strength <inline-formula><mml:math id="inf2"><mml:mi>λ</mml:mi></mml:math></inline-formula>, matching, respectively, the default parameters employed in <xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>, and some of those used in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>, see ‘Sampling sequences from Potts models’ for details. For each protein family, we thus obtained four different MSAs of the same depth: the natural one, the one generated by our iterative masking procedure using MSA Transformer, and the two MSAs sampled from the inferred Potts model. To characterize each sequence, we consider four different scores (see ‘Scoring individual sequences’). First, we assess the quality of the generated sequences as homologs of the protein family of interest; we do this via the HMMER (<ext-link ext-link-type="uri" xlink:href="http://hmmer.org">http://hmmer.org</ext-link>) score of the hidden Markov model employed by Pfam to retrieve natural homologs. Second, we consider a score that accounts for coevolution between amino-acid sites, namely the statistical energy score from the Potts model fitted on the natural MSA. Third, we determine AlphaFold’s confidence in its determination of the three-dimensional structure of these sequences, via the predicted local-distance difference test (pLDDT) score. Fourth, to assess whether the predicted structures are similar to the native ones, we compute the root-mean-squared deviation (RMSD) between a reference experimental structure and the AlphaFold predicted structures. The first three scores are such that higher values are better, while smaller RMSD values indicate that predicted structures are similar to the native ones. Together, these scores account for very different aspects of proteins, namely homology, coevolution and structure.</p><p>Let us first consider the 14 large protein families in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>, where MSAs are deep enough to accurately fit Potts models using bmDCA (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>). <xref ref-type="fig" rid="fig1">Figure 1</xref> shows that, for all these protein families, and for these four different scores, the sequences generated by MSA Transformer using our iterative masking procedure have scores that are at least as good as those of natural sequences, and better than those of sequences generated by bmDCA with default parameters (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>), as confirmed by the Kolmogorov–Smirnov test (see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). Decreasing the sampling temperature and the regularization strength used with bmDCA improves the statistical energy score as expected (<xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>), but also other scores. These other scores, and most importantly our two structural scores, are similar or better for MSA-Transformer–generated sequences compared to those generated by bmDCA with non-default parameters. In particular, the median pLDDT score is larger for the former than for the latter in 11 protein families out of 14, by a margin larger than the standard deviation in 4 of them (see <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>). These results demonstrate that MSA Transformer is a good candidate to generate synthetic sequences from protein families, and that our iterative masking procedure allows to perform such generation.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Comparison of homology, coevolution, and structure-based scores between natural sequences and sequences generated by MSA Transformer or Boltzmann machine DCA (bmDCA).</title><p>For each Pfam family in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>, we compare a natural MSA from Pfam and three synthetic MSAs of the same depth. The first synthetic MSA was obtained using MSA Transformer via our iterative masking procedure, and the second and third ones were generated by a Potts model inferred from the natural MSA using bmDCA with two different pairs <inline-formula><mml:math id="inf3"><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo mathvariant="normal" rspace="4.2pt">,</mml:mo><mml:mi>T</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of regularization strength <inline-formula><mml:math id="inf4"><mml:mi>λ</mml:mi></mml:math></inline-formula> and sampling temperature <inline-formula><mml:math id="inf5"><mml:mi>T</mml:mi></mml:math></inline-formula>. For each of the four scores described in ‘Scoring individual sequences’, we show the distributions of score values among sequences in each MSA as a violin plot. Higher score values are better for all scores except root-mean-squared deviation (RMSD) (bottom panel), where smaller values indicate a closer match to an experimental structure. Top panel: For each Pfam family, HMMER scores are divided by the highest score found in the natural MSA. Note that sequences below HMMER’s default homology detection score (<italic>E</italic>-value larger than 10), and whose HMMER score is thus 0, are not shown (the median over families of the fraction of such sequences is 2% for bmDCA (10<sup>−2</sup>, 1.00)-generated MSAs, while there are no such sequences among the MSA-Transformer–generated ones). Second panel: Statistical energy scores are defined as minus the bmDCA statistical energies. To accommodate the highly family-dependent ranges of these scores, for each Pfam family we show their values after shifting by the mean score in the natural MSA, and normalizing by the standard deviation of natural MSA scores. Third panel: AlphaFold’s predicted local-distance difference test (pLDDT) confidence scores. Bottom panel: RMSD of predicted structures with respect to the experimental structures in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>. Structural scores (pLDDT and RMSD) were computed on 200 randomly chosen sequences from each MSA. All kernel-smoothed histograms are normalized such that all violins have the same maximal width. Outliers (less than 1% in all cases) were discarded for legibility.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Multiple sequence alignment (MSA) diversity for each protein family and each generation method.</title><p>We show the relative effective depth of each MSA (natural or generated), that is <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext mathvariant="normal">eff</mml:mtext></mml:msub><mml:mo mathvariant="normal">/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>M</mml:mi><mml:mtext mathvariant="normal">eff</mml:mtext></mml:msub></mml:math></inline-formula> is effective MSA depth and <inline-formula><mml:math id="inf8"><mml:mi>M</mml:mi></mml:math></inline-formula> is actual MSA depth, versus the similarity threshold <inline-formula><mml:math id="inf9"><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mo mathvariant="normal">-</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:math></inline-formula> used to define the effective depth (see <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig1-figsupp1-v2.tif"/></fig></fig-group><p>How different are these synthetic sequences from the natural ones? In particular, are the best-scoring sequences novel, or are they almost copies of natural sequences? In <xref ref-type="fig" rid="fig2">Figure 2</xref> we show, for two example protein families (PF00072 and PF00153), the HMMER score and the DCA statistical energy score versus the sequence’s Hamming distance to its closest natural sequence in the natural MSA.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Homology and coevolution scores versus distance to the natural multiple sequence alignment (MSA), for protein families PF00072 and PF00153.</title><p>We show contour plots of the HMMER score and the statistical energy score (defined as minus the DCA statistical energy, shifted by its mean value in the natural MSA) versus the Hamming distance of each sequence to the closest natural sequence (which is not itself, in the case of natural sequences). Results are shown for natural sequences and for sequences generated using MSA Transformer and Boltzmann machine DCA (bmDCA) (the same two <inline-formula><mml:math id="inf10"><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>T</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> pairs as in <xref ref-type="fig" rid="fig1">Figure 1</xref> are used for bmDCA). The lightest contours shown include 99% of the cumulative probability mass.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig2-v2.tif"/></fig><p>From the marginal distributions of the Hamming distances in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we observe that MSA Transformer generates sequences with variable distances to their closest natural sequences, and that these distances are overall larger than those between natural sequences and their closest neighbors (excluding themselves). With default parameters, bmDCA generates sequences which are generally very different from the natural ones, but decreasing sampling temperature makes bmDCA-generated sequences more similar to natural ones and to each other, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. Besides, the marginal distributions of scores illustrate the general observation made on <xref ref-type="fig" rid="fig1">Figure 1</xref> and in <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref> that MSA-Transformer–generated sequences have good scores. Moreover, the plots in <xref ref-type="fig" rid="fig2">Figure 2</xref> reveal that the MSA-Transformer–generated sequences featuring the highest HMMER scores tend to have large Hamming distances to natural sequences, that is to be truly novel (see also ‘Choosing parameters in the iterative masking procedure’). We observe these trends for most large protein families studied, and they are robust to using BLOSUM similarity scores (<xref ref-type="bibr" rid="bib40">Henikoff and Henikoff, 1992</xref>) instead of Hamming distances. Therefore, our sequence generation method based on MSA Transformer is not reaching good scores by just reproducing natural sequences. Besides, the diversity of MSA-Transformer–generated MSAs, as measured by their effective depth (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), is only slightly smaller than that of natural MSAs (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Conversely, bmDCA at low temperature produces highly redundant sequences (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), which are concentrated in specific regions of the score versus distance space in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Indeed, sequence generation by bmDCA is then constrained to exploring the local minima of the Potts model energy landscapes.</p></sec><sec id="s2-2"><title>Sequence generation by the iterative masking procedure is successful for small protein families</title><p>Accurately fitting Potts models requires deep and diverse MSAs, as evidenced by the strong dependence of structural contact prediction by Potts models on MSA depth (<xref ref-type="bibr" rid="bib53">Marks et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Morcos et al., 2011</xref>). By contrast, MSA Transformer was trained on many MSAs, and is able to transfer knowledge across protein families. It outperforms Potts models at unsupervised contact prediction most strongly for shallow MSAs (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>). How does sequence generation using our iterative masking procedure based on MSA Transformer compare to bmDCA in the case of small protein families?</p><p>To address this question, we generated synthetic MSAs starting from seven small families, using both our iterative masking procedure based on MSA Transformer and bmDCA with default parameters and with low sampling temperature. <xref ref-type="fig" rid="fig3">Figure 3</xref> reports all four scores discussed above in the case of these seven small families, listed in <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>. We observe that MSA-Transformer–generated sequences have similar HMMER scores and structural scores to natural sequences. MSA-Transformer–generated sequences also generally have better HMMER scores and structural scores than those generated by bmDCA with default parameters. While low-temperature bmDCA yields better statistical energy scores (as expected), and also gives HMMER scores and structural scores comparable to natural sequences, it in fact generates sequences that are almost exact copies of natural ones (see <xref ref-type="fig" rid="fig3">Figure 3</xref>, bottom row). By contrast, MSA Transformer produces sequences that are quite different from natural ones, and have very good scores. Thus, our method based on MSA Transformer is particularly promising in the tricky case of small protein families.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Application of our sequence generation method based on MSA Transformer to small protein families.</title><p>We consider seven small protein families, with natural MSAs that comprise from nine to a few hundreds of sequences, see <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>. As in <xref ref-type="fig" rid="fig1">Figure 1</xref>, for each family, we compare the natural MSA and three synthetic MSAs of the same depth. In all cases, we show violin plots of the same four scores as for large families in <xref ref-type="fig" rid="fig1">Figure 1</xref>, as well as of the Hamming distance to the closest natural sequence, which is not itself in the case of natural sequences (‘Distance’). For the three smallest families (left panel; fewer than 40 sequences), we also show the score of each individual sequence as a swarm plot. Note that while we employ the same sampling temperatures <inline-formula><mml:math id="inf11"><mml:mi>T</mml:mi></mml:math></inline-formula> as in <xref ref-type="fig" rid="fig1">Figure 1</xref> for Boltzmann machine DCA (bmDCA), here, we use regularization strength <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> throughout, due to MSA shallowness (see ‘Sampling sequences from Potts models’).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig3-v2.tif"/></fig></sec><sec id="s2-3"><title>Higher-order statistics are better reproduced by MSA Transformer, while lower-order statistics are better reproduced by bmDCA</title><p>How well do synthetic MSAs generated by our method based on MSA Transformer, and by bmDCA, reproduce the statistics of amino-acid usage observed in natural MSAs? To address this question, we consider the r20 score (<xref ref-type="bibr" rid="bib37">Haldane et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">McGee et al., 2021</xref>), which quantifies the statistical similarity of two datasets at various orders (see ‘Analyzing the statistics of MSAs’). We compute it between each of our synthetic MSAs and the corresponding natural one, for the 14 large protein families in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>. We also present as reference an assumption-free null model, namely the r20 score between two subsets of each natural MSA. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows that bmDCA with default parameters is most often the best method at reproducing lower-order statistics, while MSA Transformer is the best at reproducing higher-order statistics, in all families considered. bmDCA at lower temperature performs more poorly at reproducing the statistics of natural MSAs than other methods, because low-temperature biases the sampling (bmDCA models are effectively learned at temperature <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Similarity of statistics between synthetic and natural multiple sequence alignments (MSAs).</title><p>To compare the statistics of synthetic and natural MSAs at various orders, we compute r20 scores (<xref ref-type="bibr" rid="bib37">Haldane et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">McGee et al., 2021</xref>), and plot them versus the number of different MSA columns that are considered (see ‘Analyzing the statistics of MSAs’ for details). All families in <xref ref-type="fig" rid="fig5">Figure 5</xref> are considered. For each of them, the reference MSA comprises either half of the natural MSA (with sequences selected uniformly at random), or 30,000 sequences from it if the natural MSA depth is larger than 60,000. The null model compares the other half of the natural MSA to this reference MSA. It yields an estimate of the expected r20 scores due only to finite-size effects in a model-free, purely data-driven way.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Ability of generated sequences to reproduce one-, two-, and three-body statistics.</title><p>In all panels, each marker represents a family in 5. For each statistical or information measure (rows), different scores (columns) comparing generated and natural sequences are the coordinates of these points. All scores are such that being close to 0 is better, and their value for multiple sequence alignments (MSAs) generated by Boltzmann machine DCA (bmDCA) with default parameters is shown versus that for MSAs generated by MSA Transformer. The statistical or information measures considered in each row are defined in ‘Analyzing the statistics of MSAs’ – from the top: one-body frequency, two- and three-body connected correlations, entropy, mutual information, and co-information. For each of them, we consider its values over all MSA columns (or pairs or triplets of columns), and all amino acids if appropriate, for both natural and synthetic MSAs. To obtain the vertical and horizontal coordinates (respectively) of the markers in each panel, we compare these values for each natural MSA with the values from the corresponding synthetic MSAs generated by bmDCA with default parameters or by our method based on MSA Transformer (respectively). We use four different scores for this comparison, and devote each column of the figure to one of these scores – from the left: <inline-formula><mml:math id="inf14"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf15"><mml:mi>ρ</mml:mi></mml:math></inline-formula> denotes the Pearson correlation; <inline-formula><mml:math id="inf16"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mtext>Slope</mml:mtext><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> where ‘Slope’ means the slope of best linear fit (see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for illustrations of these first two quantities in the case of two- and three-body connected correlations for families PF00072 and PF00153); the Jensen–Shannon divergence between the distributions of values; the Wasserstein distance between these distributions. For each statistical or information measure (row) and each score (column), and for each family in 5, we have one value of the score comparing the natural and bmDCA-generated MSAs and another one comparing the natural and MSA-Transformer–generated MSAs. We plot the former value versus the latter, yielding one marker per protein family in each plot. Thus, each plot compares the ability of bmDCA and MSA Transformer to reproduce the statistics of the natural data. Blue markers (above the diagonal) mean that the scores for MSA-Transformer–generated MSAs are better, while green markers (below the diagonal) mean the opposite.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Two- and three-body connected correlations estimated from generated multiple sequence alignments (MSAs) versus the natural one, for family PF00072.</title><p>Relationships between connected correlations estimated from the MSA generated by MSA Transformer or Boltzmann machine DCA (bmDCA), and those estimated from the natural MSA, are shown as binned scatter plots both for two-body (top row) and three-body (bottom row) statistics. We include a null model (third column) obtained by splitting the natural MSA in half and comparing the statistics of one half with those of the other. It yields an estimate of the expected dispersion in these plots due only to finite-size effects in a model-free, purely data-driven way. Pearson correlation coefficients, <inline-formula><mml:math id="inf17"><mml:mi>ρ</mml:mi></mml:math></inline-formula> and slopes of lines of best fit, are reported in each case. For the comparisons involving MSA Transformer, we used <xref ref-type="disp-formula" rid="equ4 equ5">Equations 4 and 5</xref> to estimate correlations. On the other hand, since bmDCA is trained to reproduce frequencies rescaled with phylogenetic weights (<italic>w</italic><sub><italic>i</italic></sub> in 8), for the comparisons involving bmDCA we rescaled the natural frequencies before using <xref ref-type="disp-formula" rid="equ4 equ5">Equations 4 and 5</xref> to estimate correlations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Two- and three-body connected correlations estimated from generated multiple sequence alignments (MSAs) versus the natural one, for family PF00153.</title><p>Same as <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, but for family PF00153.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig4-figsupp3-v2.tif"/></fig></fig-group><p>To have a more detailed insight into lower-order correlations, we estimate frequencies and information theory measures, at the one-, two-, and three-body level, from our natural and synthetic MSAs, and compare them (see ‘Analyzing the statistics of MSAs’). <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows that one- and two-body statistics are generally better reproduced by bmDCA with default parameters than by MSA Transformer, while results are more mixed for three-body statistics. <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> show a comparison of second- and third-order connected correlations for PF00072 and PF00153. For PF00072, bmDCA reproduces better the second- but also third-order connected correlations of the natural data than MSA Transformer, while for PF00153, MSA Transformer reproduces the third-order connected correlations better than bmDCA, consistently with <xref ref-type="fig" rid="fig4">Figure 4</xref>. Potts models are pairwise maximum entropy models constrained to match the one- and two-body frequencies from natural MSAs. Thus, bmDCA is trained to reproduce these frequencies, and achieves these objectives quite well, although the comparison to the null model in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> hints that further improvements remain possible, see <xref ref-type="bibr" rid="bib59">Meshulam et al., 2021</xref>. MSA Transformer has entirely different training objectives, but, interestingly, it performs comparably at reproducing three-body statistics and is better at reproducing even higher-order statistics than bmDCA.</p></sec><sec id="s2-4"><title>MSA Transformer captures well the distribution of sequences in sequence space</title><p>How are synthetic MSAs generated by MSA Transformer and bmDCA impacted by the heterogeneous repartition of natural sequences in sequence space? While natural protein sequences in a family have evolved from a common ancestor along a phylogeny, synthetic sequences do not have a real evolutionary history. However, as bmDCA and MSA Transformer are trained on natural data, they can capture phylogenetic correlations (<xref ref-type="bibr" rid="bib48">Lupo et al., 2022</xref>). Besides, inferred Potts models are known to be impacted by phylogenetic correlations (<xref ref-type="bibr" rid="bib86">Weigt et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Marks et al., 2011</xref>; <xref ref-type="bibr" rid="bib65">Qin and Colwell, 2018</xref>; <xref ref-type="bibr" rid="bib84">Vorberg et al., 2018</xref>; <xref ref-type="bibr" rid="bib71">Rodriguez Horta et al., 2019</xref>; <xref ref-type="bibr" rid="bib72">Rodriguez Horta and Weigt, 2021</xref>; <xref ref-type="bibr" rid="bib54">Marmier et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Colavin et al., 2022</xref>; <xref ref-type="bibr" rid="bib33">Gerardos et al., 2022</xref>; <xref ref-type="bibr" rid="bib23">Dietler et al., 2023</xref>).</p><p>To analyze the overall distribution of MSA sequences in sequence space, we first perform a principal component (PC) analysis of one-hot encoded MSAs, and focus on the top two PCs (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>) (see ‘Characterizing the distribution of sequences in MSAs’). <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the distribution of sequences in the space spanned by these top two PCs, for natural and synthetic MSAs, in the cases of PF00072 and PF00153. We observe that MSA Transformer is able to generate sequences with a distribution in sequence space that is very similar to that of the natural MSA. Conversely, bmDCA captures the overall shape of this distribution, but appears to smooth it compared to the natural data with default parameters and to restrict to sparse regions of the sequence space at low temperature, consistently with our previous results. These observations are general across all the deep MSAs we considered (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Note that a limitation of this analysis is that the top two PCs explain a small fraction of the variance in all cases (see <xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Distribution of sequences in sequence space, for families PF00072 and PF00153.</title><p>We show the distribution of one-hot encoded natural and synthetic sequences projected in the subspace of the first two principal components of the natural multiple sequence alignment (MSA). The same axis limits are used within one family, except for Boltzmann machine DCA (bmDCA) (10<sup>−3</sup>, 0.33) in the case of PF00072. Note that the fraction of the total variance explained by the first two principal components of each MSA is less than 4% for all families and all generation methods.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Distribution of sequences in sequence space for all large protein families in our dataset (part 1).</title><p>We show the distribution of one-hot encoded natural and synthetic sequences projected in the subspace of the first two principal components of the natural multiple sequence alignment (MSA), as in 5 for families PF00072 and PF00153, but also including Boltzmann machine DCA (bmDCA) at <inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>T</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:msup><mml:mn mathvariant="normal">10</mml:mn><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mn mathvariant="normal">3</mml:mn></mml:mrow></mml:msup><mml:mo mathvariant="normal">,</mml:mo><mml:mn mathvariant="normal">0.66</mml:mn><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Distribution of sequences in sequence space for all large protein families in our dataset (part 2).</title><p>We show the distribution of one-hot encoded natural and synthetic sequences projected in the subspace of the first two principal components of the natural multiple sequence alignment (MSA), as in 5 for families PF00072 and PF00153, but also including Boltzmann machine DCA (bmDCA) at <inline-formula><mml:math id="inf19"><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>T</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:msup><mml:mn mathvariant="normal">10</mml:mn><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mn mathvariant="normal">3</mml:mn></mml:mrow></mml:msup><mml:mo mathvariant="normal">,</mml:mo><mml:mn mathvariant="normal">0.66</mml:mn><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Neighbors of natural and synthetic sequences, for families PF00072 and PF00153.</title><p>We show the distribution of the number of neighbors of sequences in the natural multiple sequence alignment (MSA), and the distribution of the number of neighbors of the closest natural sequence to each of our generated sequences. Given a sequence in a natural MSA, its number of neighbors is the number of natural sequences that are within a (normalized) Hamming distance <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> from it. The moving average of the results is shown, using a window representing 5% of the total number of points.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig5-figsupp3-v2.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Comparing phylogenies inferred from natural and generated multiple sequence alignments (MSAs) for families PF00072 and PF00153.</title><p>We show the averaged spectra of modified graph Laplacian (MGL) matrices computed, for each tree inferred from an MSA, by using the leaves of multiple sub-trees made of 500 randomly sampled sequences from the MSA of interest. Specifically, in each case, we perform an average over 200 different sub-trees of the histograms of counts of the eigenvalues (see ‘Characterizing the distribution of sequences in MSAs’). We compare MSAs generated using either MSA Transformer (left) or Boltzmann machine DCA (bmDCA) (center and right) to the natural MSA.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig5-figsupp4-v2.tif"/></fig></fig-group><p>Next, to assess whether generated sequences most resemble natural ones that are well represented in their family or, rather, rare ones, we consider the closest natural sequence to each synthetic sequence, and count the neighbors of this natural sequence in the natural MSA (see ‘Characterizing the distribution of sequences in MSAs’). <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref> compares the distribution of these numbers of neighbors for natural sequences and for the closest natural sequences to generated sequences, in the cases of PF00072 and PF00153. It shows that bmDCA generates sequences similar to natural sequences with fewer neighbors than typical in the natural data. Conversely, MSA Transformer generates sequences whose closest natural sequences have a distribution of number of neighbors similar to that of the natural MSA. This suggests that our generation method based on MSA Transformer tends to sample from denser regions of the sequence space than bmDCA, while not reproducing natural sequences (see also <xref ref-type="fig" rid="fig2">Figure 2</xref> and ‘Choosing parameters in the iterative masking procedure’).</p><p>Finally, to analyze in more detail the apparent relatedness of generated sequences, and compare it to real phylogenetic relationships in natural sequences, we infer phylogenetic trees from each synthetic and natural MSA, and analyze the eigenvalue spectrum of their modified graph Laplacian (MGL) to compare them (<xref ref-type="bibr" rid="bib46">Lewitus and Morlon, 2016</xref>) (see ‘Characterizing the distribution of sequences in MSAs’). <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref> compares the density of these eigenvalue spectra for natural and synthetic MSAs regarding families PF00072 and PF00153. The skewness and the position of such distributions are indicators of the topology of the tree. In particular, distributions with negative skewness (right unbalanced) or which are shifted to the right, correspond to ‘tippy’ trees, while the opposite case corresponds to ‘stemmy’ trees (<xref ref-type="bibr" rid="bib46">Lewitus and Morlon, 2016</xref>), which feature an accumulation of recent speciation events (short leaves length) (<xref ref-type="bibr" rid="bib61">Molina‐Venegas, 2021</xref>). In this light, <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref> shows that both MSA Transformer and low-temperature bmDCA generate sequences with an apparent phylogeny that is more stemmy than the natural one, while bmDCA with default parameters yields a slightly more tippy tree. This is consistent with our observations regarding sequence diversity, which is larger than in natural data for bmDCA with default parameters, slightly smaller than in natural data using MSA Transformer and much lower using low-temperature bmDCA (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s2-5"><title>Comparison with published experimental datasets</title><p>How do the sequences generated by our method based on MSA Transformer compare to published protein design experimental datasets? Recently, sequences sampled from a bmDCA Potts model of the chorismate mutase protein family were experimentally demonstrated to be functional (<xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>). In <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, we show plots analogous to those in <xref ref-type="fig" rid="fig2">Figure 2</xref>, plus additional ones for our two structural scores (pLDDT and RMSD), in the case of chorismate mutase. This allows a detailed comparison between the sequences we generate using MSA Transformer and the sequences designed in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref> using bmDCA with a combination of different temperatures and regularization strengths. We find that our method based on MSA Transformer produces sequences that score as well as artificial sequences which have been tested experimentally. Besides, we obtained these results without fine-tuning the parameters of our generative procedure to this family, while several specific combinations of parameters were used in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>.</p><p>To further compare our generated sequences to those tested experimentally in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>, we consider relative enrichment, which is the experimental score used in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref> to assess the function of chorismate mutase enzymes. This score was measured in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref> for all sequences in the natural MSA and for sequences generated with bmDCA. We estimate the expected relative enrichment of our generated sequences as the relative enrichment of the closest natural sequence. To test our estimation procedure, we estimate the relative enrichments of the bmDCA-generated sequences from <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>, and we compare them to the experimentally measured values. We focus on the top third of sequences in terms of pLDDT scores, as it was shown in <xref ref-type="bibr" rid="bib51">Malbranke et al., 2021</xref> that good structural scores help to select functional sequences. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows that in this ensemble, sequences with a high (resp. low) estimated score have a high (resp. low) experimental score too. Next, we compare the distributions of estimated relative enrichment for sequences generated using our method based on MSA Transformer and for the bmDCA-generated sequences from <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows that they are quite similar to each other. This holds both when focusing on the top third of sequences in terms of pLDDT scores for each generation method, and when considering all generated sequences. Furthermore, in the high-pLDDT case, these distributions are quite similar to the distribution of measured relative enrichment for bmDCA-generated sequences. Importantly, a similar fraction of MSA-Transformer–generated sequences and of bmDCA-generated sequences have a large estimated relative enrichment. This suggests that our method based on MSA Transformer should be able to generate functional sequences.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Comparison of our generated sequences to those experimentally tested in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>, for the chorismate mutase family.</title><p>Left: The estimated relative enrichment (r.e.) scores of the Boltzmann machine DCA (bmDCA)-generated sequences that are in the top 33% in terms of predicted local-distance difference test (pLDDT) scores are plotted versus their experimentally measured counterparts from <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>. We estimate the expected r.e. of these generated sequences as the r.e. of the closest natural sequence measured in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>. We observe that high estimated r.e. is associated with high measured r.e., as 71% of sequences with estimated r.e. &gt; 0.4 (green) also have measured r.e. &gt; 0.4. Note that in the top marginals (showing the measured r.e. for bmDCA-generated sequences), the green and yellow histograms are stacked on top of each other. Thus, the stacked histogram shows the distribution of all measured r.e. values for bmDCA-generated sequences that are in the top 33% in terms of pLDDT scores. Top right: Overlaid histograms of estimated r.e. are shown for our MSA-Transformer–generated sequences and for the bmDCA-generated ones from <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>, restricting in both cases to the sequences with top 33% pLDDT scores. Bottom right: Same as top right, but considering all generated sequences.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Homology, coevolution, and structural scores versus distance to the natural multiple sequence alignment (MSA), for the chorismate mutase family.</title><p>We assess the performance of our generative method in the case of the chorismate mutase family, by comparing our generated sequences (‘MSA Tr.’) to natural ones, and to the sequences generated in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref> using Boltzmann machine DCA (bmDCA) at various values of the sampling temperature and of the regularization strength. Our synthetic sequences were generated using the iterative masking procedure based on MSA Transformer, starting from the natural alignment used in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>. Our sequences score similar to natural sequences and to the bmDCA-generated sequences from <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>, which were experimentally validated there.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Deep mutational scanning (DMS) scores for families PF00595 and PF13354.</title><p>In DMS experiments, the fitness effects of all possible one-point mutations from a reference natural sequence are measured. To assign a DMS score to natural and generated sequences, we align each of them to the reference sequence, and sum the experimentally measured fitness effects of the relevant amino acids at each position. Higher values of the DMS score are better, as they mean higher fitness. We show normalized histograms of the DMS scores for sequences in the natural and generated multiple sequence alignments (MSAs) of protein families PF00595 and PF13354, based on the DMS experiments in <xref ref-type="bibr" rid="bib57">McLaughlin et al., 2012</xref>; <xref ref-type="bibr" rid="bib78">Stiffler et al., 2015</xref>, respectively. For generated sequences, we restrict to those whose Hamming distance to their closest natural neighbor is larger than <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>δ</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">0.2</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig6-figsupp2-v2.tif"/></fig></fig-group><p>While the data from <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref> is particularly well suited to retrospectively evaluate our sequence generation method, we also propose a comparison of the distributions of scores based on experimental deep mutational scans (DMS) for protein families PF00595 (<xref ref-type="bibr" rid="bib57">McLaughlin et al., 2012</xref>) and PF13354 (<xref ref-type="bibr" rid="bib78">Stiffler et al., 2015</xref>). We compute these DMS scores for each natural and synthetic sequence, by summing the experimentally measured effects of the relevant single-point mutations with respect to the reference sequence of the experimental studies. <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> shows the distribution of the DMS scores of natural and generated sequences for these two families. Our sequence generation method based on MSA Transformer better reproduces the DMS score distribution of natural sequences than bmDCA, and generates sequences with better average DMS scores. Despite the potential limitations of our DMS scores, for example their additivity, these results corroborate our other findings and provide further encouragement for our sequence generation method based on MSA Transformer.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we proposed an iterative masking procedure which directly exploits the MLM objective of protein language models to generate sequences using the MSA-based neural language model MSA Transformer. We found that these sequences score as well as natural ones on three very different aspects, namely homology, coevolution, and structure. For large protein families, our synthetic sequences have homology and structure-based scores at least as good as bmDCA-generated sequences, and have similar properties to experimentally validated ones. Moreover, our generation method based on MSA Transformer is less limited by shallow MSAs than bmDCA, and is thus particularly promising for small protein families. Besides, MSA-Transformer–generated sequences better reproduce the higher-order statistics and the distribution of sequences in sequence space of natural data than bmDCA-generated ones. Conversely, bmDCA, with default parameters, better reproduces first- and second-order statistics, consistently with its training objective.</p><p>Our results are highly promising for sequence generation by MSA-based protein language models, and we hope that they will motivate further studies, especially experimental tests. They also show that protein deep learning models based on the MLM objective have great generative potential, despite not being obvious generative models. More generally, our results reinforce the new promising ‘coevolution-driven’ protein design approach of learning from sequences of evolutionarily related proteins the constraints associated to protein structure and function. This concept differs from structure- and physics-based de novo design (<xref ref-type="bibr" rid="bib21">Dahiyat and Mayo, 1997</xref>; <xref ref-type="bibr" rid="bib44">Kuhlman et al., 2003</xref>; <xref ref-type="bibr" rid="bib47">Liang et al., 2009</xref>), and from the new possibility to use supervised deep learning models able to accurately predict protein structures (<xref ref-type="bibr" rid="bib43">Jumper et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Baek et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Chowdhury et al., 2023</xref>) for structure-driven sequence generation (<xref ref-type="bibr" rid="bib2">Anishchenko et al., 2021</xref>). One can view the coevolution-driven approach as intermediate between structure-based approaches and directed evolution ones (<xref ref-type="bibr" rid="bib3">Arnold, 2018</xref>). The coevolution-driven approach was recently experimentally validated in the case of bmDCA Potts models, which capture pairwise coevolution patterns in MSAs (<xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>), and for variational autoencoders (<xref ref-type="bibr" rid="bib38">Hawkins-Hooker et al., 2021a</xref>; <xref ref-type="bibr" rid="bib55">McGee et al., 2021</xref>). Protein language models trained on MSAs provide state-of-the-art unsupervised contact prediction and are able to capture coevolutionary patterns in their tied row attentions (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>), and capture phylogenetic relationships in column attentions (<xref ref-type="bibr" rid="bib48">Lupo et al., 2022</xref>). This makes them ideal candidates to generate new protein sequences from given families. However, contrary to Potts models and variational autoencoders (<xref ref-type="bibr" rid="bib55">McGee et al., 2021</xref>), they do not allow direct sampling from a probability distribution over sequences (<xref ref-type="bibr" rid="bib34">Goyal et al., 2021</xref>). Here, we demonstrated the power of a simple generation method directly based on the MLM objective used for the training of MSA-based protein language models. It differs from using a decoder, which, though designed to perform autoregressive generation of amino acids to form a new sequence, requires training a full encoder–decoder model and learning a parametric function mapping an MSA to a distribution over its sequences (<xref ref-type="bibr" rid="bib39">Hawkins-Hooker et al., 2021b</xref>). We instead directly employed the representation of protein families captured by the self-supervised model MSA Transformer to generate sequences. More sophisticated sampling methods could be considered along this line (<xref ref-type="bibr" rid="bib34">Goyal et al., 2021</xref>), but our minimal approach already gives very promising results.</p><p>We have focused on a large protein language model and compared it to the simplest model capturing coevolution, namely the Potts model, but we note that interpretable models of intermediate complexity such as restricted Boltzmann machines (<xref ref-type="bibr" rid="bib81">Tubiana et al., 2019</xref>) could also be explored for coevolution-driven protein design. All these methods rely on MSAs; this is very useful to capture coevolution, but also means that one has to rely on potentially imperfect alignments. Thus, starting from alignment-free methods (<xref ref-type="bibr" rid="bib11">Bileschi et al., 2022</xref>; <xref ref-type="bibr" rid="bib76">Shin et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Madani et al., 2021</xref>) also constitutes a promising direction.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Using MSA Transformer to generate sequences via an iterative masking procedure</title><sec id="s4-1-1"><title>Iterative masking procedure</title><p>In order to generate new sequences using MSA Transformer, we directly leverage the model’s ability to assign, to arbitrary masked residue positions, a probability for each of the possible amino-acid tokens, given by the softmax of the model’s output logits (<xref ref-type="bibr" rid="bib85">Wang and Cho, 2019</xref>; <xref ref-type="bibr" rid="bib34">Goyal et al., 2021</xref>; <xref ref-type="bibr" rid="bib68">Rao et al., 2021b</xref>). Indeed, in its pre-training, MSA Transformer applies the MLM objective to a training set of 26 million MSAs (<xref ref-type="bibr" rid="bib68">Rao et al., 2021b</xref>). For this, it minimizes a pseudolikelihood loss, which reads, for an MSA <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a version <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in which some amino acids (those in a ‘mask’) are masked:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mtext>MLM</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:mspace width="thinmathspace"/><mml:mtext>mask</mml:mtext></mml:mrow></mml:munder><mml:mrow><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the amino acid at the <inline-formula><mml:math id="inf26"><mml:mi>i</mml:mi></mml:math></inline-formula>th residue position in the <inline-formula><mml:math id="inf27"><mml:mi>m</mml:mi></mml:math></inline-formula>th sequence of <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf29"><mml:mi>θ</mml:mi></mml:math></inline-formula> denotes all model parameters. For each position <inline-formula><mml:math id="inf30"><mml:mi>i</mml:mi></mml:math></inline-formula> in each sequence <inline-formula><mml:math id="inf31"><mml:mi>m</mml:mi></mml:math></inline-formula>, the model outputs one value (‘logit’) per amino-acid/gap symbol, and softmax-normalizing all values from this location in the MSA yields the conditional probabilities <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, which are then summed over the subset of masked MSA locations.</p><p>We propose an iterative masking procedure (see <xref ref-type="fig" rid="fig7">Figure 7</xref>) which, given an arbitrary MSA <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> of natural sequences, proceeds as follows:</p><list list-type="order"><list-item><p>If necessary, subsample <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to obtain an input MSA <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for MSA Transformer. The depth of <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is chosen given the memory footprint of MSA Transformer. In practice, we use input MSAs containing 600 sequences picked uniformly at random from our natural MSA. (During training, the authors of <xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref> kept <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mn>14</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf38"><mml:mi>L</mml:mi></mml:math></inline-formula> is sequence length and <inline-formula><mml:math id="inf39"><mml:mi>M</mml:mi></mml:math></inline-formula> is MSA depth. However, we found that during inference we can use 2<sup>17</sup> tokens on an Nvidia V100 32GB GPU.) Note that, for large protein families, multiple 600-sequence MSAs obtained using the procedure presented here are then combined into a single MSA of the same depth as the natural one (see below).</p></list-item><list-item><p>Randomly mask each residue of <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> with a masking probability <inline-formula><mml:math id="inf41"><mml:mi>p</mml:mi></mml:math></inline-formula> , otherwise leave it unchanged. In practice, we choose <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> (see ‘Choosing parameters in the iterative masking procedure’).</p></list-item><list-item><p>Feed the masked MSA to the model, and fill each masked entry with the token with highest probability (obtained from the model’s output logits).</p></list-item><list-item><p>Repeat Steps 2–3 a number of times. In practice, we stop the algorithm after <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> iterations.</p></list-item></list><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Iterative masking procedure to generate sequences using MSA Transformer.</title><p>Here, the red hashtag (#) stands for a masked amino acid, while blue uppercase letters stand for predicted amino acids at the masked positions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Evolution of mean scores during the iterative masking procedure, for family PF00153.</title><p>Average scores of the generated sequences are reported for different iteration numbers and masking probabilities. The scores employed are: (<bold>A</bold>) Hamming distances to the closest natural sequence, (<bold>B</bold>) Mutual information between synthetic and natural columns of the MSAs, (<bold>C</bold>) HMMER scores – see ‘Scoring individual sequences’, (<bold>D</bold>) Statistical energy scores (negative DCA statistical energies, shifted by their mean value for natural sequences – see ‘Scoring individual sequences’), (<bold>E</bold>) Pairwise Hamming distances between the generated sequences, and (<bold>F</bold>) Mean square deviations (MSD) of the predicted contact maps at each iteration from the initial one (zero iterations). The synthetic MSAs, comprising 5000 sequences, were generated with MSA Transformer starting from natural sequences of family PF00153.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Evolution of mean scores during the iterative masking procedure, for family PF00096.</title><p>Same as in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, but for PF00096. This family has the shortest length among those considered here, namely <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>L</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">23</mml:mn></mml:mrow></mml:math></inline-formula> (see <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig7-figsupp2-v2.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Evolution of mean scores during the iterative masking procedure, for family PF13354.</title><p>Same as in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, but for PF13354. This family has the largest length among those considered here, namely <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>L</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">198</mml:mn></mml:mrow></mml:math></inline-formula> (see <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig7-figsupp3-v2.tif"/></fig><fig id="fig7s4" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 4.</label><caption><title>Evolution of inferred contact maps during the iterative masking procedure, for family PF00153.</title><p>Contact maps obtained from MSA Transformer for different iteration numbers and masking probabilities are reported. Probabilities of contacts are computed using the logistic regression on the output attention matrices from <xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>. The input of the model consists of 100 different sequences chosen uniformly at random from the synthetic MSA generated at each iteration of our iterative masking procedure. These contact maps were employed to compute the mean square deviations shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1F</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79854-fig7-figsupp4-v2.tif"/></fig></fig-group><p>As natural MSAs, we use Pfam full MSAs for 14 protein families, described in ‘Datasets’. For each natural MSA <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we repeat the procedure above multiple times, sampling sequences each time from <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> without replacement to obtain a different input MSA <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in Step 1, until all the sequences in <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are used. Note that sequences remain aligned at all times during the procedure. Combining the MSAs resulting from all these batches then yields a synthetic MSA with the same depth as the natural one, which ensures that the statistical properties of the synthetic MSA are subject to the same magnitude of finite-size errors as those of the natural MSA.</p></sec><sec id="s4-1-2"><title>Choosing parameters in the iterative masking procedure</title><p><xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> illustrates, in the case of Pfam family PF00153, for different values of the masking probability <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, how different properties of the generated MSAs evolve with the number <inline-formula><mml:math id="inf51"><mml:mi>I</mml:mi></mml:math></inline-formula> of iterations in the iterative masking procedure. For <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, we observe a gradual divergence from the initial natural sequences (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A, B</xref>) and a simultaneous increase of scores (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1C, D</xref>, see ‘Scoring individual sequences’ for definitions) and decrease of MSA diversity (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref>), and then a saturation of these various measures, as <inline-formula><mml:math id="inf53"><mml:mi>I</mml:mi></mml:math></inline-formula> increases. Our choice <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> is motivated by the fact that plateaus are reached at this point. However, the final values of all scores depend on <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4</xref> shows the contact maps inferred by MSA Transformer (using the logistic regression on tied row attentions trained in <xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>) from generated sequences, for various values of <inline-formula><mml:math id="inf56"><mml:mi>I</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, in the case of family PF00153. We observe that the contact map characteristic of the protein family of interest gets gradually lost as <inline-formula><mml:math id="inf58"><mml:mi>I</mml:mi></mml:math></inline-formula> is increased for larger values of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1F</xref> and <xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4</xref>). These issues when <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is increased are understandable, given that the pseudolikelihood loss used for the MLM objective in MSA Transformer ignores dependencies between masked entries. We note that despite this, larger values of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> yield overall better average HMMER scores (<xref ref-type="bibr" rid="bib26">Eddy, 1998</xref>) and statistical energy scores (for <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Our choice of <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is motivated by the fact that this value is close to that employed in the training of the model (<inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.12</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>), and that it better preserves contact maps. The product <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> gives the average number of times that each amino acid of the MSA is changed during the generation process. With our choices, each amino acid is masked 20 times on average.</p><p>The behaviors observed in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> for PF00153 are generic across the protein families we studied, as can be seen in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> and <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>, which show the same data as in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> for Pfam families PF00096 and PF13354 (which have different sequence lengths). This demonstrates that our sequence generation method is robust. In particular, as the parameters <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> yield satisfactory convergence of MSA properties and preservation of contact maps in all cases, we used these parameters throughout, without any family-specific fine-tuning.</p><p>The sequences thus generated by our method do not coincide with natural ones. The fraction of MSA-Transformer–generated sequences which are identical to sequences in the input natural MSAs is below <inline-formula><mml:math id="inf68"><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for all large families considered, except three families with low diversity and/or very short sequence length (PF00096, PF00397, and PF00595).</p></sec><sec id="s4-1-3"><title>Variants of the iterative masking procedure</title><p>In our algorithm, we mask tokens randomly throughout the input MSA. We also explored an alternative procedure where masking is restricted to the first sequence of the input MSA. Thus, all other sequences act as a context for the first sequence which is gradually modified. This can be done either with a fixed context, or by sampling different sequences from the natural MSA at each iteration to form a variable context. Note that the procedure with fixed context is reminiscent of the non-iterative one used in <xref ref-type="bibr" rid="bib58">Meier et al., 2021</xref> to compute DMS scores from MSA Transformer. For the same masking probability <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> as in our standard procedure (note that fewer iterations are needed for convergence, in practice <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> suffices), the alternative procedure with fixed context yields sequences that are overall slightly less different from natural ones than the standard iterative masking procedure, while the opposite holds with variable context. Besides, both alternative procedures yield sequences with better HMMER scores, but worse statistical energy scores, than natural ones – see <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>. Finally, the two- and three-body statistics (defined in ‘Analyzing the statistics of MSAs’) of the natural MSA are less well reproduced using these alternative procedures than the standard one – see <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>. We also note that these variants are computationally more demanding. In this context, we decided to focus on the standard iterative masking procedure.</p><p>There are also different ways of selecting the token to fill each masked position. We have chosen a greedy sampling method where the token with highest probability is selected. We also explored an alternative method where the new token to fill the masked position is chosen by sampling the probability distribution given by the softmax of the logits, see <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. This method allows to introduce a sampling temperature <inline-formula><mml:math id="inf71"><mml:mi>T</mml:mi></mml:math></inline-formula> into the softmax operation and compute the probability as <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf73"><mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:math></inline-formula> is the logit vector. Note that the greedy method that we employ corresponds to sampling at <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. We found that MSAs generated with higher values of <inline-formula><mml:math id="inf75"><mml:mi>T</mml:mi></mml:math></inline-formula> are farther from the corresponding natural MSAs, showing that increasing this sampling temperature promotes originality. However, they are of lower quality according to our HMMER and statistical energy scores, and reproduce the statistics of the natural data less well. These results, summarized in <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>, motivated us to mainly consider greedy sampling.</p><p>Finally, in our iterative masking procedure, we subsample the initial natural MSAs uniformly at random. We also tried diversity maximizing sampling (<xref ref-type="bibr" rid="bib67">Rao et al., 2021a</xref>), but we found that random sampling gives slightly better results.</p></sec></sec><sec id="s4-2"><title>Sampling sequences from Potts models</title><p>To sample independent equilibrium sequences from Potts models, we used the strategy described in <xref ref-type="bibr" rid="bib48">Lupo et al., 2022</xref>. Specifically, we fitted Potts models on each of our natural MSAs using bmDCA (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>) (<ext-link ext-link-type="uri" xlink:href="https://github.com/ranganathanlab/bmDCA">https://github.com/ranganathanlab/bmDCA</ext-link>; <xref ref-type="bibr" rid="bib32">Figliuzzi and Barrat-Charlaix, 2020</xref>). Using bmDCA is known to yield Potts models with good generative power (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>; <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>).</p><p>Consider a sequence of <inline-formula><mml:math id="inf76"><mml:mi>L</mml:mi></mml:math></inline-formula> amino-acid sites. We denote by <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the state of site <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:math></inline-formula> is the number of possible states, namely the 20 natural amino acids and the alignment gap. The Potts model Hamiltonian of a sequence <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> reads (<xref ref-type="bibr" rid="bib86">Weigt et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Cocco et al., 2018</xref>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For each MSA <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>, we inferred parameters <inline-formula><mml:math id="inf82"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by bmDCA (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>; <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>). The Potts model probability distribution is then given by the Boltzmann distribution associated to the Hamiltonian <inline-formula><mml:math id="inf84"><mml:mi>H</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>Z</mml:mi></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf85"><mml:mi>Z</mml:mi></mml:math></inline-formula> is a constant ensuring normalization and <inline-formula><mml:math id="inf86"><mml:mi>T</mml:mi></mml:math></inline-formula> is a parameter whose default value is 1. To generate a synthetic MSA from <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we performed equilibrium MCMC sampling from the Potts model with Hamiltonian <inline-formula><mml:math id="inf88"><mml:mi>H</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. Specifically, we used the implementation in <xref ref-type="bibr" rid="bib48">Lupo et al., 2022</xref> of the Metropolis–Hastings algorithm, in which each step is a proposed mutation at a single amino-acid site. We started from a set of <inline-formula><mml:math id="inf89"><mml:mi>M</mml:mi></mml:math></inline-formula> randomly and independently initialized sequences, where <inline-formula><mml:math id="inf90"><mml:mi>M</mml:mi></mml:math></inline-formula> is the depth of <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and made a total number <inline-formula><mml:math id="inf92"><mml:mi>N</mml:mi></mml:math></inline-formula> of Monte Carlo steps on each sequence. For each <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, suitable values for <inline-formula><mml:math id="inf94"><mml:mi>N</mml:mi></mml:math></inline-formula> are estimated by bmDCA during its training, to ensure that Metropolis–Hastings sampling reaches thermal equilibrium after <inline-formula><mml:math id="inf95"><mml:mi>N</mml:mi></mml:math></inline-formula> steps when starting from a randomly initialized sequence (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>). We thus used the value of <inline-formula><mml:math id="inf96"><mml:mi>N</mml:mi></mml:math></inline-formula> estimated by bmDCA at the end of training. This yielded, for each MSA in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>, a synthetic MSA of the same depth, composed of independent equilibrium sequences.</p><p>This procedure allows to tune the sampling temperature <inline-formula><mml:math id="inf97"><mml:mi>T</mml:mi></mml:math></inline-formula>, in a similar spirit as for MSA Transformer, cf. ‘Variants of the iterative masking procedure’. This amounts to tuning the selection strength. Recall that Potts models are inferred at <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, which is thus the default value. Using MCMC sampling as described above, we first generated synthetic MSAs at <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and using regularization strength <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. These correspond to the default parameters <inline-formula><mml:math id="inf101"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, matching those employed in <xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>, and allowing direct comparison with those results. Importantly, using sampling temperature <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> means that the distribution learnt from natural data is directly sampled. However, it was found in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref> that sequences generated at <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> have worse statistical energy scores than natural sequences, due at least in part to high regularization, and that this can be corrected by lower-temperature sampling. Therefore, for completeness, we also considered all parameter combinations <inline-formula><mml:math id="inf104"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> used in <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref> for PF00072. <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref> shows that decreasing sampling temperature strongly improves the mean statistical energy score, as it should, and somewhat improves HMMER scores and structural scores. However, this comes at the cost of decreasing MSA diversity and getting sequences substantially more similar to natural ones. It also strongly impairs the fitting of the one- and two-body statistics. The effect of changing regularization strength (at inference) appears to be more minor, but decreasing it allows to somewhat mitigate the loss of diversity associated to lowering temperature. In light of these results, and to make our comparison to bmDCA comprehensive, we used <inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0.33</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref>) in addition to <inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>) throughout our analysis of deep MSAs. In the case of shallow MSAs (3), we employed <inline-formula><mml:math id="inf107"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0.33</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf108"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0.33</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> because shallow MSAs require stronger regularization strengths.</p></sec><sec id="s4-3"><title>Scoring individual sequences</title><p>We use different scores to compare natural and generated sequences.</p><p>First, HMMER scores (<xref ref-type="bibr" rid="bib26">Eddy, 1998</xref>) are computed, for each sequence, from the Pfam profile Hidden Markov Models (HMM), employing the function <monospace>hmmsearch</monospace> from the HMMER Suite version 3.3.2 (<ext-link ext-link-type="uri" xlink:href="http://hmmer.org">http://hmmer.org</ext-link>). HMMER scores are homology scores, which are in particular used in Pfam to search sequence databases for sequence homologs and to construct full MSAs starting from curated seed MSAs. Higher HMMER scores indicate better matches to the Pfam HMM.</p><p>Second, DCA statistical energy scores are computed for each sequence using the Potts model Hamiltonian <inline-formula><mml:math id="inf109"><mml:mi>H</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> with the couplings and the fields inferred by bmDCA on the natural MSA of the family of interest (see ‘Sampling sequences from Potts models’). The statistical energy score is then defined as the opposite of the statistical energy, that is <inline-formula><mml:math id="inf110"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for a sequence <inline-formula><mml:math id="inf111"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula>, so that, here too, higher values mean better scores.</p><p>We also compute AlphaFold (<xref ref-type="bibr" rid="bib43">Jumper et al., 2021</xref>) structural prediction confidence scores, that is pLDDT values. Given the computational cost, for each natural or generated MSA, we evaluate pLDDT values for a subset of 200 randomly sampled sequences.</p><p>Finally, we compute the root-mean-square deviation (RMSD) between a reference experimental structure of the family of focus (see list in <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>) and the AlphaFold predicted structures, also for a subset of 200 randomly sampled sequences in each MSA.</p><p>Because AlphaFold takes MSAs as input, we compute these two structural scores using the whole natural MSA of the family of interest as context in all cases. In addition, for the protein family PF00072, we also used fully synthetic MSAs as input to AlphaFold. Structural scores are then very similar to those obtained using natural context (see <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>).</p></sec><sec id="s4-4"><title>Analyzing the statistics of MSAs</title><p>To compare the generated MSAs to the natural ones, we consider different statistical measures.</p><p>First, to analyze how faithfully the generated MSAs reproduce the statistics of the natural ones at various orders, we compute the r20 score (<xref ref-type="bibr" rid="bib37">Haldane et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">McGee et al., 2021</xref>). Specifically, to obtain <xref ref-type="fig" rid="fig4">Figure 4</xref>, we analyze the frequency of subsequences spanning 2–10 non-contiguous columns. In each of 1000 randomly sampled sets of columns for each subsequence length, we compute the frequency of the 20 most frequent words in natural and synthetic MSAs of the family considered, and evaluate the Pearson correlation between these top 20 frequencies in the MSA of focus and those in a reference MSA. We then average these Pearson correlation values over all sets of 1000 columns, yielding the r20 score.</p><p>To further inspect low-order statistics, in each MSA, we compute the one-body frequencies of occurrence of each amino acid at each site, the two-body frequencies of each pair of amino acids at each pair of sites, and the three-body frequencies associated to triplets. We denote them by <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf113"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf114"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf115"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf116"><mml:mi>j</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf117"><mml:mi>k</mml:mi></mml:math></inline-formula> denote sites, while <inline-formula><mml:math id="inf118"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf119"><mml:mi>y</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf120"><mml:mi>z</mml:mi></mml:math></inline-formula> represent amino acids (see ‘Sampling sequences from Potts models’). We then estimate the second- and third-order connected correlations as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We also compute the ‘plug-in’ estimates of the Shannon entropy of each site <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, and of the two- and three-body joint entropies <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, from the frequencies. They yield the plug-in estimates of the mutual information <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between two columns, and of the <italic>co-information</italic> <inline-formula><mml:math id="inf125"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between three columns:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded></mml:mrow></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Co-information is a measure of higher-order statistical dependencies (<xref ref-type="bibr" rid="bib56">McGill, 1954</xref>; <xref ref-type="bibr" rid="bib80">Timme et al., 2014</xref>; <xref ref-type="bibr" rid="bib66">Quax et al., 2017</xref>; <xref ref-type="bibr" rid="bib74">Rosas et al., 2019</xref>), which generalizes mutual information to triplets of random variables, vanishes for independent variables, and reflects the balance between redundancy and synergy in these triplets (<xref ref-type="bibr" rid="bib87">Williams and Beer, 2010</xref>; <xref ref-type="bibr" rid="bib73">Rosas et al., 2016</xref>). A systematic finite-size error occurs when estimating entropies using the plug-in estimate from frequencies measured in finite datasets (<xref ref-type="bibr" rid="bib10">Bialek, 2012</xref>), and it affects entropy-derived quantities such as mutual information and co-information. Here, we do not attempt to correct it. Rather, we only make comparisons between MSAs of the same length and depth, which are affected by the same finite-size errors.</p></sec><sec id="s4-5"><title>Characterizing the distribution of sequences in MSAs</title><p>Another way of studying the properties of generated MSAs is to analyze the distribution of their sequences in sequence space, and to compare it to that of natural sequences in the same family.</p><p>First, to assess whether generated sequences most resemble natural ones that are well represented in their family or, rather, rare ones, we consider for each synthetic sequence its closest natural sequence. We then count the number of neighbors of this natural sequence in the natural MSA, that is the number of natural sequences that have (normalized) Hamming distance below <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> with the sequence of interest. Note that the inverse of this number of neighbors gives the sequence weight <italic>w</italic><sub><italic>i</italic></sub> introduced in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>.</p><p>Second, to explore the distributions in sequence space of sequences within each MSA, and compare synthetic and natural MSAs, we associate to each sequence the concatenation of the one-hot encodings of each of its amino acids (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>). We perform a PC analysis of the matrix corresponding to the natural MSA in this representation. We can then represent natural and synthetic sequences as points projected in the space defined by the first two PCs of the natural MSA.</p><p>Third, to analyze in more detail the apparent relatedness of generated sequences, and compare it to real phylogenetic relationships in natural sequences, we infer phylogenetic trees from each MSA using <monospace>FastTree 2</monospace> (<xref ref-type="bibr" rid="bib64">Price et al., 2010</xref>). To quantitatively compare the topologies of these trees, which do not have the same leaves, we analyze the eigenvalue spectrum of their MGL (<xref ref-type="bibr" rid="bib46">Lewitus and Morlon, 2016</xref>). The MGL of a phylogenetic tree is defined as the difference between its degree matrix (a diagonal matrix whose <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>th diagonal entry is the sum of the branch lengths from node <inline-formula><mml:math id="inf128"><mml:mi>i</mml:mi></mml:math></inline-formula> to all other nodes in the tree) and the matrix of patristic distances (whose <inline-formula><mml:math id="inf129"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> th entry is the branch length between nodes <inline-formula><mml:math id="inf130"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf131"><mml:mi>j</mml:mi></mml:math></inline-formula>). Given the computational cost of running such an analysis on our deep MSAs, we use a bootstrap-aggregating strategy in the spirit of <xref ref-type="bibr" rid="bib19">Colijn and Plazzotta, 2018</xref>. Namely, for each MSA we compute 200 different trees, each one inferred from a different sub-MSA of 500 sequences, itself randomly sampled from the whole MSA. Then, for each of these trees, we compute the eigenvalue spectrum of the MGL. Next, we merge all these spectra together to obtain a single eigenvalue spectral density. Note that this method has the advantage of not depending on the details of the topology of one large inferred tree, which are known to be sensitive to the choice of phylogeny reconstruction algorithm.</p></sec><sec id="s4-6"><title>Datasets</title><p>To generate synthetic MSAs with MSA Transformer and bmDCA and compare them to their natural counterparts, we consider the deep Pfam ‘full’ alignments (<xref ref-type="bibr" rid="bib60">Mistry et al., 2021</xref>) associated to 14 different protein domains (<xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>). Each MSA is a matrix <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf133"><mml:mi>L</mml:mi></mml:math></inline-formula> columns, representing the different amino-acid sites, and <inline-formula><mml:math id="inf134"><mml:mi>M</mml:mi></mml:math></inline-formula> rows. Each row <inline-formula><mml:math id="inf135"><mml:mi>i</mml:mi></mml:math></inline-formula>, denoted by <inline-formula><mml:math id="inf136"><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, represents one sequence of the alignment. We refer to <inline-formula><mml:math id="inf137"><mml:mi>L</mml:mi></mml:math></inline-formula> as the MSA length, and to <inline-formula><mml:math id="inf138"><mml:mi>M</mml:mi></mml:math></inline-formula> as its depth. For all our MSAs, <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>36000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. These alignments are the same as in <xref ref-type="bibr" rid="bib48">Lupo et al., 2022</xref>, except that we removed PF13354 (Beta-lactamase2) from this set of deep MSAs because of its smaller depth. However, this family is included in our additional analyses (see <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>).</p><p>Deep MSAs generally include some highly similar sequences due to phylogenetic relatedness. This can be characterized via the effective depth (<xref ref-type="bibr" rid="bib86">Weigt et al., 2009</xref>)<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>:=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mtext>with</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>δ</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf140"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi mathvariant="normal">H</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the (normalized) Hamming distance between two sequences <inline-formula><mml:math id="inf141"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf142"><mml:mi mathvariant="bold-italic">e</mml:mi></mml:math></inline-formula>, that is the fraction of sites where the amino acids differ, and we set <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>. Note that the inverse of the sequence weight <italic>w</italic><sub><italic>i</italic></sub> in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> is the number of neighbors in ‘Characterizing the distribution of sequences in MSAs’, and that <inline-formula><mml:math id="inf144"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mi>eff</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> can be as low as 0.06 for our natural MSAs.</p><p>All these families were previously shown to be well fitted by Potts models inferred by bmDCA (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>), making our results on sequence generation by bmDCA readily comparable with previous results. Our domains’ short lengths are convenient because bmDCA is computationally demanding, and also in view of MSA Transformer’s large memory footprint, which is <inline-formula><mml:math id="inf145"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, their large depth is crucial to our comparisons, as it allows Potts models to be accurately fitted (<xref ref-type="bibr" rid="bib31">Figliuzzi et al., 2018</xref>).</p><p>We extended our study to small protein families by considering seven additional families, listed in <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>, for which we also started from Pfam ‘full’ MSAs. These families comprise from nine to a few hundreds of sequences. We also considered two additional protein families, also listed in <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>, for our comparison with published experimental datasets.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Validation, Investigation, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-79854-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Python code for generating sequences using the iterative masking procedure is archived at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7684052">https://doi.org/10.5281/zenodo.7684052</ext-link>. Raw data were collected from two public sources: (1) MSAs from the Pfam database (<ext-link ext-link-type="uri" xlink:href="https://pfam.xfam.org/">https://pfam.xfam.org/</ext-link>); (2) further MSAs from <ext-link ext-link-type="uri" xlink:href="https://github.com/matteofigliuzzi/bmDCA">https://github.com/matteofigliuzzi/bmDCA</ext-link> (<xref ref-type="bibr" rid="bib6">Barrat-Charlaix, 2017</xref>). We generated sequences with bmDCA using code publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ranganathanlab/bmDCA">https://github.com/ranganathanlab/bmDCA</ext-link> (<xref ref-type="bibr" rid="bib32">Figliuzzi and Barrat-Charlaix, 2020</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no. 851173, to A-FB).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alley</surname><given-names>EC</given-names></name><name><surname>Khimulya</surname><given-names>G</given-names></name><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>AlQuraishi</surname><given-names>M</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title><source>Nature Methods</source><volume>16</volume><fpage>1315</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0598-1</pub-id><pub-id pub-id-type="pmid">31636460</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Pellock</surname><given-names>SJ</given-names></name><name><surname>Chidyausiku</surname><given-names>TM</given-names></name><name><surname>Ramelot</surname><given-names>TA</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Hao</surname><given-names>J</given-names></name><name><surname>Bafna</surname><given-names>K</given-names></name><name><surname>Norn</surname><given-names>C</given-names></name><name><surname>Kang</surname><given-names>A</given-names></name><name><surname>Bera</surname><given-names>AK</given-names></name><name><surname>DiMaio</surname><given-names>F</given-names></name><name><surname>Carter</surname><given-names>L</given-names></name><name><surname>Chow</surname><given-names>CM</given-names></name><name><surname>Montelione</surname><given-names>GT</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>De novo protein design by deep network hallucination</article-title><source>Nature</source><volume>600</volume><fpage>547</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04184-w</pub-id><pub-id pub-id-type="pmid">34853475</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnold</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Directed evolution: bringing new chemistry to life</article-title><source>Angewandte Chemie International Edition</source><volume>57</volume><fpage>4143</fpage><lpage>4148</lpage><pub-id pub-id-type="doi">10.1002/anie.201708408</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baek</surname><given-names>M</given-names></name><name><surname>DiMaio</surname><given-names>F</given-names></name><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Dauparas</surname><given-names>J</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Cong</surname><given-names>Q</given-names></name><name><surname>Kinch</surname><given-names>LN</given-names></name><name><surname>Schaeffer</surname><given-names>RD</given-names></name><name><surname>Millán</surname><given-names>C</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Adams</surname><given-names>C</given-names></name><name><surname>Glassman</surname><given-names>CR</given-names></name><name><surname>DeGiovanni</surname><given-names>A</given-names></name><name><surname>Pereira</surname><given-names>JH</given-names></name><name><surname>Rodrigues</surname><given-names>AV</given-names></name><name><surname>van Dijk</surname><given-names>AA</given-names></name><name><surname>Ebrecht</surname><given-names>AC</given-names></name><name><surname>Opperman</surname><given-names>DJ</given-names></name><name><surname>Sagmeister</surname><given-names>T</given-names></name><name><surname>Buhlheller</surname><given-names>C</given-names></name><name><surname>Pavkov-Keller</surname><given-names>T</given-names></name><name><surname>Rathinaswamy</surname><given-names>MK</given-names></name><name><surname>Dalwadi</surname><given-names>U</given-names></name><name><surname>Yip</surname><given-names>CK</given-names></name><name><surname>Burke</surname><given-names>JE</given-names></name><name><surname>Garcia</surname><given-names>KC</given-names></name><name><surname>Grishin</surname><given-names>NV</given-names></name><name><surname>Adams</surname><given-names>PD</given-names></name><name><surname>Read</surname><given-names>RJ</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Accurate prediction of protein structures and interactions using a three-track neural network</article-title><source>Science</source><volume>373</volume><fpage>871</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1126/science.abj8754</pub-id><pub-id pub-id-type="pmid">34282049</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural machine translation by jointly learning to align and translate</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Barrat-Charlaix</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>bmDCA</data-title><version designator="e1d93fa">e1d93fa</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/matteofigliuzzi/bmDCA">https://github.com/matteofigliuzzi/bmDCA</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bhattacharya</surname><given-names>N</given-names></name><name><surname>Thomas</surname><given-names>N</given-names></name><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Dauparas</surname><given-names>J</given-names></name><name><surname>Koo</surname><given-names>PK</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name><name><surname>Song</surname><given-names>YS</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Single Layers of Attention Suffice to Predict Protein Contacts</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.21.423882</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bhattacharya</surname><given-names>N</given-names></name><name><surname>Thomas</surname><given-names>N</given-names></name><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Dauparas</surname><given-names>J</given-names></name><name><surname>Koo</surname><given-names>PK</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name><name><surname>Song</surname><given-names>YS</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Interpreting potts and transformer protein models through the lens of simplified attention</article-title><conf-name>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</conf-name><fpage>34</fpage><lpage>45</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Ranganathan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Rediscovering the Power of Pairwise Interactions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/0712.4397">https://arxiv.org/abs/0712.4397</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Biophysics: Searching for principles</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bileschi</surname><given-names>ML</given-names></name><name><surname>Belanger</surname><given-names>D</given-names></name><name><surname>Bryant</surname><given-names>DH</given-names></name><name><surname>Sanderson</surname><given-names>T</given-names></name><name><surname>Carter</surname><given-names>B</given-names></name><name><surname>Sculley</surname><given-names>D</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>DePristo</surname><given-names>MA</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Using deep learning to annotate the protein universe</article-title><source>Nature Biotechnology</source><volume>40</volume><fpage>932</fpage><lpage>937</lpage><pub-id pub-id-type="doi">10.1038/s41587-021-01179-w</pub-id><pub-id pub-id-type="pmid">35190689</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bitbol</surname><given-names>AF</given-names></name><name><surname>Dwyer</surname><given-names>RS</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name><name><surname>Wingreen</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inferring interaction partners from protein sequences</article-title><source>PNAS</source><volume>113</volume><fpage>12180</fpage><lpage>12185</lpage><pub-id pub-id-type="doi">10.1073/pnas.1606762113</pub-id><pub-id pub-id-type="pmid">27663738</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casari</surname><given-names>G</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Valencia</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>A method to predict functional residues in proteins</article-title><source>Nature Structural Biology</source><volume>2</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nsb0295-171</pub-id><pub-id pub-id-type="pmid">7749921</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>RR</given-names></name><name><surname>Morcos</surname><given-names>F</given-names></name><name><surname>Levine</surname><given-names>H</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Toward rationally redesigning bacterial two-component signaling systems using coevolutionary information</article-title><source>PNAS</source><volume>111</volume><fpage>E563</fpage><lpage>E571</lpage><pub-id pub-id-type="doi">10.1073/pnas.1323734111</pub-id><pub-id pub-id-type="pmid">24449878</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>RR</given-names></name><name><surname>Nordesjö</surname><given-names>O</given-names></name><name><surname>Hayes</surname><given-names>RL</given-names></name><name><surname>Levine</surname><given-names>H</given-names></name><name><surname>Flores</surname><given-names>SC</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name><name><surname>Morcos</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Connecting the sequence-space of bacterial signaling proteins to phenotypes using coevolutionary landscapes</article-title><source>Molecular Biology and Evolution</source><volume>33</volume><fpage>3054</fpage><lpage>3064</lpage><pub-id pub-id-type="doi">10.1093/molbev/msw188</pub-id><pub-id pub-id-type="pmid">27604223</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chowdhury</surname><given-names>R</given-names></name><name><surname>Bouatta</surname><given-names>N</given-names></name><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>Rochereau</surname><given-names>C</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name><name><surname>Sorger</surname><given-names>PK</given-names></name><name><surname>AlQuraishi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Single-Sequence Protein Structure Prediction Using Language Models from Deep Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.08.02.454840</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Feinauer</surname><given-names>C</given-names></name><name><surname>Figliuzzi</surname><given-names>M</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inverse statistical physics of protein sequences: A key issues review</article-title><source>Reports on Progress in Physics. Physical Society</source><volume>81</volume><elocation-id>032601</elocation-id><pub-id pub-id-type="doi">10.1088/1361-6633/aa9965</pub-id><pub-id pub-id-type="pmid">29120346</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colavin</surname><given-names>A</given-names></name><name><surname>Atolia</surname><given-names>E</given-names></name><name><surname>Bitbol</surname><given-names>AF</given-names></name><name><surname>Huang</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Extracting phylogenetic dimensions of coevolution reveals hidden functional signals</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>820</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-04260-1</pub-id><pub-id pub-id-type="pmid">35039514</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colijn</surname><given-names>C</given-names></name><name><surname>Plazzotta</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A metric on phylogenetic tree shapes</article-title><source>Systematic Biology</source><volume>67</volume><fpage>113</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1093/sysbio/syx046</pub-id><pub-id pub-id-type="pmid">28472435</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cong</surname><given-names>Q</given-names></name><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Protein interaction networks revealed by proteome coevolution</article-title><source>Science</source><volume>365</volume><fpage>185</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1126/science.aaw6718</pub-id><pub-id pub-id-type="pmid">31296772</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahiyat</surname><given-names>BI</given-names></name><name><surname>Mayo</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>De novo protein design: Fully automated sequence selection</article-title><source>Science</source><volume>278</volume><fpage>82</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1126/science.278.5335.82</pub-id><pub-id pub-id-type="pmid">9311930</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de la Paz</surname><given-names>JA</given-names></name><name><surname>Nartey</surname><given-names>CM</given-names></name><name><surname>Yuvaraj</surname><given-names>M</given-names></name><name><surname>Morcos</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Epistatic contributions promote the unification of incompatible models of neutral molecular evolution</article-title><source>PNAS</source><volume>117</volume><fpage>5873</fpage><lpage>5882</lpage><pub-id pub-id-type="doi">10.1073/pnas.1913071117</pub-id><pub-id pub-id-type="pmid">32123092</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietler</surname><given-names>N</given-names></name><name><surname>Lupo</surname><given-names>U</given-names></name><name><surname>Bitbol</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Impact of phylogeny on structural contact inference from protein sequence data</article-title><source>Journal of the Royal Society, Interface</source><volume>20</volume><elocation-id>20220707</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2022.0707</pub-id><pub-id pub-id-type="pmid">36751926</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>SD</given-names></name><name><surname>Wahl</surname><given-names>LM</given-names></name><name><surname>Gloor</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction</article-title><source>Bioinformatics</source><volume>24</volume><fpage>333</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btm604</pub-id><pub-id pub-id-type="pmid">18057019</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dwyer</surname><given-names>RS</given-names></name><name><surname>Ricci</surname><given-names>DP</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name><name><surname>Silhavy</surname><given-names>TJ</given-names></name><name><surname>Wingreen</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predicting functionally informative mutations in <italic>Escherichia coli</italic> bama using evolutionary covariance analysis</article-title><source>Genetics</source><volume>195</volume><fpage>443</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1534/genetics.113.155861</pub-id><pub-id pub-id-type="pmid">23934888</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eddy</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Profile hidden markov models</article-title><source>Bioinformatics</source><volume>14</volume><fpage>755</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/14.9.755</pub-id><pub-id pub-id-type="pmid">9918945</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekeberg</surname><given-names>M</given-names></name><name><surname>Lövkvist</surname><given-names>C</given-names></name><name><surname>Lan</surname><given-names>Y</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>Aurell</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Improved contact prediction in proteins: Using pseudolikelihoods to infer potts models</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>87</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.87.012707</pub-id><pub-id pub-id-type="pmid">23410359</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Rehawi</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gibbs</surname><given-names>T</given-names></name><name><surname>Feher</surname><given-names>T</given-names></name><name><surname>Angerer</surname><given-names>C</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Bhowmik</surname><given-names>D</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>ProtTrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing</article-title><conf-name>IEEE Transactions on Pattern Analysis and Machine Intelligence</conf-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><name><surname>Schmidt</surname><given-names>S</given-names></name><name><surname>Höcker</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Prot GPT2 is a deep unsupervised language model for protein design</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>4348</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-32007-7</pub-id><pub-id pub-id-type="pmid">35896542</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Figliuzzi</surname><given-names>M</given-names></name><name><surname>Jacquier</surname><given-names>H</given-names></name><name><surname>Schug</surname><given-names>A</given-names></name><name><surname>Tenaillon</surname><given-names>O</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Coevolutionary landscape inference and the context-dependence of mutations in beta-lactamase TEM-1</article-title><source>Molecular Biology and Evolution</source><volume>33</volume><fpage>268</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1093/molbev/msv211</pub-id><pub-id pub-id-type="pmid">26446903</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Figliuzzi</surname><given-names>M</given-names></name><name><surname>Barrat-Charlaix</surname><given-names>P</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How pairwise coevolutionary models capture the collective residue variability in proteins?</article-title><source>Molecular Biology and Evolution</source><volume>35</volume><fpage>1018</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1093/molbev/msy007</pub-id><pub-id pub-id-type="pmid">29351669</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Figliuzzi</surname><given-names>M</given-names></name><name><surname>Barrat-Charlaix</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Boltzmann-machine direct coupling analysis (bmdca)</data-title><version designator="aba5168">aba5168</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ranganathanlab/bmDCA">https://github.com/ranganathanlab/bmDCA</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerardos</surname><given-names>A</given-names></name><name><surname>Dietler</surname><given-names>N</given-names></name><name><surname>Bitbol</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Correlations from structure and phylogeny combine constructively in the inference of protein partners from sequences</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1010147</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010147</pub-id><pub-id pub-id-type="pmid">35576238</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goyal</surname><given-names>K</given-names></name><name><surname>Dyer</surname><given-names>C</given-names></name><name><surname>Berg-Kirkpatrick</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis–Hastings</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2106.02736">https://arxiv.org/abs/2106.02736</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>AG</given-names></name><name><surname>Elhabashy</surname><given-names>H</given-names></name><name><surname>Brock</surname><given-names>KP</given-names></name><name><surname>Maddamsetti</surname><given-names>R</given-names></name><name><surname>Kohlbacher</surname><given-names>O</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Large-scale discovery of protein interactions at residue resolution using co-evolution calculated from genomic sequences</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>1396</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-21636-z</pub-id><pub-id pub-id-type="pmid">33654096</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gueudré</surname><given-names>T</given-names></name><name><surname>Baldassi</surname><given-names>C</given-names></name><name><surname>Zamparo</surname><given-names>M</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>Pagnani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Simultaneous identification of specifically interacting paralogs and interprotein contacts by direct coupling analysis</article-title><source>PNAS</source><volume>113</volume><fpage>12186</fpage><lpage>12191</lpage><pub-id pub-id-type="doi">10.1073/pnas.1607570113</pub-id><pub-id pub-id-type="pmid">27729520</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Flynn</surname><given-names>WF</given-names></name><name><surname>He</surname><given-names>P</given-names></name><name><surname>Levy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Coevolutionary landscape of kinase family proteins: Sequence probabilities and functional motifs</article-title><source>Biophysical Journal</source><volume>114</volume><fpage>21</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2017.10.028</pub-id><pub-id pub-id-type="pmid">29320688</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins-Hooker</surname><given-names>A</given-names></name><name><surname>Depardieu</surname><given-names>F</given-names></name><name><surname>Baur</surname><given-names>S</given-names></name><name><surname>Couairon</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Bikard</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Generating functional protein variants with variational autoencoders</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008736</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008736</pub-id><pub-id pub-id-type="pmid">33635868</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hawkins-Hooker</surname><given-names>A</given-names></name><name><surname>Jones</surname><given-names>DT</given-names></name><name><surname>Paige</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>MSA-Conditioned generative protein language models for fitness landscape modelling and design</article-title><conf-name>In Machine Learning for Structural Biology Workshop NeurIPS</conf-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henikoff</surname><given-names>S</given-names></name><name><surname>Henikoff</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Amino acid substitution matrices from protein blocks</article-title><source>PNAS</source><volume>89</volume><fpage>10915</fpage><lpage>10919</lpage><pub-id pub-id-type="doi">10.1073/pnas.89.22.10915</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hie</surname><given-names>BL</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name><name><surname>Shanker</surname><given-names>VR</given-names></name><name><surname>Weidenbacher</surname><given-names>PA</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Efficient Evolution of Human Antibodies from General Protein Language Models and Sequence Information Alone</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.04.10.487811</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>SR</given-names></name><name><surname>Monaco</surname><given-names>S</given-names></name><name><surname>Massie</surname><given-names>K</given-names></name><name><surname>Syed</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Generating Novel Protein Sequences Using Gibbs Sampling of Masked Language Models</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.01.26.428322</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Figurnov</surname><given-names>M</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Bates</surname><given-names>R</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Potapenko</surname><given-names>A</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Ballard</surname><given-names>AJ</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Romera-Paredes</surname><given-names>B</given-names></name><name><surname>Nikolov</surname><given-names>S</given-names></name><name><surname>Jain</surname><given-names>R</given-names></name><name><surname>Adler</surname><given-names>J</given-names></name><name><surname>Back</surname><given-names>T</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Reiman</surname><given-names>D</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Zielinski</surname><given-names>M</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Pacholska</surname><given-names>M</given-names></name><name><surname>Berghammer</surname><given-names>T</given-names></name><name><surname>Bodenstein</surname><given-names>S</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Highly accurate protein structure prediction with alphafold</article-title><source>Nature</source><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhlman</surname><given-names>B</given-names></name><name><surname>Dantas</surname><given-names>G</given-names></name><name><surname>Ireton</surname><given-names>GC</given-names></name><name><surname>Varani</surname><given-names>G</given-names></name><name><surname>Stoddard</surname><given-names>BL</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Design of a novel globular protein fold with atomic-level accuracy</article-title><source>Science</source><volume>302</volume><fpage>1364</fpage><lpage>1368</lpage><pub-id pub-id-type="doi">10.1126/science.1089427</pub-id><pub-id pub-id-type="pmid">14631033</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lapedes</surname><given-names>AS</given-names></name><name><surname>Giraud</surname><given-names>BG</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Stormo</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Correlated mutations in models of protein sequences: Phylogenetic and structural effects</article-title><conf-name>Statistics in Molecular Biology and Genetics - IMS Lecture Notes - Monograph Series</conf-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewitus</surname><given-names>E</given-names></name><name><surname>Morlon</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Characterizing and comparing phylogenies from their laplacian spectrum</article-title><source>Systematic Biology</source><volume>65</volume><fpage>495</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1093/sysbio/syv116</pub-id><pub-id pub-id-type="pmid">26658901</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Fan</surname><given-names>K</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Jin</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>C</given-names></name><name><surname>Tang</surname><given-names>C</given-names></name><name><surname>Lai</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>De novo design of a βαβ motif</article-title><source>Angewandte Chemie International Edition</source><volume>48</volume><fpage>3301</fpage><lpage>3303</lpage><pub-id pub-id-type="doi">10.1002/anie.200805476</pub-id><pub-id pub-id-type="pmid">19347908</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lupo</surname><given-names>U</given-names></name><name><surname>Sgarbossa</surname><given-names>D</given-names></name><name><surname>Bitbol</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Protein language models trained on multiple sequence alignments learn phylogenetic relationships</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>6298</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-34032-y</pub-id><pub-id pub-id-type="pmid">36273003</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>McCann</surname><given-names>B</given-names></name><name><surname>Naik</surname><given-names>N</given-names></name><name><surname>Keskar</surname><given-names>NS</given-names></name><name><surname>Anand</surname><given-names>N</given-names></name><name><surname>Eguchi</surname><given-names>RR</given-names></name><name><surname>Huang</surname><given-names>PS</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>ProGen: Language Modeling for Protein Generation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.07.982272</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>Krause</surname><given-names>B</given-names></name><name><surname>Greene</surname><given-names>ER</given-names></name><name><surname>Subramanian</surname><given-names>S</given-names></name><name><surname>Mohr</surname><given-names>BP</given-names></name><name><surname>Holton</surname><given-names>JM</given-names></name><name><surname>Olmos</surname><given-names>JL</given-names></name><name><surname>Xiong</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>ZZ</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Fraser</surname><given-names>JS</given-names></name><name><surname>Naik</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep Neural Language Modeling Enables Functional Protein Generation across Families</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.18.452833</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malbranke</surname><given-names>C</given-names></name><name><surname>Bikard</surname><given-names>D</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Improving sequence-based modeling of protein families using secondary structure quality assessment</article-title><source>Bioinformatics</source><volume>37</volume><fpage>4083</fpage><lpage>4090</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab442</pub-id><pub-id pub-id-type="pmid">34117879</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malinverni</surname><given-names>D</given-names></name><name><surname>Marsili</surname><given-names>S</given-names></name><name><surname>Barducci</surname><given-names>A</given-names></name><name><surname>De Los Rios</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Large-scale conformational transitions and dimerization are encoded in the amino-acid sequences of hsp70 chaperones</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004262</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004262</pub-id><pub-id pub-id-type="pmid">26046683</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>DS</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name><name><surname>Sheridan</surname><given-names>R</given-names></name><name><surname>Hopf</surname><given-names>TA</given-names></name><name><surname>Pagnani</surname><given-names>A</given-names></name><name><surname>Zecchina</surname><given-names>R</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Protein 3D structure computed from evolutionary sequence variation</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e28766</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0028766</pub-id><pub-id pub-id-type="pmid">22163331</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marmier</surname><given-names>G</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>Bitbol</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Phylogenetic correlations can suffice to infer protein partners from sequences</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007179</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007179</pub-id><pub-id pub-id-type="pmid">31609984</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGee</surname><given-names>F</given-names></name><name><surname>Hauri</surname><given-names>S</given-names></name><name><surname>Novinger</surname><given-names>Q</given-names></name><name><surname>Vucetic</surname><given-names>S</given-names></name><name><surname>Levy</surname><given-names>RM</given-names></name><name><surname>Carnevale</surname><given-names>V</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The generative capacity of probabilistic protein sequence models</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6302</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26529-9</pub-id><pub-id pub-id-type="pmid">34728624</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGill</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Multivariate information transmission</article-title><source>Psychometrika</source><volume>19</volume><fpage>97</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1007/BF02289159</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLaughlin</surname><given-names>RN</given-names></name><name><surname>Poelwijk</surname><given-names>FJ</given-names></name><name><surname>Raman</surname><given-names>A</given-names></name><name><surname>Gosal</surname><given-names>WS</given-names></name><name><surname>Ranganathan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The spatial architecture of protein function and adaptation</article-title><source>Nature</source><volume>491</volume><fpage>138</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/nature11500</pub-id><pub-id pub-id-type="pmid">23041932</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Language Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.09.450648</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Meshulam</surname><given-names>L</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Successes and Failures of Simplified Models for a Network of Real Neurons</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2112.14735">https://arxiv.org/abs/2112.14735</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mistry</surname><given-names>J</given-names></name><name><surname>Chuguransky</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>L</given-names></name><name><surname>Qureshi</surname><given-names>M</given-names></name><name><surname>Salazar</surname><given-names>GA</given-names></name><name><surname>Sonnhammer</surname><given-names>ELL</given-names></name><name><surname>Tosatto</surname><given-names>SCE</given-names></name><name><surname>Paladin</surname><given-names>L</given-names></name><name><surname>Raj</surname><given-names>S</given-names></name><name><surname>Richardson</surname><given-names>LJ</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pfam: The protein families database in 2021</article-title><source>Nucleic Acids Research</source><volume>49</volume><fpage>D412</fpage><lpage>D419</lpage><pub-id pub-id-type="doi">10.1093/nar/gkaa913</pub-id><pub-id pub-id-type="pmid">33125078</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molina‐Venegas</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>What are “tippy” and “stemmy” phylogenies? resolving a phylogenetic terminological tangle</article-title><source>Journal of Systematics and Evolution</source><volume>59</volume><fpage>403</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1111/jse.12686</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morcos</surname><given-names>F</given-names></name><name><surname>Pagnani</surname><given-names>A</given-names></name><name><surname>Lunt</surname><given-names>B</given-names></name><name><surname>Bertolino</surname><given-names>A</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Zecchina</surname><given-names>R</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Direct-coupling analysis of residue coevolution captures native contacts across many protein families</article-title><source>PNAS</source><volume>108</volume><fpage>E1293</fpage><lpage>E1301</lpage><pub-id pub-id-type="doi">10.1073/pnas.1111471108</pub-id><pub-id pub-id-type="pmid">22106262</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morcos</surname><given-names>F</given-names></name><name><surname>Jana</surname><given-names>B</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Coevolutionary signals across protein lineages help capture multiple protein conformations</article-title><source>PNAS</source><volume>110</volume><fpage>20533</fpage><lpage>20538</lpage><pub-id pub-id-type="doi">10.1073/pnas.1315625110</pub-id><pub-id pub-id-type="pmid">24297889</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>MN</given-names></name><name><surname>Dehal</surname><given-names>PS</given-names></name><name><surname>Arkin</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>FastTree 2--approximately maximum-likelihood trees for large alignments</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e9490</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0009490</pub-id><pub-id pub-id-type="pmid">20224823</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>C</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Power law tails in phylogenetic systems</article-title><source>PNAS</source><volume>115</volume><fpage>690</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1073/pnas.1711913115</pub-id><pub-id pub-id-type="pmid">29311320</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quax</surname><given-names>R</given-names></name><name><surname>Har-Shemesh</surname><given-names>O</given-names></name><name><surname>Sloot</surname><given-names>PMA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Quantifying synergistic information using intermediate stochastic variables</article-title><source>Entropy</source><volume>19</volume><elocation-id>85</elocation-id><pub-id pub-id-type="doi">10.3390/e19020085</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RM</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Canny</surname><given-names>J</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>MSA Transformer</article-title><conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name><fpage>18</fpage><lpage>24</lpage></element-citation></ref><ref id="bib68"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Transformer protein language models are unsupervised structure learners</article-title><conf-name>In International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Goyal</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>PNAS</source><volume>118</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id><pub-id pub-id-type="pmid">33876751</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rocklin</surname><given-names>GJ</given-names></name><name><surname>Chidyausiku</surname><given-names>TM</given-names></name><name><surname>Goreshnik</surname><given-names>I</given-names></name><name><surname>Ford</surname><given-names>A</given-names></name><name><surname>Houliston</surname><given-names>S</given-names></name><name><surname>Lemak</surname><given-names>A</given-names></name><name><surname>Carter</surname><given-names>L</given-names></name><name><surname>Ravichandran</surname><given-names>R</given-names></name><name><surname>Mulligan</surname><given-names>VK</given-names></name><name><surname>Chevalier</surname><given-names>A</given-names></name><name><surname>Arrowsmith</surname><given-names>CH</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Global analysis of protein folding using massively parallel design, synthesis, and testing</article-title><source>Science</source><volume>357</volume><fpage>168</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1126/science.aan0693</pub-id><pub-id pub-id-type="pmid">28706065</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez Horta</surname><given-names>E</given-names></name><name><surname>Barrat-Charlaix</surname><given-names>P</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Toward inferring potts models for phylogenetically correlated sequence data</article-title><source>Entropy</source><volume>21</volume><elocation-id>1090</elocation-id><pub-id pub-id-type="doi">10.3390/e21111090</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez Horta</surname><given-names>E</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>On the effect of phylogenetic correlations in coevolution-based contact prediction in proteins</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008957</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008957</pub-id><pub-id pub-id-type="pmid">34029316</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosas</surname><given-names>F</given-names></name><name><surname>Ntranos</surname><given-names>V</given-names></name><name><surname>Ellison</surname><given-names>CJ</given-names></name><name><surname>Pollin</surname><given-names>S</given-names></name><name><surname>Verhelst</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Understanding interdependency through complex information sharing</article-title><source>Entropy</source><volume>18</volume><elocation-id>38</elocation-id><pub-id pub-id-type="doi">10.3390/e18020038</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Mediano</surname><given-names>PAM</given-names></name><name><surname>Gastpar</surname><given-names>M</given-names></name><name><surname>Jensen</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Quantifying high-order interdependencies via multivariate extensions of the mutual information</article-title><source>Physical Review. E</source><volume>100</volume><elocation-id>032305</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.100.032305</pub-id><pub-id pub-id-type="pmid">31640038</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russ</surname><given-names>WP</given-names></name><name><surname>Figliuzzi</surname><given-names>M</given-names></name><name><surname>Stocker</surname><given-names>C</given-names></name><name><surname>Barrat-Charlaix</surname><given-names>P</given-names></name><name><surname>Socolich</surname><given-names>M</given-names></name><name><surname>Kast</surname><given-names>P</given-names></name><name><surname>Hilvert</surname><given-names>D</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>Ranganathan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An evolution-based model for designing chorismate mutase enzymes</article-title><source>Science</source><volume>369</volume><fpage>440</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1126/science.aba3304</pub-id><pub-id pub-id-type="pmid">32703877</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>JE</given-names></name><name><surname>Riesselman</surname><given-names>AJ</given-names></name><name><surname>Kollasch</surname><given-names>AW</given-names></name><name><surname>McMahon</surname><given-names>C</given-names></name><name><surname>Simon</surname><given-names>E</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Manglik</surname><given-names>A</given-names></name><name><surname>Kruse</surname><given-names>AC</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Protein design and variant prediction using autoregressive generative models</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2403</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22732-w</pub-id><pub-id pub-id-type="pmid">33893299</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Socolich</surname><given-names>M</given-names></name><name><surname>Lockless</surname><given-names>SW</given-names></name><name><surname>Russ</surname><given-names>WP</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Gardner</surname><given-names>KH</given-names></name><name><surname>Ranganathan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Evolutionary information for specifying a protein fold</article-title><source>Nature</source><volume>437</volume><fpage>512</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1038/nature03991</pub-id><pub-id pub-id-type="pmid">16177782</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stiffler</surname><given-names>MA</given-names></name><name><surname>Hekstra</surname><given-names>DR</given-names></name><name><surname>Ranganathan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Evolvability as a function of purifying selection in TEM-1 β-lactamase</article-title><source>Cell</source><volume>160</volume><fpage>882</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.01.035</pub-id><pub-id pub-id-type="pmid">25723163</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sułkowska</surname><given-names>JI</given-names></name><name><surname>Morcos</surname><given-names>F</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Genomics-aided structure prediction</article-title><source>PNAS</source><volume>109</volume><fpage>10340</fpage><lpage>10345</lpage><pub-id pub-id-type="doi">10.1073/pnas.1207864109</pub-id><pub-id pub-id-type="pmid">22691493</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Timme</surname><given-names>N</given-names></name><name><surname>Alford</surname><given-names>W</given-names></name><name><surname>Flecker</surname><given-names>B</given-names></name><name><surname>Beggs</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Synergy, redundancy, and multivariate information measures: An experimentalist’s perspective</article-title><source>Journal of Computational Neuroscience</source><volume>36</volume><fpage>119</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1007/s10827-013-0458-4</pub-id><pub-id pub-id-type="pmid">23820856</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tubiana</surname><given-names>J</given-names></name><name><surname>Cocco</surname><given-names>S</given-names></name><name><surname>Monasson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning protein constitutive motifs from sequence data</article-title><source>eLife</source><volume>8</volume><elocation-id>e39397</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.39397</pub-id><pub-id pub-id-type="pmid">30857591</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>5998</fpage><lpage>6008</lpage></element-citation></ref><ref id="bib83"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vig</surname><given-names>J</given-names></name><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>Varshney</surname><given-names>LR</given-names></name><name><surname>Xiong</surname><given-names>C</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Rajani</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>BERTology Meets Biology: Interpreting Attention in Protein Language Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.15222">https://arxiv.org/abs/2006.15222</ext-link></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vorberg</surname><given-names>S</given-names></name><name><surname>Seemayer</surname><given-names>S</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synthetic protein alignments by ccmgen quantify noise in residue-residue contact prediction</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006526</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006526</pub-id><pub-id pub-id-type="pmid">30395601</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>BERT Has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1902.04094">https://arxiv.org/abs/1902.04094</ext-link></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>White</surname><given-names>RA</given-names></name><name><surname>Szurmant</surname><given-names>H</given-names></name><name><surname>Hoch</surname><given-names>JA</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Identification of direct residue contacts in protein-protein interaction by message passing</article-title><source>PNAS</source><volume>106</volume><fpage>67</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1073/pnas.0805923106</pub-id><pub-id pub-id-type="pmid">19116270</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>PL</given-names></name><name><surname>Beer</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Nonnegative Decomposition of Multivariate Information</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1004.2515">https://arxiv.org/abs/1004.2515</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>p values of the Kolmogorov–Smirnov test comparing the distributions of homology, coevolution, and structure-based scores across natural and synthetic multiple sequence alignments (MSAs).</title><p>For each score except the root-mean-squared deviation (RMSD), we test the null hypothesis that the scores of MSA-Transformer–generated sequences are greater or equal than those of Boltzmann machine DCA (bmDCA)-generated sequences, in the (stringent) sense that the cumulative distribution function of the former is always below that of the latter. Here, bmDCA1 stands for bmDCA with <inline-formula><mml:math id="inf146"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0.33</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and bmDCA2 for bmDCA with <inline-formula><mml:math id="inf147"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For the RMSD, the null hypothesis is that the scores of MSA-Transformer–generated sequences are smaller or equal than those of bmDCA-generated sequences (recall that smaller RMSDs are better). In all cases, a p value close to one (resp. zero) means that the null hypothesis tested should be accepted (resp. rejected). Reported zero p values are too small to be properly assessed by the algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Pfam ID</th><th align="center" valign="bottom" colspan="2">HMMER score</th><th align="center" valign="bottom" colspan="2">Statistical energy score</th><th align="center" valign="bottom" colspan="2">pLDDT confidence</th><th align="center" valign="bottom" colspan="2">RMSD</th></tr><tr><th align="center" valign="bottom">MSA Tr. ≥bmDCA1</th><th align="center" valign="bottom">MSA Tr. ≥bmDCA2</th><th align="center" valign="bottom">MSA Tr. ≥bmDCA1</th><th align="center" valign="bottom">MSA Tr. ≥bmDCA2</th><th align="center" valign="bottom">MSA Tr. ≥bmDCA1</th><th align="center" valign="bottom">MSA Tr. ≥bmDCA2</th><th align="center" valign="bottom">MSA Tr. ≤bmDCA1</th><th align="center" valign="bottom">MSA Tr. ≤bmDCA2</th></tr></thead><tbody><tr><td align="left" valign="bottom">PF00004</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.20</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.33</td><td align="center" valign="bottom">0.96</td></tr><tr><td align="left" valign="bottom">PF00005</td><td align="center" valign="bottom">0.93</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">6.5 · 10<sup>−19</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.78</td><td align="center" valign="bottom">0.96</td></tr><tr><td align="left" valign="bottom">PF00041</td><td align="center" valign="bottom">0.99</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">2.9 · 10<sup>−14</sup></td><td align="center" valign="bottom">5.9 · 10 <sup>−3</sup></td></tr><tr><td align="left" valign="bottom">PF00072</td><td align="center" valign="bottom">4.7 · 10<sup>−12</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">9.1 · 10<sup>−6</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">3.4 · 10<sup>−6</sup></td><td align="center" valign="bottom">0.96</td></tr><tr><td align="left" valign="bottom">PF00076</td><td align="center" valign="bottom">9.5 · 10<sup>−134</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">6.5 · 10<sup>−19</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">9.2 · 10<sup>−5</sup></td><td align="center" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">PF00096</td><td align="center" valign="bottom">0.04</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.01</td><td align="center" valign="bottom">0.92</td><td align="center" valign="bottom">0.92</td><td align="center" valign="bottom">2.4 · 10<sup>−5</sup></td></tr><tr><td align="left" valign="bottom">PF00153</td><td align="center" valign="bottom">0.91</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.98</td><td align="center" valign="bottom">0.84</td><td align="center" valign="bottom">9.3 · 10<sup>−10</sup></td><td align="center" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">PF00271</td><td align="center" valign="bottom">5.9 · 10<sup>−30</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.33</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.02</td><td align="center" valign="bottom">0.88</td></tr><tr><td align="left" valign="bottom">PF00397</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.5 · 10<sup>−3</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">4.8 · 10<sup>−10</sup></td><td align="center" valign="bottom">0.38</td></tr><tr><td align="left" valign="bottom">PF00512</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">4.3 · 10<sup>−3</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.96</td><td align="center" valign="bottom">0.67</td></tr><tr><td align="left" valign="bottom">PF00595</td><td align="center" valign="bottom">0.83</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0 · 10<sup>−15</sup></td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">7.2 · 10<sup>−24</sup></td><td align="center" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">PF01535</td><td align="center" valign="bottom">0.98</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0.78</td><td align="center" valign="bottom">4.1 · 10<sup>−8</sup></td></tr><tr><td align="left" valign="bottom">PF02518</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">PF07679</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">1.0</td><td align="center" valign="bottom">4.3 · 10<sup>−3</sup></td><td align="center" valign="bottom">1.0</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Median homology, coevolution, and structure-based scores in natural and synthetic sequences.</title><p>We report the median values of each of the scores shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, as well as their standard deviations (between parentheses), for natural sequences (’Nat.’), for sequences generated by our method based on MSA Transformer, and for sequences generated by Boltzmann machine DCA (bmDCA) at low temperature, that is with <inline-formula><mml:math id="inf148"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0.33</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (denoted by ‘bmDCA’). Scores are normalized as <xref ref-type="fig" rid="fig1">Figure 1</xref>, except that, for statistical energy, we subtract the median of natural scores instead of the mean for clarity (therefore, all natural MSAs have median 0 and standard deviation 1 for this score). For all scores, the best median among those of the two synthetic MSAs is shown in bold, and for the predicted local-distance difference test (pLDDT) score, it is shown in red if it is better than that the other synthetic MSA by a margin larger than the largest standard deviation. Recall that higher values are better for all scores, except root-mean-squared deviation (RMSD), for which the opposite holds.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="center" valign="bottom" colspan="3">HMMER score</th><th align="center" valign="bottom" colspan="3">Statistical energy score</th><th align="center" valign="bottom" colspan="3">pLDDT confidence (%)</th><th align="center" valign="bottom" colspan="3">RMSD (Å)</th></tr><tr><th align="left" valign="bottom">Pfam ID</th><th align="center" valign="bottom">Nat.</th><th align="center" valign="bottom">MSA Tr.</th><th align="center" valign="bottom">bmDCA</th><th align="center" valign="bottom">Nat.</th><th align="center" valign="bottom">MSA Tr.</th><th align="center" valign="bottom">bmDCA</th><th align="center" valign="bottom">Nat.</th><th align="center" valign="bottom">MSA Tr.</th><th align="center" valign="bottom">bmDCA</th><th align="center" valign="bottom">Nat.</th><th align="center" valign="bottom">MSA Tr.</th><th align="center" valign="bottom">bmDCA</th></tr></thead><tbody><tr><td align="left" valign="bottom">PF00004</td><td align="center" valign="bottom">0.5 (0.2)</td><td align="center" valign="bottom">0.6 (0.2)</td><td align="center" valign="bottom"><bold>0.8</bold> (0.2)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">0.8 (0.9)</td><td align="center" valign="bottom"><bold>1.6</bold> (0.1)</td><td align="center" valign="bottom">85.4 (4.1)</td><td align="center" valign="bottom"><bold>85.8</bold> (4.5)</td><td align="center" valign="bottom">81.7 (0.7)</td><td align="center" valign="bottom">3.4 (0.8)</td><td align="center" valign="bottom"><bold>2.8</bold> (0.7)</td><td align="center" valign="bottom">3.6 (0.5)</td></tr><tr><td align="left" valign="bottom">PF00005</td><td align="center" valign="bottom">0.7 (0.1)</td><td align="center" valign="bottom">0.8 (0.1)</td><td align="center" valign="bottom">0.8 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">1.8 (0.9)</td><td align="center" valign="bottom"><bold>3.1</bold> (0.2)</td><td align="center" valign="bottom">83.0 (6.9)</td><td align="center" valign="bottom">89.0 (4.2)</td><td align="center" valign="bottom"><bold>91.6</bold> (1.6)</td><td align="center" valign="bottom">3.8 (1.2)</td><td align="center" valign="bottom">2.8 (1.0)</td><td align="center" valign="bottom">2.8 (0.8)</td></tr><tr><td align="left" valign="bottom">PF00041</td><td align="center" valign="bottom">0.6 (0.1)</td><td align="center" valign="bottom"><bold>0.9</bold> (0.2)</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">1.5 (1.0)</td><td align="center" valign="bottom"><bold>4.9</bold> (0.5)</td><td align="center" valign="bottom">90.0 (4.5)</td><td align="center" valign="bottom"><named-content content-type="author-callout-style-a3"><bold>92.0</bold> </named-content>(3.2)</td><td align="center" valign="bottom">79.2 (2.7)</td><td align="center" valign="bottom">2.1 (2.1)</td><td align="center" valign="bottom"><bold>2.9</bold> (0.5)</td><td align="center" valign="bottom">3.4 (2.2)</td></tr><tr><td align="left" valign="bottom">PF00072</td><td align="center" valign="bottom">0.7 (0.1)</td><td align="center" valign="bottom"><bold>0.9</bold> (0.1)</td><td align="center" valign="bottom">0.8 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">2.1 (0.8)</td><td align="center" valign="bottom"><bold>3.8</bold> (0.3)</td><td align="center" valign="bottom">94.5 (3.4)</td><td align="center" valign="bottom"><bold>94.9</bold> (1.9)</td><td align="center" valign="bottom">94.1 (0.5)</td><td align="center" valign="bottom">2.4 (0.3)</td><td align="center" valign="bottom">2.3 (0.1)</td><td align="center" valign="bottom"><bold>2.1</bold> (0.1)</td></tr><tr><td align="left" valign="bottom">PF00076</td><td align="center" valign="bottom">0.6 (0.1)</td><td align="center" valign="bottom">0.8 (0.2)</td><td align="center" valign="bottom">0.8 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">1.5 (0.8)</td><td align="center" valign="bottom"><bold>3.5</bold> (0.2)</td><td align="center" valign="bottom">82.2 (4.4)</td><td align="center" valign="bottom">84.6 (4.8)</td><td align="center" valign="bottom"><bold>87.6</bold> (1.5)</td><td align="center" valign="bottom">1.8 (0.5)</td><td align="center" valign="bottom">1.4 (0.6)</td><td align="center" valign="bottom">1.4 (0.1)</td></tr><tr><td align="left" valign="bottom">PF00096</td><td align="center" valign="bottom">0.8 (0.0)</td><td align="center" valign="bottom">0.9 (0.0)</td><td align="center" valign="bottom">0.9 (0.0)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">2.2 (0.8)</td><td align="center" valign="bottom"><bold>2.8</bold> (0.3)</td><td align="center" valign="bottom">93.0 (2.0)</td><td align="center" valign="bottom"><bold>94.0</bold> (0.8)</td><td align="center" valign="bottom">93.7 (0.2)</td><td align="center" valign="bottom">0.6 (0.1)</td><td align="center" valign="bottom">0.4 (0.1)</td><td align="center" valign="bottom">0.4 (0.0)</td></tr><tr><td align="left" valign="bottom">PF00153</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom"><bold>0.6</bold> (0.1)</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">0.6 (0.8)</td><td align="center" valign="bottom"><bold>2.6</bold> (0.3)</td><td align="center" valign="bottom">65.0 (5.0)</td><td align="center" valign="bottom"><bold>66.6</bold> (6.2)</td><td align="center" valign="bottom">64.9 (4.3)</td><td align="center" valign="bottom">5.1 (1.8)</td><td align="center" valign="bottom">4.4 (1.5)</td><td align="center" valign="bottom">4.3 (1.1)</td></tr><tr><td align="left" valign="bottom">PF00271</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom">0.5 (0.2)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">1.0 (0.9)</td><td align="center" valign="bottom"><bold>2.4</bold> (0.3)</td><td align="center" valign="bottom">78.4 (4.6)</td><td align="center" valign="bottom"><bold>86.4</bold> (5.4)</td><td align="center" valign="bottom">83.8 (2.2)</td><td align="center" valign="bottom">2.0 (0.8)</td><td align="center" valign="bottom">2.3 (0.6)</td><td align="center" valign="bottom"><bold>1.8</bold> (0.1)</td></tr><tr><td align="left" valign="bottom">PF00397</td><td align="center" valign="bottom">0.7 (0.1)</td><td align="center" valign="bottom"><bold>0.8</bold> (0.1)</td><td align="center" valign="bottom">0.8 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">0.5 (0.9)</td><td align="center" valign="bottom"><bold>2.1</bold> (0.2)</td><td align="center" valign="bottom">88.1 (2.2)</td><td align="center" valign="bottom"><bold>88.9</bold> (2.4)</td><td align="center" valign="bottom">88.2 (1.0)</td><td align="center" valign="bottom">0.9 (0.3)</td><td align="center" valign="bottom">0.9 (0.3)</td><td align="center" valign="bottom">0.9 (0.1)</td></tr><tr><td align="left" valign="bottom">PF00512</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom">0.8 (0.2)</td><td align="center" valign="bottom">0.7 (0.2)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">1.5 (1.0)</td><td align="center" valign="bottom"><bold>3.2</bold> (0.3)</td><td align="center" valign="bottom">91.0 (4.0)</td><td align="center" valign="bottom"><bold>90.2</bold> (4.0)</td><td align="center" valign="bottom">89.5 (1.5)</td><td align="center" valign="bottom">2.1 (0.6)</td><td align="center" valign="bottom"><bold>2.2</bold> (0.5)</td><td align="center" valign="bottom">3.1 (0.2)</td></tr><tr><td align="left" valign="bottom">PF00595</td><td align="center" valign="bottom">0.7 (0.1)</td><td align="center" valign="bottom">0.7 (0.1)</td><td align="center" valign="bottom">0.7 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">0.5 (0.9)</td><td align="center" valign="bottom"><bold>2.6</bold> (0.3)</td><td align="center" valign="bottom">93.4 (4.5)</td><td align="center" valign="bottom">94.0 (1.8)</td><td align="center" valign="bottom"><bold>95.1</bold> (0.8)</td><td align="center" valign="bottom">1.8 (0.4)</td><td align="center" valign="bottom">1.7 (0.5)</td><td align="center" valign="bottom"><bold>1.4</bold> (0.2)</td></tr><tr><td align="left" valign="bottom">PF01535</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom"><bold>0.9</bold> (0.2)</td><td align="center" valign="bottom">0.6 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">2.3 (1.1)</td><td align="center" valign="bottom"><bold>4.1</bold> (0.2)</td><td align="center" valign="bottom">82.4 (6.2)</td><td align="center" valign="bottom"><named-content content-type="author-callout-style-a3"><bold>94.3</bold> </named-content>(5.5)</td><td align="center" valign="bottom">77.9 (3.6)</td><td align="center" valign="bottom">1.0 (1.1)</td><td align="center" valign="bottom"><bold>0.4</bold> (0.7)</td><td align="center" valign="bottom">0.5 (0.4)</td></tr><tr><td align="left" valign="bottom">PF02518</td><td align="center" valign="bottom">0.6 (0.2)</td><td align="center" valign="bottom"><bold>0.8</bold> (0.2)</td><td align="center" valign="bottom">0.7 (0.2)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">1.9 (0.9)</td><td align="center" valign="bottom"><bold>3.5</bold> (0.2)</td><td align="center" valign="bottom">88.0 (6.0)</td><td align="center" valign="bottom"><named-content content-type="author-callout-style-a3"><bold>91.0</bold> </named-content>(6.3)</td><td align="center" valign="bottom">73.6 (2.3)</td><td align="center" valign="bottom">4.1 (0.9)</td><td align="center" valign="bottom"><bold>3.9</bold> (0.5)</td><td align="center" valign="bottom">4.7 (1.1)</td></tr><tr><td align="left" valign="bottom">PF07679</td><td align="center" valign="bottom">0.5 (0.1)</td><td align="center" valign="bottom"><bold>0.7</bold> (0.2)</td><td align="center" valign="bottom">0.4 (0.1)</td><td align="center" valign="bottom">0 (1)</td><td align="center" valign="bottom">1.7 (1.0)</td><td align="center" valign="bottom"><bold>5.2</bold> (0.6)</td><td align="center" valign="bottom">93.5 (3.8)</td><td align="center" valign="bottom"><named-content content-type="author-callout-style-a3"><bold>95.3</bold> </named-content>(2.9)</td><td align="center" valign="bottom">89.8 (2.2)</td><td align="center" valign="bottom">1.3 (1.0)</td><td align="center" valign="bottom">1.2 (0.5)</td><td align="center" valign="bottom">1.2 (0.2)</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Comparing different generation methods of MSA Transformer.</title><p>Various scores are shown for the natural MSA of protein family PF00153 and for synthetic MSAs generated in different ways from this family (each synthetic MSA comprises 10,000 sequences). For generation using MSA Transformer (see ‘Using MSA Transformer to generate sequences via an iterative masking procedure’), our standard iterative masking procedure is shown with its default greedy sampling (corresponding to <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>T</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">0</mml:mn></mml:mrow></mml:math></inline-formula>) and two higher temperatures. Variants of the procedure where only the first sequence is masked (‘Context’, either fixed or variable, both with greedy sampling) are also shown. We report the mean Hamming distance to the closest natural sequence, which is not itself in the case of natural sequences (‘Distance’) as well as the mean HMMER and statistical energy scores (‘-Energy’) described in ‘Scoring individual sequences’. Note that statistical energy scores are shifted by the mean value obtained for the natural MSA (which is −235.8). We also report the Pearson correlations between the two- and three-body statistics of the natural and the generated MSAs, denoted, respectively, by <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal">[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo mathvariant="normal">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal">[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>j</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo mathvariant="normal">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (for the natural MSA we report the Pearson correlation between two halves of this MSA), as illustrated in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="3">Score</th><th align="center" valign="bottom" rowspan="3">Natural sequences</th><th align="center" valign="bottom" colspan="5">MSA Transformer</th></tr><tr><th align="center" valign="bottom" colspan="3">Iterative masking</th><th align="center" valign="bottom" colspan="2">Context (greedy)</th></tr><tr><th align="center" valign="bottom">Greedy</th><th align="center" valign="bottom"><inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula></th><th align="center" valign="bottom"><inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula></th><th align="center" valign="bottom">Fixed</th><th align="center" valign="bottom">Variable</th></tr></thead><tbody><tr><td align="left" valign="bottom">Distance</td><td align="center" valign="bottom">0.155</td><td align="center" valign="bottom">0.271</td><td align="center" valign="bottom">0.305</td><td align="center" valign="bottom">0.514</td><td align="center" valign="bottom">0.232</td><td align="center" valign="bottom">0.262</td></tr><tr><td align="left" valign="bottom">HMMER</td><td align="center" valign="bottom">48.0</td><td align="center" valign="bottom">58.2</td><td align="center" valign="bottom">58.1</td><td align="center" valign="bottom">48.4</td><td align="center" valign="bottom">58.7</td><td align="center" valign="bottom">63.8</td></tr><tr><td align="left" valign="bottom">− Energy</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">13.0</td><td align="center" valign="bottom">8.5</td><td align="center" valign="bottom">−42.0</td><td align="center" valign="bottom">−15.4</td><td align="center" valign="bottom">−13.2</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="bottom">0.94</td><td align="center" valign="bottom">0.84</td><td align="center" valign="bottom">0.84</td><td align="center" valign="bottom">0.62</td><td align="center" valign="bottom">0.73</td><td align="center" valign="bottom">0.81</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="bottom">0.89</td><td align="center" valign="bottom">0.80</td><td align="center" valign="bottom">0.76</td><td align="center" valign="bottom">0.41</td><td align="center" valign="bottom">0.66</td><td align="center" valign="bottom">0.77</td></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Impact of regularization strength and sampling temperature on sequence generation by Boltzmann machine DCA (bmDCA), for family PF00072.</title><p>We compare MSAs obtained using bmDCA with different regularization strengths <inline-formula><mml:math id="inf156"><mml:mi>λ</mml:mi></mml:math></inline-formula> (for inference) and sampling temperatures <inline-formula><mml:math id="inf157"><mml:mi>T</mml:mi></mml:math></inline-formula> (for generation) with the natural and the MSA-Transformer–generated MSAs. In each case, we report the average of the Hamming distances of each sequence to their closest natural neighbor, which is not itself in the case of natural sequences (‘Distance’), as well as the effective MSA depth, the scores defined in ‘Scoring individual sequences’, and the Pearson correlation coefficients of the two- and three-body connected correlations computed from natural and generated MSAs (<inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). For MSA Transformer ('MSA Tr.'), bmDCA (0.01,1) and bmDCA (0.001,0.33), we also computed structural scores by feeding the entire synthetic MSA to Alphafold as context MSA (instead of using the natural MSA as context, see ‘Scoring individual sequences’). Structural scores are then very similar to those obtained using natural context.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Type</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf160"><mml:mi>λ</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf161"><mml:mi>T</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom">Distance</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf162"><mml:msubsup><mml:mi>M</mml:mi><mml:mtext>eff</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th align="left" valign="bottom">HMMER</th><th align="left" valign="bottom">- Energy</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></th><th align="left" valign="bottom">pLDDT (%)</th><th align="left" valign="bottom">RMSD (Å)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Natural</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="char" char="." valign="bottom">0.193</td><td align="char" char="." valign="bottom">40,180</td><td align="char" char="." valign="bottom">90.3</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.99</td><td align="char" char="." valign="bottom">0.88</td><td align="char" char="." valign="bottom">93.6</td><td align="char" char="." valign="bottom">2.5</td></tr><tr><td align="left" valign="bottom">MSA Tr.</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="char" char="." valign="bottom">0.348</td><td align="char" char="." valign="bottom">9304</td><td align="char" char="." valign="bottom">119.1</td><td align="char" char="." valign="bottom">59.1</td><td align="char" char="." valign="bottom">0.73</td><td align="char" char="." valign="bottom">0.53</td><td align="char" char="." valign="bottom">94.7</td><td align="char" char="." valign="bottom">2.35</td></tr><tr><td align="left" valign="bottom" colspan="3"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="5">Same as above, with synthetic context:</td><td align="char" char="." valign="bottom">95.1</td><td align="char" char="." valign="bottom">2.37</td></tr><tr><td align="left" valign="bottom">bmDCA</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0.55</td><td align="char" char="." valign="bottom">73,062</td><td align="char" char="." valign="bottom">66.5</td><td align="char" char="." valign="bottom">−37.0</td><td align="char" char="." valign="bottom">0.96</td><td align="char" char="." valign="bottom">0.58</td><td align="char" char="." valign="bottom">84.3</td><td align="char" char="." valign="bottom">2.58</td></tr><tr><td align="left" valign="bottom" colspan="3"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="5">Same as above, with synthetic context:</td><td align="char" char="." valign="bottom">83.9</td><td align="char" char="." valign="bottom">2.70</td></tr><tr><td align="left" valign="bottom">bmDCA</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">0.66</td><td align="char" char="." valign="bottom">0.294</td><td align="char" char="." valign="bottom">18,911</td><td align="char" char="." valign="bottom">101.7</td><td align="char" char="." valign="bottom">92.2</td><td align="char" char="." valign="bottom">0.48</td><td align="char" char="." valign="bottom">0.11</td><td align="char" char="." valign="bottom">94.2</td><td align="char" char="." valign="bottom">2.61</td></tr><tr><td align="left" valign="bottom">bmDCA</td><td align="char" char="." valign="bottom">0.01</td><td align="char" char="." valign="bottom">0.33</td><td align="char" char="." valign="bottom">0.251</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">103.2</td><td align="char" char="." valign="bottom">118.3</td><td align="char" char="." valign="bottom">0.42</td><td align="char" char="." valign="bottom">0.05</td><td align="char" char="." valign="bottom">94.2</td><td align="char" char="." valign="bottom">2.55</td></tr><tr><td align="left" valign="bottom">bmDCA</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0.525</td><td align="char" char="." valign="bottom">73,062</td><td align="char" char="." valign="bottom">86.9</td><td align="char" char="." valign="bottom">−18.3</td><td align="char" char="." valign="bottom">0.97</td><td align="char" char="." valign="bottom">0.63</td><td align="char" char="." valign="bottom">89.7</td><td align="char" char="." valign="bottom">2.44</td></tr><tr><td align="left" valign="bottom">bmDCA</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.66</td><td align="char" char="." valign="bottom">0.296</td><td align="char" char="." valign="bottom">21,294</td><td align="char" char="." valign="bottom">103.9</td><td align="char" char="." valign="bottom">89.3</td><td align="char" char="." valign="bottom">0.48</td><td align="char" char="." valign="bottom">0.19</td><td align="char" char="." valign="bottom">94.3</td><td align="char" char="." valign="bottom">2.6</td></tr><tr><td align="left" valign="bottom">bmDCA</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.33</td><td align="char" char="." valign="bottom">0.274</td><td align="char" char="." valign="bottom">14</td><td align="char" char="." valign="bottom">107.7</td><td align="char" char="." valign="bottom">109.6</td><td align="char" char="." valign="bottom">0.4</td><td align="char" char="." valign="bottom">0.13</td><td align="char" char="." valign="bottom">94.0</td><td align="char" char="." valign="bottom">2.14</td></tr><tr><td align="left" valign="bottom" colspan="3"/><td align="left" valign="bottom"/><td align="left" valign="bottom" colspan="5">Same as above, with synthetic context:</td><td align="char" char="." valign="bottom">94.2</td><td align="char" char="." valign="bottom">2.24</td></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Pfam families and natural MSAs used in our analysis.</title><p><inline-formula><mml:math id="inf165"><mml:mi>L</mml:mi></mml:math></inline-formula> denotes the length of an MSA, <inline-formula><mml:math id="inf166"><mml:mi>M</mml:mi></mml:math></inline-formula> its depth, and <inline-formula><mml:math id="inf167"><mml:msubsup><mml:mi>M</mml:mi><mml:mtext mathvariant="normal">eff</mml:mtext><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mn mathvariant="normal">0.2</mml:mn><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> its effective depth with distance threshold <inline-formula><mml:math id="inf168"><mml:mrow><mml:mi>δ</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">0.2</mml:mn></mml:mrow></mml:math></inline-formula>, see <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>. The reference experimental PDB structures used for our root-mean-squared deviation (RMSD) calculations, and their resolutions (‘Resol.’), are also reported.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Pfam ID</th><th align="left" valign="bottom">Family name</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf169"><mml:mi>L</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf170"><mml:mi>M</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf171"><mml:msubsup><mml:mi>M</mml:mi><mml:mtext>eff</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th align="left" valign="bottom">PDB ID</th><th align="left" valign="bottom">Resol.</th></tr></thead><tbody><tr><td align="left" valign="bottom">PF00004</td><td align="left" valign="bottom">AAA</td><td align="char" char="." valign="bottom">132</td><td align="char" char="." valign="bottom">39,277</td><td align="char" char="." valign="bottom">9049</td><td align="char" char="." valign="bottom">4D81</td><td align="left" valign="bottom">2.40 Å</td></tr><tr><td align="left" valign="bottom">PF00005</td><td align="left" valign="bottom">ABC_tran</td><td align="char" char="." valign="bottom">137</td><td align="char" char="." valign="bottom">68,891</td><td align="char" char="." valign="bottom">43,881</td><td align="char" char="." valign="bottom">1L7V</td><td align="left" valign="bottom">3.20 Å</td></tr><tr><td align="left" valign="bottom">PF00041</td><td align="left" valign="bottom">fn3</td><td align="char" char="." valign="bottom">85</td><td align="char" char="." valign="bottom">42,721</td><td align="char" char="." valign="bottom">17,782</td><td align="char" char="." valign="bottom">3UP1</td><td align="left" valign="bottom">2.15 Å</td></tr><tr><td align="left" valign="bottom">PF00072</td><td align="left" valign="bottom">Response_reg</td><td align="char" char="." valign="bottom">112</td><td align="char" char="." valign="bottom">73,063</td><td align="char" char="." valign="bottom">40,180</td><td align="char" char="." valign="bottom">3ILH</td><td align="left" valign="bottom">2.59 Å</td></tr><tr><td align="left" valign="bottom">PF00076</td><td align="left" valign="bottom">RRM_1</td><td align="char" char="." valign="bottom">69</td><td align="char" char="." valign="bottom">51,964</td><td align="char" char="." valign="bottom">20,273</td><td align="char" char="." valign="bottom">3NNH</td><td align="left" valign="bottom">2.75 Å</td></tr><tr><td align="left" valign="bottom">PF00096</td><td align="left" valign="bottom">zf-C2H2</td><td align="char" char="." valign="bottom">23</td><td align="char" char="." valign="bottom">38,996</td><td align="char" char="." valign="bottom">12,581</td><td align="char" char="." valign="bottom">4R2A</td><td align="left" valign="bottom">1.59 Å</td></tr><tr><td align="left" valign="bottom">PF00153</td><td align="left" valign="bottom">Mito_carr</td><td align="char" char="." valign="bottom">94</td><td align="char" char="." valign="bottom">93,776</td><td align="char" char="." valign="bottom">17,859</td><td align="char" char="." valign="bottom">1OCK</td><td align="left" valign="bottom">2.20 Å</td></tr><tr><td align="left" valign="bottom">PF00271</td><td align="left" valign="bottom">Helicase_C</td><td align="char" char="." valign="bottom">111</td><td align="char" char="." valign="bottom">66,809</td><td align="char" char="." valign="bottom">25,017</td><td align="char" char="." valign="bottom">3EX7</td><td align="left" valign="bottom">2.30 Å</td></tr><tr><td align="left" valign="bottom">PF00397</td><td align="left" valign="bottom">WW</td><td align="char" char="." valign="bottom">31</td><td align="char" char="." valign="bottom">39,045</td><td align="char" char="." valign="bottom">3361</td><td align="char" char="." valign="bottom">4REX</td><td align="left" valign="bottom">1.60 Å</td></tr><tr><td align="left" valign="bottom">PF00512</td><td align="left" valign="bottom">HisKA</td><td align="char" char="." valign="bottom">66</td><td align="char" char="." valign="bottom">154,998</td><td align="char" char="." valign="bottom">67,303</td><td align="char" char="." valign="bottom">3DGE</td><td align="left" valign="bottom">2.80 Å</td></tr><tr><td align="left" valign="bottom">PF00595</td><td align="left" valign="bottom">PDZ</td><td align="char" char="." valign="bottom">82</td><td align="char" char="." valign="bottom">71,303</td><td align="char" char="." valign="bottom">4053</td><td align="char" char="." valign="bottom">1BE9</td><td align="left" valign="bottom">1.82 Å</td></tr><tr><td align="left" valign="bottom">PF01535</td><td align="left" valign="bottom">PPR</td><td align="char" char="." valign="bottom">31</td><td align="char" char="." valign="bottom">109,064</td><td align="char" char="." valign="bottom">37,514</td><td align="char" char="." valign="bottom">4M57</td><td align="left" valign="bottom">2.86 Å</td></tr><tr><td align="left" valign="bottom">PF02518</td><td align="left" valign="bottom">HATPase_c</td><td align="char" char="." valign="bottom">111</td><td align="char" char="." valign="bottom">80,714</td><td align="char" char="." valign="bottom">59,189</td><td align="char" char="." valign="bottom">3G7E</td><td align="left" valign="bottom">2.20 Å</td></tr><tr><td align="left" valign="bottom">PF07679</td><td align="left" valign="bottom">I-set</td><td align="char" char="." valign="bottom">90</td><td align="char" char="." valign="bottom">36,141</td><td align="char" char="." valign="bottom">14,611</td><td align="char" char="." valign="bottom">1FHG</td><td align="left" valign="bottom">2.00 Å</td></tr></tbody></table></table-wrap><table-wrap id="app1table6" position="float"><label>Appendix 1—table 6.</label><caption><title>Other Pfam families and natural MSAs used in our analysis.</title><p><inline-formula><mml:math id="inf172"><mml:mi>L</mml:mi></mml:math></inline-formula> denotes the length of an MSA and <inline-formula><mml:math id="inf173"><mml:mi>M</mml:mi></mml:math></inline-formula> its depth. The reference experimental PDB structures used for our root-mean-squared deviation (RMSD) calculations, and their resolutions, are also reported.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Pfam ID</th><th align="left" valign="bottom">Family name</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf174"><mml:mi>L</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf175"><mml:mi>M</mml:mi></mml:math></inline-formula></th><th align="left" valign="bottom">PDB ID</th><th align="left" valign="bottom">Resol.</th></tr></thead><tbody><tr><td align="left" valign="bottom">PF01356</td><td align="left" valign="bottom">A_amylase_inhib</td><td align="char" char="." valign="bottom">68</td><td align="char" char="." valign="bottom">51</td><td align="char" char="." valign="bottom">1OK0</td><td align="left" valign="bottom">0.93 Å</td></tr><tr><td align="left" valign="bottom">PF03440</td><td align="left" valign="bottom">APT</td><td align="char" char="." valign="bottom">87</td><td align="char" char="." valign="bottom">14</td><td align="char" char="." valign="bottom">6RO0</td><td align="left" valign="bottom">2.13 Å</td></tr><tr><td align="left" valign="bottom">PF04008</td><td align="left" valign="bottom">Adenosine_kin</td><td align="char" char="." valign="bottom">154</td><td align="char" char="." valign="bottom">342</td><td align="char" char="." valign="bottom">1WVQ</td><td align="left" valign="bottom">1.45 Å</td></tr><tr><td align="left" valign="bottom">PF06351</td><td align="left" valign="bottom">Allene_ox_cyc</td><td align="char" char="." valign="bottom">175</td><td align="char" char="." valign="bottom">378</td><td align="char" char="." valign="bottom">2BRJ</td><td align="left" valign="bottom">1.50 Å</td></tr><tr><td align="left" valign="bottom">PF06355</td><td align="left" valign="bottom">Aegerolysin</td><td align="char" char="." valign="bottom">131</td><td align="char" char="." valign="bottom">322</td><td align="char" char="." valign="bottom">6MYI</td><td align="left" valign="bottom">1.15 Å</td></tr><tr><td align="left" valign="bottom">PF16747</td><td align="left" valign="bottom">Adhesin_E</td><td align="char" char="." valign="bottom">125</td><td align="char" char="." valign="bottom">31</td><td align="char" char="." valign="bottom">6GUT</td><td align="left" valign="bottom">1.63 Å</td></tr><tr><td align="left" valign="bottom">PF18648</td><td align="left" valign="bottom">ADPRTs_Tse2</td><td align="char" char="." valign="bottom">155</td><td align="char" char="." valign="bottom">9</td><td align="char" char="." valign="bottom">5AKO</td><td align="left" valign="bottom">2.40 Å</td></tr><tr><td align="left" valign="bottom">PF13354</td><td align="left" valign="bottom">Beta-lactamase2</td><td align="char" char="." valign="bottom">198</td><td align="char" char="." valign="bottom">4642</td><td align="char" char="." valign="bottom">6QW8</td><td align="left" valign="bottom">1.10 Å</td></tr><tr><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Chorismate mutase <xref ref-type="bibr" rid="bib75">Russ et al., 2020</xref></td><td align="char" char="." valign="bottom">96</td><td align="char" char="." valign="bottom">1130</td><td align="char" char="." valign="bottom">1ECM</td><td align="left" valign="bottom">2.20 Å</td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79854.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Colwell</surname><given-names>Lucy J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Cambridge University</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.04.14.488405" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.14.488405"/></front-stub><body><p>This important study proposes a method to sample novel sequences from a protein language model that could have exciting applications in protein sequence design. The claims are supported by a solid benchmarking of the designed sequences in terms of quality, novelty and diversity.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79854.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Colwell</surname><given-names>Lucy J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Cambridge University</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.14.488405">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.04.14.488405v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Generative power of a protein language model trained on multiple sequence alignments&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by Lucy Colwell as Reviewing Editor and José Faraldo-Gómez as Senior Editor. All reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>(1) Please address the technical points raised by each referee below, paying particular attention to those around the generalizability of hyperparameter choices, the training of bmDCA, and the comparison between bmDCA and the MSA transformer and the sampling temperatures used, and the evaluation of sample diversity.</p><p>(2) Both reviewers note the lack of experimental validation. Adding some level of experimental validation of the proposed method would significantly improve the manuscript. If this is not possible, an alternative option might be to build a retrospective model evaluation using previously published protein design experimental datasets.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Sampling hyperparameters:</p><p>Figures S1/2 convincingly motivate the choice of parameters for one protein family, but it is unclear whether these would generalize well for other proteins. In particular, it is unclear whether the number of iterations should be the same across protein sizes.</p><p>bmDCA training:</p><p>In figure 3 middle left panel (comparing MSA covariance versus bmDCA-generated covariance) shows a fairly small regression line slope (0.67). This suggests too large a regularization on couplings and/or an insufficient number of iterations. Have you checked that the algorithm successfully converged, or was the maximum number of iterations reached? What about using a smaller regularization strength (two values were used in Russ et al. Science 2020, one small and one large)? I know that parameter tuning is a weakness of bmDCA, but the proposed method also has free parameters and it is not clear whether they are universal for all protein lengths. This could also explain the discrepancy in the topology of the sequence distribution shown in Figure 5 (weak couplings cannot create &quot;ferromagnetic&quot; multimodal distributions).</p><p>Evaluation of sequence quality:</p><p>I feel that the comparison of Figure 1 is a bit unfair. On the one hand, the proposed MSA-Transformer sampling method generates samples from the T=0 conditional distribution (according to step 3, the bottom of p9), whereas the bmDCA generates samples from the T=1 distribution. It is known that samples from the T=1 distribution have worse statistical energy scores than natural sequences due to the regularization of fields/couplings and that this must be adequately compensated by low-temperature sampling (Russ et al. Science 2020). The authors argue p13 that low-temperature sampling results in distortion of the first and second-order moments of the data, but I think this is not a practical problem for protein design (by definition, an optimal design protocol will discard all suboptimal amino acids for a given position and only retain the most favorable ones).</p><p>The comparison of AlphaFold confidence scores between MSAtransformer-generated and natural sequences is interesting but the results are not discussed at all in the main text. Is there a statistically significant difference between the distributions? Moreover, it is not shown here whether the predicted structures are similar to the native ones. Please provide a fold similarity metric such as backbone RMSD values.</p><p>Evaluation of sample diversity:</p><p>The authors convincingly show that sequences generated by MSAtransformer differ substantially from natural sequences. However, given that the MSA-Transformer samples are essentially obtained by zero-temperature dynamics, it is important to assess how diverse are the generated sequences. This is partially addressed in Figure S5, but consider using simpler metrics. Can the authors provide basic estimates of sample diversity? (e.g., effective number of sequences as a function of the number of samples).</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I have a series of questions that if resolved could help, in my opinion, to create an improved version of this interesting study.</p><p>1. The manuscript describes how MSA Transformers could lead to metrics that are &quot;even better&quot; than the natural sequences. I feel that this statement is a bit misleading as the natural sequences are a baseline to compare. There are no better properties than those of the natural sequences because that is what we observe in nature. I would suggest removing those statements as they might produce confusion. On the other hand, if the authors would provide examples of sequences that are &quot;better&quot; in terms of function, stability, or any other quantifiable metric, then I would agree that these statements would make more sense.</p><p>2. The authors mention methods that can be trained in an unsupervised way to predict structure based on a single sequence. I wonder if this statement can be clarified, as it is obvious that those methods use several sequences as training. Are these methodologies based on physical principles instead of learning from sequences?</p><p>3. It is not clear to me what is the importance of the correlation between HMMER and Hamming distance, the authors should provide more intuition on why this is a relevant metric. Since those correlations are quite low for both models I am not sure if it contributes to the analysis in a meaningful way.</p><p>4. Are the contact maps created from the generated MSA of Transformer better than those of bmDCA? My understanding is that the value of p=0.1 optimizes contact map accuracy, but how are those compared against DCA maps?</p><p>5. Haldane et al. (https://doi.org/10.1016/j.bpj.2017.10.028) has proposed how pairwise statistics seem to be enough to recapitulate higher order mutational patterns in natural sequences. Could the authors comment on this, and mention if there is a substantial advantage in capturing 3-body statistics in the process of protein design or generative modeling?</p><p>6. It would be interesting to see how Alphafold would perform if some of these generated MSAs are used as input. I don't think it is needed to test it with all the families, but it would be an interesting experiment for a single family.</p><p>7. The families used in this study appear to have enough statistics to perform well, this is reasonable, however, for the sake of comparison, it would be interesting to see how the MSA Transformer would compare against bmDCA for a family with just a few sequence members. Are the trends the same? Or do we see a change in performance between the two generative methodologies?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79854.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>(1) Please address the technical points raised by each referee below, paying particular attention to those around the generalizability of hyperparameter choices, the training of bmDCA, and the comparison between bmDCA and the MSA transformer and the sampling temperatures used, and the evaluation of sample diversity.</p></disp-quote><p>We thank the editor for highlighting the technical points raised by the reviewers. We have treated all of these points thoroughly. Each of them is explained below in our point-by-point response to each reviewer. Here is a brief summary of how we addressed these points.</p><p>– Regarding the generalizability of hyperparameter choices, the values we chose yielded satisfactory convergence of MSA properties and preservation of contact maps for all families considered. This holds true in our revised version, where we considered 9 additional protein families (listed in Table 6), with various lengths and depths. In this context, we chose not to engage in family-specific parameter tuning. We now illustrate this robustness better, in two new figures (Figure 7—figure supplements 2 and 3).</p><p>– About the training of bmDCA, we did our best to reproduce the published bmDCA results, and to train bmDCA as in the original papers. In our response to reviewer 1 below, we show quantitative comparisons of the fitting performance of two-body correlations between our bmDCA results and published ones.</p><p>– About the comparison between bmDCA and MSA Transformer and the sampling temperatures used, we have now performed a comprehensive comparison of our</p><p>MSA-Transformer–generated data not only to bmDCA-generated data at sampling temperature T=1, but also at lower sampling temperatures and regularization strengths, following [Russ et al. 2020]. We also now provide an analysis of the statistical significance of the difference between the distributions of all scores in our various generated datasets in Table 1, using the Kolmogorov-Smirnov test, and in Table 2, focusing on the median and standard deviation of scores.</p><p>– Regarding the evaluation of sample diversity, we now provide a study of the effective MSA depth in Figure 1—figure supplement 1, and discuss this point thoroughly.</p><disp-quote content-type="editor-comment"><p>(2) Both reviewers note the lack of experimental validation. Adding some level of experimental validation of the proposed method would significantly improve the manuscript. If this is not possible, an alternative option might be to build a retrospective model evaluation using previously published protein design experimental datasets.</p></disp-quote><p>We thank the editor and the reviewers for these very relevant suggestions. While we were not in a position to conduct experiments, we performed a retrospective model evaluation using previously published protein design experimental datasets, and added a new section about this at the end of Results. There, we present a detailed comparison of our generated sequences to those experimentally validated in [Russ et al. 2020], and a calculation of deep mutational scanning scores for two other protein families. The corresponding data is shown in our new Figure 6 and Figure 6—figure supplements 1 and 2.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Sampling hyperparameters:</p><p>Figures S1/2 convincingly motivate the choice of parameters for one protein family, but it is unclear whether these would generalize well for other proteins. In particular, it is unclear whether the number of iterations should be the same across protein sizes.</p></disp-quote><p>We thank the reviewer for raising this important question. We agree that optimal hyperparameters could potentially depend on protein family characteristics. Throughout our work, we used the following hyperparameter values: masking probability p=0.1 and number of iterations I=200, because they worked well throughout. To illustrate this robustness better, we produced two new figures (Figure 7—figure supplement 2 and Figure 7—figure supplement 3) similar to Figure S1 (now Figure 7—figure supplement 1) for two other protein families, namely PF00096 and PF13354, which are respectively the shortest (L=23) and the longest (L=198) considered in our study. We also added a paragraph in Methods that discusses this point:</p><p>&quot;The behaviors observed in Figure 7—figure supplement 1 for PF00153 are generic across the protein families we studied, as can be seen in Figure 7—figure supplement 2 and Figure 7—figure supplement 3, which show the same data as in Figure 7—figure supplement 1 for Pfam families PF00096 and PF13354 (which have different sequence lengths). This demonstrates that our sequence generation method is robust. In particular, as the parameters p = 0.1 and I = 200 yield satisfactory convergence of MSA properties and preservation of contact maps in all cases, we used these parameters throughout, without any family-specific fine-tuning.&quot;</p><disp-quote content-type="editor-comment"><p>bmDCA training:</p><p>In figure 3 middle left panel (comparing MSA covariance versus bmDCA-generated covariance) shows a fairly small regression line slope (0.67). This suggests too large a regularization on couplings and/or an insufficient number of iterations. Have you checked that the algorithm successfully converged, or was the maximum number of iterations reached? What about using a smaller regularization strength (two values were used in Russ et al. Science 2020, one small and one large)? I know that parameter tuning is a weakness of bmDCA, but the proposed method also has free parameters and it is not clear whether they are universal for all protein lengths. This could also explain the discrepancy in the topology of the sequence distribution shown in Figure 5 (weak couplings cannot create &quot;ferromagnetic&quot; multimodal distributions).</p></disp-quote><p>We did our best to reproduce the published bmDCA results, and to train bmDCA as in published papers. Our baseline fitting method employs the default parameters of [Figliuzzi et al. 2018] and uses the exact same convergence criteria, and our sampling method employs the equilibration time determined there. We chose to employ the default parameters to allow for direct comparison with the results of [Figliuzzi et al. 2018]. Below, we show quantitative comparisons of the fitting performance of two-body correlations between our bmDCA results and published ones.</p><p>More generally, we agree with the reviewer about the importance of regularization strength and temperature in bmDCA, as reported in [Russ et al. 2020]. Throughout our revised manuscript, we now present results with parameter values used in [Russ et al. 2020], in addition to our results with default parameters. We explain this important point in more detail in our response to the next reviewer question.</p><p>Details about bmDCA fitting quality:</p><p>We checked convergence and we double-checked that our results are consistent with these published results and those of [Trinquier et al. 2021]. In particular, the Pearson correlation values between the pairwise correlations in the natural and bmDCA-generated data are consistent with those reported in [Figliuzzi et al. 2018] and [Trinquier et al. 2021]:</p><table-wrap id="sa2table1" position="float"><label>Author response table 1.</label><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Family</th><th valign="bottom">Pearson (Figliuzzi/Trinquier)</th><th valign="bottom">Pearson (ours)</th></tr></thead><tbody><tr><td align="left" valign="bottom">PF00004</td><td align="char" char="." valign="bottom">0.95/-</td><td align="char" char="." valign="bottom">0.98</td></tr><tr><td align="left" valign="bottom">PF00005</td><td align="char" char="." valign="bottom">0.95/-</td><td align="char" char="." valign="bottom">0.96</td></tr><tr><td align="left" valign="bottom">PF00041</td><td align="char" char="." valign="bottom">0.97/-</td><td align="char" char="." valign="bottom">0.96</td></tr><tr><td align="left" valign="bottom">PF00072</td><td align="char" char="." valign="bottom">0.98/0.93</td><td align="char" char="." valign="bottom">0.96</td></tr><tr><td align="left" valign="bottom">PF00076</td><td align="char" char="." valign="bottom">0.98/0.97</td><td align="char" char="." valign="bottom">0.95</td></tr><tr><td align="left" valign="bottom">PF00096</td><td align="char" char="." valign="bottom">0.99/-</td><td align="char" char="." valign="bottom">0.93</td></tr><tr><td align="left" valign="bottom">PF00153</td><td align="char" char="." valign="bottom">0.97/-</td><td align="char" char="." valign="bottom">0.92</td></tr><tr><td align="left" valign="bottom">PF00595</td><td align="char" char="." valign="bottom">-/0.97</td><td align="char" char="." valign="bottom">0.93</td></tr><tr><td align="left" valign="bottom">PF01535</td><td align="char" char="." valign="bottom">0.99/-</td><td align="char" char="." valign="bottom">0.98</td></tr><tr><td align="left" valign="bottom">PF02518</td><td align="char" char="." valign="bottom">0.97/-</td><td align="char" char="." valign="bottom">0.97</td></tr><tr><td align="left" valign="bottom">PF07679</td><td align="char" char="." valign="bottom">0.95/-</td><td align="char" char="." valign="bottom">0.96</td></tr></tbody></table></table-wrap><p>As can be observed from this table, our results are similar to the published ones, and the range of differences between our results and published ones appears comparable to that between the results of two papers by the same group.</p><p>Regarding the slope of 0.67 mentioned by the reviewer for PF00153 (now shown in Figure 4—figure supplement 3), unfortunately, we cannot compare it directly to previous results, because the value of this slope was not reported in previous works. However, there are 3 other families for which this slope is explicitly reported in [Trinquier et al. 2021], and here is the comparison in this case:</p><table-wrap id="sa2table2" position="float"><label>Author response table 2.</label><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Family</th><th valign="bottom">Slope (Trinquier)</th><th valign="bottom">Slope (ours)</th></tr></thead><tbody><tr><td align="left" valign="bottom">PF00072</td><td align="char" char="." valign="bottom">0.74</td><td align="char" char="." valign="bottom">0.88</td></tr><tr><td align="left" valign="bottom">PF00076</td><td align="char" char="." valign="bottom">0.92</td><td align="char" char="." valign="bottom">0.82</td></tr><tr><td align="left" valign="bottom">PF00595</td><td align="char" char="." valign="bottom">0.88</td><td align="char" char="." valign="bottom">0.75</td></tr></tbody></table></table-wrap><p>From these three examples, our slopes do not seem further from one than in [Trinquier et al. 2021].</p><disp-quote content-type="editor-comment"><p>Evaluation of sequence quality:</p><p>I feel that the comparison of Figure 1 is a bit unfair. On the one hand, the proposed MSA-Transformer sampling method generates samples from the T=0 conditional distribution (according to step 3, the bottom of p9), whereas the bmDCA generates samples from the T=1 distribution. It is known that samples from the T=1 distribution have worse statistical energy scores than natural sequences due to the regularization of fields/couplings and that this must be adequately compensated by low-temperature sampling (Russ et al. Science 2020). The authors argue p13 that low-temperature sampling results in distortion of the first and second-order moments of the data, but I think this is not a practical problem for protein design (by definition, an optimal design protocol will discard all suboptimal amino acids for a given position and only retain the most favorable ones).</p></disp-quote><p>We thank both reviewers for raising this very interesting point. We have now performed a comprehensive comparison of our MSA-Transformer–generated data not only to bmDCA-generated data at sampling temperature T=1 but also at lower sampling temperatures. We considered the two temperature values chosen in [Russ et al. 2020], namely T=0.33 and T=0.66. For completeness, we also considered the two values of regularization strength λ from [Russ et al. 2020] for these three temperatures, in the case of family PF00072, as reported in Table 4. Given the relatively small impact of λ observed there, we kept only one value of λ for each value of T in the rest of our manuscript – namely, λ=0.01 for T=1 to match the parameters in [Figliuzzi et al. 2018], and λ=0.001 for T=0.33 and T=0.66 as it gave slightly better scores in Table 4. Note that for our additional study of small protein families (see below), we employed λ=0.01 throughout because it is better suited to small families.</p><p>In particular, to make Figure 1 fairer, as per the reviewer's remark, we now include results obtained for bmDCA at λ=0.001 and T=0.33 in this figure. For completeness, we also include them in all other figures of the revised manuscript. Results for T=0.66 were quite similar to those obtained for T=0.33, and we also report them in Table 4 and Figure 5—figure supplements 1-2 for completeness.</p><p>Our general findings, which are discussed in the revised manuscript, are that decreasing T indeed improves the scores of bmDCA-generated sequences. However, the main improvement regards statistical energy (as expected from lowering T), while the improvements of other scores (HMMER score, and, more importantly, structural scores) are more modest. Even using T=0.33 for bmDCA, our MSA-Transformer–generated sequences have similar or better scores compared to bmDCA-generated sequences, apart from statistical energy (see Figure 1 and Table 1 and 1c). Moreover, we find that decreasing T with bmDCA substantially decreases MSA diversity, while MSA-Transformer–generated sequences do not suffer from such an issue (see Figure 1—figure supplement 1). In fact, at low T, bmDCA concentrates on local minima of the statistical energy landscape (see Figures 2, 5 and Figure 5—figure supplements 1-2), resulting in low diversity.</p><p>Overall, these new results confirm that our procedure for generating sequences using MSA Transformer is promising, featuring scores at least comparable with low-temperature bmDCA sequences, and high diversity.</p><disp-quote content-type="editor-comment"><p>The comparison of AlphaFold confidence scores between MSAtransformer-generated and natural sequences is interesting but the results are not discussed at all in the main text. Is there a statistically significant difference between the distributions? Moreover, it is not shown here whether the predicted structures are similar to the native ones. Please provide a fold similarity metric such as backbone RMSD values.</p></disp-quote><p>We thank the reviewer for these important remarks. We now present an analysis of the statistical significance of the difference between the distributions of all scores in the various generated datasets in Table 1, using the Kolmogorov-Smirnov test, and we also compare the median and standard deviation of all scores in the natural and generated datasets in Table 2. We revised the discussion in the main text accordingly.</p><p>In addition, we agree that checking whether the predicted structures are similar to the native ones is an important test that goes beyond the AlphaFold pLDDT. We therefore added an additional score throughout our manuscript, which is the RMSD between a reference experimental structure of the family (see Table 5) and the AlphaFold structure predicted for each sequence studied. The results from the RMSD analysis corroborate those obtained with pLDDT and show that predicted structures are indeed similar to the native ones. These results are now discussed in the main text. We believe that this point strengthens our conclusions, and we thank the reviewer for suggesting this analysis.</p><disp-quote content-type="editor-comment"><p>Evaluation of sample diversity:</p><p>The authors convincingly show that sequences generated by MSAtransformer differ substantially from natural sequences. However, given that the MSA-Transformer samples are essentially obtained by zero-temperature dynamics, it is important to assess how diverse are the generated sequences. This is partially addressed in Figure S5, but consider using simpler metrics. Can the authors provide basic estimates of sample diversity? (e.g., effective number of sequences as a function of the number of samples).</p></disp-quote><p>We thank the reviewer for asking this interesting question. We added a new supplementary figure, Figure 1—figure supplement 1, to address this point. In this figure, we show the effective number of sequences Meff for the MSAs we generated, versus the similarity threshold employed to define Meff. We find that the sequence diversity of MSA-Transformer–generated MSAs is slightly smaller than that of the natural MSAs, but remains of the same order of magnitude. Therefore, the method we propose to generate sequences using MSA Transformer preserves diversity to a large extent, despite using zero-temperature dynamics. This is probably enabled by the fact that we start from an MSA and not from a single sequence, and that MSA</p><p>Transformer uses the whole MSA as context.</p><p>Conversely, Figure 1—figure supplement 1 shows that low-temperature bmDCA sampling leads to substantially reduced Meff values, consistently with the idea that only the local minima of the energy landscape are then sampled. More precisely, we observe a regular increase of Meff with the similarity threshold for MSA-Transformer–generated data as well as for natural data, while the increase is more stepwise for low-temperature bmDCA.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I have a series of questions that if resolved could help, in my opinion, to create an improved version of this interesting study.</p><p>1. The manuscript describes how MSA Transformers could lead to metrics that are &quot;even better&quot; than the natural sequences. I feel that this statement is a bit misleading as the natural sequences are a baseline to compare. There are no better properties than those of the natural sequences because that is what we observe in nature. I would suggest removing those statements as they might produce confusion. On the other hand, if the authors would provide examples of sequences that are &quot;better&quot; in terms of function, stability, or any other quantifiable metric, then I would agree that these statements would make more sense.</p></disp-quote><p>We agree with the reviewer that natural sequences are the reference here. We changed the wording and we no longer claim that generated sequences have &quot;better scores&quot;, in order to avoid any confusion about this.</p><disp-quote content-type="editor-comment"><p>2. The authors mention methods that can be trained in an unsupervised way to predict structure based on a single sequence. I wonder if this statement can be clarified, as it is obvious that those methods use several sequences as training. Are these methodologies based on physical principles instead of learning from sequences?</p></disp-quote><p>We agree that this statement was a bit misleading, and we thank the reviewer for pointing it out. We rephrased it to clarify that these models, which are trained on a large database of sequences, are then able to predict the structure taking as input either a single sequence (ESM1b and now ESM2) or an MSA (MSA Transformer). The corresponding sentence now reads:</p><p>“These pre-trained models are able to predict structure in an unsupervised way [21], either taking as input a single sequence [20] or an MSA [25], potentially by transferring knowledge from their large training set [26, 27].”</p><disp-quote content-type="editor-comment"><p>3. It is not clear to me what is the importance of the correlation between HMMER and Hamming distance, the authors should provide more intuition on why this is a relevant metric. Since those correlations are quite low for both models I am not sure if it contributes to the analysis in a meaningful way.</p></disp-quote><p>We thank the reviewer for raising this important point. One may have feared that generated sequences that are most similar to natural ones might always have higher scores, which would show that the model has difficulty to generalize and may be overfitting. The fact that HMMER scores tend to be positively correlated with Hamming distances for sequences generated using our method based on MSA Transformer is one of several pieces of evidence that this is not the case. However, we agree that our detailed discussion of the correlations between HMMER scores and Hamming distances gave too much emphasis to this point. Therefore, we have now strongly reduced this discussion.</p><disp-quote content-type="editor-comment"><p>4. Are the contact maps created from the generated MSA of Transformer better than those of bmDCA? My understanding is that the value of p=0.1 optimizes contact map accuracy, but how are those compared against DCA maps?</p></disp-quote><p>It was shown in the original MSA Transformer paper, [Rao et al. 2021], that MSA Transformer outperforms DCA at unsupervised contact prediction on natural data. In addition, in Figure 7—figure supplements 1 to 4, we show that for small values of the masking probability p (e.g. p=0.1), the contact maps inferred by MSA Transformer from our MSAs generated using our iterative masking procedure based on MSA Transformer reproduce quite well those of natural MSAs of the same protein family. The goal here was to check that MSA Transformer was remaining within the protein family of focus when using our generation method. We now clarified this point.</p><p>In addition, for the protein family PF00072, we now also used fully synthetic MSAs (either generated using our method based on MSA Transformer or generated by bmDCA) as input to AlphaFold (Table 4). We find that structural scores (both pLDDT and RMSD) are fully in line with those of natural data, both for our method based on MSA Transformer and for low-temperature bmDCA, while bmDCA at T=1 gives poorer results.</p><disp-quote content-type="editor-comment"><p>5. Haldane et al. (https://doi.org/10.1016/j.bpj.2017.10.028) has proposed how pairwise statistics seem to be enough to recapitulate higher order mutational patterns in natural sequences. Could the authors comment on this, and mention if there is a substantial advantage in capturing 3-body statistics in the process of protein design or generative modeling?</p></disp-quote><p>We thank the reviewer for pointing out this interesting study that we now cite. We took inspiration from it, and from the later study [McGee et al. 2021] by some of the same authors, and investigated higher-order statistics in natural and synthetic MSAs (beyond 2- and 3-body ones that were already studied in the first version of our manuscript). Our new Figure 4 shows the r20 score that quantifies the similarity of statistics at various orders between two datasets. It also presents as reference an assumption-free null model, namely the r20 score between two subsets of each natural MSA. As now mentioned in the main text,</p><p>&quot;Figure 4 shows that bmDCA with default parameters is most often the best method at reproducing lower-order statistics, but that MSA Transformer is the best at reproducing higher-order statistics, in all families considered.&quot;</p><p>While pairwise models are extremely successful at modeling the statistics of protein MSAs and at making predictions e.g. about their structure, interactions, and mutational effects, this does not necessarily imply that higher-order statistics are always fully captured by these models. Besides, the training of these models is difficult and subject to finite size issues, as discussed e.g. in [McGee et al. 2021]. We note that in the context of neuroscience [Meshulam et al. 2021], a better agreement is obtained on 3-body correlations using pairwise maximum entropy models.</p><p>We now mention this point: &quot;Thus, bmDCA is trained to reproduce these frequencies, and achieves these objectives quite well, although the comparison to the null model in Figure 4—figure supplement 2 and Figure 4—figure supplement 3 hints that further improvements remain possible, see [51].&quot; Either way, here, we take these results as a pragmatic indication that lower-order statistics are better reproduced by bmDCA while higher-order statistics are better reproduced by our method based on MSA Transformer.</p><disp-quote content-type="editor-comment"><p>6. It would be interesting to see how Alphafold would perform if some of these generated MSAs are used as input. I don't think it is needed to test it with all the families, but it would be an interesting experiment for a single family.</p></disp-quote><p>We thank the reviewer for this interesting suggestion. As mentioned above, for the protein family PF00072, we now used fully synthetic MSAs (either generated using our method based on MSA Transformer or generated by bmDCA) as input to AlphaFold (Table 4). We find that structural scores (both pLDDT and RMSD) are then very similar to those obtained using natural context (which is our usual procedure). They are also fully in line with those of natural data for our method based on MSA Transformer, illustrating the robustness of the good structural scores of our synthetic sequences. We have added the sentence:</p><p>“In addition, for the protein family PF00072, we also used fully synthetic MSAs as input to AlphaFold. Structural scores are then very similar to those obtained using natural context (see Table 4)”.</p><disp-quote content-type="editor-comment"><p>7. The families used in this study appear to have enough statistics to perform well, this is reasonable, however, for the sake of comparison, it would be interesting to see how the MSA Transformer would compare against bmDCA for a family with just a few sequence members. Are the trends the same? Or do we see a change in performance between the two generative methodologies?</p></disp-quote><p>We thank the reviewer for asking this extremely interesting question. We now present new results for smaller protein families, listed in Table 6, whose shallow MSAs make it more challenging to accurately fit Potts models. Our results are shown in the new Figure 3, and discussed in the main text, in a new section titled &quot;Sequence generation by the iterative masking procedure is successful for small protein families&quot; at the end of Results. As mentioned there,</p><p>“We observe that MSA-Transformer–generated sequences have similar HMMER scores and structural scores to natural sequences. MSA-Transformer–generated sequences also generally have better HMMER scores and structural scores than those generated by bmDCA with default parameters. While low-temperature bmDCA yields better statistical energy scores (as expected), and also gives HMMER scores and structural scores comparable to natural sequences, it in fact generates sequences that are almost exact copies of natural ones. By contrast, MSA Transformer produces sequences that are quite different from natural ones, and have very good scores. Thus, our method based on MSA Transformer is particularly promising in the tricky case of small protein families.”</p><p>This analysis shows that our method not only performs as well as bmDCA for large families, but also has a broader scope, as it is less limited by MSA depth than bmDCA. We believe that this makes our manuscript stronger, and we thank the reviewer again for suggesting this very relevant additional study.</p></body></sub-article></article>