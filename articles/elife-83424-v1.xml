<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83424</article-id><article-id pub-id-type="doi">10.7554/eLife.83424</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Precise control of neural activity using dynamically optimized electrical stimulation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Shah</surname><given-names>Nishal Pradeepbhai</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1275-0381</contrib-id><email>bhaishahster@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Phillips</surname><given-names>AJ</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9109-6551</contrib-id><email>andrewjp@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Madugula</surname><given-names>Sasidhar</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lotlikar</surname><given-names>Amrith</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Gogliettino</surname><given-names>Alex R</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hays</surname><given-names>Madeline Rose</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Grosberg</surname><given-names>Lauren</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Brown</surname><given-names>Jeff</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Dusi</surname><given-names>Aditya</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tandon</surname><given-names>Pulkit</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hottowy</surname><given-names>Pawel</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Dabrowski</surname><given-names>Wladyslaw</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Sher</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Litke</surname><given-names>Alan M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3973-3642</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Mitra</surname><given-names>Subhasish</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con15"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5613-0248</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund12"/><xref ref-type="other" rid="fund13"/><xref ref-type="fn" rid="con16"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Electrical Engineering</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Neurosurgery</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Hansen Experimental Physics Laboratory, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Neurosciences PhD Program</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Bioengineering</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00bas1c41</institution-id><institution>AGH University of Science and Technology, Faculty of Physics and Applied Computer Science</institution></institution-wrap><addr-line><named-content content-type="city">Krakow</named-content></addr-line><country>Poland</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03s65by71</institution-id><institution>Santa Cruz Institute for Particle Physics, University of California, Santa Cruz, CA</institution></institution-wrap><addr-line><named-content content-type="city">Santa Cruz</named-content></addr-line><country>United States</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Ophthalmology</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Beyeler</surname><given-names>Michael</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>07</day><month>11</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e83424</elocation-id><history><date date-type="received" iso-8601-date="2022-09-13"><day>13</day><month>09</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-07-15"><day>15</day><month>07</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-07-28"><day>28</day><month>07</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.26.501643"/></event></pub-history><permissions><copyright-statement>© 2024, Shah, Phillips et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Shah, Phillips et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83424-v1.pdf"/><abstract><p>Neural implants have the potential to restore lost sensory function by electrically evoking the complex naturalistic activity patterns of neural populations. However, it can be difficult to predict and control evoked neural responses to simultaneous multi-electrode stimulation due to nonlinearity of the responses. We present a solution to this problem and demonstrate its utility in the context of a bidirectional retinal implant for restoring vision. A dynamically optimized stimulation approach encodes incoming visual stimuli into a rapid, greedily chosen, temporally dithered and spatially multiplexed sequence of simple stimulation patterns. Stimuli are selected to optimize the reconstruction of the visual stimulus from the evoked responses. Temporal dithering exploits the slow time scales of downstream neural processing, and spatial multiplexing exploits the independence of responses generated by distant electrodes. The approach was evaluated using an experimental laboratory prototype of a retinal implant: large-scale, high-resolution multi-electrode stimulation and recording of macaque and rat retinal ganglion cells ex vivo. The dynamically optimized stimulation approach substantially enhanced performance compared to existing approaches based on static mapping between visual stimulus intensity and current amplitude. The modular framework enabled parallel extensions to naturalistic viewing conditions, incorporation of perceptual similarity measures, and efficient implementation for an implantable device. A direct closed-loop test of the approach supported its potential use in vision restoration.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>sensory prosthesis</kwd><kwd>electrical stimulation</kwd><kwd>brain–computer interface</kwd><kwd>retina</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd><kwd>Long-Evans rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000971</institution-id><institution>ALS Association</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Shah</surname><given-names>Nishal Pradeepbhai</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation Graduate Research Fellowship</institution></institution-wrap></funding-source><award-id>2146755</award-id><principal-award-recipient><name><surname>Phillips</surname><given-names>AJ</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008982</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1828993</award-id><principal-award-recipient><name><surname>Phillips</surname><given-names>AJ</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>F30-EY-030776-03</award-id><principal-award-recipient><name><surname>Madugula</surname><given-names>Sasidhar</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>T32MH-020016</award-id><principal-award-recipient><name><surname>Gogliettino</surname><given-names>Alex R</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>F31-EY-033636</award-id><principal-award-recipient><name><surname>Gogliettino</surname><given-names>Alex R</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009152</institution-id><institution>The Fondation Bertarelli</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gogliettino</surname><given-names>Alex R</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>The Stanford Neurosciences Graduate Program</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gogliettino</surname><given-names>Alex R</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004382</institution-id><institution>Polish Academy of Sciences</institution></institution-wrap></funding-source><award-id>DEC-2013/10/M/NZ4/00268</award-id><principal-award-recipient><name><surname>Hottowy</surname><given-names>Pawel</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01-EY021271</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01-EY029247</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>P30-EY019005</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund13"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF/CRCNS</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel method of electrical stimulation to precisely control neural activity for sensory restoration exhibits improvements in visual stimulus reconstruction, enables efficient hardware design, and extends to naturalistic conditions.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A major goal of sensory neuroscience is to leverage our understanding of neural circuits to develop implantable devices that can artificially control neural activity for restoring senses such as vision (<xref ref-type="bibr" rid="bib31">Humayun et al., 2012</xref>; <xref ref-type="bibr" rid="bib61">Stingl et al., 2013</xref>; <xref ref-type="bibr" rid="bib47">Palanker et al., 2020</xref>; <xref ref-type="bibr" rid="bib2">Beauchamp et al., 2020</xref>; <xref ref-type="bibr" rid="bib10">Chen et al., 2020</xref>), audition (<xref ref-type="bibr" rid="bib22">Gaylor et al., 2013</xref>), and somatosensation (<xref ref-type="bibr" rid="bib34">Johnson et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Flesher et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Salas et al., 2018</xref>). Recent innovations in large-scale and high-resolution electrical stimulation hardware hold great promise for such applications. However, the effect of stimulation with many closely spaced electrodes on neural activity is generally complex and nonlinear, which severely limits the ability to produce the desired spatiotemporal patterns of neural activity for sensory restoration.</p><p>This paper presents a novel approach to this problem in the context of an epiretinal implant for restoring vision in people blinded by photoreceptor degeneration (<xref ref-type="bibr" rid="bib31">Humayun et al., 2012</xref>; <xref ref-type="bibr" rid="bib5">Beyeler et al., 2019</xref>; <xref ref-type="bibr" rid="bib6">Bloch, 2020</xref>). Epiretinal implants electrically activate retinal ganglion cells (RGCs) that have survived degeneration, causing them to send artificial visual signals to the brain. After initial successes, progress toward restoring high-fidelity natural vision using this approach has slowed, likely in part due to indiscriminate activation of many RGCs of different cell types and a resulting inaccurate neural representation of the target stimulus. One reason for this indiscriminate activation is the difficulty of predicting the neural activity evoked by multi-electrode stimulation based on the activity evoked by single-electrode stimulation.</p><p>Here, we present a data-driven optimization approach to bypass this problem by dynamically combining simpler stimulation patterns (<xref ref-type="bibr" rid="bib12">Choi et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Beauchamp et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Tafazoli et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Haji Ghaffari et al., 2021</xref>; <xref ref-type="bibr" rid="bib68">Vasireddy et al., 2023</xref>) using temporal dithering and spatial multiplexing. The presented solution is divided into three steps, allowing it to be modified for a wide range of neural systems and implants. First, we develop a simple, explicit model of how the visual image can be reconstructed from the activity of many RGCs of diverse types accessed by the multi-electrode array. Second, we avoid the complexity of nonlinear electrical stimulation by empirically calibrating RGC responses to a collection of simple single-electrode stimuli which can then be combined asynchronously and sparsely to reproduce patterns of neural activity. Finally, we optimize visual scene reconstruction by greedily selecting a sequence of these simple stimuli, temporally dithered to exploit the high speed of electrically evoked neural responses, and spatially multiplexed to avoid interactions between nearby electrodes. These three steps result in a dynamically optimized stimulation paradigm: the visual stimulus is transformed into a spatiotemporal pattern of electrical stimuli designed to produce a pattern of neural activity that is most effective for vision restoration given the measured limitations of the neural interface.</p><p>This dynamic optimization approach was tested and evaluated using large-scale multi-electrode stimulation and recording ex vivo from the macaque and rat retinas, a lab prototype for a future implantable system. The method produced substantial improvements in stimulus reconstruction compared to existing methods, by appropriately activating ON and OFF RGCs over space. The algorithm was useful in identifying a subset of the most effective electrodes for a particular retina, which could substantially reduce power consumption in an implant. Extensions of the algorithm can in principle be used to translate the approach to naturalistic viewing conditions with eye movements, and to exploit perceptual metrics to further enhance the quality of reconstruction.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>First, we frame the translation of a visual stimulus into electrical stimulation as an optimization problem and present a greedy temporal dithering algorithm to solve it efficiently. Then, we use the ex vivo lab prototype to evaluate the performance of the approach. We compare it with existing methods and develop extensions for spatial multiplexing, natural viewing with eye movements and perceptual quality measures.</p><p>The ex vivo lab prototype consists of electrical recording and stimulation of the macaque and rat retinas with a large-scale high-density multi-electrode array (512 electrodes, 60 μm spacing). The visual and electrical response properties of all recorded cells are estimated by direct measurements, using experimental methods described previously (<xref ref-type="bibr" rid="bib19">Field and Chichilnisky, 2007</xref>; <xref ref-type="bibr" rid="bib32">Jepson et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Grosberg et al., 2017</xref>) (see Methods). These data provide reliable experimental access to complete populations of ON and OFF parasol cell types in macaque retina, so these two cell types are the focus of the empirical analysis.</p><sec id="s2-1"><title>Dynamic optimization to approximately replicate neural code</title><p>Converting a visual stimulus into effective electrical stimulation can be framed as an optimization problem. Using the terminology of optimization, the three key components are the <italic>objective function</italic>, the <italic>constraints</italic>, and the <italic>algorithm</italic> (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The <italic>objective</italic> function to be minimized is identified as the difference between the target visual stimulus and a reconstruction of the stimulus from the neural responses, as a proxy for how the brain could use the signal for visual inference. However, certain <italic>constraints</italic> are imposed by electrical stimulation, which provides imperfect control over the activity of a population of cells. Hence, the optimization <italic>algorithm</italic> must convert incoming visual stimuli into electrical stimuli, such that the stimulus reconstructed from electrically evoked responses matches the true stimulus as closely as possible.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Algorithmic components of the proposed framework for electrical stimulation.</title><p>(<bold>A</bold>) In a healthy retina, the visual stimulus is encoded in the neural response pattern of retinal ganglion cells (RGCs; top row). In a retina with an implant, the visual stimulus is encoded into current patterns, which generate neural response patterns (bottom row). In either case, the neural responses are eventually processed by the brain to elicit perception, through a process assumed to involve reconstruction of the image. Selecting the appropriate electrical stimulation can be framed as an optimization problem, in which the goal is to identify an <italic>algorithm</italic> (prosthesis encoding) that achieves an <italic>objective</italic> (reconstruction error) while operating under <italic>constraints</italic> (electrical stimulation). (<bold>B</bold>) <italic>Objective</italic>: Linear reconstruction of visual stimulus by summing cell-specific spatial filters, weighted by spike counts. Receptive fields of ON (blue) and OFF (red) parasol cells in a population are shown. (<bold>C</bold>) <italic>Constraint</italic>: Characterizing electrically evoked RGC responses with a dictionary of stimulation patterns. Example dictionary elements, with cells shaded according to evoked response probability. A single-electrode stimulated multiple cells, indicating poor selectivity. (<bold>D</bold>) <italic>Algorithm</italic>: Run-time usage of the artificial retina. Exploiting the slow visual integration time, distant electrodes are stimulated in fast sequence. The resulting neural response is the summation of spikes elicited in each time step.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig1-v1.tif"/></fig><sec id="s2-1-1"><title>Objective: reconstructing the visual stimulus from neural responses</title><p>The <italic>objective</italic> of electrical stimulation in this context is to reproduce, as closely as possible, a visual sensation that would be produced by normal light-evoked responses. However, it is not known how the brain interprets RGC light responses, and thus how to frame the problem computationally. As a simple proxy, the objective is defined by reconstructing the visual image as accurately as possible from evoked RGC spikes (see Discussion) and then evaluating the difference between the reconstruction and the original image. For simplicity, linear reconstruction is assumed, and the objective is the minimum squared error between the target image and the reconstructed image.</p><p>Specifically, for a target image shown to the retina in the experimental lab prototype setting, the reconstructed stimulus is modeled as the linear superposition of spatial filters, each associated with a particular RGC, weighted by the corresponding RGC response (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="bibr" rid="bib73">Warland et al., 1997</xref>; <xref ref-type="bibr" rid="bib8">Brackbill et al., 2020</xref>). The optimal linear reconstruction filter for each ON and OFF parasol cell was approximated using the measured spatial receptive field of the cell obtained with white noise stimulation, scaled to predict the average spike count recorded within 50 ms of the onset of a flashed checkerboard stimulus. Note that in an implanted blind retina, the reconstruction filter for each cell would have to be estimated in a different way (<xref ref-type="bibr" rid="bib79">Zaidi et al., 2022</xref>, see Discussion).</p></sec><sec id="s2-1-2"><title>Constraint: calibrating the collection of neural responses that can be electrically evoked</title><p>The limited precision of electrical stimulation <italic>constrains</italic> our ability to produce desired response patterns in the RGC population. To optimize stimulation under this constraint, it would be ideal to have a model that characterizes how RGCs respond to arbitrary electrical stimulus patterns produced with the electrode array. Unfortunately, estimating this model is difficult due to nonlinear interactions in neural activation resulting from current passed simultaneously through multiple electrodes (<xref ref-type="bibr" rid="bib33">Jepson et al., 2014</xref>).</p><p>An alternative is to use a limited <italic>dictionary</italic> of responses evoked by simple current patterns. This dictionary was <italic>calibrated</italic> empirically in advance (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Specifically, current was passed through each of the 512 electrodes individually at each of 40 current levels (logarithmically spaced over the range 0.1–4 µA) and the response probability for each recorded cell was estimated using the fraction of trials in which an evoked spike was recorded from the cell. The evoked spikes were identified using a custom spike sorting algorithm that estimated the electrical artifact produced by stimulation and matched the residual recorded voltage to template waveforms of cells previously identified during visual stimulation (<xref ref-type="bibr" rid="bib45">Mena et al., 2017</xref>). In general, even though some electrical stimuli selectively activated one cell, many stimuli simultaneously activated two or more cells, often due to axonal stimulation (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="bibr" rid="bib26">Grosberg et al., 2017</xref>). Also, high-amplitude stimuli tended to evoke spikes in cells with receptive fields off the electrode array via their axons. These cases were detected by identifying bidirectional spike propagation to the edge of the electrode array and were removed from the dictionary (<xref ref-type="bibr" rid="bib26">Grosberg et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Tandon et al., 2021</xref>).</p></sec><sec id="s2-1-3"><title>Algorithm: greedy temporal dithering to approximate the optimal spatiotemporal electrical stimulus</title><p>Because a single-electrode stimulus generally cannot create a pattern of activity across the RGC population that accurately encodes a visual image, multiple stimuli must be combined, while also avoiding the nonlinear interactions mentioned above. This was achieved by rapid interleaving or <italic>temporal dithering</italic> of a diverse collection of single-electrode stimuli. The effectiveness of this method relies on assuming that if many such stimuli are provided in rapid succession (e.g. 0.1-ms interval), they evoke visual sensations similar to those that would be produced by simultaneous stimulation (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), because of long visual integration times in the brain (e.g. tens of ms, see Discussion).</p><p>Under this assumption, the optimization problem reduces to finding a sequence of dictionary elements <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that minimizes the expected squared error between the target visual stimulus and reconstructed responses based on the total spike count:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the target visual stimulus, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>x</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the stimulus reconstruction filter, and <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is a vector of the spike counts in the population of cells generated by stimulation <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, with spikes being drawn according to Bernoulli processes with probabilities <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In order to create an effective stimulation sequence, a straightforward, real-time method is to greedily select a stimulus at each step that minimizes the predicted error between the reconstruction and the target image. Although the greedy approach is not necessarily optimal (as explained later), it does allow for effective real-time implementation.</p><p>A crucial assumption of the algorithm is that the total expected spike count evoked by a sequence of electrical stimuli is the sum of the expected spikes for all the individual stimuli. However, when a cell is repeatedly stimulated, the activation probabilities associated with later stimuli are reduced, because of biophysical refractoriness. To avoid this non-independence, the stimulus at each time step is chosen from a ‘valid’ subset of the dictionary that does not include cells that were targeted recently (see Methods).</p></sec></sec><sec id="s2-2"><title>Greedy temporal dithering outperforms existing static methods</title><p>The greedy temporal dithering algorithm was evaluated on data collected using the laboratory experimental prototype for a retinal implant. After calibrating the responses of all recorded RGCs to all available single-electrode stimuli, the greedy temporal dithering stimulation sequence corresponding to a specific visual target (usually, a random checkerboard image) was calculated, as described above (<xref ref-type="disp-formula" rid="equ5">Equation 1</xref>). Then, the visual target was linearly reconstructed from the stimulation sequence using samples drawn from the single-electrode calibration data, under the assumption that temporal dithering maintains the independence of the responses evoked by single-electrode stimuli. Note that this assumption was later tested (see below).</p><p>The results of the analysis obtained by sampling from calibration data reveal the inferred visual reconstruction that is possible with greedy temporal dithering. During the stimulation sequence, the reconstructed image slowly built up to a spatially smooth version of the target image (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Not surprisingly, ON (OFF) parasol cells were stimulated more than OFF (ON) parasol cells in bright (dark) regions of the target stimulus. Moreover, the reconstruction for individual trials was similar to the average across multiple trials, indicating that the noise from inter-trial response variation was relatively small.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visual stimulus reconstruction achieved using the greedy temporal dithering algorithm.</title><p>White noise target image shown on left. First column: cumulative stimulation count across electrodes after 500, 3000, and 10,000 electrical stimuli (A, B, and C, respectively). Second column: responses for ON (blue) and OFF (red) parasol cells, sampled according to the single-electrode calibration data. Shade indicates the cumulative number of spikes. Third column: single-trial and trial-averaged reconstruction of the target stimulus.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig2-v1.tif"/></fig><p>To quantify the performance of the greedy temporal dithering algorithm, its reconstruction error was compared to the error of an optimized approach for existing retinal implants, which map the intensity of the visual stimulus near each electrode to its stimulation current amplitude or frequency (<xref ref-type="bibr" rid="bib31">Humayun et al., 2012</xref>; <xref ref-type="bibr" rid="bib61">Stingl et al., 2013</xref>; <xref ref-type="bibr" rid="bib47">Palanker et al., 2020</xref>). The performance of this static pixel-wise mapping was simulated with the lab prototype. Specifically, the current passed through each electrode was determined by a sigmoidal function of the intensity of the visual stimulus near the electrode, optimized at each electrode to minimize the reconstruction error across a training set of random checkerboard images (see Methods). This approach provides a generous upper bound to the performance of existing implants, because it uses actual response probabilities to optimize each sigmoidal function rather than relying on much more limited patient feedback, as is the case in existing retinal implants. Even using this generous upper bound, static pixel-wise mapping resulted in significantly less accurate reconstruction of the target image (<xref ref-type="fig" rid="fig3">Figure 3D, H</xref>), likely due to coactivation of overlapping ON and OFF cell types.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Quantifying the performance of dynamically optimized stimulation.</title><p>(<bold>A</bold>) A sample target checkerboard image. (<bold>B</bold>) ON and OFF receptive fields shaded with the expected summed response from greedy temporal dithering. Achieved reconstructions are shown for (<bold>C</bold>) greedy temporal dithering using calibrated responses to single-electrode stimulation (8448 electrical stimuli), (<bold>D</bold>) static pixel-wise mapping approximating existing open-loop systems (5023 stimuli), (<bold>E</bold>) a lower error bound on the optimal algorithm for a single-electrode dictionary (1850 stimuli), and (<bold>F</bold>) perfect control with the available reconstruction filters. (<bold>G</bold>) Reconstruction error (relative mean squared error) between target and the expected achieved perception for 20 different targets (blue lines), with the example from C indicated with the green line. (<bold>H</bold>) Histogram of relative performance of the above approaches across 20 target images.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig3-v1.tif"/></fig></sec><sec id="s2-3"><title>Greedy temporal dithering is nearly optimal given the interface constraints</title><p>What factors could improve the performance of the dynamically optimized stimulation approach? Broadly, performance could be limited either by the algorithm (greedy selection of electrical stimuli) or by the constraints of the interface (limited control of neural activity afforded by single-electrode stimulation).</p><p>To test whether performance could be improved with a different algorithm, the greedy approach was compared with a nearly optimal algorithm. The original optimization problem can be reformulated as:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>w</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf7"><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the target stimulus, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the reconstruction filter, <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a matrix of all response probabilities in the dictionary, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the variance associated with dictionary elements and <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates the number of times each dictionary element is used. Because <inline-formula><mml:math id="inf12"><mml:mi>w</mml:mi></mml:math></inline-formula> is an integer, this optimization problem is difficult to solve. However, an upper bound on the performance gap between the greedy algorithm and the optimal algorithm can be obtained by allowing non-integer values of <inline-formula><mml:math id="inf13"><mml:mi>w</mml:mi></mml:math></inline-formula>. Across multiple target images, the gap was low (&lt;10%, ‘optimal algorithm’ in <xref ref-type="fig" rid="fig3">Figure 3E, H</xref>), suggesting that the approximate nature of the greedy algorithm is not a substantial source of error in the present conditions.</p><p>To test whether performance could be improved with a more precise neural interface, the reconstruction error was compared with perfect control, in which any desired response pattern can be produced. The performance with perfect control was estimated by the solving the following optimization problem:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the vector of spike counts. This optimization problem was solved by relaxing the integer constraint on <inline-formula><mml:math id="inf15"><mml:mi>r</mml:mi></mml:math></inline-formula> to obtain an upper bound on the performance gap between a single-electrode dictionary and an ideal dictionary. Across multiple targets, this gap was substantial (&gt;40%, ‘perfect control’ in <xref ref-type="fig" rid="fig3">Figure 3F, H</xref>).</p><p>Thus, although the greedy algorithm is nearly optimal for a single-electrode dictionary, the reconstruction performance of an artificial retina could be improved by enhancing the dictionary (e.g. by using calibrated, optimized multi-electrode stimulation patterns; <xref ref-type="bibr" rid="bib33">Jepson et al., 2014</xref>; <xref ref-type="bibr" rid="bib68">Vasireddy et al., 2023</xref>).</p></sec><sec id="s2-4"><title>Closed-loop experimental validation of greedy temporal dithering</title><p>The performance of the greedy temporal dithering algorithm was next tested empirically using closed-loop recording and stimulation in the isolated rat retina (see Methods). We compared the reconstruction of the visual stimulus from RGC responses evoked by the stimulation sequence to the reconstruction obtained using samples drawn from the single-electrode calibration data. First, reconstruction filters were obtained using visual stimulation and recording, and the responses to single-electrode stimulation were calibrated as described above. Then, the greedy temporal dithering stimulation sequence was computed during the experiment and delivered to the retina at an expanded 3-ms stimulation interval to facilitate spike sorting (see Methods). The evoked RGC responses to this sequence were then recorded and analyzed. The reconstructions obtained with the evoked RGC responses captured much of the spatial structure in each target image (<xref ref-type="fig" rid="fig4">Figure 4A–D</xref>). Notably, the spatial structure of reconstructions using the calibrated responses to single-electrode stimulation (‘calibrated’ in <xref ref-type="fig" rid="fig4">Figure 4</xref>, similar to <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>) and the RGC responses evoked by the stimulation sequence (‘evoked’ in <xref ref-type="fig" rid="fig4">Figure 4</xref>) were similar. These reconstructions reached asymptotic quality with 225 ± 29 stimulations (not shown), a significantly smaller number than in tests obtained with macaque retina (<xref ref-type="fig" rid="fig3">Figure 3G</xref>), likely in part because of the smaller number of cells in the rat recordings (see Discussion). Importantly, the reconstruction performance benefited from differential activation of ON and OFF cells over space in a way that reflected the spatial distribution of intensities in each target image (<xref ref-type="fig" rid="fig4">Figure 4E, F</xref>). This observation highlights the importance of electrical stimulation which approximates naturalistic RGC responses, in comparison with the static pixel-wise approaches used in existing implants.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Experimental validation of dynamically optimized stimulation in the rat retina.</title><p>(<bold>A</bold>) Four sample target checkerboard images. Achieved reconstructions for these images are shown (<bold>B</bold>) assuming perfect control of retinal ganglion cell (RGC) firing with the available reconstruction filters, (<bold>C</bold>) using greedy temporal dithering based on calibrated single-electrode responses, and (<bold>D</bold>) and using the RGC responses evoked during electrical stimulation with greedy temporal dithering. (<bold>E, F</bold>) ON and OFF receptive fields shaded with the total number of evoked spikes. (<bold>G</bold>) Reconstruction error (relative mean squared error) across 20 target images for perfect control vs. greedy temporal dithering using calibrated responses (top) and for greedy temporal dithering using calibrated responses vs. evoked responses (bottom). Red points correspond to the four targets shown. Evoked RGC responses are averaged over 25 trials for each target (<bold>D–G</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig4-v1.tif"/></fig><p>The limitations to the performance of the dynamically optimized stimulation were further explored with two comparisons. First, the reconstruction error obtained using calibrated responses to single-electrode stimulation was higher than the error obtained under an assumption of ‘perfect control’ (i.e. that all recorded cells can be activated independently), reflecting the limitations of single-electrode stimulation (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, top), as shown earlier (<xref ref-type="fig" rid="fig3">Figure 3H</xref>). Second, the reconstruction error obtained using the RGC responses evoked by the stimulation sequence was higher than the error obtained using calibrated responses to single-electrode stimulation (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, bottom), presumably reflecting non-stationarity in the ex vivo recordings and/or failures of independence due to temporal dithering (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, see Discussion).</p></sec><sec id="s2-5"><title>Spatial multiplexing increases throughput within the visual integration window</title><p>The main requirement of temporal dithering – the independence of responses generated by individual electrical stimuli, and their summation within a time window for visual perception – could limit the throughput of electrical stimulation. Although independence can be ensured by spacing single-electrode stimuli widely in time, this approach could make it difficult to deliver many electrical stimuli within a visual integration window (e.g. tens of ms). One approach to maximizing the number of stimuli that can be delivered is <italic>spatial multiplexing</italic>, in which multiple electrodes are used simultaneously for stimulation if they are known to affect the firing probabilities of disjoint sets of cells. A simple example would be if electrodes separated by more than a particular distance <italic>D</italic> always influenced the firing of disjoint sets of cells. In this case, implementing a circular <italic>exclusion zone</italic> with radius <italic>D</italic> around each electrode at each time step of temporal dithering would be expected to produce the same cellular activation as was obtained with calibrated single-electrode stimulation. As in the original temporal dithering approach, at the subsequent time step, electrical stimuli with nonzero activation probability for any recently targeted cell would be omitted from consideration (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Spatial multiplexing by simultaneous stimulation of distant electrodes.</title><p>(<bold>A</bold>) Visualization of temporally dithered and spatially multiplexed stimulation. At each time step, multiple single-electrode stimuli are chosen greedily (gray circles) across the electrode array (black dots), separated by a spatial exclusion radius (red circles). (<bold>B</bold>) Estimation of the spatial exclusion radius using 754 total electrode pairs across 7 parasol cells from 4 peripheral macaque retina preparations (see Methods). Interaction between electrodes is measured by fractional deviation in activation threshold for a given cell on a primary electrode (ordinate) resulting from simultaneous stimulation of another electrode with identical current amplitude at varying separations (abscissa). Baseline represents the variability associated with estimating single-electrode activation thresholds.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig5-v1.tif"/></fig><p>To identify a spatial exclusion zone and test its effectiveness, activation curves corresponding to single-electrode stimulation were compared to activation curves obtained with additional simultaneous stimulation using a nearby secondary electrode at the same current level (see Methods). Examination of many electrode pairs over a range of distances and multiple cells (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) revealed a systematic decrease of the interaction between stimulating electrodes as a function of distance. On average, the activation probability of a cell in response to single-electrode stimulation was affected relatively little (&lt;5% fractional change in threshold) for a secondary electrode 200µm away. Thus, two electrodes more than this distance apart were unlikely to substantially influence the activation probability of the same cell(s). This suggests that spatial multiplexing of stimuli outside a spatial exclusion radius is a practical strategy for high-throughput stimulation with temporal dithering.</p></sec><sec id="s2-6"><title>Dynamic optimization framework enables data-driven hardware design</title><p>The dynamic optimization framework suggests further optimizations for hardware efficiency. Because the greedy temporal dithering algorithm chooses electrodes in a spatially non-uniform manner over the array (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), restricting stimulation to a more frequently chosen subset of electrodes could enhance efficiency. To test this possibility, the algorithm was applied with dictionaries restricted to a subset of the most frequently used electrodes, and calibrated performance was evaluated on twenty visual targets. In general, restricted dictionaries would be expected to reduce reconstruction performance. However, a minimal (&lt;5%) increase in reconstruction error was observed if the number of available electrodes was reduced by up to 50% (<xref ref-type="fig" rid="fig6">Figure 6B–F</xref>). Note that this increase was not due to the greedy nature of stimulation, because a lower bound computed for an optimal algorithm showed similar behavior (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). These observations suggest a strategy for efficient implant operation in a retina-specific manner: identify the most frequently used ~50% of electrodes during calibration and permanently turn off the remaining electrodes during run-time usage. Such a reduction in the set of stimulated electrodes could lead to reduced memory access and power consumption, with little loss in performance. Thus, applying the algorithmic framework to the ex vivo lab prototype leads to insights relevant to the development of an in vivo implant.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Subsampling electrodes for hardware efficiency.</title><p>(<bold>A</bold>) Frequency of stimulating different electrodes (size of gray circles), overlaid with axons (lines), and somas (colored circles) inferred from spatiotemporal spike waveform across the electrode array recorded from each cell. (<bold>B</bold>) Reconstruction error as a function of the fraction of electrodes included in the dictionary (black, thin lines correspond to different target images) and average over 20 target images (black, thick line). Different collections of target stimuli were used for electrode selection and reconstruction performance evaluation. Lower bound on error of any algorithm for the subsampled dictionaries for individual targets (green, thin lines) and averaged across targets (green, thick line). (<bold>C</bold>) Example target image. (<bold>D–F</bold>) Reconstructed images using the dictionary with most frequently used 20%, 60%, and 100% of electrodes, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig6-v1.tif"/></fig></sec><sec id="s2-7"><title>Dynamic optimization framework extends to naturalistic viewing conditions</title><p>For practical application, the dynamically optimized stimulation approach must be extended to naturalistic viewing conditions, in which saccadic and fixational eye movements move the fovea over the scene for high-resolution vision (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Similarly, an implant fixed on the retina would move over the scene as the eye moves, and would only transmit the information about its restricted view of the image. The dynamic optimization framework extends naturally to this situation.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Extension of dynamically optimized stimulation to naturalistic conditions with eye movements.</title><p>(<bold>A</bold>) Conversion of a visual scene into dynamic stimulus. A target visual scene (left), with sample eye movement trajectory (blue). For each eye position, the population of ganglion cells accessible by the implant views a small portion of the visual scene (top right). The reconstructed stimulus for each patch captures the local stimulus information (bottom right). (<bold>B</bold>) Spike trains passed through a spatiotemporal reconstruction filter of the dynamic stimulus video. For simplicity, a rank one filter was used, which spatially filtered each spike bin independently, and then filtered the reconstructed stimulus video in time. (<bold>C</bold>) Final reconstruction performance over a sequence of saccades, in the absence (left) and the presence (right) of small fixational eye movements. (<bold>D</bold>) Reduction in reconstruction error of the visual scene as a function of the number of saccades, in the absence (blue) and the presence (orange) of fixational eye movements.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig7-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-83424-fig7-video1.mp4" id="fig7video1"><label>Figure 7—video 1.</label><caption><title>Greedy temporal dithering and spatial multiplexing in natural viewing conditions.</title><p>Top row: operation of the proposed approach when only saccadic eye movements are used. Left: target visual scene with overlaid eye movement trajectory (red). Middle: target stimulus for the cells underneath the retinal implant. Right: the assembled visual scene after generating responses with the selected stimulation sequence. Bottom row: similar to top, with fixational eye movements in addition to saccadic eye movements.</p></caption></media></fig-group><p>First, the objective, the constraint, and an algorithm for the corresponding optimization problem are identified. The objective function is modified to minimize the error between the original stimulus video and a video reconstructed from the RGC spike trains. For simplicity, a spatiotemporal reconstruction filter is used with separable spatial and temporal components and the same time course (with opposite polarity) for ON and OFF parasol cells (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, see Methods). The constraints (measured electrical stimulation properties) are unchanged. Finally, the algorithm is adapted by choosing a dictionary element for each time step greedily to minimize the average error between the recent frames of the target stimulus seen by the implant and the corresponding frames of the reconstruction (see Methods).</p><p>This modified algorithm was evaluated using simulations of naturalistic viewing. For a given scene, a dynamic visual stimulus was generated by simulating saccadic eye movements with random inter-saccade intervals and random fixation locations with a preference for regions of the scene with high spatial-frequency content (<xref ref-type="bibr" rid="bib78">Yarbus, 1967</xref>). Optionally, fixational eye movements were simulated by jittering the visual stimulus with Brownian motion (see Methods). As before, stimulation patterns were determined using greedy temporal dithering, and reconstruction was performed from calibrated responses to single-electrode stimulation. The dynamic visual stimulus covering the collection of recorded cells was reconstructed from the evoked spikes and the full visual scene was then assembled by stitching together the parts of the scene covering the cells at each time step.</p><p>The assembled visual scene closely matched the target, capturing many of its fine details (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, <xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref>). Interestingly, the reconstructed visual scene was smoother and more accurate (lower reconstruction error) when fixational eye movements were simulated along with saccades (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Specifically, for the same final reconstruction error, a ~4× reduction in the number of required saccades and hence the number of required electrical stimuli was observed in the presence of fixational eye movements (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). Hence, the performance of greedy temporal dithering translates to natural viewing conditions and reveals that more accurate image reconstruction is possible with fixational eye movements (<xref ref-type="bibr" rid="bib76">Wu et al., 2024</xref>).</p></sec><sec id="s2-8"><title>Optimizing stimulation using a perceptual similarity measure</title><p>The framework provides a natural way to use alternative metrics to optimize visual perception evoked by electrical stimulation. Specifically, the mean squared error (MSE) measure of reconstruction accuracy, while convenient, does not accurately capture perceived differences in image content, whereas error metrics such as Structural Similarity (SSIM) more closely parallel perception (<xref ref-type="bibr" rid="bib72">Wang et al., 2004</xref>). To identify a nearly optimal sequence of stimuli with SSIM as the objective, an exhaustive approach was used to optimize across all possible stimulation sequences for every eye location (see Methods). The SSIM and MSE metrics produced similar reconstructions when the number of electrical stimulation patterns was unlimited (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). This suggests that the choice of reconstruction error metric may not be important for an implant that can stimulate at high rates. However, SSIM optimization produced higher-quality reconstructions when the number of electrical stimuli was limited (<xref ref-type="fig" rid="fig8">Figure 8C</xref>), a constraint that may be relevant with short visual integration times or low power limits in an implant. Thus, the greedy dithering approach with a perceptually accurate reconstruction metric could lead to higher performance in an implanted device, though additional developments will be needed before such an optimization can be performed in real time.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Extension of dynamically optimized stimulation using Structural Similarity (SSIM) perceptual error metric.</title><p>(<bold>A</bold>) Two target images. (<bold>B</bold>) Reconstruction with MSE and SSIM error metrics for greedy temporal dithering, with a high budget. (<bold>C</bold>) Same as B, with a low budget.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83424-fig8-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This paper presents a dynamically optimized electrical stimulation approach to improve the performance of sensory electronic implants. Greedy temporal dithering and spatial multiplexing address the challenges of precisely controlling the activity of diverse cell types in a neural population by rapidly delivering a sequence of simple electrical stimuli with independent effects within a visual integration window. This approach avoids nonlinear interactions resulting from simultaneous multi-electrode stimulation while providing enough flexibility to elicit rich spatiotemporal response patterns using a sequence of single-electrode stimulation patterns. The greedy temporal dithering and spatial multiplexing approach outperforms existing approaches (<xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig5">5</xref>) and enables efficient neural interface development (<xref ref-type="fig" rid="fig6">Figure 6</xref>), potentially incorporating naturalistic viewing with eye movements (<xref ref-type="fig" rid="fig7">Figure 7</xref>) and/or perceptual similarity metrics (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><p>The performance of the temporal dithering algorithm was primarily evaluated using calibrated responses from single-electrode stimulation (<xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref>, and <xref ref-type="fig" rid="fig6">6</xref>—<xref ref-type="fig" rid="fig8">8</xref>). In addition, validation of temporal dithering (<xref ref-type="fig" rid="fig4">Figure 4</xref>) was performed in closed-loop experiments by delivering the optimized electrical stimulation sequence and analyzing the evoked RGC responses. The reconstructions using these evoked RGC responses captured much of the spatial structure in each target image. However, these reconstructions were less accurate than the reconstructions based on the calibrated responses to single-electrode stimulation (which were used to compute the temporal dithering stimulation sequence during the experiment). At least two factors could contribute to this discrepancy: (1) non-stationarities in the electrical responses of RGCs in the ex vivo retina preparation, due to physiological changes and/or movement, and (2) failures of independence of interleaved stimulation over time relative to isolated single-electrode stimulation. Further experimental work will be needed to distinguish these possibilities. In addition, validation of temporal dithering at shorter stimulation intervals more relevant for in vivo application will require the development of spike sorting approaches that reliably operate in the presence of complex electrical artifacts.</p><sec id="s3-1"><title>Assumptions underlying the temporal dithering and spatial multiplexing approach</title><p>The presented approach relies on several assumptions regarding how the brain uses RGC responses for vision. A major assumption is that downstream processing of RGC responses is slow, integrating over tens of milliseconds, so that perception depends primarily on the total number of evoked spikes within this time interval. The assumption of slow visual processing is based on evidence ranging from flicker fusion experiments to the time scale of synaptic transfer to neurophysiological tests of temporal integration (<xref ref-type="bibr" rid="bib71">Wandell, 1995</xref>; <xref ref-type="bibr" rid="bib62">Tadin et al., 2010</xref>; <xref ref-type="bibr" rid="bib58">Samaha and Postle, 2015</xref>; <xref ref-type="bibr" rid="bib77">Wutz et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Borghuis et al., 2019</xref>). However, there is also empirical evidence that the temporal precision of spikes in RGCs in certain conditions is on the order of 1 ms (<xref ref-type="bibr" rid="bib3">Berry et al., 1997</xref>; <xref ref-type="bibr" rid="bib51">Reich et al., 1997</xref>; <xref ref-type="bibr" rid="bib4">Berry and Meister, 1998</xref>; <xref ref-type="bibr" rid="bib35">Keat et al., 2001</xref>; <xref ref-type="bibr" rid="bib67">Uzzell and Chichilnisky, 2004</xref>) and that downstream mechanisms could potentially read out these temporally precise RGC signals (<xref ref-type="bibr" rid="bib1">Alonso et al., 1996</xref>). It remains unclear from these studies exactly how this high temporal precision would be important for vision. Studies of readout of RGC signals from the macaque retina have shown that for reconstruction of images from responses to flashed stimuli, and for speed and direction discrimination with moving stimuli, ~10 ms temporal resolution of readout from RGC signals is optimal (<xref ref-type="bibr" rid="bib11">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib21">Frechette et al., 2005</xref>; <xref ref-type="bibr" rid="bib8">Brackbill et al., 2020</xref>; <xref ref-type="bibr" rid="bib76">Wu et al., 2024</xref>). Nonetheless, finer temporal precision could be part of the neural code of RGCs under certain visual stimulus conditions, such as compensation for fixational eye drift (<xref ref-type="bibr" rid="bib76">Wu et al., 2024</xref>). In sum, much evidence supports the idea that the temporal resolution of RGC signal readout in the brain is likely to be on the order of tens of milliseconds for many visual tasks, but this may not be true for all conditions.</p><p>Another important assumption is that visual sensations produced in the brain are based on linear reconstruction of the incident image from retinal inputs. This is a first-order approximation to facilitate real-time optimization of electrical stimulation, and is almost certainly wrong in detail. A more accurate model could involve replacing this with a nonlinear and/or biologically realistic reconstruction (<xref ref-type="bibr" rid="bib48">Parthasarathy et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Kim et al., 2021</xref>; <xref ref-type="bibr" rid="bib75">Wu et al., 2022</xref>). However, these approaches are far more complex and cannot yet be optimized in real time.</p><p>This work also relies on several empirically tested assumptions about electrical stimulation. First, the brief (150 µs) and low (&lt;4 µA) current pulses used in this study typically evoke a single spike with low and precise latencies (e.g. 0.73 ± 0.05 ms <xref ref-type="bibr" rid="bib59">Sekirnjak et al., 2011</xref>), in part because the mechanism of activation is direct depolarization rather than network-mediated excitation. Second, electrical stimuli at the same electrode separated by at least 10 ms generate approximately independent responses (<xref ref-type="bibr" rid="bib64">Talaminos-Barroso et al., 2020</xref>), making temporal dithering possible. Third, distant electrodes generate independent responses (<xref ref-type="fig" rid="fig5">Figure 5</xref>), making spatial multiplexing possible. While these electrical stimulation properties may be widely applicable, they should be tested and quantified in each neural circuit before applying the temporal dithering and spatial multiplexing approach.</p><p>Finally, the approach relies on the assumption that it is possible to deliver a sufficiently large number of electrical stimuli within a visual integration time to produce high quality artificial vision. The total number of electrical stimuli required depends on the number of cells targeted, their expected firing rates for the visual image, and distribution of RGC activation probabilities in the electrical stimulus dictionary. Future work should identify how these factors vary across individuals, species, and neural circuits.</p></sec><sec id="s3-2"><title>Extensions and broader applicability of the proposed approach</title><p>The modular nature of the dynamic optimization approach enables several potential extensions. The single-electrode dictionary could be enhanced with multi-electrode stimulation patterns designed to optimize cellular selectivity, response diversity, or ideally the overall expected algorithm performance (<xref ref-type="bibr" rid="bib33">Jepson et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Fan et al., 2019</xref>; <xref ref-type="bibr" rid="bib70">Vilkhu et al., 2021</xref>; <xref ref-type="bibr" rid="bib68">Vasireddy et al., 2023</xref>). Additionally, the efficient and real-time greedy algorithm could be replaced with an algorithm that identifies the optimal stimulation sequence for multiple time steps, perhaps accounting for predicted future saccade locations. Finally, each module could be optimized for metrics such as performance, hardware efficiency (e.g. <xref ref-type="fig" rid="fig6">Figure 6B</xref>), or stability/robustness for chronic function.</p><p>The greedy temporal dithering and spatial multiplexing approach relies on the ability to efficiently compute and deliver optimal electrical stimuli in a bidirectional electronic implant. Although this procedure exceeds the capabilities of current devices, two features of the approach support implementation on implantable hardware with a limited size and power budget.</p><p>First, the hardware requirements of the proposed closed-loop procedure – calibration of single-electrode stimuli followed by dynamically optimized stimulation for encoding visual scenes – are less stringent than those of a real-time closed-loop system. In the latter, one records and analyzes the results of every electrical stimulus to determine the stimulus for the next time step. Instead, the present approach relies only on identifying the <italic>average</italic> electrical response properties of each cell in advance. After this initial calibration, the stimulation sequence is decided in an open-loop manner by optimizing the <italic>expected</italic> visual reconstruction associated with each electrical stimulus.</p><p>Second, the approach can effectively exploit non-selective activation. Selective activation of every cell in a region of the retina, if achievable, would make it possible to create arbitrary patterns of neural activity, but in practice this is difficult with real neural interfaces. The presented framework uses all available stimulation patterns as efficiently as possible by directly optimizing for stimulus reconstruction. In fact, the approach frequently exploits non-selective activation, in order to evoke desired spiking activity in fewer time steps (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p></sec><sec id="s3-3"><title>Translational potential</title><p>The physiological similarities between human and macaque retina (<xref ref-type="bibr" rid="bib13">Cowan et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Kling et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Soto et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Rodieck, 1998</xref>), including their responses to electrical stimulation (<xref ref-type="bibr" rid="bib43">Madugula et al., 2022</xref>), suggest that the benefits of the present stimulation approach could translate from the ex vivo lab prototype with healthy macaque retina to an in vivo implant in the degenerated human retina. However, several technical innovations are required to enable chronic in vivo recording and stimulation. First, new surgical methods must be developed to implant a tiny and high-density chip on the surface of the retina with close and stable contact, to create a lasting, stable interface with RGCs. Second, modifications to the dynamic optimization approach are necessary to mitigate non-stationarities in electrical response properties, which are common in chronic recordings with multi-electrode array implants (<xref ref-type="bibr" rid="bib49">Perge et al., 2013</xref>; <xref ref-type="bibr" rid="bib17">Downey et al., 2018</xref>). Third, receptive field locations, cell types, and reconstruction filters must be inferred using spike waveform features and spontaneous activity rather than light-evoked responses in a blind retina (<xref ref-type="bibr" rid="bib59">Sekirnjak et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Li et al., 2015</xref>; <xref ref-type="bibr" rid="bib54">Richard et al., 2015</xref>; <xref ref-type="bibr" rid="bib79">Zaidi et al., 2022</xref>). Fourth, the stimulation approach must be modified to account for changes in spontaneous/oscillatory activity in the degenerated retina (<xref ref-type="bibr" rid="bib59">Sekirnjak et al., 2011</xref>; <xref ref-type="bibr" rid="bib24">Goo et al., 2015</xref>; <xref ref-type="bibr" rid="bib66">Trenholm and Awatramani, 2015</xref>). Finally, the approach must be tested with the visual and electrical properties in the central retina (<xref ref-type="bibr" rid="bib23">Gogliettino et al., 2023</xref>), the most clinically relevant location for a retinal implant.</p><p>The present approach could also leverage other methods developed for improving the performance of existing, low-resolution implants. Examples of these methods are context-dependent image preprocessing (<xref ref-type="bibr" rid="bib9">Cha et al., 1992</xref>; <xref ref-type="bibr" rid="bib44">McCarthy et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Lieby et al., 2011</xref>; <xref ref-type="bibr" rid="bib69">Vergnieux et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Ho et al., 2019</xref>), limiting to sparse stimulation (<xref ref-type="bibr" rid="bib42">Loudin et al., 2007</xref>), exploiting the adaptation of sensory systems (<xref ref-type="bibr" rid="bib56">Rouger et al., 2007</xref>; <xref ref-type="bibr" rid="bib46">Merabet and Pascual-Leone, 2010</xref>), and exploiting perceived phosphenes due to axon bundle activation for optimizing stimulation (<xref ref-type="bibr" rid="bib25">Granley et al., 2022</xref>; <xref ref-type="bibr" rid="bib15">de Ruyter van Steveninck et al., 2022</xref>; <xref ref-type="bibr" rid="bib52">Relic et al., 2022</xref>). Ideally, a unified framework such as the one presented here would include these and potentially other approaches to optimal stimulation.</p><p>Electrical stimulation of the visual cortex has also been tested for vision restoration (<xref ref-type="bibr" rid="bib10">Chen et al., 2020</xref>), and one study (<xref ref-type="bibr" rid="bib2">Beauchamp et al., 2020</xref>) deployed a dynamic stimulation approach that demonstrated impressive performance in human participants. However, both studies only considered simple visual stimuli which can be described by lines (such as English letters and numbers) or a few dots. The dynamic optimization framework presented here could provide a way to precisely control neural activity for arbitrarily complex stimuli and improve the performance of a range of cortical sensory implants.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Retinal preparation</title><p>Extracellular multi-electrode recording and stimulation in macaque retina were performed as described previously (<xref ref-type="bibr" rid="bib32">Jepson et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Grosberg et al., 2017</xref>). Briefly, eyes were obtained from terminally anesthetized macaque monkeys used for experiments in other laboratories, in accordance with Institutional Animal Care and Use Committee guidelines. After enucleation, the eyes were hemisected and the vitreous humor was removed. The hemisected eye cups were stored in oxygenated bicarbonate-buffered Ames’ solution (Sigma) during transport to the laboratory. The retina was then isolated from the pigment epithelium under infrared illumination and held RGC side down on a custom multi-electrode array (see below). Throughout the experiments, the retina was superfused with Ames’ solution at 35°C.</p><p>For experimental validation of temporal dithering, eyes were obtained from adult Long-Evans rats in accordance with Institutional Animal Care and Use Committee guidelines. Immediately after enucleation, the anterior portion of the eye and vitreous humor were removed under infrared illumination and the eye cup placed in oxygenated bicarbonate-buffered Ames’ solution. The retina was then isolated under infrared illumination and held RGC side down on a custom multi-electrode array. Throughout the experiments, the retina was superfused with Ames’ solution at 31°C.</p></sec><sec id="s4-2"><title>Electrical recordings</title><p>A custom 512-electrode stimulation and recording system (<xref ref-type="bibr" rid="bib29">Hottowy et al., 2008</xref>; <xref ref-type="bibr" rid="bib30">Hottowy et al., 2012</xref>) was used to deliver electrical stimuli and record spikes from RGCs. The electrodes were organized in a 16 × 32 isosceles triangular lattice arrangement, with 60 μm spacing between electrodes (<xref ref-type="bibr" rid="bib40">Litke et al., 2004</xref>). Electrodes were 10 μm in diameter and electroplated with platinum black. For recording, raw voltage signals from the electrodes were amplified, filtered (43–5000 Hz), and multiplexed with custom circuitry. These voltage signals were sampled with commercial data acquisition hardware (National Instruments) at 20 kHz per channel. For recording and stimulation, a platinum ground wire circling the recording chamber served as a distant ground.</p></sec><sec id="s4-3"><title>Electrical stimulation</title><p>For electrical stimulation, custom hardware (<xref ref-type="bibr" rid="bib30">Hottowy et al., 2012</xref>) was controlled by commercial multifunction cards (National Instruments). Current was passed through each of the 512 electrodes individually, with 40 different amplitudes (0.1–4 μA, logarithmically spaced), 27 times each. For each amplitude, charge-balanced triphasic current pulses with relative amplitudes of 2:−3:1 and phase widths of 50 μs (total duration 150 μs) were delivered through the stimulating electrode (amplitude corresponds to the magnitude of the second, cathodal phase of the pulse). This pulse shape was chosen to reduce stimulation artifacts in the recordings. Custom circuitry disconnected the recording amplifiers during stimulation, reducing stimulation artifacts and making it possible to identify elicited spikes on the stimulating electrode as well as nearby electrodes (<xref ref-type="bibr" rid="bib30">Hottowy et al., 2012</xref>; <xref ref-type="bibr" rid="bib32">Jepson et al., 2013</xref>).</p></sec><sec id="s4-4"><title>Visual stimulation</title><p>Recordings obtained with visual stimulation were analyzed to identify spike waveforms of distinct RGCs recorded, using spike sorting methods described previously (<xref ref-type="bibr" rid="bib19">Field and Chichilnisky, 2007</xref>; <xref ref-type="bibr" rid="bib40">Litke et al., 2004</xref>). Specifically, the spike times of each cell were typically identified using relatively large spikes detected near the soma. Then, the complete spatiotemporal signature of the spikes from each cell over all electrodes (the <italic>electrical image</italic>) was computed by averaging the voltage waveforms on all electrodes at and near the times of its recorded spikes (<xref ref-type="bibr" rid="bib40">Litke et al., 2004</xref>). This electrical image provided a template of the cell’s spatiotemporal spike waveform, which was then used to identify spikes evoked from cells by electrical stimulation.</p><p>Distinct RGC types were identified by their visual responses to a 30-min long white noise stimulus (80 × 40 pixel grid, ~44 µm pixels, refresh rate 120 Hz, low photopic light level). After the stimulus presentation the average stimulus that preceded a spike in each RGC was computed, producing the spike-triggered average (STA) stimulus (<xref ref-type="bibr" rid="bib11">Chichilnisky and Kalmar, 2002</xref>). The STA summarizes the spatial, temporal, and chromatic properties of light responses. Spatial receptive fields were obtained using the spatial sensitivity profile of the STA (<xref ref-type="bibr" rid="bib11">Chichilnisky and Kalmar, 2002</xref>). Features of the STA were used to segregate functionally distinct RGC types (<xref ref-type="bibr" rid="bib53">Rhoades et al., 2019</xref>). For each identified RGC type, the receptive fields formed a regular mosaic covering the region of retina recorded (<xref ref-type="bibr" rid="bib16">Devries and Baylor, 1997</xref>; <xref ref-type="bibr" rid="bib19">Field and Chichilnisky, 2007</xref>), confirming the correspondence to a morphologically distinct RGC type (<xref ref-type="bibr" rid="bib14">Dacey, 1993</xref>; <xref ref-type="bibr" rid="bib74">Wässle et al., 1981</xref>), and in some cases revealing complete recordings from the population. The density and light responses of the five most frequently recorded RGC types uniquely identified them as ON and OFF midget, ON and OFF parasol and small bistratified cells. Subsequent analysis was restricted to two numerically dominant RGC types in the macaque retina – ON and OFF parasol cells – which were sampled efficiently in our experiment and formed nearly complete mosaics covering the region recorded. In the rat retina, analysis was restricted to two RGC types – putative ON brisk transient and OFF brisk transient cells identified by their receptive fields and spiking autocorrelation (<xref ref-type="bibr" rid="bib50">Ravi et al., 2018</xref>) – which formed reasonably complete mosaics covering the region recorded and had properties broadly resembling those of ON and OFF parasol cells in the macaque retina.</p></sec><sec id="s4-5"><title>Temporal dithering algorithm</title><p>Given the stimulus reconstruction filter and electrical stimulation dictionary, the goal of the greedy temporal dithering algorithm is to identify a sequence of electrical stimuli that encodes a target visual stimulus.</p><p>Let <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the target visual stimulus, <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the stimulus reconstruction filter, and <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the observed response vector (consisting of a zero or a one for each cell) produced in the population of cells stimulated using electrical stimulation dictionary element <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with the associated probability vector <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Multiple dictionary elements must be combined to generate rich spatiotemporal population responses that capture the visual information in a target visual stimulus. We therefore define the objective as finding a sequence of dictionary elements <inline-formula><mml:math id="inf21"><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula> that minimizes the expected mean squared error between the target visual stimulus and the image reconstructed from the sequence of responses <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The responses are stochastic with <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and we assume that responses are generated independently across time steps. The resulting objective function is:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>To efficiently solve this optimization problem, the right-hand side can be decomposed into bias and variance terms as follows:<disp-formula id="equ5"><label>(2)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(3)</label><mml:math id="m6"><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>E</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><label>(4)</label><mml:math id="m7"><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>E</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(5)</label><mml:math id="m8"><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>⋯</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <xref ref-type="disp-formula" rid="equ5">Equation 2</xref> follows from adding and subtracting <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="disp-formula" rid="equ6">Equation 3</xref> expands the square of summation and uses the fact that <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>A</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to zero out the cross term; and <xref ref-type="disp-formula" rid="equ7">Equation 4</xref> uses the fact that neural responses at different time steps are independent of each other.</p><p>The expression (<inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext> </mml:mtext><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) corresponds to the total variance across all pixels of the visual image for the dictionary element <inline-formula><mml:math id="inf27"><mml:mi>c</mml:mi></mml:math></inline-formula> chosen at step <italic>i</italic>. When cells respond independently to electrical stimulation, this variance term simplifies to <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the activation probability of cell <inline-formula><mml:math id="inf30"><mml:mi>n</mml:mi></mml:math></inline-formula> with dictionary element <inline-formula><mml:math id="inf31"><mml:mi>c</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the reconstruction filter for cell <inline-formula><mml:math id="inf33"><mml:mi>n</mml:mi></mml:math></inline-formula> (the <inline-formula><mml:math id="inf34"><mml:mi>n</mml:mi></mml:math></inline-formula>th column of <inline-formula><mml:math id="inf35"><mml:mi>A</mml:mi></mml:math></inline-formula>).</p><p>Below, we present two methods to solve the above optimization problem and two additional methods to solve relaxed optimization problems for comparison purposes. The first method provides a greedy solution which can be deployed in real time and handles dependencies between successive stimuli. The second method provides an upper bound on the optimal solution by jointly optimizing a vector of the number of times each dictionary element is used. A third method relaxes the above objective to provide an upper bound on performance with no electrical stimulation constraints by directly optimizing the number of spikes for each cell. Finally, a fourth method approximates the function of present-day retinal implants by optimizing a mapping between visual stimulus intensity and current amplitude for each electrode.</p><p>For the last three methods which generate solutions irrespective of the order of stimulation, a temporal dithering strategy for ordering the electrical stimuli was assumed so that the stimuli would not interfere with one another. The performance for all methods was evaluated in the same manner, by linearly reconstructing the target stimulus from the identified electrical stimulation sequence using samples drawn from the single-electrode calibration data.</p><sec id="s4-5-1"><title>Greedy optimization</title><p>Instead of jointly optimizing for the whole stimulation sequence, which is difficult, a greedy approach is used for efficiency. This approach optimizes the choice of stimulation at time step <inline-formula><mml:math id="inf36"><mml:mi>t</mml:mi></mml:math></inline-formula> after fixing the stimulation sequence up to step <inline-formula><mml:math id="inf37"><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. The choice of dictionary element <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at time step <inline-formula><mml:math id="inf39"><mml:mi>t</mml:mi></mml:math></inline-formula> is only affected by the first and last terms of <xref ref-type="disp-formula" rid="equ8">Equation 5</xref>. Hence, the greedy objective function for choosing the dictionary element at time step <inline-formula><mml:math id="inf40"><mml:mi>t</mml:mi></mml:math></inline-formula> is given by:<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Instead of using a fixed dictionary of stimulation patterns <inline-formula><mml:math id="inf41"><mml:mi>D</mml:mi></mml:math></inline-formula> for all time steps, biological and hardware constraints on the stimulation sequence can be incorporated by changing the dictionary elements <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> available at each time step. For example, interactions between time steps produced by refractoriness were avoided by removing dictionary elements that activate cells with probability &gt;0.1 if those cells were targeted with probability &gt;0.1 in the last 100 steps.</p></sec><sec id="s4-5-2"><title>Approximate joint optimization</title><p>Instead of selecting the dictionary elements step by step, the optimal number of times each dictionary element should ideally be selected (irrespective of the order of stimulation) can be identified by reformulating the objective function as follows:<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>w</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector of non-negative integers corresponding to the number of times each dictionary element is stimulated, <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a matrix of activation probabilities for each cell and dictionary element, and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the variance in decoding corresponding to each dictionary element (given by <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>The optimization problem is NP complete due to the integer constraints on <inline-formula><mml:math id="inf47"><mml:mi>w</mml:mi></mml:math></inline-formula>. However, it can be approximately solved by relaxing the integer constraint, resulting in the following optimization problem and upper bound on performance:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>w</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5-3"><title>Perfect control optimization</title><p>To give an estimate of the best possible reconstruction when there is no constraint on electrical stimulation, the number of spikes for each cell can be directly optimized (irrespective of the order of stimulation) using the following objective function:<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector of non-negative integers corresponding to the number of times each cell spike. Again, this optimization problem can be approximately solved by relaxing the integer constraint on <inline-formula><mml:math id="inf49"><mml:mi>r</mml:mi></mml:math></inline-formula> resulting in the following optimization problem and upper bound on performance:<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5-4"><title>Static pixel-wise optimization</title><p>To approximate the function of present-day retinal implants, a mapping was learned between the intensity of the visual stimulus near each electrode and the intensity of the current passed through that electrode, to determine which electrical stimuli to deliver (irrespective of the order of stimulation).</p><p>First, an affine transformation mapped the visual stimulus onto the electrode array. Second, the average visual stimulus intensity was identified over an approximately 130 µm × 130 µm region around the electrode location. Third, the average visual stimulus intensity on the electrode <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> was mapped to the current amplitude <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> using a scaled sigmoid:<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This electrical stimulus was then delivered <inline-formula><mml:math id="inf52"><mml:mi>n</mml:mi></mml:math></inline-formula> times at that electrode. All parameters <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for each electrode were simultaneously optimized to minimize reconstruction error across a training set of random checkerboard images.</p></sec></sec><sec id="s4-6"><title>Analysis of temporal dithering using calibrated responses</title><p>Greedy temporal dithering was analyzed in <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref>, and <xref ref-type="fig" rid="fig6">6</xref> using calibrated responses to visual stimulation (stimulus reconstruction filter) and electrical stimulation (dictionary of single-electrode response probabilities).</p><p>The stimulus reconstruction filter for each RGC was approximated using the scaled receptive field (<xref ref-type="bibr" rid="bib8">Brackbill et al., 2020</xref>). Briefly, the receptive field was obtained by computing the spatial component of the rank 1 approximation of the STA. The receptive field was then denoised by computing the robust standard deviation (<inline-formula><mml:math id="inf54"><mml:mi>σ</mml:mi></mml:math></inline-formula>) of the magnitudes of all pixels, zeroing out pixels with absolute value less than 2.5<inline-formula><mml:math id="inf55"><mml:mi>σ</mml:mi></mml:math></inline-formula>, and retaining the largest spatially contiguous component. Finally, the receptive fields were scaled such that a linear-rectified response model most accurately predicted the average spike count recorded within 50 ms of the onset of static flashed checkerboard stimuli. Note that because the stimulus reconstruction filter is proportional to the receptive field, this approximation matches an optimal linear decoder only if the receptive fields of the cells are orthogonal. This is approximately true: RGC receptive fields form a uniform mosaic sampling the visual field with little overlap (<xref ref-type="bibr" rid="bib16">Devries and Baylor, 1997</xref>; <xref ref-type="bibr" rid="bib11">Chichilnisky and Kalmar, 2002</xref>).</p><p>Response probabilities for each single-electrode stimulation pattern were identified after removing electrical artifacts using custom spike sorting software (<xref ref-type="bibr" rid="bib45">Mena et al., 2017</xref>). Briefly, the spike sorting software estimates the electrical stimulation artifacts by modeling the artifact change across amplitudes with a Gaussian process, subtracts the estimated electrical artifacts from the recording and then matches the residual spikes to cell waveforms obtained from recordings obtained with a visual stimulus. The cell activation probabilities for each of the 40 different amplitudes were replaced by values from a sigmoid fitted to all levels, and collected into a dictionary. Each element in the dictionary consisted of an electrode, a stimulus current level, and the evoked spike probability for all recorded cells (typically, for any given electrode and current level, only a few nearby cells had nonzero spike probability).</p><p>Dictionary elements that involved activating distant cells along their axons were removed due to the unknown receptive field locations and thus uncertain contribution to stimulus reconstruction (<xref ref-type="bibr" rid="bib26">Grosberg et al., 2017</xref>). Briefly, the responses to electrical stimulation were mapped to a collection of weighted graphs, and graph partitioning and graph traversal algorithms were applied to identify axon bundle activity. The focus was on two characteristic features of axon bundle signals: bidirectional propagation, and growth of signal amplitude with stimulation current (<xref ref-type="bibr" rid="bib65">Tandon et al., 2021</xref>).</p><p>Finally, only dictionary elements that activated at least one cell with probability at least 0.01 were retained, resulting in 1000–5000 dictionary elements. A single dictionary element that does not activate any cell (probability = 0) was added to allow the greedy algorithm to avoid stimulation when no available stimulation pattern would decrease error.</p><p>Once visual and electrical responses were calibrated, the greedy temporal dithering sequence was applied to 20 static black and white random checkerboard targets, each for up to 10,000 steps. Responses were randomly sampled using calibrated dictionary probabilities and used to reconstruct the stimulus. Reconstruction error was reported as the squared error between the target and reconstruction normalized by the target squared (<italic>relative mean squared error</italic>).</p></sec><sec id="s4-7"><title>Validation of temporal dithering using experimentally evoked responses</title><p>Responses evoked by the greedy temporal dithering approach during a closed-loop experiment with the rat retina were analyzed in <xref ref-type="fig" rid="fig4">Figure 4</xref>. A 15-min long white noise visual stimulus (80 × 40 pixel grid, ~44 µm pixels, refresh rate 30 Hz, low photopic light level) recording was used to identify cell locations, types, and spike waveforms as described above. Subsequent analysis was restricted to two numerically dominant ON and OFF cell types, which each formed nearly complete mosaics across the array. Next, an optimal linear reconstruction filter was computed for these cells (<xref ref-type="bibr" rid="bib23">Gogliettino et al., 2023</xref>). Briefly, a linear-nonlinear cascade model was used to simulate RGC responses to white noise images by half-rectifying the inner product of the STAs and the visual stimuli. The response model was scaled to predict the average spike count recorded within 250 ms of the onset of static flashed checkerboard stimuli presented to the retina during the experiment. The optimal linear reconstruction filter was computed using linear least-squares regression of the model responses against the stimuli for a training set of checkerboard stimuli with varying pixel sizes (352, 220, 176, 110, 88, and 55 μm), using 10,000 training images each (60,000 images total).</p><p>A single-electrode stimulation scan (42 amplitudes at each of the 512 electrodes individually, 0.1–4 μA, linearly spaced, 15 times each) was then used to compute an electrical stimulus dictionary. Response probabilities for each electrical stimulus were identified using a custom template matching approach (<xref ref-type="bibr" rid="bib23">Gogliettino et al., 2023</xref>) and axon bundle activation thresholds were determined by automated methods as described above (<xref ref-type="bibr" rid="bib65">Tandon et al., 2021</xref>). Briefly, the custom template matching approach performs unsupervised clustering on the voltage traces for each stimulation pattern, then iteratively compares the difference signals between the clusters to cell waveforms estimated from visual stimulus recordings. Response probabilities for each stimulation pattern were smoothed with a sigmoid fitted across amplitudes and collected in the dictionary. Dictionary elements that elicited axon bundle activity or had at least one cell with a poor sigmoid fit were permanently removed from the dictionary. A single dictionary element that did not activate any cell was included to avoid stimulation when any real stimulation pattern would increase error.</p><p>After calibration of visual and electrical response properties, a greedy stimulation sequence was computed for each of a collection of 20 random checkerboard visual targets (10 × 5 pixel grid, ~352 µm pixels) during the closed-loop experiment using a greedy temporal dithering implementation optimized for speed (<xref ref-type="bibr" rid="bib41">Lotlikar et al., 2023</xref>). This implementation evaluates dictionary elements in parallel across several distinct regions involving disjoint groups of cells in order to speed up selection of the electrical stimulation sequence. Each stimulation sequence was assembled until the objective could not be decreased further, resulting in 171–288 stimulations. The stimulation sequence for each target was delivered 25 times at an expanded 3-ms stimulation interval to separate the recorded voltage signals from prior and future electrical stimulation artifacts. The expanded stimulation interval was necessary to determine evoked RGC response probabilities for each target, which were identified using the same custom template matching approach used for the single-electrode stimulation scan.</p></sec><sec id="s4-8"><title>Characterizing spatial exclusion radius for spatial multiplexing</title><p>The spatial exclusion radius was estimated using a bi-electrode stimulation experiment. The initial response dictionary was characterized using single-electrode stimulation as described above. A target cell was chosen, and the activation curve over the standard current range (42 amplitudes, 0.1–4 µA, linearly spaced) was determined for this cell using the electrode that recorded the largest amplitude spike waveform (primary electrode). Stimulation characterization was then repeated on this electrode with equal current passed simultaneously through a secondary electrode, and the changes in the activation curve relative to the original curve were examined. All secondary electrodes within 400 μm of the primary electrode were tested. The single-electrode activation curve was also re-estimated using secondary electrodes more than 800 μm from the primary electrode and not overlapping the axon of the target cell. Evoked spikes were identified using the spike sorting approach described for the validation experiment above.</p><p>The two-electrode stimulation produced an activation curve for each electrode pair, from which the activation threshold (estimated current amplitude producing 50% spike probability) was determined. The fractional change from the single-electrode activation threshold was computed for each secondary electrode, revealing the degree to which the presence of a secondary stimulating electrode influences the responses generated by a particular primary electrode. <xref ref-type="fig" rid="fig5">Figure 5B</xref> summarizes the absolute change in threshold with increasing distance between stimulating electrodes, generated by computing the weighted mean and the resampled standard error of the weighted mean for test pairs near each distance. The weighting for each electrode pair was inversely proportional to the variance of the single-electrode activation threshold for that cell.</p></sec><sec id="s4-9"><title>Extension of greedy dithering to natural scenes with eye movements</title><p>The greedy temporal dithering approach was extended to natural viewing by modifications to visual stimulus target generation and reconstruction. For a given natural image, a dynamic visual target was generated by simulating eye movements. A sequence of five hundred fixation locations were sampled, preferentially in the high spatial-frequency regions of the image, with a mean duration of 300 ms (SD 100 ms) between saccades. A patch of size 40 × 80 was taken around each saccade location to generate the dynamic visual stimulus. In some cases, fixational eye movements were also simulated by perturbing the fixation location with a brownian motion (3 pixel SD).</p><p>The greedy algorithm was modified such that the stimulation choice at each step considered multiple recent frames of the target. The dynamic target was discretized on the display at 120 Hz, and 83 stimulation choices were made within each frame (corresponding to a stimulation every 0.1 ms). To accommodate the dynamic stimulus, the spatial reconstruction filter was replaced with a spatiotemporal reconstruction filter. For efficiency, the spatiotemporal reconstruction filter was modeled as rank 1 (space–time separable), with the identical time course for all cells and opposite polarity for ON and OFF cells. Hence, each evoked spike influences the reconstruction at multiple subsequent time steps. The straightforward extension of the greedy algorithm is then to choose a stimulation pattern at each time step such that it minimizes the total error over multiple time steps.</p><p>For a given stimulation sequence, the image is assembled by first reconstructing each frame of the dynamic visual stimulus using the spatiotemporal reconstruction filter. Then, each frame of the reconstructed dynamic stimulus is ‘pasted’ at the fixation location at the time of the spike. The intensity for each pixel in the final reconstructed image is estimated by averaging the intensity across all fixation locations in which the recorded cells have reconstruction filters that include the pixel.</p></sec><sec id="s4-10"><title>Incorporating perceptual similarity metrics</title><p>Possible improvements to the approach that could be produced by optimizing perceptual similarity (rather than mean squared error) in the stimulation objective were analyzed after simplifying modifications. First, instead of image-dependent and random fixation locations, all possible saccade locations were considered. This corresponds to a uniform distribution of fixation locations, and the visual scene is reconstructed by averaging the reconstruction of image patches corresponding to various fixation locations. Next, for each fixation location, the corresponding image patch was reconstructed using <italic>expected</italic> responses (rather than measured, stochastic responses). Note that unlike the algorithm presented above, this formulation does not account for inter-trial variability. Finally, instead of greedily optimizing the stimulation sequence, the number of stimuli for all dictionary elements and fixation locations were jointly optimized. Given these simplifications, the following optimization problem was solved:<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">#</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf56"><mml:mi>d</mml:mi></mml:math></inline-formula> is the measure of similarity, <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the target visual stimulus,  <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the reconstruction filter, <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo>×</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the dictionary, <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of times each dictionary element is stimulated for patch <italic>i</italic>, and <inline-formula><mml:math id="inf61"><mml:mi>G</mml:mi></mml:math></inline-formula> is an operator that averages the reconstruction of individual patches to assemble the entire image. To explore the reconstruction under different stimulation budgets, <inline-formula><mml:math id="inf62"><mml:mi>λ</mml:mi></mml:math></inline-formula> is varied to penalize stimulating a large number of dictionary elements.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Data curation, Software, Investigation</p></fn><fn fn-type="con" id="con5"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Resources, Software, Investigation, Methodology</p></fn><fn fn-type="con" id="con7"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con12"><p>Resources</p></fn><fn fn-type="con" id="con13"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con14"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con15"><p>Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con16"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83424-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and code are available on Dryad at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.pk0p2ngrv">https://doi.org/10.5061/dryad.pk0p2ngrv</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>N</given-names></name><name><surname>Phillips</surname><given-names>AJ</given-names></name><name><surname>Madugula</surname><given-names>S</given-names></name><name><surname>Lotlikar</surname><given-names>A</given-names></name><name><surname>Gogliettino</surname><given-names>A</given-names></name><name><surname>Hays</surname><given-names>M</given-names></name><name><surname>Grosberg</surname><given-names>L</given-names></name><name><surname>Brown</surname><given-names>J</given-names></name><name><surname>Dusi</surname><given-names>A</given-names></name><name><surname>Tandon</surname><given-names>P</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>A</given-names></name><name><surname>Mitra</surname><given-names>S</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><source>Dryad Digital Repository</source><year iso-8601-date="2024">2024</year><data-title>Data from: Precise control of neural activity using dynamically optimized electrical stimulation</data-title><pub-id pub-id-type="doi">10.5061/dryad.pk0p2ngrv</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank J Carmena, K Bankiewicz, T Moore, W Newsome, M Taffe, T Albright, E Callaway, H Fox, R Krauzlis, S Moriarty, and the California National Primate Research Center for access to macaque retinas. We thank the Stanford Artificial Retina team for helpful discussions. We thank ALS Association Milton Safenowitz fellowship (NPS), NSF Graduate Research Fellowship Grant No. 2146755 and NSF Grant No. 1828993 (AJP), NIH NEI F30-EY-030776-03 (SM), NIH NIMH T32MH-020016, NIH NEI F31-EY-033636, the Fondation Bertarelli, the Stanford Neurosciences Graduate Program (AG), Polish Academy of Sciences DEC-2013/10/M/NZ4/00268 (PH), Research to Prevent Blindness Stein Innovation Award, Wu Tsai Neurosciences Institute Big Ideas, NIH NEI R01-EY021271, NIH NEI R01-EY029247, NIH NEI P30-EY019005, and NSF/CRCNS grant (EJC) for funding this work.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname><given-names>JM</given-names></name><name><surname>Usrey</surname><given-names>WM</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Precisely correlated firing in cells of the lateral geniculate nucleus</article-title><source>Nature</source><volume>383</volume><fpage>815</fpage><lpage>819</lpage><pub-id pub-id-type="doi">10.1038/383815a0</pub-id><pub-id pub-id-type="pmid">8893005</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Oswalt</surname><given-names>D</given-names></name><name><surname>Sun</surname><given-names>P</given-names></name><name><surname>Foster</surname><given-names>BL</given-names></name><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Niketeghad</surname><given-names>S</given-names></name><name><surname>Pouratian</surname><given-names>N</given-names></name><name><surname>Bosking</surname><given-names>WH</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamic stimulation of visual cortex produces form vision in sighted and blind humans</article-title><source>Cell</source><volume>181</volume><fpage>774</fpage><lpage>783</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.04.033</pub-id><pub-id pub-id-type="pmid">32413298</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Warland</surname><given-names>DK</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The structure and precision of retinal spike trains</article-title><source>PNAS</source><volume>94</volume><fpage>5411</fpage><lpage>5416</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.10.5411</pub-id><pub-id pub-id-type="pmid">9144251</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Refractoriness and neural precision</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>2200</fpage><lpage>2211</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-06-02200.1998</pub-id><pub-id pub-id-type="pmid">9482804</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beyeler</surname><given-names>M</given-names></name><name><surname>Nanduri</surname><given-names>D</given-names></name><name><surname>Weiland</surname><given-names>JD</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name><name><surname>Fine</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A model of ganglion axon pathways accounts for percepts elicited by retinal implants</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>9199</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-45416-4</pub-id><pub-id pub-id-type="pmid">31235711</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloch</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The argus ii retinal prosthesis system</article-title><source>Prosthesis</source><volume>01</volume><elocation-id>e4947</elocation-id><pub-id pub-id-type="doi">10.5772/intechopen.84947</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghuis</surname><given-names>BG</given-names></name><name><surname>Tadin</surname><given-names>D</given-names></name><name><surname>Lankheet</surname><given-names>MJM</given-names></name><name><surname>Lappin</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Temporal limits of visual motion processing: psychophysics and neurophysiology</article-title><source>Vision</source><volume>01</volume><elocation-id>e0005</elocation-id><pub-id pub-id-type="doi">10.3390/vision3010005</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Rhoades</surname><given-names>C</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reconstruction of natural images from responses of primate retinal ganglion cells</article-title><source>eLife</source><volume>9</volume><elocation-id>e58516</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58516</pub-id><pub-id pub-id-type="pmid">33146609</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cha</surname><given-names>K</given-names></name><name><surname>Horch</surname><given-names>KW</given-names></name><name><surname>Normann</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Mobility performance with a pixelized vision system</article-title><source>Vision Research</source><volume>32</volume><fpage>1367</fpage><lpage>1372</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(92)90229-c</pub-id><pub-id pub-id-type="pmid">1455709</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Fernandez</surname><given-names>E</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Shape perception via a high-channel-count neuroprosthesis in monkey visual cortex</article-title><source>Science</source><volume>370</volume><fpage>1191</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1126/science.abd7435</pub-id><pub-id pub-id-type="pmid">33273097</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Kalmar</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Functional asymmetries in ON and OFF ganglion cells of primate retina</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>2737</fpage><lpage>2747</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-07-02737.2002</pub-id><pub-id pub-id-type="pmid">11923439</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>JS</given-names></name><name><surname>Brockmeier</surname><given-names>AJ</given-names></name><name><surname>McNiel</surname><given-names>DB</given-names></name><name><surname>von Kraus</surname><given-names>L</given-names></name><name><surname>Príncipe</surname><given-names>JC</given-names></name><name><surname>Francis</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Eliciting naturalistic cortical responses with a sensory prosthesis via optimized microstimulation</article-title><source>Journal of Neural Engineering</source><volume>13</volume><elocation-id>056007</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/13/5/056007</pub-id><pub-id pub-id-type="pmid">27518368</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname><given-names>CS</given-names></name><name><surname>Renner</surname><given-names>M</given-names></name><name><surname>Gross-Scherf</surname><given-names>B</given-names></name><name><surname>Goldblum</surname><given-names>D</given-names></name><name><surname>Munz</surname><given-names>M</given-names></name><name><surname>Krol</surname><given-names>J</given-names></name><name><surname>Szikra</surname><given-names>T</given-names></name><name><surname>Papasaikas</surname><given-names>P</given-names></name><name><surname>Cuttat</surname><given-names>R</given-names></name><name><surname>Waldt</surname><given-names>A</given-names></name><name><surname>Diggelmann</surname><given-names>R</given-names></name><name><surname>Patino-Alvarez</surname><given-names>CP</given-names></name><name><surname>Gerber-Hollbach</surname><given-names>N</given-names></name><name><surname>Schuierer</surname><given-names>S</given-names></name><name><surname>Hou</surname><given-names>Y</given-names></name><name><surname>Srdanovic</surname><given-names>A</given-names></name><name><surname>Balogh</surname><given-names>M</given-names></name><name><surname>Panero</surname><given-names>R</given-names></name><name><surname>Hasler</surname><given-names>PW</given-names></name><name><surname>Kusnyerik</surname><given-names>A</given-names></name><name><surname>Szabo</surname><given-names>A</given-names></name><name><surname>Stadler</surname><given-names>MB</given-names></name><name><surname>Orgül</surname><given-names>S</given-names></name><name><surname>Hierlemann</surname><given-names>A</given-names></name><name><surname>Scholl</surname><given-names>HPN</given-names></name><name><surname>Roma</surname><given-names>G</given-names></name><name><surname>Nigsch</surname><given-names>F</given-names></name><name><surname>Roska</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cell types of the human retina and its organoids at single-cell resolution: developmental convergence, transcriptomic identity, and disease map</article-title><source>Cell</source><volume>182</volume><fpage>1623</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.08.013</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The mosaic of midget ganglion cells in the human retina</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>5334</fpage><lpage>5355</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.13-12-05334.1993</pub-id><pub-id pub-id-type="pmid">8254378</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Ruyter van Steveninck</surname><given-names>J</given-names></name><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Wezel</surname><given-names>R</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>End-to-end optimization of prosthetic vision</article-title><source>Journal of Vision</source><volume>22</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1167/jov.22.2.20</pub-id><pub-id pub-id-type="pmid">35703408</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devries</surname><given-names>SH</given-names></name><name><surname>Baylor</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Mosaic arrangement of ganglion cell receptive fields in rabbit retina</article-title><source>Journal of Neurophysiology</source><volume>78</volume><fpage>2048</fpage><lpage>2060</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.78.4.2048</pub-id><pub-id pub-id-type="pmid">9325372</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downey</surname><given-names>JE</given-names></name><name><surname>Schwed</surname><given-names>N</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Collinger</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Intracortical recording stability in human brain-computer interface users</article-title><source>Journal of Neural Engineering</source><volume>15</volume><elocation-id>046016</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aab7a0</pub-id><pub-id pub-id-type="pmid">29553484</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>VH</given-names></name><name><surname>Grosberg</surname><given-names>LE</given-names></name><name><surname>Madugula</surname><given-names>SS</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Epiretinal stimulation with local returns enhances selectivity at cellular resolution</article-title><source>Journal of Neural Engineering</source><volume>16</volume><elocation-id>025001</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aaeef1</pub-id><pub-id pub-id-type="pmid">30523958</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Information processing in the primate retina: circuitry and coding</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>1</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094252</pub-id><pub-id pub-id-type="pmid">17335403</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesher</surname><given-names>SN</given-names></name><name><surname>Collinger</surname><given-names>JL</given-names></name><name><surname>Foldes</surname><given-names>ST</given-names></name><name><surname>Weiss</surname><given-names>JM</given-names></name><name><surname>Downey</surname><given-names>JE</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Boninger</surname><given-names>ML</given-names></name><name><surname>Gaunt</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Intracortical microstimulation of human somatosensory cortex</article-title><source>Science Translational Medicine</source><volume>8</volume><elocation-id>361ra141</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aaf8083</pub-id><pub-id pub-id-type="pmid">27738096</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frechette</surname><given-names>ES</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Grivich</surname><given-names>MI</given-names></name><name><surname>Petrusca</surname><given-names>D</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fidelity of the ensemble code for visual motion in primate retina</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>119</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1152/jn.01175.2004</pub-id><pub-id pub-id-type="pmid">15625091</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaylor</surname><given-names>JM</given-names></name><name><surname>Raman</surname><given-names>G</given-names></name><name><surname>Chung</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>M</given-names></name><name><surname>Lau</surname><given-names>J</given-names></name><name><surname>Poe</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cochlear implantation in adults: a systematic review and meta-analysis</article-title><source>JAMA Otolaryngology-- Head &amp; Neck Surgery</source><volume>139</volume><fpage>265</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1001/jamaoto.2013.1744</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Madugula</surname><given-names>SS</given-names></name><name><surname>Grosberg</surname><given-names>LE</given-names></name><name><surname>Vilkhu</surname><given-names>RS</given-names></name><name><surname>Brown</surname><given-names>J</given-names></name><name><surname>Nguyen</surname><given-names>H</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Dąbrowski</surname><given-names>W</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>High-fidelity reproduction of visual signals by electrical stimulation in the central primate retina</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>4625</fpage><lpage>4641</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1091-22.2023</pub-id><pub-id pub-id-type="pmid">37188516</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goo</surname><given-names>YS</given-names></name><name><surname>Park</surname><given-names>DJ</given-names></name><name><surname>Ahn</surname><given-names>JR</given-names></name><name><surname>Senok</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spontaneous oscillatory rhythms in the degenerating mouse retina modulate retinal ganglion cell responses to electrical stimulation</article-title><source>Frontiers in Cellular Neuroscience</source><volume>9</volume><elocation-id>512</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2015.00512</pub-id><pub-id pub-id-type="pmid">26793063</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Granley</surname><given-names>J</given-names></name><name><surname>Relic</surname><given-names>L</given-names></name><name><surname>Beyeler</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Hybrid Neural Autoencoder for Sensory Neuroprostheses and Its Applications in Bionic Vision</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2205.13623">http://arxiv.org/abs/2205.13623</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosberg</surname><given-names>LE</given-names></name><name><surname>Ganesan</surname><given-names>K</given-names></name><name><surname>Goetz</surname><given-names>GA</given-names></name><name><surname>Madugula</surname><given-names>SS</given-names></name><name><surname>Bhaskhar</surname><given-names>N</given-names></name><name><surname>Fan</surname><given-names>V</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Mitra</surname><given-names>S</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Activation of ganglion cells and axon bundles using epiretinal electrical stimulation</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>1457</fpage><lpage>1471</lpage><pub-id pub-id-type="doi">10.1152/jn.00750.2016</pub-id><pub-id pub-id-type="pmid">28566464</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haji Ghaffari</surname><given-names>D</given-names></name><name><surname>Akwaboah</surname><given-names>AD</given-names></name><name><surname>Mirzakhalili</surname><given-names>E</given-names></name><name><surname>Weiland</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Real-time optimization of retinal ganglion cell spatial activity in response to epiretinal stimulation</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>29</volume><fpage>2733</fpage><lpage>2741</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2021.3138297</pub-id><pub-id pub-id-type="pmid">34941514</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>E</given-names></name><name><surname>Boffa</surname><given-names>J</given-names></name><name><surname>Palanker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Performance of complex visual tasks using simulated prosthetic vision via augmented-reality glasses</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.1167/19.13.22</pub-id><pub-id pub-id-type="pmid">31770773</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Dąbrowski</surname><given-names>W</given-names></name><name><surname>Skoczeń</surname><given-names>A</given-names></name><name><surname>Wiącek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>An integrated multichannel waveform generator for large-scale spatio-temporal stimulation of neural tissue</article-title><source>Analog Integrated Circuits and Signal Processing</source><volume>55</volume><fpage>239</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1007/s10470-007-9125-x</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Skoczeń</surname><given-names>A</given-names></name><name><surname>Gunning</surname><given-names>DE</given-names></name><name><surname>Kachiguine</surname><given-names>S</given-names></name><name><surname>Mathieson</surname><given-names>K</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Wiącek</surname><given-names>P</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Dąbrowski</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Properties and application of a multichannel integrated circuit for low-artifact, patterned electrical stimulation of neural tissue</article-title><source>Journal of Neural Engineering</source><volume>9</volume><elocation-id>066005</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/9/6/066005</pub-id><pub-id pub-id-type="pmid">23160018</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humayun</surname><given-names>MS</given-names></name><name><surname>Dorn</surname><given-names>JD</given-names></name><name><surname>da Cruz</surname><given-names>L</given-names></name><name><surname>Dagnelie</surname><given-names>G</given-names></name><name><surname>Sahel</surname><given-names>JA</given-names></name><name><surname>Stanga</surname><given-names>PE</given-names></name><name><surname>Cideciyan</surname><given-names>AV</given-names></name><name><surname>Duncan</surname><given-names>JL</given-names></name><name><surname>Eliott</surname><given-names>D</given-names></name><name><surname>Filley</surname><given-names>E</given-names></name><name><surname>Ho</surname><given-names>AC</given-names></name><name><surname>Santos</surname><given-names>A</given-names></name><name><surname>Safran</surname><given-names>AB</given-names></name><name><surname>Arditi</surname><given-names>A</given-names></name><name><surname>Del Priore</surname><given-names>LV</given-names></name><name><surname>Greenberg</surname><given-names>RJ</given-names></name><collab>Argus II Study Group</collab></person-group><year iso-8601-date="2012">2012</year><article-title>Interim results from the international trial of Second Sight's visual prosthesis</article-title><source>Ophthalmology</source><volume>119</volume><fpage>779</fpage><lpage>788</lpage><pub-id pub-id-type="doi">10.1016/j.ophtha.2011.09.028</pub-id><pub-id pub-id-type="pmid">22244176</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepson</surname><given-names>LH</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Mathieson</surname><given-names>K</given-names></name><name><surname>Gunning</surname><given-names>DE</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Focal electrical stimulation of major ganglion cell types in the primate retina for the design of visual prostheses</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>7194</fpage><lpage>7205</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4967-12.2013</pub-id><pub-id pub-id-type="pmid">23616529</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepson</surname><given-names>LH</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Mathieson</surname><given-names>K</given-names></name><name><surname>Gunning</surname><given-names>DE</given-names></name><name><surname>Dąbrowski</surname><given-names>W</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spatially patterned electrical stimulation to enhance resolution of retinal prostheses</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4871</fpage><lpage>4881</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2882-13.2014</pub-id><pub-id pub-id-type="pmid">24695706</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>LA</given-names></name><name><surname>Wander</surname><given-names>JD</given-names></name><name><surname>Sarma</surname><given-names>D</given-names></name><name><surname>Su</surname><given-names>DK</given-names></name><name><surname>Fetz</surname><given-names>EE</given-names></name><name><surname>Ojemann</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Direct electrical stimulation of the somatosensory cortex in humans using electrocorticography electrodes: a qualitative and quantitative report</article-title><source>Journal of Neural Engineering</source><volume>10</volume><elocation-id>036021</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/10/3/036021</pub-id><pub-id pub-id-type="pmid">23665776</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keat</surname><given-names>J</given-names></name><name><surname>Reinagel</surname><given-names>P</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Predicting every spike: a model for the responses of visual neurons</article-title><source>Neuron</source><volume>30</volume><fpage>803</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00322-1</pub-id><pub-id pub-id-type="pmid">11430813</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>YJ</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Batty</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Tong</surname><given-names>W</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Nonlinear decoding of natural images from large-scale primate retinal ganglion recordings</article-title><source>Neural Computation</source><volume>33</volume><fpage>1719</fpage><lpage>1750</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01395</pub-id><pub-id pub-id-type="pmid">34411268</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Wu</surname><given-names>EG</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Silva</surname><given-names>RA</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Functional organization of midget and parasol ganglion cells in the human retina</article-title><source>Neuroscience</source><volume>01</volume><elocation-id>e0762</elocation-id><pub-id pub-id-type="doi">10.1101/2020.08.07.240762</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>PH</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Schiff</surname><given-names>M</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Ahn</surname><given-names>D</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Anatomical identification of extracellularly recorded cells in large-scale multielectrode recordings</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>4663</fpage><lpage>4675</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3675-14.2015</pub-id><pub-id pub-id-type="pmid">25788683</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lieby</surname><given-names>P</given-names></name><name><surname>Barnes</surname><given-names>N</given-names></name><name><surname>McCarthy</surname><given-names>C</given-names></name><name><surname>Dennett</surname><given-names>H</given-names></name><name><surname>Walker</surname><given-names>JG</given-names></name><name><surname>Botea</surname><given-names>V</given-names></name><name><surname>Scott</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Substituting Depth for Intensity and Real-Time Phosphene Rendering: Visual Navigation under Low Vision Conditions</article-title><conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><fpage>8017</fpage><lpage>8020</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2011.6091977</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Bezayiff</surname><given-names>N</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Cunningham</surname><given-names>W</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Grillo</surname><given-names>AA</given-names></name><name><surname>Grivich</surname><given-names>M</given-names></name><name><surname>Grybos</surname><given-names>P</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Kachiguine</surname><given-names>S</given-names></name><name><surname>Kalmar</surname><given-names>RS</given-names></name><name><surname>Mathieson</surname><given-names>K</given-names></name><name><surname>Petrusca</surname><given-names>D</given-names></name><name><surname>Rahman</surname><given-names>M</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What does the eye tell the brain?: Development of a system for the large-scale recording of retinal output activity</article-title><source>IEEE Transactions on Nuclear Science</source><volume>51</volume><fpage>1434</fpage><lpage>1440</lpage><pub-id pub-id-type="doi">10.1109/TNS.2004.832706</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lotlikar</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Vilkhu</surname><given-names>R</given-names></name><name><surname>Madugula</surname><given-names>S</given-names></name><name><surname>Grosberg</surname><given-names>L</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>A</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Mitra</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Partitioned Temporal Dithering for Efficient Epiretinal Electrical Stimulation</article-title><conf-name>11th International IEEE/EMBS Conference on Neural Engineering</conf-name><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/NER52421.2023.10123787</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loudin</surname><given-names>JD</given-names></name><name><surname>Simanovskii</surname><given-names>DM</given-names></name><name><surname>Vijayraghavan</surname><given-names>K</given-names></name><name><surname>Sramek</surname><given-names>CK</given-names></name><name><surname>Butterwick</surname><given-names>AF</given-names></name><name><surname>Huie</surname><given-names>P</given-names></name><name><surname>McLean</surname><given-names>GY</given-names></name><name><surname>Palanker</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Optoelectronic retinal prosthesis: system design and performance</article-title><source>Journal of Neural Engineering</source><volume>4</volume><fpage>S72</fpage><lpage>S84</lpage><pub-id pub-id-type="doi">10.1088/1741-2560/4/1/S09</pub-id><pub-id pub-id-type="pmid">17325419</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madugula</surname><given-names>SS</given-names></name><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Zaidi</surname><given-names>M</given-names></name><name><surname>Aggarwal</surname><given-names>G</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Brown</surname><given-names>JB</given-names></name><name><surname>Vilkhu</surname><given-names>R</given-names></name><name><surname>Hays</surname><given-names>MR</given-names></name><name><surname>Nguyen</surname><given-names>H</given-names></name><name><surname>Fan</surname><given-names>V</given-names></name><name><surname>Wu</surname><given-names>EG</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Silva</surname><given-names>RA</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Focal electrical stimulation of human retinal ganglion cells for vision restoration</article-title><source>Journal of Neural Engineering</source><volume>19</volume><pub-id pub-id-type="doi">10.1088/1741-2552/aca5b5</pub-id><pub-id pub-id-type="pmid">36533865</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McCarthy</surname><given-names>C</given-names></name><name><surname>Barnes</surname><given-names>N</given-names></name><name><surname>Lieby</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>33rd Annual International Conference</article-title><conf-name>33rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><fpage>4457</fpage><lpage>4460</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2011.6091105</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mena</surname><given-names>GE</given-names></name><name><surname>Grosberg</surname><given-names>LE</given-names></name><name><surname>Madugula</surname><given-names>S</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Litke</surname><given-names>A</given-names></name><name><surname>Cunningham</surname><given-names>J</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Electrical stimulus artifact cancellation and neural spike detection on large multi-electrode arrays</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005842</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005842</pub-id><pub-id pub-id-type="pmid">29131818</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural reorganization following sensory loss: the opportunity of change</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>44</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1038/nrn2758</pub-id><pub-id pub-id-type="pmid">19935836</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palanker</surname><given-names>D</given-names></name><name><surname>Le Mer</surname><given-names>Y</given-names></name><name><surname>Mohand-Said</surname><given-names>S</given-names></name><name><surname>Muqit</surname><given-names>M</given-names></name><name><surname>Sahel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Photovoltaic restoration of central vision in atrophic age-related macular degeneration</article-title><source>Ophthalmology</source><volume>127</volume><fpage>1097</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1016/j.ophtha.2020.02.024</pub-id><pub-id pub-id-type="pmid">32249038</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname><given-names>N</given-names></name><name><surname>Batty</surname><given-names>E</given-names></name><name><surname>Falcon</surname><given-names>W</given-names></name><name><surname>Rutten</surname><given-names>T</given-names></name><name><surname>Rajpal</surname><given-names>M</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural networks for efficient bayesian decoding of natural images from retinal neurons</article-title><source>Neuroscience</source><volume>01</volume><elocation-id>e3759</elocation-id><pub-id pub-id-type="doi">10.1101/153759</pub-id><pub-id pub-id-type="pmid">29184197</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perge</surname><given-names>JA</given-names></name><name><surname>Homer</surname><given-names>ML</given-names></name><name><surname>Malik</surname><given-names>WQ</given-names></name><name><surname>Cash</surname><given-names>S</given-names></name><name><surname>Eskandar</surname><given-names>E</given-names></name><name><surname>Friehs</surname><given-names>G</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Intra-day signal instabilities affect decoding performance in an intracortical neural interface system</article-title><source>Journal of Neural Engineering</source><volume>10</volume><elocation-id>036004</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/10/3/036004</pub-id><pub-id pub-id-type="pmid">23574741</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravi</surname><given-names>S</given-names></name><name><surname>Ahn</surname><given-names>D</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pathway-specific asymmetries between on and off visual signals</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9728</fpage><lpage>9740</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2008-18.2018</pub-id><pub-id pub-id-type="pmid">30249795</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reich</surname><given-names>DS</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Knight</surname><given-names>BW</given-names></name><name><surname>Ozaki</surname><given-names>T</given-names></name><name><surname>Kaplan</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Response variability and timing precision of neuronal spike trains in vivo</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>2836</fpage><lpage>2841</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.5.2836</pub-id><pub-id pub-id-type="pmid">9163398</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Relic</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Tuan</surname><given-names>YL</given-names></name><name><surname>Beyeler</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep Learning–Based Perceptual Stimulus Encoder for Bionic Vision</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2203.05604">https://arxiv.org/abs/2203.05604</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhoades</surname><given-names>CE</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Manookin</surname><given-names>MB</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Goetz</surname><given-names>G</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unusual physiological properties of smooth monostratified ganglion cell types in primate retina</article-title><source>Neuron</source><volume>103</volume><fpage>658</fpage><lpage>672</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.036</pub-id><pub-id pub-id-type="pmid">31227309</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Richard</surname><given-names>E</given-names></name><name><surname>Goetz</surname><given-names>GA</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Recognizing Retinal Ganglion Cells in the Dark</chapter-title><source>In Advances in Neural Information Processing Systems 28</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>2476</fpage><lpage>2484</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rodieck</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>The First Steps in Seeing</source><publisher-name>Sinauer Associates Incorporated</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouger</surname><given-names>J</given-names></name><name><surname>Lagleyre</surname><given-names>S</given-names></name><name><surname>Fraysse</surname><given-names>B</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Deguine</surname><given-names>O</given-names></name><name><surname>Barone</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Evidence that cochlear-implanted deaf patients are better multisensory integrators</article-title><source>PNAS</source><volume>104</volume><fpage>7295</fpage><lpage>7300</lpage><pub-id pub-id-type="doi">10.1073/pnas.0609419104</pub-id><pub-id pub-id-type="pmid">17404220</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salas</surname><given-names>A</given-names></name><name><surname>Michelle</surname><given-names>LB</given-names></name><name><surname>Kellis</surname><given-names>S</given-names></name><name><surname>Jafari</surname><given-names>M</given-names></name><name><surname>Jo</surname><given-names>H</given-names></name><name><surname>Kramer</surname><given-names>D</given-names></name><name><surname>Shanfield</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Proprioceptive and cutaneous sensations in humans elicited by intracortical microstimulation</article-title><source>eLife</source><volume>07</volume><elocation-id>e2904</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32904</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samaha</surname><given-names>J</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The speed of alpha-band oscillations predicts the temporal resolution of visual perception</article-title><source>Current Biology</source><volume>25</volume><fpage>2985</fpage><lpage>2990</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.10.007</pub-id><pub-id pub-id-type="pmid">26526370</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekirnjak</surname><given-names>C</given-names></name><name><surname>Jepson</surname><given-names>LH</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Changes in physiological properties of rat ganglion cells during retinal degeneration</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>2560</fpage><lpage>2571</lpage><pub-id pub-id-type="doi">10.1152/jn.01061.2010</pub-id><pub-id pub-id-type="pmid">21389304</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soto</surname><given-names>F</given-names></name><name><surname>Hsiang</surname><given-names>JC</given-names></name><name><surname>Rajagopal</surname><given-names>R</given-names></name><name><surname>Piggott</surname><given-names>K</given-names></name><name><surname>Harocopos</surname><given-names>GJ</given-names></name><name><surname>Couch</surname><given-names>SM</given-names></name><name><surname>Custer</surname><given-names>P</given-names></name><name><surname>Morgan</surname><given-names>JL</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient coding by midget and parasol ganglion cells in the human retina</article-title><source>Neuron</source><volume>107</volume><fpage>656</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.05.030</pub-id><pub-id pub-id-type="pmid">32533915</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stingl</surname><given-names>K</given-names></name><name><surname>Bartz-Schmidt</surname><given-names>KU</given-names></name><name><surname>Besch</surname><given-names>D</given-names></name><name><surname>Braun</surname><given-names>A</given-names></name><name><surname>Bruckmann</surname><given-names>A</given-names></name><name><surname>Gekeler</surname><given-names>F</given-names></name><name><surname>Greppmaier</surname><given-names>U</given-names></name><name><surname>Hipp</surname><given-names>S</given-names></name><name><surname>Hörtdörfer</surname><given-names>G</given-names></name><name><surname>Kernstock</surname><given-names>C</given-names></name><name><surname>Koitschev</surname><given-names>A</given-names></name><name><surname>Kusnyerik</surname><given-names>A</given-names></name><name><surname>Sachs</surname><given-names>H</given-names></name><name><surname>Schatz</surname><given-names>A</given-names></name><name><surname>Stingl</surname><given-names>KT</given-names></name><name><surname>Peters</surname><given-names>T</given-names></name><name><surname>Wilhelm</surname><given-names>B</given-names></name><name><surname>Zrenner</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Artificial vision with wirelessly powered subretinal electronic implant alpha-IMS</article-title><source>Proceedings. Biological Sciences</source><volume>280</volume><elocation-id>20130077</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2013.0077</pub-id><pub-id pub-id-type="pmid">23427175</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadin</surname><given-names>D</given-names></name><name><surname>Lappin</surname><given-names>JS</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name><name><surname>Glasser</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>High temporal precision for perceiving event offsets</article-title><source>Vision Research</source><volume>50</volume><fpage>1966</fpage><lpage>1971</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.07.005</pub-id><pub-id pub-id-type="pmid">20650287</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>MacDowell</surname><given-names>CJ</given-names></name><name><surname>Che</surname><given-names>Z</given-names></name><name><surname>Letai</surname><given-names>KC</given-names></name><name><surname>Steinhardt</surname><given-names>CR</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning to control the brain through adaptive closed-loop patterned stimulation</article-title><source>Journal of Neural Engineering</source><volume>17</volume><elocation-id>056007</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/abb860</pub-id><pub-id pub-id-type="pmid">32927437</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Talaminos-Barroso</surname><given-names>A</given-names></name><name><surname>Reina-Tosina</surname><given-names>J</given-names></name><name><surname>Roa-Romero</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Models Based on Cellular Automata for the Analysis of Biomedical Systems</chapter-title><source>In Control Applications for Biomedical Engineering Systems</source><publisher-name>Elsevier</publisher-name><fpage>405</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-817461-6.00014-7</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tandon</surname><given-names>P</given-names></name><name><surname>Bhaskhar</surname><given-names>N</given-names></name><name><surname>Shah</surname><given-names>N</given-names></name><name><surname>Madugula</surname><given-names>S</given-names></name><name><surname>Grosberg</surname><given-names>L</given-names></name><name><surname>Fan</surname><given-names>VH</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Mitra</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Automatic identification of axon bundle activation for epiretinal prosthesis</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>29</volume><fpage>2496</fpage><lpage>2502</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2021.3128486</pub-id><pub-id pub-id-type="pmid">34784278</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trenholm</surname><given-names>S</given-names></name><name><surname>Awatramani</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Origins of spontaneous activity in the degenerating retina</article-title><source>Frontiers in Cellular Neuroscience</source><volume>9</volume><elocation-id>277</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2015.00277</pub-id><pub-id pub-id-type="pmid">26283914</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uzzell</surname><given-names>VJ</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Precision of spike trains in primate retinal ganglion cells</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>780</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.1152/jn.01171.2003</pub-id><pub-id pub-id-type="pmid">15277596</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vasireddy</surname><given-names>PK</given-names></name><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Brown</surname><given-names>JB</given-names></name><name><surname>Vilkhu</surname><given-names>RS</given-names></name><name><surname>Madugula</surname><given-names>SS</given-names></name><name><surname>Phillips</surname><given-names>AJ</given-names></name><name><surname>Mitral</surname><given-names>S</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Efficient Modeling and Calibration of Multi-Electrode Stimuli for Epiretinal Implants</article-title><conf-name>2023 11th International IEEE/EMBS Conference on Neural Engineering (NER</conf-name><conf-loc>Baltimore, MD, USA</conf-loc><pub-id pub-id-type="doi">10.1109/NER52421.2023.10123907</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergnieux</surname><given-names>V</given-names></name><name><surname>Macé</surname><given-names>MJM</given-names></name><name><surname>Jouffrais</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Simplification of visual rendering in simulated prosthetic vision facilitates navigation</article-title><source>Artificial Organs</source><volume>41</volume><fpage>852</fpage><lpage>861</lpage><pub-id pub-id-type="doi">10.1111/aor.12868</pub-id><pub-id pub-id-type="pmid">28321887</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vilkhu</surname><given-names>RS</given-names></name><name><surname>Madugula</surname><given-names>SS</given-names></name><name><surname>Grosberg</surname><given-names>LE</given-names></name><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Hottowy</surname><given-names>P</given-names></name><name><surname>Dabrowski</surname><given-names>W</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Mitra</surname><given-names>S</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spatially patterned bi-electrode epiretinal stimulation for axon avoidance at cellular resolution</article-title><source>Journal of Neural Engineering</source><volume>18</volume><pub-id pub-id-type="doi">10.1088/1741-2552/ac3450</pub-id><pub-id pub-id-type="pmid">34710857</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Foundations of Vision</source><publisher-name>Sinauer Associates, Incorporated</publisher-name></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Sheikh</surname><given-names>HR</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Image quality assessment: from error visibility to structural similarity</article-title><source>IEEE Transactions on Image Processing</source><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/tip.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warland</surname><given-names>DK</given-names></name><name><surname>Reinagel</surname><given-names>P</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Decoding visual information from a population of retinal ganglion cells</article-title><source>Journal of Neurophysiology</source><volume>78</volume><fpage>2336</fpage><lpage>2350</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.78.5.2336</pub-id><pub-id pub-id-type="pmid">9356386</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wässle</surname><given-names>H</given-names></name><name><surname>Peichl</surname><given-names>L</given-names></name><name><surname>Boycott</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Dendritic territories of cat retinal ganglion cells</article-title><source>Nature</source><volume>292</volume><fpage>344</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1038/292344a0</pub-id><pub-id pub-id-type="pmid">7254331</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>EG</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Maximum a posteriori natural scene reconstruction from retinal ganglion cells with deep denoiser priors</article-title><source>Neuroscience</source><volume>01</volume><elocation-id>e2737</elocation-id><pub-id pub-id-type="doi">10.1101/2022.05.19.492737</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>EG</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Rhoades</surname><given-names>C</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Fixational Eye Movements Enhance the Precision of Visual Information Transmitted by the Primate Retina</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.08.12.552902</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wutz</surname><given-names>A</given-names></name><name><surname>Muschter</surname><given-names>E</given-names></name><name><surname>van Koningsbruggen</surname><given-names>MG</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name><name><surname>Melcher</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Temporal Integration Windows in Neural Processing and Perception Aligned to Saccadic Eye Movements</article-title><source>Current Biology</source><volume>26</volume><fpage>1659</fpage><lpage>1668</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.04.070</pub-id><pub-id pub-id-type="pmid">27291050</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yarbus</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="1967">1967</year><source>Eye Movements and Vision</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4899-5379-7</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidi</surname><given-names>M</given-names></name><name><surname>Aggarwal</surname><given-names>G</given-names></name><name><surname>Shah</surname><given-names>NP</given-names></name><name><surname>Karniol-Tambour</surname><given-names>O</given-names></name><name><surname>Goetz</surname><given-names>G</given-names></name><name><surname>Madugula</surname><given-names>S</given-names></name><name><surname>Gogliettino</surname><given-names>AR</given-names></name><name><surname>Wu</surname><given-names>EG</given-names></name><name><surname>Kling</surname><given-names>A</given-names></name><name><surname>Brackbill</surname><given-names>N</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Inferring Light Responses of Primate Retinal Ganglion Cells Using Intrinsic Electrical Signatures</article-title><source>J Neural Eng</source><volume>20</volume><elocation-id>e3858</elocation-id><pub-id pub-id-type="doi">10.1101/2022.05.29.493858</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83424.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Beyeler</surname><given-names>Michael</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>This valuable study proposes a new algorithm for determining the electrical stimulation delivered through a sensory-neural/retinal implant with the aim of improving the perceptual benefit to implant users. The evidence supporting the conclusions is solid, with additional experiments and analyses submitted during the revision having significantly strengthened the study. The work will be of interest to both neuroscientists and neuroengineers.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83424.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Beyeler</surname><given-names>Michael</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Shivdasani</surname><given-names>Mohit</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03r8z3t63</institution-id><institution>University of New South Wales</institution></institution-wrap><country>Australia</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Precise control of neural activity using temporally dithered and spatially multiplexed electrical stimulation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Mohit Shivdasani (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Included here is a brief evaluation summary and list of revisions the reviewers and review editor deem essential for the authors to address. The public summaries and full, individual reviewers' recommendations for the authors are also appended below. The authors are advised to address the public summaries briefly, and the individual recommendations in a detailed, point-by-point manner.</p><p>As you will be able to read below, reviewers appreciated the importance of the study and its potentially broad interest. The approach to formulating the problem of choosing electrical stimuli for visual prostheses as a data-driven optimization problem holds promise for several sensory-neural prostheses. The writing was relatively clear, the figures appropriate, and the methods mostly rigorous. However, reviewers raised concerns with regard to some of the claims made, particularly as it pertains to the full greedy, dithering, multiplexed algorithm and its potential to greatly improve the quality of vision delivered by a retinal implant. The key points that need to be addressed can be summarized as follows:</p><p>1) Please provide more experimental data (specifically: reconstructed images and reconstruction errors) to substantiate the claim that the algorithm can improve the quality of vision in an ex vivo setting. The main evidence that is presented about the quality of vision that might be achieved is a computer simulation; that is, the image reconstructions and reconstruction errors given in Figures 2 &amp; 3 with the dithered, but not multiplexed version of the algorithm. However, the same cannot be said about the ex vivo experiment. While the outputs of the dithered &amp; multiplex version were indeed applied to ex vivo retina, the output is all simulated and no experimental validation data is presented. However, it is possible that the experimentally observed retinal output might differ from (the assumption of) a linear sum of dictionary elements. All reviewers agreed that if the authors could report on the results of the experiments in which algorithmic stimulation is applied to ex vivo retina, and report this in terms of image reconstructions and reconstruction error, this would greatly improve the strength of evidence.</p><p>2) Please expand the discussion on the theoretical assumptions regarding visual processing in the brain and the perception of phosphenes through electrical stimulation that the study is based on, as it may limit the translational impact of the work. All reviewers agreed that the study relies on several significant assumptions about neural coding in the retina, the visual brain, and interactions between electrodes, some of which have been recently challenged. This includes the assumption that neural coding in the retina is solely based on a firing-rate code, that visual perception is solely based on the number of spikes within a slow temporal integration window, and that non-simultaneous interleaved electrical stimulation does not lead to neural interactions. At a minimum, the authors should address these limitations clearly in their Discussion, and comment on the potential implications of the failure of these assumptions on their algorithm performance.</p><p>3) Please clarify which figures/results are from simulations and which are from experimental data.</p><p><italic>Reviewer #1:</italic></p><p>Shah et al. propose an algorithm to precisely control RGC activation using electrical stimulation, using temporal dithering, and spatial multiplexing. The main assumption is that the brain has perceptual integration windows, during which a visual percept can be built up by stimulating single (or small groups of) neurons in rapid succession. Which electrodes to stimulate to achieve a desired percept is dictated by a dictionary of stimulation patterns. The authors demonstrate the effectiveness of their method on ex vivo recordings of ON and OFF parasol cells.</p><p>The biggest strengths of the study are the theoretical contributions and the experimental recordings used to demonstrate the effectiveness of their algorithm. The thinking follows a number of recent efforts in the field to think about visual prosthetic stimulation as a closed-loop data-driven optimization problem. This may have benefits over other open-loop stimulation techniques.</p><p>However, the biggest weakness of the study is a reliance on a number of controversial assumptions about the neural code of vision. The first is the existence of a slow temporal integration window during which the brain cannot distinguish the order of stimuli presented and/or sums up RGC activity to decode the presented stimulus. The paper presents only limited (and dated) evidence for this. Second, the assumption of a Bernoulli distribution is at the very least limiting, as neurons may respond with multiple spikes to a stimulation pattern and some spatial features may be encoded by the relative timing of spikes across neurons. Third, even though the temporal dithering may avoid electrical crosstalk, there may still be neuronal crosstalk on longer timescales, thus challenging the independence assumption. Extending the delay between stimuli in order to avoid neuronal crosstalk may severely limit the utility of the proposed algorithm since it would cap the number of stimuli that could be delivered in one of the assumed temporal integration windows.</p><p>Even if the assumptions hold, the presented evidence of the ex vivo recordings would need to provide some additional detail before the practical utility of the proposed algorithm could be judged accordingly. Although the study reports reconstruction errors and example reconstructed images for the simulation experiment, the same cannot be said about the ex vivo experiment. In addition, real-world implementation of the algorithm would presumably include not just stimulus delivery but also in-the-loop stimulus decoding, and it is not clear how quickly that could be done. Lastly, the linear filters would have to be estimated in a degenerated retina, where one could not rely on responses to light stimuli. The paper notes that this could be done by considering the spontaneous activity of cells – I can see how that could allow you to distinguish between ON and OFF cells, for instance, but it is not clear to me how that would allow you to determine the linear filter for each cell. For those reasons, it is somewhat hard to judge the practical utility and potential significance of the proposed algorithm.</p><p>Recommendations for the authors:</p><p>Methods: The assumption of a Bernoulli distribution seems limiting, as neurons may respond with more than one spike. The decoding overall does not take into account that (at least some) visual information may be encoded in the relative timing of spikes across neurons.</p><p>p.7 algorithm section: The major underlying assumption here is that temporally dithered stimuli will be linearly integrated by the retina. Dithering may avoid electrical crosstalk, but neurons may exhibit &quot;crosstalk&quot; on longer timescales due to the relatively slow (as compared to electrical stimulation) temporal dynamics of ion channels. The authors seem to be aware of that as they say &quot;presumably&quot; and mention the idea of a perceptual integration window. But it would have been great to refer to some existing (and more recent) literature on the topic (if any).</p><p>p.8 algorithm section: Greedy algorithms are often not guaranteed to find the global optimum. Can the authors show that their proposed algorithm does not get stuck in local optima? How is the final optimization performed at time t, given the dictionary elements?</p><p>Ex vivo experiments: It would be helpful to see reconstruction errors and reconstructed images, similar to how they were presented for the simulation study. How long does the decoding/stimulus selection take? Presumably, the dictionary is quite large given the number of neurons. Is there a more efficient way to search the dictionary than O(N)? Or how is that done, and how quickly could it be done in a real-world implementation? I am concerned that this step may severely limit the number of stimuli that could be realistically delivered in a temporal integration window.</p><p>In all equations, it would help if the authors used proper formatting to label vectors vs. matrices. For example, is the stimulus reconstruction filter in Eq. 1 a vector and the product is elementwise? What is the size of (and what are the rows/columns in) D on page 8? Etc.</p><p><italic>Reviewer #2:</italic></p><p>This study proposes a new algorithm for determining the electrical stimulation delivered through a sensory neural implant with the aim of improving the perceptual benefit to implant users. The algorithm is evaluated using data from an ex vivo prototype of a retinal prosthesis to computer-simulate the retinal responses expected from applying the algorithm and later by applying stimuli from the full temporally dithered, spatially multiplex algorithm to ex vivo retina.</p><p>Presently, stimulation algorithms used clinically are calibrated using limited perceptual data from the user. In contrast, the proposed algorithm uses detailed measurements of retinal responses to electrical stimulation to optimize the stimulation. This is achieved by minimizing the error between a target image and a version of that image reconstructed from the evoked response that is predicted by the algorithm based on the detailed measurements. The use of a data-driven, optimization approach is similar to several other recently proposed neural stimulation algorithms (which are not cited by the study). The distinguishing feature of the algorithm proposed in this study is that it seeks to stimulate in a way that minimizes the interactions between electrodes that can occur when stimulating neural responses. This avoids the need for the algorithm to account for such interactions.</p><p>Overall, the main advantage of the proposed approach is that it frames the problem of how to deliver perceptually beneficial electrical stimulation with an implant as a closed-loop/data-driven optimization problem. This has the potential to improve over presently used open-loop strategies. It then provides an algorithm for solving this optimization problem to a good approximation. Applying the algorithm using data recorded from ex vivo retina with a prototype implant is a strength. However, the evaluation of the efficacy of the algorithm is limited. In the first instance, it is limited to computer simulation of the retinal response for the version of the algorithm that uses just temporal dithering. While this analysis supports the conclusion that the proposed algorithm could provide improved visual perception relative to the clinical open-loop strategy, much stronger evidence would be provided by applying the optimal stimuli from the proposed algorithm directly to the ex vivo retinal preparation and measuring the retinal response. This approach to testing the algorithm directly to the ex vivo retina is done for the full version of the algorithm that combines spatial multiplexing with temporal dithering. However, in contrast to simulated results, the study does not report on the reconstructed images that result from applying the algorithm to ex vivo retina, nor on the reconstruction errors. This makes it difficult to evaluate the efficacy of the algorithm.</p><p>Section: Introduction.</p><p>The motivation for using a temporally dithered, spatially multiplexed algorithm to optimize stimulation stems from the desire to minimize the interactions caused by simultaneous stimulation of the electrodes in evoking a neural response. While this is an important strategy to investigate, the interactions are not typically as &quot;complex&quot; as claimed in the manuscript. Indeed, previous studies in several labs (including the Chichilnisky lab) show that these interactions can typically be described by a linear, weighted sum of the electrode currents followed by a simple static nonlinearity to predict the probability of spiking (a small minority of retinal ganglion cells require more complex nonlinear descriptions) [1 -3]. This model and others have been the basis for alternative data-driven, closed-loop stimulation strategies that optimize the stimulation in a way that seeks to take advantage of the interactions between electrodes to improve the spatial resolution of evoked retinal activity through &quot;current steering&quot;.</p><p>Section: Greedy temporal dithering to replicate neural code.</p><p>Data-driven optimization: The data required for the proposed algorithm is of two types. An exhaustive dictionary of response probabilities to single electrode stimulation across all current amplitudes, and a set of responses used to reconstruct the target image from the predicted response to electrical stimulation. For the latter, reconstruction of the image is achieved by applying linear filters to the predicted response. In the study, these linear filters were derived from cells' receptive fields, obtained by measured responses in the retina to light stimulation. It is noted that this would not be possible in a clinical implant, as the retina is degenerate. However, it is not clear how a set of filters would be obtained in this case. The authors mention that distinct cell types can be identified from spontaneous activity. However, this does not explain how receptive field size and location would be estimated in this situation.</p><p>The reconstruction of the image is achieved through linear filtering with a matrix A, with columns, A_j, that are the (scaled) receptive field filters (Eq. 1). However, this is only correct if the receptive field filters of the different cells are orthogonal, i.e. the inner product of each pair of receptive fields is zero. More generally, appropriate linear filtering should be performed by applying the pseudo inverse of the transpose of A. This is because the retinal spike rates are being approximated as the inner product of the receptive field and the image (A_j transposed, matrix-multiplied by the image vector), half-wave rectified. For the receptive fields of ON and OFF parasol cells given in the study, it appears that the receptive fields are approximately orthogonal for the two separate populations due to the non-overlapping tiling of the visual field by each population (e.g. Figure 2). However, it is not clear whether this situation would prevail in the blind retina, as the filters have not been specified in the case.</p><p>The greedy optimization algorithm is insufficiently explained in the Methods, including the following points:</p><p>• A derivation justifying splitting the objective function into the terms due to the mean and variance is required.</p><p>• The terminology for the terms tr(var(A R_i)) is not clearly explained. I assume it is the matrix trace of the covariance matrix of the random variable A R_i.</p><p>• The assumption of a Bernoulli random variable for the response, i.e. 1 or 0 spikes, is limiting, given there may be multiple spikes in response to electrical stimulation, especially for activation via the retinal network.</p><p>• The expression that was derived for the term tr(var(A R_i)) in the case of Bernoulli random variables should be given.</p><p>• It is not explained how the algorithm performs the final optimization at time step t, given elements in a restricted dictionary D_t.</p><p>Section: Greedy temporal dithering outperforms open loop methods.</p><p>The image reconstruction shown in Figure 2 uses 500, 3000, and 10000 electrical stimuli (shown in A, B and C respectively). However, these are unrealistically large numbers of stimuli: given the temporal perceptual window of 50 ms, mentioned in the Introduction as the time over which retinal responses would be perceptually integrated, and the pulse duration of 0.15 ms used in the study, a maximum of 333 stimuli could be applied during the window. Consequently, the use of 3000 and 10,000 electrical stimuli in the simulations provides unrealistic estimates of the degree to which the image can be reconstructed.</p><p>A full comparison of the proposed greedy, closed-loop algorithm to the conventional open-loop algorithm is difficult to evaluate based on the results presented. First, the number of electrical stimuli applied in making the comparison (Figure 3H) is not given. However, it seems likely, given the data in Figure 3G that an unrealistically large 10,000 stimuli were used. If instead a realistic 300-400 stimuli were used there may be little difference between the greedy-closed loop algorithm and the conventional open-loop algorithm.</p><p>A second limitation is that, in this subsection, the greedy, closed-loop algorithm appears to have only been tested in simulation. E.g. &quot;For random checkerboard visual stimulus targets, the greedy dithering stimulation sequence was calculated, neural responses were sampled using measured response probabilities evoked by the individual selected stimuli, and then the target image was linearly reconstructed from these responses.&quot; Given that all the relevant data required to run the algorithm for the ex vivo retina and implant prototype had been collected during the experiment, it is unclear why the algorithm was not applied to test it by directly measuring responses to the algorithm's stimulation. This would have tested a critical assumption of the greedy-temporal dithering algorithm: that the responses to successive stimuli are statistically independent. Instead, the simulation assumes this to be the case.</p><p>A third limitation is that the reconstructed image for the conventional open-loop algorithm does not resemble the phosphene images reported by most retinal implant users. Most implant users report predominantly bright, rather than dark, localized phosphenes [4]. The open-loop reconstruction shown in Figure 3d appears to be largely a gray averaging of light and dark phosphenes, likely due to the linear reconstruction method used.</p><p>Some details of the implementation of the open-loop strategy are unclear including:</p><p>• How the area that was &quot;near&quot; the electrode was selected when calculating the intensity of the visual stimulus.</p><p>• How the temporal sequence of the electrodes was chosen. It seems that the open-loop strategy is also likely, temporally dithered, but without the benefit of data-driven optimization.</p><p>Section: Greedy temporal dithering is nearly optimal given the interface constraints.</p><p>The comparison of the greedy, closed-loop approximately optimal algorithm to truly optimal algorithms is an important comparison in principle. However, again it is not clear if a realistic number of stimulation pulses were used in performing this comparison (i.e. &lt; 400).</p><p>Some details of the implementation of the optimal comparison strategy are unclear including:</p><p>• The meaning and purpose of the term V^T w in the objective function.</p><p>• Whether w&gt;=0 was required after the integer requirement was relaxed in the optimization.</p><p>Section: Spatial multiplexing for fitting multiple stimuli in a visual integration window.</p><p>The idea to use spatial multiplexing of stimuli to overcome the limitation in the number of stimuli that can be delivered during a perceptual temporal window is a good idea to investigate. The aim is to choose stimuli on different electrodes that affect neural response independently. However, the initial formulation of what is meant by independence is not correct. This is stated as: &quot;For independence to hold, the following condition must be met: if <italic>p</italic>1 is the activation probability of a given cell with stimulation on electrode 1, and p2 is the activation probability of the same cell with electrode 2, then the activation probability with simultaneous stimulation must be <italic>p</italic>1+<italic>p</italic>2.&quot; That this is incorrect can be seen because this formulation could give a probability greater than 1. However, the subsequent description of what is actually implemented appears correct. A general, in-principle way of describing what independence means is that if p1 is the probability of stimulating one cell with electrode 1 and p2 is the probability of stimulating a different cell with electrode 2, then the probability of stimulating both cell 1 and cell 2 using simultaneous stimulation with electrodes 1 and 2 is the product of those probabilities, p1.p2.</p><p>In contrast to greedy dithering alone, the use of both greedy dithering and spatial multiplexing was tested in a closed-loop experiment by recording responses to stimuli produced by the algorithm. However, the paper does not report on the image reconstructions, nor the reconstruction errors that were obtained.</p><p>Instead, the reported results of the greedy dithering-plus-multiplexing (Figure 4) show only that it is possible to select eight multiplexed electrodes with sufficient separation to ensure minimal interference. This could potentially increase the number of electrodes stimulated with the greedy, closed-loop algorithm by a factor of 8, bringing it to around 2,700 stimuli. This is closer to the 3000 electrode stimulations used in Figure 2b that gave errors that approached the asymptotic limit. However, the results in Figure 4 were obtained using stimulation every 2 ms, not every 0.15 ms (= pulse duration). With this limitation, this reduces the number of electrode stimuli to 200 in a 50 ms perceptual window, which again is not likely to give a good reconstruction error according to the simulations.</p><p>Other Results sections.</p><p>The sections on hardware constraints, naturalistic viewing conditions, and the use of perceptual similarity measures make useful observations about the potential benefits of the optimization framework for algorithmically determining the electrical stimulation.</p><p>Discussion.</p><p>The discussion covers many important points well. Regarding the translational potential, I would agree that an important point is &quot;First, new surgical methods must be developed to implant a tiny chip on the surface of the retina with stable contact.&quot; But add that it must also be in extremely close contact for retinal ganglion cell spikes to be recorded. Further, a very high-density array (~ 60 μm pitch) and associated electronics for both stimulation and recording must be developed which is suitable in size, form factor, and power consumption for clinical use.</p><p>References</p><p>[1] Jepson, L. H., Hottowy, P., Mathieson, K., Gunning, D. E., Dąbrowski, W., Litke, A. M., &amp; Chichilnisky, E. J. (2014). Spatially patterned electrical stimulation to enhance resolution of retinal prostheses. Journal of Neuroscience, 34(14), 4871-4881.</p><p>[2] Lorach, H., Goetz, G., Smith, R., Lei, X., Mandel, Y., Kamins, T.,.… &amp; Palanker, D. (2015). Photovoltaic restoration of sight with high visual acuity. Nature medicine, 21(5), 476-482.</p><p>[3] Maturana, M. I., Apollo, N. V., Hadjinicolaou, A. E., Garrett, D. J., Cloherty, S. L., Kameneva, T.,.… &amp; Meffin, H. (2016). A simple and accurate model to predict responses to multi-electrode stimulation in the retina. PLoS Computational Biology, 12(4), e1004849.</p><p>[4] Humayun, M. S., Weiland, J. D., Fujii, G. Y., Greenberg, R., Williamson, R., Little, J., et al. (2003) Visual perception in a blind subject with a chronic microelectronic retinal prosthesis. Vision Research, 43, (2573-2581).</p><p>Recommendations for the authors:</p><p>Overall, it appears that the approach may offer some important benefits for sensory-neural implant users. However, the reporting of results is not sufficiently complete to draw strong conclusions about the potential benefits. In addition to the Public Review, I have some related suggestions below.</p><p>Reconstruction model in the blind retina.</p><p>• It would be helpful to provide more detail about how the image reconstruction would work in the blind retinas, beyond what is mentioned regarding the identification of ON and OFF retinal ganglion cell type. How would the size and location of receptive fields be estimated?</p><p>• The assumptions underlying the reconstruction model should be described, especially with respect to the orthogonality of the receptive field filters. It would be helpful to describe an approach in the methods that do not rely on this assumption, as I describe in my public comments.</p><p>Greed optimization algorithm: There are several aspects of this that could be better explained. These include:</p><p>• A derivation justifying splitting the objective function into the terms due to the mean and variance is required.</p><p>• The terminology for the terms tr(var(A R_i)) is not clearly explained. I assume it is the matrix trace of the covariance matrix of the random variable A R_i.</p><p>• The assumption of a Bernoulli random variable, i.e. 1 or 0 spikes, is limiting, given there may be multiple spikes in response to electrical stimulation, especially for activation via the retinal network.</p><p>• The expression derived for the term tr(var(A R_i)) in the case of Bernoulli random variables should be given.</p><p>• It is not explained how the algorithm performs the final optimization at time step t, given elements in a restricted dictionary D_t.</p><p>• It is not explained how to determine the time for which recently used dictionary elements are excluded from current use.</p><p>Section: Greedy temporal dithering outperforms open loop methods</p><p>Regarding the number of single-electrode stimuli used in image reconstruction, it would be better to place the numbers used in the context of what is possible in the perceptual time window. It would recommend using the value of 333 instead of 500, as this corresponds to the number of 0.15 pulses that could be fit into a 50 ms window. The value of 3000 roughly corresponds to what might be achieved with spatial multiplexing. The value of 10,000 corresponds to the upper limit that is achievable through this algorithm.</p><p>I think it would be beneficial to make it clearer that the results in Figure 3 are simulated. It would also strengthen the study to perform validation in ex vivo retina to apply the greedy temporal dithering stimuli to the retina and reconstruct the image from the responses. If there is a good reason not to do this, this should be explained.</p><p>It would improve the study if a reconstruction algorithm that provides an image with a better match to the perception of phosphenes by retinal implant users was used. If this cannot be done, it should be discussed as a limitation of the study.</p><p>It would be helpful to clarify some details of the implementation of the open-loop strategy including:</p><p>• How the area that was &quot;near&quot; the electrode was selected when calculating the intensity of the visual stimulus.</p><p>• How the temporal sequence of the electrodes was chosen.</p><p>Section: Greedy temporal dithering is nearly optimal given the interface constraints</p><p>A realistic number of stimulation pulses should be used in performing this comparison e.g. &lt; 400 for the pure temporal dithering or &lt; 3000 for the spatially multiplexed, temporal dithering.</p><p>It would be helpful to clarify some details of the implementation of the open-loop strategy including:</p><p>• The meaning and purpose of the term V^T.w in the objective function.</p><p>• Whether w&gt;=0 was required after the integer requirement was relaxed in the optimization.</p><p>Section: Spatial multiplexing for fitting multiple stimuli in a visual integration window</p><p>As described in my public review, the description of independence is not correct. I have suggested an alternative description that I believe accords with what was actually implemented.</p><p>It was surprising that the results of the validation experiments on ex vivo retina with the spatially multiplexed, temporally dithered algorithm were not reported more thoroughly. It is important to provide figures showing the image reconstruction that was achieved and the statistics for the reconstruction error.</p><p><italic>Reviewer #3:</italic></p><p>In this study, Shah and colleagues propose an interesting solution to the non-linear interactions caused by simultaneously stimulating multiple electrodes within a retinal implant. Through high-resolution recordings of ON and OFF parasol retinal ganglion cells, the authors demonstrate that a greedy dithering and spatially multiplexed algorithm, which can also work in the presence of saccadic eye movements, is able to faithfully reconstruct images represented by total numbers of spikes in a given time window across multiple retinal ganglion cells. Essentially, Shah and colleagues propose and demonstrate a method to only stimulate single or groups of 8 electrodes at a time from a pre-established dictionary, but then interleave stimulation of multiple electrodes or groups rapidly across the dictionary to additively build an image. Through their very rigorous and elegant ex vivo recordings in 180 ON and OFF parasol cells across four primate retina preparations, the authors compellingly demonstrate that (i) their greedy algorithm performs better than an open loop algorithm, similar to an optimal algorithm considering the interface constraints, and close but not equal to an ideal control using only a single-electrode dictionary; (ii) that groups of electrodes can be simultaneously activated with a high-resolution neural interface without any retinal interactions provided that they are at least 160 μm apart; (iii) that the algorithm performs just as well even with only 50% of the electrodes on the interface and (iv) that the algorithm can work in the presence of saccadic eye movements and performs better when both saccadic and fixational eye movements are made as opposed to saccadic movements alone.</p><p>The experimental recordings and performance of the algorithm in various conditions are the biggest strengths of this study and the authors certainly demonstrate that their algorithm can reproduce spiking numbers across an array of cells that resemble closely spiking numbers evoked by visual stimuli for these conditions. In other words, the authors' primary claim that the neural code for visual images in the retina (in the form of spiking numbers) can be faithfully reproduced with electrical stimulation using such an algorithm, is well supported by evidence.</p><p>A major weakness in the study however is the reliance of this algorithm on several significant assumptions about neural coding in the retina, neural coding in the visual brain, and interactions between electrodes even with non-simultaneous stimulation. Some of these assumptions have already been highly challenged in several studies in the visual neuroscience field and in studies involving the perception of phosphenes with interleaved stimulation of single electrodes. Therefore, in light of what is currently known about visual encoding and artificial vision, the study whilst showcasing an elegant computational tool perhaps provides only little hope that such an algorithm will actually work in practice to recreate the perception of images with electrical stimulation but instead does lay a foundation for further work to be done with the assessment of future algorithms. The main assumptions that the authors rely on include:</p><p>1) That neural coding in the retina is simply based on a number of spikes evoked by populations of cells ignoring any temporal patterns of responses. A plethora of studies has indicated that relative spike timing between groups of retinal ganglion cells for example can encode complex visual features but the greedy algorithm does not aim to mimic these spike timing features.</p><p>2) That perception within the brain is solely based on a number of spikes within a slow temporal integration window (the authors cite a 1995 reference for this). Since 1995 though, this has also been challenged, therefore extending the authors' claims of reproducing spike numbers in the retina to reproducing perception in the brain would be contentious.</p><p>3) That neural interactions with non-simultaneous interleaved electrical stimulation are absent. There is in silico, electrophysiological and perceptual evidence with retinal implants that interleaving of electrodes still results in neural interactions and that perception with interleaved stimulation with multiple electrodes does not result in a linear summed perception of phosphenes evoked by single electrodes i.e. dictionary elements. Therefore, the algorithm would only work if such interactions are minimal or absent, for example with larger than 0.1 ms intervals between stimulations or more than 160 μm electrode separation. Note, interactions with interleaving also exist with cochlear implants as the current spread is large.</p><p>4) That even if the above 3 assumptions were applied and true, the algorithm can faithfully extrapolate to reconstruct moving images at 24 per second. This seems unlikely as presumably the total time required to linearly reconstruct a single static image would extend to many tens or even hundreds of ms given the number of times each dictionary element needs to be accessed to enable reproduction of similar spiking numbers between visual and electrical stimulation, runs in the thousands.</p><p>In spite of major reliance on these assumptions, the authors do demonstrate a very useful tool in the form of the greedy algorithm for situations perhaps other than the visual system, where perception with artificial stimulation may be more predictable and interactions with non-simultaneous stimulation may be simpler.</p><p>Recommendations for the authors:</p><p>It may be possible to address at least some of the limitations in particular (1) and (4) mentioned in the public review. For limitation (1), the authors could try and experiment with their algorithm and reanalyse data to examine if and how well spike timing features (perhaps relative first spike latencies between RGCs or other temporal patterns of spikes) are reproducible. For limitation (4) the authors could at least perform calculations of time taken by the algorithm in each of the situations and targets presented, to examine if these times are realistic.</p><p>For limitations (2) and (3), the authors at a minimum should address these clearly in their discussion and the potential implications of the failure of these assumptions on their algorithm performance.</p><p>Other things that the authors should consider is including some example raw data from their retinas before and after artifact subtraction in response to both visual targets and their greedy algorithm as a figure.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83424.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Included here is a brief evaluation summary and list of revisions the reviewers and review editor deem essential for the authors to address. The public summaries and full, individual reviewers' recommendations for the authors are also appended below. The authors are advised to address the public summaries briefly, and the individual recommendations in a detailed, point-by-point manner.</p><p>As you will be able to read below, reviewers appreciated the importance of the study and its potentially broad interest. The approach to formulating the problem of choosing electrical stimuli for visual prostheses as a data-driven optimization problem holds promise for several sensory-neural prostheses. The writing was relatively clear, the figures appropriate, and the methods mostly rigorous. However, reviewers raised concerns with regard to some of the claims made, particularly as it pertains to the full greedy, dithering, multiplexed algorithm and its potential to greatly improve the quality of vision delivered by a retinal implant. The key points that need to be addressed can be summarized as follows:</p><p>1) Please provide more experimental data (specifically: reconstructed images and reconstruction errors) to substantiate the claim that the algorithm can improve the quality of vision in an ex vivo setting. The main evidence that is presented about the quality of vision that might be achieved is a computer simulation; that is, the image reconstructions and reconstruction errors given in Figures 2 &amp; 3 with the dithered, but not multiplexed version of the algorithm. However, the same cannot be said about the ex vivo experiment. While the outputs of the dithered &amp; multiplex version were indeed applied to ex vivo retina, the output is all simulated and no experimental validation data is presented. However, it is possible that the experimentally observed retinal output might differ from (the assumption of) a linear sum of dictionary elements. All reviewers agreed that if the authors could report on the results of the experiments in which algorithmic stimulation is applied to ex vivo retina, and report this in terms of image reconstructions and reconstruction error, this would greatly improve the strength of evidence.</p></disp-quote><p>We agree that this was the main missing element in the submitted manuscript and have spent recent months addressing this issue directly with experiments. The new results are given in the new section <italic>Experimental validation of greedy temporal dithering</italic> accompanied by the new Figure 4. In this analysis, we apply greedy dithered sequences to the retina ex vivo and directly compute image reconstructions and errors from the measured, evoked neural responses. We also compare these results to our previous approach of using measured responses from the electrical stimulus calibration phase of the experiment, and show that the experimental results align with these expectations. Due to pandemic-era limitations on the availability of monkey retinas, we performed the new experimental validation in the rat retina, and have thus added sections corresponding to this analysis to <italic>Methods</italic>. We think the addition of these experiments has substantially increased the impact of the paper and are grateful to the reviewers for highlighting its importance. We have also outlined the limitations of this experimental validation in the Discussion.</p><disp-quote content-type="editor-comment"><p>2) Please expand the discussion on the theoretical assumptions regarding visual processing in the brain and the perception of phosphenes through electrical stimulation that the study is based on, as it may limit the translational impact of the work. All reviewers agreed that the study relies on several significant assumptions about neural coding in the retina, the visual brain, and interactions between electrodes, some of which have been recently challenged. This includes the assumption that neural coding in the retina is solely based on a firing-rate code, that visual perception is solely based on the number of spikes within a slow temporal integration window, and that non-simultaneous interleaved electrical stimulation does not lead to neural interactions. At a minimum, the authors should address these limitations clearly in their Discussion, and comment on the potential implications of the failure of these assumptions on their algorithm performance.</p></disp-quote><p>We appreciate the reviewers’ focus on what is assumed/tested in our approach, something we have attempted to be very up-front about. As requested, we have provided additional clarification of our assumptions regarding neural coding in the visual brain and electrical stimulation in the Discussion. Here, we summarize the rationale for the three specific assumptions raised by the reviewers, paralleling the new Discussion text:</p><p>Firing rate code in the retina: We agree that in addition to firing rate, other features of the retinal code such as relative latency have been shown to carry information about the visual stimulus in some conditions and species (e.g. (Gollisch &amp; Meister, 2008)). However, firing rate is thought to be the dominant feature of the neural code in the primate visual system (Shadlen &amp; Newsome, 1994), so we made a first-order approximation to focus on it in this work. The importance of the firing rate code is also supported by recent work in which we found that macaque RGC spike counts integrated over ~150 ms provide greater image reconstruction accuracy than spike latencies (Brackbill et al., 2020). Based on this work, we think that while our approach may not capture all of the information normally present in RGC visual signals, it likely captures a large fraction of what is useful for vision. However, we agree that features of RGC responses other than firing rate could be important in certain circumstances (e.g. (Meister, 1996)), and have clarified this in the Discussion.</p><p>Visual perception based on slow temporal integration: We appreciate that the reviewers raised this important and subtle point, which has valid arguments on both sides. We try to describe our perspective on it more fully here. There is ample evidence that visual signals in the brain are integrated over tens of milliseconds to produce perception. This evidence ranges from flicker fusion experiments to the time scale of synaptic signal transfer to neurophysiological tests of temporal integration (Borghuis et al., 2019; Samaha &amp; Postle, 2015; Tadin et al., 2010; Wutz et al., 2016). It is also consistent with the widespread use of display technology with ~60-100 Hz refresh rates. This known coarse temporal resolution of vision likely arises from long time constants in phototransduction and synaptic transfer. However, in principle, signals transmitted by RGCs to the brain could have finer temporal precision than signals in the photoreceptors, and visual centers in the brain could be sensitive to the precise timing of RGC spikes (as described in modeling studies, e.g. (Gütig et al., 2013)). Indeed, there is some empirical evidence that the temporal precision of spikes in RGCs can, in certain conditions, be on the order of 1 ms (Berry et al., 1997; Berry &amp; Meister, 1998; Keat et al., 2001; Reich et al., 1997; Uzzell &amp; Chichilnisky, 2004). Furthermore, the idea that downstream mechanisms in the brain could “read out” RGC signals with millisecond temporal precision is supported to a limited degree by empirical studies of precisely correlated activity (e.g. (Alonso et al., 1996)). However, it is unclear from these studies exactly how this high temporal precision would be useful for vision. We have performed several in-depth studies of the temporal resolution of readout of RGC signals from the macaque retina. This work has shown that for reconstruction of images from flashed stimuli, and for speed/direction discrimination with moving stimuli, the optimal temporal resolution of primate RGC signal readout is ~10 ms or coarser (Brackbill et al., 2020; Chichilnisky &amp; Kalmar, 2003; Frechette et al., 2005; Wu et al., 2023). On the other hand, high temporal precision could be part of the neural code of RGCs in other specific visual stimulus conditions. For example, we recently found that image reconstruction from macaque RGC spikes in the presence of fixational eye drift is sensitive to spike timing in the 2-5 ms range (Wu et al., 2023). In sum, much evidence leans toward the idea that the temporal resolution of RGC signal readout in the brain is likely to be on the order of tens of milliseconds for many visual tasks. However, this may not be true for all conditions. We have clarified this in the Discussion.</p><p>Neural interactions for non-simultaneous interleaved electrical stimulation: While such interactions have been observed in previous studies of electrical stimulation (Ho et al., 2020; Sekhar et al., 2020; Yoon et al., 2020), the current work relies on very low current levels (&lt;4 µA) delivered in brief pulses (150 µsec) that typically produce a single spike, directly evoked by membrane depolarization, with a latency of a few milliseconds (Sekirnjak et al., 2006, 2008) and no network-mediated activation. This suggests that the primary temporal interactions are likely to be the relative refractory period of neurons (~10 ms). This interpretation is supported by the findings in our new experimental validation. We have clarified these considerations in the Discussion.</p><disp-quote content-type="editor-comment"><p>3) Please clarify which figures/results are from simulations and which are from experimental data.</p></disp-quote><p>None of the figures in the paper are based on simulations of retinal responses – all figures use real recorded data. Importantly, however, Figures 2, 3, 6, 7, and 8 use calibrated responses to single-electrode stimulation to compute the reconstruction that is possible if dithered and multiplexed stimulation is used — the data are from real recordings, but the independence and stability of evoked responses is assumed, and samples are drawn from the measured spike probabilities to compute the reconstruction quality. To test whether these assumptions are valid, the new manuscript also includes closed-loop experiments (Figures 4 and 5) which validate the entire calibration-stimulation pipeline. We have added clarifying text throughout the Results and also summarized this information in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>1.1) Shah et al. propose an algorithm to precisely control RGC activation using electrical stimulation, using temporal dithering, and spatial multiplexing. The main assumption is that the brain has perceptual integration windows, during which a visual percept can be built up by stimulating single (or small groups of) neurons in rapid succession. Which electrodes to stimulate to achieve a desired percept is dictated by a dictionary of stimulation patterns. The authors demonstrate the effectiveness of their method on ex vivo recordings of ON and OFF parasol cells.</p><p>The biggest strengths of the study are the theoretical contributions and the experimental recordings used to demonstrate the effectiveness of their algorithm. The thinking follows a number of recent efforts in the field to think about visual prosthetic stimulation as a closed-loop data-driven optimization problem. This may have benefits over other open-loop stimulation techniques.</p></disp-quote><p>Thank you for recognizing the strengths of the study.</p><disp-quote content-type="editor-comment"><p>1.2) However, the biggest weakness of the study is a reliance on a number of controversial assumptions about the neural code of vision. The first is the existence of a slow temporal integration window during which the brain cannot distinguish the order of stimuli presented and/or sums up RGC activity to decode the presented stimulus. The paper presents only limited (and dated) evidence for this.</p></disp-quote><p>We agree that this was limited in the original submission, and have clarified this important issue in our response to Essential Revisions 2 paragraph 3 above as well as in changes to the Discussion</p><disp-quote content-type="editor-comment"><p>1.3) Second, the assumption of a Bernoulli distribution is at the very least limiting, as neurons may respond with multiple spikes to a stimulation pattern and some spatial features may be encoded by the relative timing of spikes across neurons.</p></disp-quote><p>Please see our response to Recommendations for Authors 1.8 below.</p><disp-quote content-type="editor-comment"><p>1.4) Third, even though the temporal dithering may avoid electrical crosstalk, there may still be neuronal crosstalk on longer timescales, thus challenging the independence assumption. Extending the delay between stimuli in order to avoid neuronal crosstalk may severely limit the utility of the proposed algorithm since it would cap the number of stimuli that could be delivered in one of the assumed temporal integration windows.</p></disp-quote><p>Please see our response to Recommendations for Authors 1.9 below.</p><disp-quote content-type="editor-comment"><p>1.5) Even if the assumptions hold, the presented evidence of the ex vivo recordings would need to provide some additional detail before the practical utility of the proposed algorithm could be judged accordingly. Although the study reports reconstruction errors and example reconstructed images for the simulation experiment, the same cannot be said about the ex vivo experiment.</p></disp-quote><p>We have now performed the key closed-loop validation experiment and provided the results in the paper. Please see our response to Essential Revisions 1 above and the changes to the manuscript.</p><disp-quote content-type="editor-comment"><p>1.6) In addition, real-world implementation of the algorithm would presumably include not just stimulus delivery but also in-the-loop stimulus decoding, and it is not clear how quickly that could be done.</p></disp-quote><p>While the real-time, in-the-loop stimulus decoding (which would use the actual evoked spikes rather than the probability of an evoked spike) could improve performance, it is computationally prohibitive as the reviewer points out. The approach in this paper does not require real-time, in-the-loop stimulus decoding, but instead uses the <italic>expected</italic> stimulus decoding based on a fixed set of calibration measurements. We show that inter-trial variability in total number of spikes is minimal, so that expected spike decoding is sufficient. We have now clarified this in the Discussion.</p><disp-quote content-type="editor-comment"><p>1.7) Lastly, the linear filters would have to be estimated in a degenerated retina, where one could not rely on responses to light stimuli. The paper notes that this could be done by considering the spontaneous activity of cells – I can see how that could allow you to distinguish between ON and OFF cells, for instance, but it is not clear to me how that would allow you to determine the linear filter for each cell. For those reasons, it is somewhat hard to judge the practical utility and potential significance of the proposed algorithm.</p></disp-quote><p>These are excellent points. A recent paper from our group (Zaidi et al., 2023) addresses these issues directly. In that paper, the firing rate and autocorrelation function are first used to classify ON and OFF parasol cell types, as the reviewer suggests. Then, the average linear spatiotemporal filter for each cell type is translated in space to an estimated receptive field location for each recorded cell using its electrical image, which we have shown provides an accurate estimate of its physical location (Li et al., 2015). This procedure was evaluated quantitatively in the aforementioned publication and shown to work well, accurately reproducing the actual measured linear filters of the cells using the autocorrelation and electrical image location. Future work will be needed for additional cell types, including additional electrical features that we measure routinely which can be used to identify more cell types, but the overall approach is expected to be similar. We have clarified this important issue in the Discussion.</p><disp-quote content-type="editor-comment"><p>Recommendations for the authors:</p><p>1.8) Methods: The assumption of a Bernoulli distribution seems limiting, as neurons may respond with more than one spike. The decoding overall does not take into account that (at least some) visual information may be encoded in the relative timing of spikes across neurons.</p></disp-quote><p>With the low amplitude electrical stimulation that we use (150µs long pulses with peak current amplitude 4µA), we typically observe zero or one directly-evoked spikes for each stimulation pulse (see (Sekirnjak et al., 2006)). This is in part because the pulses are short and the mechanism of activation is direct depolarization (Sekirnjak et al., 2006), rather than a network-mediated excitation. Under these conditions, the Bernoulli assumption is reasonable. We agree that our approach would not translate to other electrical stimulation patterns, such as pulse trains or high current levels or network-mediated activation, which could elicit many spikes. We note that, unlike the present approach, those stimulation paradigms make it difficult/impossible to replicate the neural code, and that evoking one spike at a time is therefore a singular advantage of our approach. We have clarified this in the Discussion.</p><p>Please see our response to Essential Revisions 2 paragraph 2 for a discussion of relative spike timing. We also note that with the very high temporal precision of our stimulation (evoked spike time variation of roughly 0.1 ms), if there were some degree of stimulus coding in the relative timing of spikes, that relative timing could certainly be reproduced by the stimulation sequences that we provide, with a suitable modification of the optimization approach.</p><disp-quote content-type="editor-comment"><p>1.9) p.7 algorithm section: The major underlying assumption here is that temporally dithered stimuli will be linearly integrated by the retina. Dithering may avoid electrical crosstalk, but neurons may exhibit &quot;crosstalk&quot; on longer timescales due to the relatively slow (as compared to electrical stimulation) temporal dynamics of ion channels. The authors seem to be aware of that as they say &quot;presumably&quot; and mention the idea of a perceptual integration window. But it would have been great to refer to some existing (and more recent) literature on the topic (if any).</p></disp-quote><p>To clarify, the assumption is <italic>not</italic> that temporally dithered stimuli are linearly integrated by the retina. The assumption is that temporally dithered evoked spikes are linearly integrated by the visual system downstream of the retina, on time scales of tens of ms. The evidence for this is discussed in our response to Essential Revision 2 paragraph 3. In short, while this assumption is not necessarily correct in all conditions, and testing it thoroughly will require an implanted device that does not yet exist, there is ample evidence to support this assumption in many stimulus conditions.</p><p>We agree that neuronal “crosstalk” on longer timescales than the temporal dithering is a possibility. Please see our response to Essential Revisions (2) paragraph 4. To avoid the primary temporal interactions due to the relative refractory period of neurons, the stimuli at each timestep are chosen from a ‘valid’ subset of the dictionary that disallows stimulation of any recently targeted cell within its relative refractory period. Our new validation experiments directly test the possibility of crosstalk in the retina ex vivo, and the results are encouraging (Figures 4 and 5). We have now highlighted the importance of independent responses for the temporal dithering approach in the Assumptions subsection of the Discussion.</p><disp-quote content-type="editor-comment"><p>1.10) p.8 algorithm section: Greedy algorithms are often not guaranteed to find the global optimum. Can the authors show that their proposed algorithm does not get stuck in local optima? How is the final optimization performed at time t, given the dictionary elements?</p></disp-quote><p>While we do not show that the greedy algorithm does not get stuck in local optima analytically, we do show empirically that the gap between lower bound on the optimal solution and greedy solution is small (Figure 3, histograms colored orange and blue). This finding indicates that even if the greedy algorithm does sometimes get stuck in local optima, the degradation in performance is insignificant compared to the benefits in reconstruction performance that the algorithm provides.</p><p>The formula for the optimization performed at time t over the available set of dictionary elements is given as Equation 5 in the Methods.</p><disp-quote content-type="editor-comment"><p>1.11) Ex vivo experiments: It would be helpful to see reconstruction errors and reconstructed images, similar to how they were presented for the simulation study.</p></disp-quote><p>We agree that direct closed-loop validation with the temporally dithered stimulation is important and have now performed these experiments. Please see our response to Essential Revision (1). In brief, the temporally dithered stimulation conveys very substantial image structure, as predicted using the measurements at the start of the experiment.</p><disp-quote content-type="editor-comment"><p>1.12) How long does the decoding/stimulus selection take? Presumably, the dictionary is quite large given the number of neurons. Is there a more efficient way to search the dictionary than O(N)? Or how is that done, and how quickly could it be done in a real-world implementation? I am concerned that this step may severely limit the number of stimuli that could be realistically delivered in a temporal integration window.</p></disp-quote><p>These are important considerations. The dictionary can indeed potentially be large (the number of elements is equal to the number of stimulation patterns tested), and searching this dictionary could thus be a computationally prohibitive step. We have recently addressed this problem in our group (Lotlikar et al., 2023) using the insight that the greedy search can be decomposed into multiple smaller searches, because far-away electrodes stimulate a disjoint collection of cells. This approach gives a drastic increase in the speed of the algorithm. Other engineering insights (such as finding the right embedded processor, building custom chips, etc.) can further increase the speed. We have decided to not focus on engineering implementation for this conceptual/theoretical paper because it is already fairly long.</p><disp-quote content-type="editor-comment"><p>1.13) In all equations, it would help if the authors used proper formatting to label vectors vs. matrices. For example, is the stimulus reconstruction filter in Eq. 1 a vector and the product is elementwise? What is the size of (and what are the rows/columns in) D on page 8? Etc.</p></disp-quote><p>Thank you for the suggestion. We have clarified vector and matrix dimensions and fixed the equation formatting in the Results.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 :</p><p>2.1)This study proposes a new algorithm for determining the electrical stimulation delivered through a sensory neural implant with the aim of improving the perceptual benefit to implant users. The algorithm is evaluated using data from an ex vivo prototype of a retinal prosthesis to computer-simulate the retinal responses expected from applying the algorithm and later by applying stimuli from the full temporally dithered, spatially multiplex algorithm to ex vivo retina.</p><p>Presently, stimulation algorithms used clinically are calibrated using limited perceptual data from the user. In contrast, the proposed algorithm uses detailed measurements of retinal responses to electrical stimulation to optimize the stimulation. This is achieved by minimizing the error between a target image and a version of that image reconstructed from the evoked response that is predicted by the algorithm based on the detailed measurements. The use of a data-driven, optimization approach is similar to several other recently proposed neural stimulation algorithms (which are not cited by the study). The distinguishing feature of the algorithm proposed in this study is that it seeks to stimulate in a way that minimizes the interactions between electrodes that can occur when stimulating neural responses. This avoids the need for the algorithm to account for such interactions.</p><p>Overall, the main advantage of the proposed approach is that it frames the problem of how to deliver perceptually beneficial electrical stimulation with an implant as a closed-loop/data-driven optimization problem. This has the potential to improve over presently used open-loop strategies. It then provides an algorithm for solving this optimization problem to a good approximation. Applying the algorithm using data recorded from ex vivo retina with a prototype implant is a strength.</p></disp-quote><p>Thank you for summarizing the work and recognizing its strengths. We have referenced additional data-driven optimization approaches for neural stimulation in the manuscript (Choi et al., 2016; Haji Ghaffari et al., 2021; Tafazoli et al., 2020; Vasireddy et al., 2023).</p><disp-quote content-type="editor-comment"><p>2.2) However, the evaluation of the efficacy of the algorithm is limited. In the first instance, it is limited to computer simulation of the retinal response for the version of the algorithm that uses just temporal dithering. While this analysis supports the conclusion that the proposed algorithm could provide improved visual perception relative to the clinical open-loop strategy, much stronger evidence would be provided by applying the optimal stimuli from the proposed algorithm directly to the ex vivo retinal preparation and measuring the retinal response. This approach to testing the algorithm directly to the ex vivo retina is done for the full version of the algorithm that combines spatial multiplexing with temporal dithering. However, in contrast to simulated results, the study does not report on the reconstructed images that result from applying the algorithm to ex vivo retina, nor on the reconstruction errors. This makes it difficult to evaluate the efficacy of the algorithm.</p></disp-quote><p>This is a crucial point. We note that the original analysis was not based on computer simulations, but samples drawn from calibration measurements at the start of each experiment. We have now clarified this, and more importantly added reconstructed images and errors using an actual closed-loop validation experiment as suggested. Please see our response to Essential Revisions 1 above and the changes to the manuscript.</p><disp-quote content-type="editor-comment"><p>2.3) Section: Introduction.</p><p>The motivation for using a temporally dithered, spatially multiplexed algorithm to optimize stimulation stems from the desire to minimize the interactions caused by simultaneous stimulation of the electrodes in evoking a neural response. While this is an important strategy to investigate, the interactions are not typically as &quot;complex&quot; as claimed in the manuscript. Indeed, previous studies in several labs (including the Chichilnisky lab) show that these interactions can typically be described by a linear, weighted sum of the electrode currents followed by a simple static nonlinearity to predict the probability of spiking (a small minority of retinal ganglion cells require more complex nonlinear descriptions) [1 -3]. This model and others have been the basis for alternative data-driven, closed-loop stimulation strategies that optimize the stimulation in a way that seeks to take advantage of the interactions between electrodes to improve the spatial resolution of evoked retinal activity through &quot;current steering&quot;.</p></disp-quote><p>We agree that there has been some progress in understanding interactions during electrical stimulation (though, we emphasize that this is distinct from interactions obtained with <italic>visual</italic> stimulation, where the LN models the reviewer describes have been fairly successful), and indeed some of this work has come from our lab. However, current studies of electrical stimulation, including our own, typically only examine one or a few electrodes at a time, and even in these situations we’ve shown that nonlinear interactions are often more complex than LN (Jepson et al., 2014). Our recent work (not shown) indicates that it occurs more with particular electrode configurations. In related work, we have recently made progress toward a closed-loop calibration strategy that uses “current steering” on 3 neighboring electrodes to improve the selectivity of stimulation (Vasireddy et al., 2023), and this work certainly requires dealing with the substantial nonlinearity that is present in some cases. However, using the above approaches to replicate the complex spatio-temporal pattern of RGC activity with hundreds or thousands of electrodes is far more complex. The approach presented here scales naturally to large numbers of electrodes, and also can include well-calibrated simultaneous-stimulation patterns (e.g. 3-electrode current steering) as “dictionary elements”, within the exact same optimization framework. We have attempted to clarify this a bit more in the Extensions subsection of the Discussion.</p><disp-quote content-type="editor-comment"><p>2.4) Section: Greedy temporal dithering to replicate neural code.</p><p>Data-driven optimization: The data required for the proposed algorithm is of two types. An exhaustive dictionary of response probabilities to single electrode stimulation across all current amplitudes, and a set of responses used to reconstruct the target image from the predicted response to electrical stimulation. For the latter, reconstruction of the image is achieved by applying linear filters to the predicted response. In the study, these linear filters were derived from cells' receptive fields, obtained by measured responses in the retina to light stimulation. It is noted that this would not be possible in a clinical implant, as the retina is degenerate. However, it is not clear how a set of filters would be obtained in this case. The authors mention that distinct cell types can be identified from spontaneous activity. However, this does not explain how receptive field size and location would be estimated in this situation.</p></disp-quote><p>These points are exactly correct. Please see our response to Public Review 1.7 above.</p><disp-quote content-type="editor-comment"><p>2.5) The reconstruction of the image is achieved through linear filtering with a matrix A, with columns, A_j, that are the (scaled) receptive field filters (Eq. 1). However, this is only correct if the receptive field filters of the different cells are orthogonal, i.e. the inner product of each pair of receptive fields is zero. More generally, appropriate linear filtering should be performed by applying the pseudo inverse of the transpose of A. This is because the retinal spike rates are being approximated as the inner product of the receptive field and the image (A_j transposed, matrix-multiplied by the image vector), half-wave rectified. For the receptive fields of ON and OFF parasol cells given in the study, it appears that the receptive fields are approximately orthogonal for the two separate populations due to the non-overlapping tiling of the visual field by each population (e.g. Figure 2). However, it is not clear whether this situation would prevail in the blind retina, as the filters have not been specified in the case.</p></disp-quote><p>Again, excellent point. Please see our response to Recommendations for Authors 2.22 below.</p><disp-quote content-type="editor-comment"><p>2.6) The greedy optimization algorithm is insufficiently explained in the Methods, including the following points:</p><p>A derivation justifying splitting the objective function into the terms due to the mean and variance is required.</p></disp-quote><p>We have added this derivation to the Methods.</p><disp-quote content-type="editor-comment"><p>2.7) The terminology for the terms tr(var(A R_i)) is not clearly explained. I assume it is the matrix trace of the covariance matrix of the random variable A R_i.</p></disp-quote><p>Yes, it is the trace of the covariance matrix. We have corrected the term in the Methods.</p><disp-quote content-type="editor-comment"><p>2.8) The assumption of a Bernoulli random variable for the response, i.e. 1 or 0 spikes, is limiting, given there may be multiple spikes in response to electrical stimulation, especially for activation via the retinal network.</p></disp-quote><p>This is a good point that needed to be clarified in the text. Please see our response to Recommendations for Authors 1.8 above.</p><disp-quote content-type="editor-comment"><p>2.9) The expression that was derived for the term tr(var(A R_i)) in the case of Bernoulli random variables should be given.</p></disp-quote><p>We have added this expression in the Methods.</p><disp-quote content-type="editor-comment"><p>2.10) It is not explained how the algorithm performs the final optimization at time step t, given elements in a restricted dictionary D_t.</p></disp-quote><p>We have clarified this point in the Methods.</p><disp-quote content-type="editor-comment"><p>2.11) Section: Greedy temporal dithering outperforms open loop methods.</p><p>The image reconstruction shown in Figure 2 uses 500, 3000, and 10000 electrical stimuli (shown in A, B and C respectively). However, these are unrealistically large numbers of stimuli: given the temporal perceptual window of 50 ms, mentioned in the Introduction as the time over which retinal responses would be perceptually integrated, and the pulse duration of 0.15 ms used in the study, a maximum of 333 stimuli could be applied during the window. Consequently, the use of 3000 and 10,000 electrical stimuli in the simulations provides unrealistic estimates of the degree to which the image can be reconstructed.</p></disp-quote><p>Please see our response to Recommendations for Authors 2.29 below.</p><disp-quote content-type="editor-comment"><p>2.12) A full comparison of the proposed greedy, closed-loop algorithm to the conventional open-loop algorithm is difficult to evaluate based on the results presented. First, the number of electrical stimuli applied in making the comparison (Figure 3H) is not given. However, it seems likely, given the data in Figure 3G that an unrealistically large 10,000 stimuli were used. If instead a realistic 300-400 stimuli were used there may be little difference between the greedy-closed loop algorithm and the conventional open-loop algorithm.</p></disp-quote><p>We have clarified the number of electrical stimuli applied for the reconstructions in Figure 3. Please see our response to Recommendations for Authors 2.29 below.</p><disp-quote content-type="editor-comment"><p>2.13) A second limitation is that, in this subsection, the greedy, closed-loop algorithm appears to have only been tested in simulation. E.g. &quot;For random checkerboard visual stimulus targets, the greedy dithering stimulation sequence was calculated, neural responses were sampled using measured response probabilities evoked by the individual selected stimuli, and then the target image was linearly reconstructed from these responses.&quot; Given that all the relevant data required to run the algorithm for the ex vivo retina and implant prototype had been collected during the experiment, it is unclear why the algorithm was not applied to test it by directly measuring responses to the algorithm's stimulation. This would have tested a critical assumption of the greedy-temporal dithering algorithm: that the responses to successive stimuli are statistically independent. Instead, the simulation assumes this to be the case.</p></disp-quote><p>This is a crucial point. We have now performed the closed-loop validation experiment and shown reconstructions. Please see our responses to Essential Revisions 1 above and the changes to the manuscript.</p><disp-quote content-type="editor-comment"><p>2.14) A third limitation is that the reconstructed image for the conventional open-loop algorithm does not resemble the phosphene images reported by most retinal implant users. Most implant users report predominantly bright, rather than dark, localized phosphenes [4]. The open-loop reconstruction shown in Figure 3d appears to be largely a gray averaging of light and dark phosphenes, likely due to the linear reconstruction method used.</p></disp-quote><p>Another good point. Please see our response to Recommendations for Authors 2.31 below.</p><disp-quote content-type="editor-comment"><p>2.15) Some details of the implementation of the open-loop strategy are unclear including:</p><p>How the area that was &quot;near&quot; the electrode was selected when calculating the intensity of the visual stimulus.</p><p>How the temporal sequence of the electrodes was chosen. It seems that the open-loop strategy is also likely, temporally dithered, but without the benefit of data-driven optimization.</p></disp-quote><p>We appreciate the suggestion to clarify. Please see our response to Recommendations for Authors 2.32 below.</p><disp-quote content-type="editor-comment"><p>2.16) Section: Greedy temporal dithering is nearly optimal given the interface constraints.</p><p>The comparison of the greedy, closed-loop approximately optimal algorithm to truly optimal algorithms is an important comparison in principle. However, again it is not clear if a realistic number of stimulation pulses were used in performing this comparison (i.e. &lt; 400). Some details of the implementation of the optimal comparison strategy are unclear including:</p><p>The meaning and purpose of the term V^T w in the objective function.</p><p>Whether w&gt;=0 was required after the integer requirement was relaxed in the optimization.</p></disp-quote><p>We have clarified the number of electrical stimuli applied for the reconstructions in Figure 3. Please see our response to Recommendations for Authors 2.34 below.</p><disp-quote content-type="editor-comment"><p>2.17) Section: Spatial multiplexing for fitting multiple stimuli in a visual integration window.</p><p>The idea to use spatial multiplexing of stimuli to overcome the limitation in the number of stimuli that can be delivered during a perceptual temporal window is a good idea to investigate. The aim is to choose stimuli on different electrodes that affect neural response independently. However, the initial formulation of what is meant by independence is not correct. This is stated as: &quot;For independence to hold, the following condition must be met: if p1 is the activation probability of a given cell with stimulation on electrode 1, and p2 is the activation probability of the same cell with electrode 2, then the activation probability with simultaneous stimulation must be p1+p2.&quot; That this is incorrect can be seen because this formulation could give a probability greater than 1. However, the subsequent description of what is actually implemented appears correct. A general, in-principle way of describing what independence means is that if p1 is the probability of stimulating one cell with electrode 1 and p2 is the probability of stimulating a different cell with electrode 2, then the probability of stimulating both cell 1 and cell 2 using simultaneous stimulation with electrodes 1 and 2 is the product of those probabilities, p1.p2.</p></disp-quote><p>Please see our response to Recommendations for Authors 2.35 below.</p><disp-quote content-type="editor-comment"><p>2.18) Instead, the reported results of the greedy dithering-plus-multiplexing (Figure 4) show only that it is possible to select eight multiplexed electrodes with sufficient separation to ensure minimal interference. This could potentially increase the number of electrodes stimulated with the greedy, closed-loop algorithm by a factor of 8, bringing it to around 2,700 stimuli. This is closer to the 3000 electrode stimulations used in Figure 2b that gave errors that approached the asymptotic limit. However, the results in Figure 4 were obtained using stimulation every 2 ms, not every 0.15 ms (= pulse duration). With this limitation, this reduces the number of electrode stimuli to 200 in a 50 ms perceptual window, which again is not likely to give a good reconstruction error according to the simulations.</p></disp-quote><p>Please see our response to Recommendations for Authors 2.29 below.</p><disp-quote content-type="editor-comment"><p>2.19) Other results sections.</p><p>The sections on hardware constraints, naturalistic viewing conditions, and the use of perceptual similarity measures make useful observations about the potential benefits of the optimization framework for algorithmically determining the electrical stimulation.</p></disp-quote><p>Thank you.</p><disp-quote content-type="editor-comment"><p>2.20) Discussion.</p><p>The discussion covers many important points well. Regarding the translational potential, I would agree that an important point is &quot;First, new surgical methods must be developed to implant a tiny chip on the surface of the retina with stable contact.&quot; But add that it must also be in extremely close contact for retinal ganglion cell spikes to be recorded. Further, a very high-density array (~ 60 μm pitch) and associated electronics for both stimulation and recording must be developed which is suitable in size, form factor, and power consumption for clinical use.</p></disp-quote><p>For the first point, we have added this to the Discussion as suggested. For the second point, please see the updated section on hardware design in the Discussion.</p><disp-quote content-type="editor-comment"><p>References</p><p>Jepson, L. H., Hottowy, P., Mathieson, K., Gunning, D. E., Dąbrowski, W., Litke, A. M., &amp; Chichilnisky, E. J. (2014). Spatially patterned electrical stimulation to enhance resolution of retinal prostheses. Journal of Neuroscience, 34(14), 4871-4881.</p><p>Lorach, H., Goetz, G., Smith, R., Lei, X., Mandel, Y., Kamins, T.,.… &amp; Palanker, D. (2015). Photovoltaic restoration of sight with high visual acuity. Nature medicine, 21(5), 476-482.</p><p>Maturana, M. I., Apollo, N. V., Hadjinicolaou, A. E., Garrett, D. J., Cloherty, S. L., Kameneva, T.,.… &amp; Meffin, H. (2016). A simple and accurate model to predict responses to multi-electrode stimulation in the retina. PLoS Computational Biology, 12(4), e1004849.</p><p>Humayun, M. S., Weiland, J. D., Fujii, G. Y., Greenberg, R., Williamson, R., Little, J., et al. (2003) Visual perception in a blind subject with a chronic microelectronic retinal prosthesis. Vision Research, 43, 2573-2581).</p></disp-quote><disp-quote content-type="editor-comment"><p>Recommendations for the authors:</p><p>2.21) Overall, it appears that the approach may offer some important benefits for sensory-neural implant users. However, the reporting of results is not sufficiently complete to draw strong conclusions about the potential benefits. In addition to the Public Review, I have some related suggestions below.</p><p>Reconstruction model in the blind retina.</p><p>It would be helpful to provide more detail about how the image reconstruction would work in the blind retinas, beyond what is mentioned regarding the identification of ON and OFF retinal ganglion cell type. How would the size and location of receptive fields be estimated?</p></disp-quote><p>Please see our response to Public Review 1.7 above.</p><disp-quote content-type="editor-comment"><p>2.22) The assumptions underlying the reconstruction model should be described, especially with respect to the orthogonality of the receptive field filters. It would be helpful to describe an approach in the methods that do not rely on this assumption, as I describe in my public comments.</p></disp-quote><p>We do indeed approximate the optimal linear reconstruction filters using the measured receptive fields of the cells. We agree that the decoder used is not the ‘inverse’ of the receptive fields, except in the case that the receptive fields of the cells are orthogonal. We can use the standard least-squares solution to move away from the assumption of orthogonality. We have clarified the important point that this is an approximation in the Results and Methods.</p><disp-quote content-type="editor-comment"><p>2.23) Greedy optimization algorithm: There are several aspects of this that could be better explained. These include:</p><p>A derivation justifying splitting the objective function into the terms due to the mean and variance is required.</p></disp-quote><p>Thank you. We have added this derivation to the Methods.</p><disp-quote content-type="editor-comment"><p>2.24) The terminology for the terms tr(var(A R_i)) is not clearly explained. I assume it is the matrix trace of the covariance matrix of the random variable A R_i.</p></disp-quote><p>Yes, it is the trace of the covariance matrix. We have corrected the term in the Methods.</p><disp-quote content-type="editor-comment"><p>2.25) The assumption of a Bernoulli random variable, i.e. 1 or 0 spikes, is limiting, given there may be multiple spikes in response to electrical stimulation, especially for activation via the retinal network.</p></disp-quote><p>Please see our response to Recommendations for Reviewers 1.8 above.</p><disp-quote content-type="editor-comment"><p>2.26) The expression derived for the term tr(var(A R_i)) in the case of Bernoulli random variables should be given.</p></disp-quote><p>We have added this expression in the Methods.</p><disp-quote content-type="editor-comment"><p>2.27) It is not explained how the algorithm performs the final optimization at time step t, given elements in a restricted dictionary D_t.</p></disp-quote><p>We have clarified this point in the Methods.</p><disp-quote content-type="editor-comment"><p>2.28) It is not explained how to determine the time for which recently used dictionary elements are excluded from current use.</p></disp-quote><p>Thank you for pointing out this missing information. We exclude dictionary elements to disallow stimulation of any recently targeted cell for 100 steps, which covers the relative refractory period (10 ms) when the stimulation frequency is less than 100Hz. We have now included this number in the Methods.</p><disp-quote content-type="editor-comment"><p>2.29) Section: Greedy temporal dithering outperforms open loop methods.</p><p>Regarding the number of single-electrode stimuli used in image reconstruction, it would be better to place the numbers used in the context of what is possible in the perceptual time window. It would recommend using the value of 333 instead of 500, as this corresponds to the number of 0.15 pulses that could be fit into a 50 ms window. The value of 3000 roughly corresponds to what might be achieved with spatial multiplexing. The value of 10,000 corresponds to the upper limit that is achievable through this algorithm.</p></disp-quote><p>This is an important and somewhat subtle point. While these calculations are correct for the primate recording, we found that 225±29 electrical stimulations were needed for asymptotic reconstruction performance of checkerboard targets in the rat retina. The total number of electrical stimulations needed depends on many factors including the number of cells targeted, their expected firing rates for the visual image, and the distribution of RGC activation probabilities in the electrical stimulation dictionary. Future work will be needed to identify how these factors vary across individuals, species and neural circuits. We have added text in the Discussion highlighting this issue.</p><disp-quote content-type="editor-comment"><p>2.30) I think it would be beneficial to make it clearer that the results in Figure 3 are simulated. It would also strengthen the study to perform validation in ex vivo retina to apply the greedy temporal dithering stimuli to the retina and reconstruct the image from the responses. If there is a good reason not to do this, this should be explained.</p></disp-quote><p>We agree that direct closed-loop experimental validation with the temporally dithered stimulation is important. We have now performed these experiments and clarified our language throughout the manuscript. Please see our responses to Essential Revisions 1 and 3.</p><disp-quote content-type="editor-comment"><p>2.31) It would improve the study if a reconstruction algorithm that provides an image with a better match to the perception of phosphenes by retinal implant users was used. If this cannot be done, it should be discussed as a limitation of the study.</p></disp-quote><p>This would be a highly relevant point if our electrical stimulation approaches had the same coarse level of control as existing implants. But in fact, the situation is quite different in the present work. For the short (150 µs) and low current (&lt;4 µA) pulses we use, only single or small groups of cells tend to be electrically activated (Figure 1C) (Sekirnjak et al., 2006, 2008). This is in part because the mechanism of activation is direct depolarization, rather than a network-mediated excitation, and in part because we explicitly avoid activation of axons (which produces large phosphenes in existing implants (Beyeler et al., 2019)) by pre-calibration. Due to these fundamental differences in stimulation compared to existing retinal implants, we do not expect the perception of phosphenes of the kind seen in present-day implants. We have clarified these considerations in the Results and Discussion.</p><disp-quote content-type="editor-comment"><p>2.32) It would be helpful to clarify some details of the implementation of the open-loop strategy including:</p><p>How the area that was &quot;near&quot; the electrode was selected when calculating the intensity of the visual stimulus.</p><p>How the temporal sequence of the electrodes was chosen.</p></disp-quote><p>We have replaced “open-loop” with “static pixel-wise mapping” to more accurately reflect the calculation we are performing which approximates the function of present-day retinal implants. Specifically, we used a mapping between the intensity of the visual stimulus incident on an electrode and the intensity of the current passed through that electrode to determine which electrical stimuli to deliver. The intention of this analysis is to provide a generous benchmark for present-day devices – it is generous in the sense that we assume a much greater degree of calibration precision than these devices can actually achieve.</p><p>First, an affine transformation mapped the visual stimulus onto the electrode array. Second, the average visual stimulus intensity was identified over an approximately 130 µm x 130 µm region around the electrode location. Third, the average visual stimulus intensity on the electrode (<italic>s</italic>) was mapped to the current amplitude (<italic>i</italic>) using a scaled sigmoid:</p><p><inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> This electrical stimulus was then delivered <italic>n</italic> times at that electrode. All parameters (i.e. five parameters {<italic>a</italic>, <italic>b</italic>, <italic>c</italic>, <italic>d</italic>, <italic>n</italic>} for each electrode) were simultaneously optimized to minimize reconstruction error across a training set of random checkerboard images.</p><p>A temporal dithering strategy for ordering the electrical stimuli was assumed so that the stimuli would not interfere with one another. The performance of this static pixel-wise mapping approach was evaluated in the same way as the dynamically optimized stimulation approach. In particular, the target visual stimulus was linearly reconstructed from the identified electrical stimulation sequence using samples drawn from the single-electrode calibration data.</p><p>The static mapping at each electrode captures the common aspect of existing approaches and highlights the crucial improvement of our dynamic approach. Again, we note that this procedure provides a generous interpretation of existing methods because it uses actual measured neural responses for optimizing the mapping rather than relying on more limited patient feedback. We have updated the text in the Results and clarified these implementation details in the Methods.</p><disp-quote content-type="editor-comment"><p>2.33) Section: Greedy temporal dithering is nearly optimal given the interface constraints.</p><p>A realistic number of stimulation pulses should be used in performing this comparison e.g. &lt; 400 for the pure temporal dithering or &lt; 3000 for the spatially multiplexed, temporal dithering.</p></disp-quote><p>Please see our response to Recommendations for Authors 2.29 above.</p><disp-quote content-type="editor-comment"><p>2.34) It would be helpful to clarify some details of the implementation of the open-loop strategy including:</p><p>The meaning and purpose of the term V^T.w in the objective function.</p><p>Whether w&gt;=0 was required after the integer requirement was relaxed in the optimization.</p></disp-quote><p>These details of the implementation of the optimal comparison strategy have been clarified in the Methods. Yes, the non-negativity constraint keeps the approximate objective closer to the original formulation.</p><disp-quote content-type="editor-comment"><p>2.35) Section: Spatial multiplexing for fitting multiple stimuli in a visual integration window.</p><p>As described in my public review, the description of independence is not correct. I have suggested an alternative description that I believe accords with what was actually implemented.</p></disp-quote><p>Thank you for pointing out the inconsistency in the description of spatial independence. We have corrected and simplified the description in the Methods.</p><disp-quote content-type="editor-comment"><p>2.36) It was surprising that the results of the validation experiments on ex vivo retina with the spatially multiplexed, temporally dithered algorithm were not reported more thoroughly. It is important to provide figures showing the image reconstruction that was achieved and the statistics for the reconstruction error.</p></disp-quote><p>We have now performed the closed-loop validation experiment, and added reconstructed images from it. Please see our responses to Essential Revisions 1 above and the changes to the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #3</p><p>3.1) In this study, Shah and colleagues propose an interesting solution to the non-linear interactions caused by simultaneously stimulating multiple electrodes within a retinal implant. Through high-resolution recordings of ON and OFF parasol retinal ganglion cells, the authors demonstrate that a greedy dithering and spatially multiplexed algorithm, which can also work in the presence of saccadic eye movements, is able to faithfully reconstruct images represented by total numbers of spikes in a given time window across multiple retinal ganglion cells. Essentially, Shah and colleagues propose and demonstrate a method to only stimulate single or groups of 8 electrodes at a time from a pre-established dictionary, but then interleave stimulation of multiple electrodes or groups rapidly across the dictionary to additively build an image. Through their very rigorous and elegant ex vivo recordings in 180 ON and OFF parasol cells across four primate retina preparations, the authors compellingly demonstrate that (i) their greedy algorithm performs better than an open loop algorithm, similar to an optimal algorithm considering the interface constraints, and close but not equal to an ideal control using only a single-electrode dictionary; (ii) that groups of electrodes can be simultaneously activated with a high-resolution neural interface without any retinal interactions provided that they are at least 160 μm apart; (iii) that the algorithm performs just as well even with only 50% of the electrodes on the interface and (iv) that the algorithm can work in the presence of saccadic eye movements and performs better when both saccadic and fixational eye movements are made as opposed to saccadic movements alone.</p><p>The experimental recordings and performance of the algorithm in various conditions are the biggest strengths of this study and the authors certainly demonstrate that their algorithm can reproduce spiking numbers across an array of cells that resemble closely spiking numbers evoked by visual stimuli for these conditions. In other words, the authors' primary claim that the neural code for visual images in the retina (in the form of spiking numbers) can be faithfully reproduced with electrical stimulation using such an algorithm, is well supported by evidence.</p><p>A major weakness in the study however is the reliance of this algorithm on several significant assumptions about neural coding in the retina, neural coding in the visual brain, and interactions between electrodes even with non-simultaneous stimulation. Some of these assumptions have already been highly challenged in several studies in the visual neuroscience field and in studies involving the perception of phosphenes with interleaved stimulation of single electrodes.</p><p>Therefore, in light of what is currently known about visual encoding and artificial vision, the study whilst showcasing an elegant computational tool perhaps provides only little hope that such an algorithm will actually work in practice to recreate the perception of images with electrical stimulation but instead does lay a foundation for further work to be done with the assessment of future algorithms. The main assumptions that the authors rely on include:</p><p>1) That neural coding in the retina is simply based on a number of spikes evoked by populations of cells ignoring any temporal patterns of responses. A plethora of studies has indicated that relative spike timing between groups of retinal ganglion cells for example can encode complex visual features but the greedy algorithm does not aim to mimic these spike timing features.</p></disp-quote><p>Please see our response to Recommendations for Authors 3.2 below.</p><disp-quote content-type="editor-comment"><p>2) That perception within the brain is solely based on a number of spikes within a slow temporal integration window (the authors cite a 1995 reference for this). Since 1995 though, this has also been challenged, therefore extending the authors' claims of reproducing spike numbers in the retina to reproducing perception in the brain would be contentious.</p></disp-quote><p>Please see our response to Essential Revisions 2 paragraph 3 above.</p><disp-quote content-type="editor-comment"><p>3) That neural interactions with non-simultaneous interleaved electrical stimulation are absent. There is in silico, electrophysiological and perceptual evidence with retinal implants that interleaving of electrodes still results in neural interactions and that perception with interleaved stimulation with multiple electrodes does not result in a linear summed perception of phosphenes evoked by single electrodes i.e. dictionary elements. Therefore, the algorithm would only work if such interactions are minimal or absent, for example with larger than 0.1 ms intervals between stimulations or more than 160 μm electrode separation. Note, interactions with interleaving also exist with cochlear implants as the current spread is large.</p></disp-quote><p>Please see our response to Essential Revisions 2 paragraph 4 above.</p><disp-quote content-type="editor-comment"><p>4) That even if the above 3 assumptions were applied and true, the algorithm can faithfully extrapolate to reconstruct moving images at 24 per second. This seems unlikely as presumably the total time required to linearly reconstruct a single static image would extend to many tens or even hundreds of ms given the number of times each dictionary element needs to be accessed to enable reproduction of similar spiking numbers between visual and electrical stimulation, runs in the thousands.</p></disp-quote><p>Please see our response to Recommendations for Authors 3.3 below.</p><disp-quote content-type="editor-comment"><p>In spite of major reliance on these assumptions, the authors do demonstrate a very useful tool in the form of the greedy algorithm for situations perhaps other than the visual system, where perception with artificial stimulation may be more predictable and interactions with non-simultaneous stimulation may be simpler.</p></disp-quote><p>Thank you for recognizing the strength of the work.</p><disp-quote content-type="editor-comment"><p>Recommendations for the authors:</p><p>3.2) It may be possible to address at least some of the limitations in particular (1) and (4) mentioned in the public review. For limitation (1), the authors could try and experiment with their algorithm and reanalyse data to examine if and how well spike timing features (perhaps relative first spike latencies between RGCs or other temporal patterns of spikes) are reproducible.</p></disp-quote><p>This is an important point. Please see our response to Essential Revisions 2 paragraph 2. We also note that with the very high temporal precision of our stimulation (evoked spike time variation of roughly 0.1 ms), if there were some degree of stimulus coding in the relative timing of spikes, that relative timing could certainly be reproduced by the stimulation sequences that we provide, with a suitable modification of the optimization approach. However, this would substantially increase the overall complexity of the algorithm and we think it is beyond the scope of this paper.</p><disp-quote content-type="editor-comment"><p>3.3) For limitation (4) the authors could at least perform calculations of time taken by the algorithm in each of the situations and targets presented, to examine if these times are realistic.</p></disp-quote><p>In fact, realistic numbers of electrical stimulations were required for the closed-loop experimental validation of greedy temporal dithering using rat retina (225±29 stimulations for asymptotic reconstruction performance, requiring 62±9 ms to deliver the temporally dithered sequence). For the thousands of stimulations reported for the macaque retina, spatial multiplexing could reduce the delivery time down to a realistic tens of milliseconds. For example, delivering 3000 stimuli at a 0.15 ms interval would require 450 ms without spatial multiplexing, but could require as short as 57 ms with spatial multiplexing delivering an average of 8 electrical stimuli per time step. While the former duration exceeds visual integration time, the latter duration approximately matches it. The exact number of electrical stimulations needed for each situation depends on many factors including the number of cells targeted, their expected firing rates for the particular visual image, and the distribution of RGC activation probabilities in the electrical stimulation dictionary. Future work will be needed to identify how these factors vary across individuals, species and neural circuits. We have added text in the Discussion highlighting this issue.</p><disp-quote content-type="editor-comment"><p>3.4) For limitations (2) and (3), the authors at a minimum should address these clearly in their discussion and the potential implications of the failure of these assumptions on their algorithm performance.</p></disp-quote><p>Please see our response to Essential Revisions 2 paragraphs 3 and 4 for a discussion of limitations (2) and (3), respectively. We have clarified these considerations in the Discussion.</p><disp-quote content-type="editor-comment"><p>3.5) Other things that the authors should consider is including some example raw data from their retinas before and after artifact subtraction in response to both visual targets and their greedy algorithm as a figure.</p></disp-quote><p>Thank you for this suggestion. While this could be beneficial, we ultimately decided against this as the spike sorting in the presence of electrical artifact is a very involved topic and has been extensively covered in other papers from our group (Gogliettino et al., 2023; Jepson et al., 2013, 2014; Madugula et al., 2022; Sekirnjak et al., 2006, 2008). We cite some of these papers in the Methods section.</p><p><bold>References</bold></p><p>Alonso, J. M., Usrey, W. M., &amp; Reid, R. C. (1996). Precisely correlated firing in cells of the lateral geniculate nucleus. <italic>Nature</italic>, <italic>383</italic>(6603), 815–819. https://doi.org/10.1038/383815a0</p><p>Berry, M. J., &amp; Meister, M. (1998). Refractoriness and Neural Precision. <italic>Journal of Neuroscience</italic>, <italic>18</italic>(6), 2200–2211.https://doi.org/10.1523/JNEUROSCI.18-06-02200.1998</p><p>Berry, M. J., Warland, D. K., &amp; Meister, M. (1997). The structure and precision of retinal spike trains. <italic>Proceedings of the National Academy of Sciences</italic>, <italic>94</italic>(10), 5411–5416. https://doi.org/10.1073/pnas.94.10.5411</p><p>Beyeler, M., Nanduri, D., Weiland, J. D., Rokem, A., Boynton, G. M., &amp; Fine, I. (2019). A model of ganglion axon pathways accounts for percepts elicited by retinal implants. <italic>Scientific Reports</italic>, <italic>9</italic>(1), 9199. https://doi.org/10.1038/s41598-019-45416-4</p><p>Borghuis, B. G., Tadin, D., Lankheet, M. J. M., Lappin, J. S., &amp; van de Grind, W. A. (2019). Temporal Limits of Visual Motion Processing: Psychophysics and Neurophysiology. <italic>Vision</italic>, <italic>3</italic>(1), 5. https://doi.org/10.3390/vision3010005</p><p>Brackbill, N., Rhoades, C., Kling, A., Shah, N. P., Sher, A., Litke, A. M., &amp; Chichilnisky, E. J. (2020). Reconstruction of natural images from responses of primate retinal ganglion cells. <italic>eLife</italic>, <italic>9</italic>, e58516. https://doi.org/10.7554/<italic>eLife</italic>.58516</p><p>Chichilnisky, E. J., &amp; Kalmar, R. S. (2003). Temporal Resolution of Ensemble Visual Motion Signals in Primate Retina. <italic>The Journal of Neuroscience</italic>, <italic>23</italic>(17), 6681–6689. https://doi.org/10.1523/JNEUROSCI.23-17-06681.2003</p><p>Choi, J. S., Brockmeier, A. J., McNiel, D. B., Kraus, L. M. von, Príncipe, J. C., &amp; Francis, J. T. (2016). Eliciting naturalistic cortical responses with a sensory prosthesis via optimized microstimulation. <italic>Journal of Neural Engineering</italic>, <italic>13</italic>(5), 056007. https://doi.org/10.1088/1741-2560/13/5/056007</p><p>Frechette, E. S., Sher, A., Grivich, M. I., Petrusca, D., Litke, A. M., &amp; Chichilnisky, E. J. (2005).</p><p>Fidelity of the Ensemble Code for Visual Motion in Primate Retina. <italic>Journal of Neurophysiology</italic>, <italic>94</italic>(1), 119–135. https://doi.org/10.1152/jn.01175.2004</p><p>Gogliettino, A. R., Madugula, S. S., Grosberg, L. E., Vilkhu, R. S., Brown, J., Nguyen, H., Kling, A., Hottowy, P., Dąbrowski, W., Sher, A., Litke, A. M., &amp; Chichilnisky, E. J. (2023).</p><p>High-Fidelity Reproduction of Visual Signals by Electrical Stimulation in the Central Primate Retina. <italic>Journal of Neuroscience</italic>, <italic>43</italic>(25), 4625–4641. https://doi.org/10.1523/JNEUROSCI.1091-22.2023</p><p>Gollisch, T., &amp; Meister, M. (2008). Rapid Neural Coding in the Retina with Relative Spike Latencies. <italic>Science</italic>, <italic>319</italic>(5866), 1108–1111. https://doi.org/10.1126/science.1149639</p><p>Gütig, R., Gollisch, T., Sompolinsky, H., &amp; Meister, M. (2013). Computing Complex Visual Features with Retinal Spike Times. <italic>PLoS ONE</italic>, <italic>8</italic>(1), e53063. https://doi.org/10.1371/journal.pone.0053063</p><p>Haji Ghaffari, D., Akwaboah, A. D., Mirzakhalili, E., &amp; Weiland, J. D. (2021). Real-Time Optimization of Retinal Ganglion Cell Spatial Activity in Response to Epiretinal Stimulation. <italic>IEEE Transactions on Neural Systems and Rehabilitation Engineering</italic>, <italic>29</italic>, 2733–2741. https://doi.org/10.1109/TNSRE.2021.3138297</p><p>Ho, E., Shmakov, A., &amp; Palanker, D. (2020). Decoding network-mediated retinal response to electrical stimulation: Implications for fidelity of prosthetic vision. <italic>Journal of Neural Engineering</italic>, <italic>17</italic>(6), 10.1088/1741-2552/abc535. https://doi.org/10.1088/1741-2552/abc535</p><p>Jepson, L. H., Hottowy, P., Mathieson, K., Gunning, D. E., Dabrowski, W., Litke, A. M., &amp; Chichilnisky, E. J. (2013). Focal electrical stimulation of major ganglion cell types in the primate retina for the design of visual prostheses. <italic>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</italic>, <italic>33</italic>(17), 7194–7205. https://doi.org/10.1523/JNEUROSCI.4967-12.2013</p><p>Jepson, L. H., Hottowy, P., Mathieson, K., Gunning, D. E., Dąbrowski, W., Litke, A. M., &amp; Chichilnisky, E. J. (2014). Spatially patterned electrical stimulation to enhance resolution of retinal prostheses. <italic>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</italic>, <italic>34</italic>(14), 4871–4881. https://doi.org/10.1523/JNEUROSCI.2882-13.2014</p><p>Keat, J., Reinagel, P., Reid, R. C., &amp; Meister, M. (2001). Predicting Every Spike: A Model for the Responses of Visual Neurons. <italic>Neuron</italic>, <italic>30</italic>(3), 803–817. https://doi.org/10.1016/S0896-6273(01)00322-1</p><p>Li, P. H., Gauthier, J. L., Schiff, M., Sher, A., Ahn, D., Field, G. D., Greschner, M., Callaway, E. M., Litke, A. M., &amp; Chichilnisky, E. J. (2015). Anatomical identification of extracellularly recorded cells in large-scale multielectrode recordings. <italic>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</italic>, <italic>35</italic>(11), 4663–4675.https://doi.org/10.1523/JNEUROSCI.3675-14.2015</p><p>Lotlikar, A., Shah, N. P., Gogliettino, A. R., Vilkhu, R., Madugula, S., Grosberg, L., Hottowy, P., Sher, A., Litke, A., Chichilnisky, E. J., &amp; Mitra, S. (2023). Partitioned Temporal Dithering for Efficient Epiretinal Electrical Stimulation. <italic>2023 11th International IEEE/EMBS Conference on Neural Engineering (NER)</italic>, 1–5. https://doi.org/10.1109/NER52421.2023.10123787</p><p>Madugula, S. S., Gogliettino, A. R., Zaidi, M., Aggarwal, G., Kling, A., Shah, N. P., Brown, J. B.,</p><p>Vilkhu, R., Hays, M. R., Nguyen, H., Fan, V., Wu, E. G., Hottowy, P., Sher, A., Litke, A. M., Silva, R. A., &amp; Chichilnisky, E. J. (2022). Focal Electrical Stimulation of Human Retinal Ganglion Cells for Vision Restoration. <italic>Journal of Neural Engineering</italic>, <italic>19</italic>(6), 10.1088/1741-2552/aca5b5. https://doi.org/10.1088/1741-2552/aca5b5</p><p>Meister, M. (1996). Multineuronal codes in retinal signaling. <italic>Proceedings of the National Academy of Sciences</italic>, <italic>93</italic>(2), 609–614. https://doi.org/10.1073/pnas.93.2.609</p><p>Reich, D. S., Victor, J. D., Knight, B. W., Ozaki, T., &amp; Kaplan, E. (1997). Response Variability and Timing Precision of Neuronal Spike Trains in vivo. <italic>Journal of Neurophysiology</italic>, <italic>77</italic>(5), 2836–2841. https://doi.org/10.1152/jn.1997.77.5.2836</p><p>Samaha, J., &amp; Postle, B. R. (2015). The speed of α-band oscillations predicts the temporal resolution of visual perception. <italic>Current Biology : CB</italic>, <italic>25</italic>(22), 2985–2990. https://doi.org/10.1016/j.cub.2015.10.007</p><p>Sekhar, S., Ramesh, P., Bassetto, G., Zrenner, E., Macke, J. H., &amp; Rathbun, D. L. (2020).Characterizing Retinal Ganglion Cell Responses to Electrical Stimulation Using Generalized Linear Models. <italic>Frontiers in Neuroscience</italic>, <italic>14</italic>, 378. https://doi.org/10.3389/fnins.2020.00378</p><p>Sekirnjak, C., Hottowy, P., Sher, A., Dabrowski, W., Litke, A. M., &amp; Chichilnisky, E. J. (2006). Electrical stimulation of mammalian retinal ganglion cells with multielectrode arrays. <italic>Journal of Neurophysiology</italic>, <italic>95</italic>(6), 3311–3327. https://doi.org/10.1152/jn.01168.2005</p><p>Sekirnjak, C., Hottowy, P., Sher, A., Dabrowski, W., Litke, A. M., &amp; Chichilnisky, E. J. (2008). High-resolution electrical stimulation of primate retina for epiretinal implant design. <italic>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</italic>, <italic>28</italic>(17), 4446–4456. https://doi.org/10.1523/JNEUROSCI.5138-07.2008</p><p>Shadlen, M. N., &amp; Newsome, W. T. (1994). Noise, neural codes and cortical organization. <italic>Current Opinion in Neurobiology</italic>, <italic>4</italic>(4), 569–579. https://doi.org/10.1016/0959-4388(94)90059-0</p><p>Tadin, D., Lappin, J. S., Blake, R., &amp; Glasser, D. M. (2010). High temporal precision for perceiving event offsets. <italic>Vision Research</italic>, <italic>50</italic>(19), 1966–1971. https://doi.org/10.1016/j.visres.2010.07.005</p><p>Tafazoli, S., MacDowell, C. J., Che, Z., Letai, K. C., Steinhardt, C. R., &amp; Buschman, T. J. (2020). Learning to control the brain through adaptive closed-loop patterned stimulation. <italic>Journal of Neural Engineering</italic>, <italic>17</italic>(5), 056007. https://doi.org/10.1088/1741-2552/abb860</p><p>Uzzell, V. J., &amp; Chichilnisky, E. J. (2004). Precision of Spike Trains in Primate Retinal Ganglion Cells. <italic>Journal of Neurophysiology</italic>, <italic>92</italic>(2), 780–789. https://doi.org/10.1152/jn.01171.2003</p><p>Vasireddy, P. K., Gogliettino, A. R., Brown, J. B., Vilkhu, R. S., Madugula, S. S., Phillips, A. J., Mitral, S., Hottowy, P., Sher, A., Litke, A., Shah, N. P., &amp; Chichilnisky, E. J. (2023).Efficient Modeling and Calibration of Multi-Electrode Stimuli for Epiretinal Implants. <italic>2023 11th International IEEE/EMBS Conference on Neural Engineering (NER)</italic>, 1–4. https://doi.org/10.1109/NER52421.2023.10123907</p><p>Wu, E. G., Brackbill, N., Rhoades, C., Kling, A., Gogliettino, A. R., Shah, N. P., Sher, A., Litke, A. M., Simoncelli, E. P., &amp; Chichilnisky, E. J. (2023). <italic>Fixational Eye Movements Enhance the Precision of Visual Information Transmitted by the Primate Retina</italic> (p.2023.08.12.552902). bioRxiv. https://doi.org/10.1101/2023.08.12.552902</p><p>Wutz, A., Muschter, E., van Koningsbruggen, M. G., Weisz, N., &amp; Melcher, D. (2016). Temporal Integration Windows in Neural Processing and Perception Aligned to Saccadic Eye Movements. <italic>Current Biology: CB</italic>, <italic>26</italic>(13), 1659–1668. https://doi.org/10.1016/j.cub.2016.04.070</p><p>Yoon, Y. J., Lee, J.-I., Jang, Y. J., An, S., Kim, J. H., Fried, S. I., &amp; Im, M. (2020). Retinal Degeneration Reduces Consistency of Network-mediated Responses Arising in Ganglion Cells to Electric Stimulation. <italic>IEEE Transactions on Neural Systems and Rehabilitation Engineering : A Publication of the IEEE Engineering in Medicine and Biology Society</italic>, <italic>28</italic>(9), 1921–1930. https://doi.org/10.1109/TNSRE.2020.3003345</p><p>Zaidi, M., Aggarwal, G., Shah, N. P., Karniol-Tambour, O., Goetz, G., Madugula, S. S., Gogliettino, A. R., Wu, E. G., Kling, A., Brackbill, N., Sher, A., Litke, A. M., &amp; Chichilnisky, E. J. (2023). Inferring light responses of primate retinal ganglion cells using intrinsic electrical signatures. <italic>Journal of Neural Engineering</italic>, <italic>20</italic>(4), 045001. https://doi.org/10.1088/1741-2552/ace657</p></body></sub-article></article>