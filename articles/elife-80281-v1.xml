<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80281</article-id><article-id pub-id-type="doi">10.7554/eLife.80281</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Representational integration and differentiation in the human hippocampus following goal-directed navigation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-279840"><name><surname>Fernandez</surname><given-names>Corey</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3901-5552</contrib-id><email>coreyf@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-187498"><name><surname>Jiang</surname><given-names>Jiefeng</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-281893"><name><surname>Wang</surname><given-names>Shao-Fang</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-281894"><name><surname>Choi</surname><given-names>Hannah Lee</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0556-8351</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-10267"><name><surname>Wagner</surname><given-names>Anthony D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0624-4543</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Graduate Program in Neurosciences, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Wu Tsai Neurosciences Institute, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/036jqmy94</institution-id><institution>Department of Psychological and Brain Sciences, University of Iowa</institution></institution-wrap><addr-line><named-content content-type="city">Iowa City</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Psychology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e80281</elocation-id><history><date date-type="received" iso-8601-date="2022-05-14"><day>14</day><month>05</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-29"><day>29</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-04-13"><day>13</day><month>04</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.04.12.488078"/></event></pub-history><permissions><copyright-statement>© 2023, Fernandez et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Fernandez et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80281-v1.pdf"/><abstract><p>As we learn, dynamic memory processes build structured knowledge across our experiences. Such knowledge enables the formation of internal models of the world that we use to plan, make decisions, and act. Recent theorizing posits that mnemonic mechanisms of differentiation and integration – which at one level may seem to be at odds – both contribute to the emergence of structured knowledge. We tested this possibility using fMRI as human participants learned to navigate within local and global virtual environments over the course of 3 days. Pattern similarity analyses on entorhinal cortical and hippocampal patterns revealed evidence that differentiation and integration work concurrently to build local and global environmental representations, and that variability in integration relates to differences in navigation efficiency. These results offer new insights into the neural machinery and the underlying mechanisms that translate experiences into structured knowledge that allows us to navigate to achieve goals.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>episodic memory</kwd><kwd>spatial learning</kwd><kwd>fmri</kwd><kwd>medial temporal lobe</kwd><kwd>hippocampus</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>The Marcus and Amalia Wallenberg Foundation</institution></institution-wrap></funding-source><award-id>MAW 2015.0043</award-id><principal-award-recipient><name><surname>Wagner</surname><given-names>Anthony D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>The Stanford Center for Cognitive and Neurobiological Imaging</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wagner</surname><given-names>Anthony D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Mnemonic mechanisms of differentiation and integration within the medial temporal lobe occur concurrently during the learning of local and global environmental knowledge.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Memory is central to who we are, providing the foundation for our sense of self, understanding of the world, and predictions about the future. While we experience life as a series of events, our ability to build knowledge across events – to abstract common themes and infer relationships between events – enables us to construct internal models of the world, or ‘cognitive maps’, that we use to plan, make decisions, and act (<xref ref-type="bibr" rid="bib82">Tolman, 1948</xref>; <xref ref-type="bibr" rid="bib59">O’Keefe and Nadel, 1978</xref>; <xref ref-type="bibr" rid="bib52">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib20">Epstein et al., 2017</xref>). One goal of memory science is to understand how neural systems integrate information across experiences, building structured knowledge about the world.</p><p>Interactions between a network of brain regions in the medial temporal lobe and neocortex are known to support episodic memory, or memory for events. Central to this network is the hippocampus, whose computations allow us to encode and recall specific episodes from the past and to abstract relationships across experiences that share common elements (<xref ref-type="bibr" rid="bib52">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Eichenbaum, 2004</xref>). In subserving rapid learning over co-occurring features, the hippocampus plays a critical role in building episodic memories and structured knowledge by binding together inputs from distributed cortical regions, forming conjunctive representations of experiences (<xref ref-type="bibr" rid="bib52">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib81">Teyler and DiScenna, 1986</xref>; <xref ref-type="bibr" rid="bib51">Libby et al., 2014</xref>). As we learn, neural populations in the hippocampus that represent the traces for individual events can vary in their relations, becoming more distinct (differentiation) or more similar (integration) to each other (<xref ref-type="bibr" rid="bib69">Ritvo et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Brunec et al., 2020</xref>; <xref ref-type="bibr" rid="bib89">Wammes et al., 2022</xref>). A central question is how these two seemingly contradictory mechanisms of representational change are employed during the construction of structured knowledge.</p><p>One hypothesized hippocampal mechanism is pattern separation, in which the hippocampus creates distinct neural representations for highly similar events (<xref ref-type="bibr" rid="bib62">O’Reilly and McClelland, 1994</xref>; <xref ref-type="bibr" rid="bib49">Leutgeb et al., 2007</xref>; <xref ref-type="bibr" rid="bib98">Yassa and Stark, 2011</xref>). Through pattern separation, the hippocampus avoids the blending of similar experiences in memory, reducing across-event interference and forgetting. Tests of the pattern separation hypothesis include functional MRI (fMRI) studies manipulating the extent to which features of events are similar. Such studies have found intriguing evidence for pattern separation when experiences have high overlap or lead to similar outcomes (<xref ref-type="bibr" rid="bib2">Bakker et al., 2008</xref>; <xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Kyle et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Ballard et al., 2019</xref>), with links to later remembering (<xref ref-type="bibr" rid="bib48">LaRocque et al., 2013</xref>). Moreover, some fMRI studies have found evidence for pattern differentiation, where event overlap appears to trigger a ‘repulsion’ of hippocampal representations beyond baseline similarity to reduce interference (<xref ref-type="bibr" rid="bib89">Wammes et al., 2022</xref>; <xref ref-type="bibr" rid="bib25">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">Chanales et al., 2017</xref>; <xref ref-type="bibr" rid="bib70">Schapiro et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Jiang et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Hulbert and Norman, 2015</xref>).</p><p>Despite evidence supporting the hippocampus’s role in forming distinct memories for events, decades of work also suggest it is essential for building structured knowledge and forming cognitive maps that capture relations between spatial, perceptual, and conceptual features that were encountered in separate events that share common elements (<xref ref-type="bibr" rid="bib82">Tolman, 1948</xref>; <xref ref-type="bibr" rid="bib59">O’Keefe and Nadel, 1978</xref>; <xref ref-type="bibr" rid="bib20">Epstein et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>; <xref ref-type="bibr" rid="bib83">Tompary and Davachi, 2017</xref>). For example, the hippocampus quickly extracts the temporal structure of an environment as defined by the transition probabilities between items in sequences (<xref ref-type="bibr" rid="bib70">Schapiro et al., 2012</xref>; <xref ref-type="bibr" rid="bib71">Schapiro et al., 2013</xref>), and it represents higher order structure when there is no variance in transition probabilities between items but an underlying community structure in a sequence (<xref ref-type="bibr" rid="bib73">Schapiro et al., 2016</xref>). Moreover, it is essential for inferring indirect relationships between items and events (<xref ref-type="bibr" rid="bib72">Schapiro et al., 2014</xref>; <xref ref-type="bibr" rid="bib101">Zeithamova et al., 2012</xref>; <xref ref-type="bibr" rid="bib87">Vaidya et al., 2021</xref>).</p><p>How do overlapping features from separate events lead to integration and generalization? Prior work suggests that retrieval-based learning, recurrent mechanisms within the episodic memory network, and memory replay enable representational integration and allow for the encoding of relationships that have never been directly experienced (<xref ref-type="bibr" rid="bib73">Schapiro et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Kumaran and McClelland, 2012</xref>; <xref ref-type="bibr" rid="bib44">Kumaran et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Schapiro et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Gupta et al., 2010</xref>; <xref ref-type="bibr" rid="bib97">Wu and Foster, 2014</xref>). For example, integrative encoding is thought to occur when a new experience (BC) triggers the recall of a related past episode (AB), and the concurrent activation of the remembered and presently experienced episodes results in the formation of an integrated representation (ABC) that can support future inference and generalization (<xref ref-type="bibr" rid="bib76">Shohamy and Wagner, 2008</xref>). Integrative encoding may relate to other models that rely on recurrent connections between entorhinal cortex (EC) and the hippocampus to recirculate hippocampal output back into the system as new input, allowing for the discovery of higher order structure at the time of knowledge expression (<xref ref-type="bibr" rid="bib43">Kumaran and McClelland, 2012</xref>; <xref ref-type="bibr" rid="bib44">Kumaran et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Schapiro et al., 2017</xref>). To the extent that the discovered structure is encoded, then across-event relationships are captured in an integrated representation. Moreover, replay within the episodic memory network is also thought to play an important role in updating mnemonic representations (<xref ref-type="bibr" rid="bib60">Ólafsdóttir et al., 2018</xref>). Online replay during periods of awake rest has been shown to integrate multiple events, rather than just replaying a single event (<xref ref-type="bibr" rid="bib31">Gupta et al., 2010</xref>). Offline replay during sharp wave-ripples is associated with memory consolidation and updates to neocortical knowledge structures (<xref ref-type="bibr" rid="bib99">Yu et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Jadhav et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Lewis and Durrant, 2011</xref>; <xref ref-type="bibr" rid="bib15">Clewett et al., 2022</xref>).</p><p>Despite a large and rapidly expanding theoretical and empirical literature, our understanding of how differentiation and integration occur across discrete experiences and contribute to the building of coherent, multi-level structured representations is still far from comprehensive. Additional insights may come from examining these processes during the building of structured spatial knowledge, or maps of the external environment. For example, if one moves to a new city, one often first learns individual routes between navigational goals (i.e., from home to work, home to grocery store). Over time, the episodic memory network links individual routes, building a spatial map that enables planning of novel routes, detours, and shortcuts. Spatial knowledge acquired across learning consists of both local and global representations that are used to different extents based on navigational demands (<xref ref-type="bibr" rid="bib12">Chrastil, 2013</xref>), and the hippocampus and EC are thought to be crucial for the acquisition of such representations. Spatial cells in these regions encode position, head-direction, speed, and other environmental features (<xref ref-type="bibr" rid="bib58">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib54">McNaughton et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Hafting et al., 2005</xref>; <xref ref-type="bibr" rid="bib40">Kropff et al., 2015</xref>), supporting Tolman’s proposal that navigation is guided by internal cognitive maps and offering insight into how these maps are neurally implemented.</p><p>Spatial learning is rich with structure and thus well-suited for exploring the role of differentiation and integration in the formation of local and global knowledge across experiences. Indeed, a previous study in which participants viewed first-person trajectories to real-world destinations found that representations of routes with overlapping sections were differentiated in the hippocampus following learning (<xref ref-type="bibr" rid="bib10">Chanales et al., 2017</xref>). Another study observed differentiation of hippocampal patterns for landmarks that were common to multiple virtual cities (<xref ref-type="bibr" rid="bib103">Zheng et al., 2021</xref>). By contrast, other studies found evidence of integration for items that share spatial and temporal context (<xref ref-type="bibr" rid="bib18">Deuker et al., 2016</xref>) and for objects located in geometrically similar positions across subspaces of segmented environments (<xref ref-type="bibr" rid="bib64">Peer et al., 2021</xref>). More work is needed to characterize experience-driven changes in population-level neural representations of local and global spaces.</p><p>Towards this goal, we developed an experimental paradigm that leverages immersive virtual navigation and fMRI to investigate how neural representations of items encountered during goal-directed navigation evolve across learning. Specifically, pattern similarity analyses and linear mixed-effects models characterized experience-driven changes in EC and hippocampus following ‘local’ and ‘global’ navigation. We hypothesized that we would find evidence for both integration and differentiation emerging at the same time points across learning, as participants build local and global representations of the virtual environment. Moreover, we predicted that early evidence of global map learning during local navigation would depend on integration and predict participants’ ability to subsequently navigate across the environment.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Knowledge of local environments</title><p>Participants (n = 23) completed two behavioral navigation tasks interspersed between 3 consecutive days of fMRI (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Participants first completed a Local Navigation Task, in which they learned to navigate to five goal locations within each of three distinct local environments – that is, three oval tracks experienced in a virtual 3D environment from a first-person perspective (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). For each track, participants completed four learning runs and six test runs (10 trials per run). At the start of each run, participants were placed on the track and rotated to orient themselves. Each trial began with a fractal cue indicating the navigational goal, followed by the selection of a heading direction and navigation (see Methods; <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). We assessed goal-location learning by measuring the proportion of test trials on which participants correctly navigated to the cued location. Navigation was considered accurate if the participant landed within 8 arbitrary units of the goal (36% of the average distance between goals).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Study design.</title><p>(<bold>A</bold>) Overview of the 3-day experimental paradigm. (<bold>B</bold>) First-person view from an example training trial (left); virtual fog limited the distance viewed, ensuring that no two landmark buildings could be seen at any one time. Overhead view of the virtual environment (right). The environment consisted of three oval tracks. Colored boxes indicate the approximate locations of landmarks, circles approximate goal locations for an individual study participant. (<bold>C</bold>) fMRI paradigm. Participants viewed images of the 12 landmarks and 15 fractals that were used in their unique virtual environment, while performing a perceptual decision-making task. On each trial, the stimulus appeared on a gray background for 1.8 s, followed by a fixation cross for 5.4 s; participants were instructed to attend to the stimuli and to determine whether a feature of the stimulus was ‘bleached out’; on ‘catch’ trials (8% of trials), a red fixation cross appeared after image offset and participants indicated a response. (<bold>D</bold>) Schematic illustration of potential representational changes driven by learning. Following goal-directed navigation, the distance between landmarks in neural state-space could have increased (differentiation) or decreased (integration).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-fig1-v1.tif"/></fig><p>Across Day 1 test trials, participants navigated to the correct location on 96% of trials, with performance close to ceiling from the beginning of testing (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, top). The proportion of correct trials was significantly above chance (0.25, or 1 of 4 possible goals; mean ± SEM: 0.961 ± 0.009; t<sub>22</sub> = 82.831, one-sample t-test; p &lt; 2.2e<sup>–16</sup>, d = 17.27) and did not differ across Day 1 test runs (<italic>β</italic> = 0.003 ± 0.002; t = 1.701; p = 0.089). To assess whether knowledge of goal locations within local environments was retained overnight, participants completed two additional test runs (Runs 13–14; preceded by two runs of ‘top-up’ learning trials, Runs 11–12). Accuracy on Day 2 (mean ± SEM: 0.985 ± 0.004) did not differ from that on the final Day 1 test run (t<sub>22</sub> = 1.418; p = 0.17, paired t-test).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Navigation performance on the Local and Global Navigation Tasks.</title><p>(<bold>A</bold>) Local Navigation accuracy on test trials was near ceiling across runs on Day 1 and Day 2 (top). Participants’ navigational efficiency improved over learning trials, and they navigated efficiently across test trials (bottom). (<bold>B</bold>) During the Global Navigation Task, participants navigated more accurately (top) and efficiently (bottom) on within- vs. across-track trials. Participants were more accurate for within-track trials during learning and during early test runs, but improved on across-track trials over the course of test runs on Day 2 (top). Accuracy improved for both trial types on Day 3, such that performance did not significantly differ between within-track and across-track trials. Participants were significantly more efficient on both trial types on Day 3 relative to Day 2. (*** p &lt; 0.001, paired t-test. Error bars denote SEM. Local Navigation Task, n = 23; Global Navigation Task, n = 21. Learning trials = trials on which the fractal marking the goal location was visible on the track; Test trials = trials on which participants had to rely on memory for the goal location, as the fractals were removed from the track).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-fig2-v1.tif"/></fig><p>Participants were instructed to take the shortest path to a goal. To assess learning of spatial relationships between goals in local environments, we computed a path inefficiency metric that represents a participant’s path length relative to the length of the shortest possible path (see Methods). During Local Navigation (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom), path inefficiency improved across the four learning runs (effect of run, <italic>β</italic> = –8.99 ± 1.262; t = –7.125; p &lt; 1.34e<sup>–12</sup>; d = 1.49), was lower on the first test run relative to the first learning run (t<sub>22</sub> = 4.272; p = 0.0003, paired t-test; d = 0.89), and was consistently low across Day 1 test trials (mean ± SEM: 9.542 ± 1.773%; no effect of run, <italic>β</italic> = –0.178 ± 0.378; t = –0.471; p = 0.638). As with accuracy, path inefficiency did not differ between Day 2 (mean ± SEM: 6.405 ± 1.681%) and the final Day 1 test run (t<sub>22</sub> = –0.619; p = 0.542, paired t-test). Together, these data demonstrate that participants quickly acquired precise knowledge of goal locations on the three tracks and the relations between locations within a track, as evidenced by their ability to successfully navigate between goals using nearly the shortest possible path.</p></sec><sec id="s2-2"><title>Knowledge of the global environment</title><p>Next, 21 of 23 participants completed a Global Navigation Task where the separately learned tracks were connected and participants were required to navigate to goals both within and across tracks. While within-track accuracy on Day 2 was lower in the Global Task (mean ± SEM: 0.913 ± 0.031; t<sub>20</sub> = –3.396; p = 0.003, paired t-test; d = 0.74, <xref ref-type="fig" rid="fig2">Figure 2b</xref>, top) than the preceding Local Task (mean ± SEM: 0.985 ± 0.004, <xref ref-type="fig" rid="fig2">Figure 2a</xref>, top), performance on the Global Task was above chance for both within-track (mean ± SEM: 0.913 ± 0.031) and across-track (mean ± SEM: 0.862 ± 0.034) trials (chance = 0.07, or 1 of 14 possible goals; within-track trials: t<sub>20</sub> = 27.029, one-sample t-test; p &lt; 2.2e<sup>–16</sup>, d = 5.90; across-track trials: t<sub>20</sub> = 23.600, one-sample t-test; p &lt; 2.2e<sup>–16</sup>, d = 5.15). During Global Navigation, within-track accuracy was higher than across-track accuracy (t<sub>20</sub> = 4.073; p = 0.0006, paired t-test; d = 0.89). However, across-track accuracy showed a trend towards increasing across test runs (effect of run, <italic>β</italic> = 0.007 ± 0.004; t = 1.792; p = 0.073; no effect of run on within-track trials, <italic>β</italic> = –0.005 ± 0.004; t = –1.103; p = 0.27), such that accuracy did not differ between trial types on the final Day 2 test run (t<sub>20</sub> = –0.029; p = 0.977, paired t-test). Before fMRI on Day 3, participants completed four additional test runs of Global Navigation. Accuracy significantly improved overnight for both trial types (within-track mean ± SEM: 0.963 ± 0.013, t<sub>20</sub> = 2.425, p = 0.025, paired t-test, d = 0.53; across-track mean ± SEM: 0.929 ± 0.025, t<sub>20</sub> = 2.841, p = 0.01, paired t-test, d = 0.62), and did not differ between them on Day 3 (t<sub>20</sub> = 1.886, p = 0.074, paired t-test; d = 0.41).</p><p>During Day 2, navigation was less efficient on the Global Task (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom) than the preceding Local Task (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom). Efficient navigation on across-track trials required a global representation of the virtual environment. During Day 2 learning runs, path inefficiency tended to be greater on across-track (mean ± SEM: 59.028 ± 7.124%) than within-track trials (mean ± SEM: 31.825 ± 13.392%; t<sub>20</sub> = 1.947, paired t-test; p = 0.066). Similarly, on Day 2 test runs, navigation was significantly more inefficient on across-track (mean ± SEM: 37.86 ± 6.389%) vs. within-track trials (mean ± SEM: 23.479 ± 5.764%; t<sub>20</sub> = 3.215, paired t-test; p = 0.004; d = 0.70). However, performance on both trial types improved over the course of Day 2 test runs (within-track trials, effect of run: <italic>β</italic> = –2.27 ± 1.012; t = –2.244; p = 0.025; d = 0.49; across-track trials, effect of run: <italic>β</italic> = –2.045 ± 0.647; t = –3.163; p = 0.002; d = 0.69), such that efficiency did not significantly differ between them during the final Day 2 test run (t<sub>20</sub> = 1.480, paired t-test; p = 0.155). As with accuracy, efficiency tended to improve overnight for both trial types (Day 3 within-track trials mean ± SEM: 17.176 ± 4.222%, t<sub>20</sub> = –3.043, paired t-test, p = 0.006, d = 0.66; Day 3 across-track trials mean ± SEM: 24.717 ± 4.741%, t<sub>20</sub> = –2.006, paired t-test, p = 0.059).</p><p>We also visually examined participants’ routes on each across-track Global Navigation Task trial at the start (first four test runs) and end (last two test runs) of the task on Day 2, and at the end (last two test runs) of Day 3, noting whether participants (a) initially switched to the correct track on the trial and (b) navigated in the correct direction after switching. At the start of the Global Task on Day 2, participants switched to the correct track on 65.78% (SD = 21.35%) of across-track trials. Of those trials, participants navigated in the correct direction after switching 77.75% (SD = 22.52%) of the time. By the end of Day 2, those numbers increased to 75.09% (SD = 20.33%) and 85.64% (SD = 16.91%) respectively. Performance continued to improve on Day 3, with participants switching to the correct track on 78.36% (SD = 21.77%) of across-track trials during the last two runs of the Global Navigation Task and navigating in the correct direction 90.48% (SD = 10.63%) of the time after switching tracks.</p><p>Taken together, these data show that participants learned both the local environments (as evidenced by within-track navigation) and the global environment (as evidenced by across-track navigation), achieving high accuracy and efficiency on within- and across-track trials prior to the final scan on Day 3.</p></sec><sec id="s2-3"><title>Individual differences in global knowledge</title><p>Examination of performance at the participant level revealed striking individual differences in path inefficiency when navigating across tracks in the Global Navigation Task on Day 2 (range = 4.219–107.237%, Q1 = 17.914%, Q3 = 54.625%; Figure 6a). Some participants were highly efficient on across-track trials from the beginning, suggesting they had acquired much of the requisite global knowledge during performance of the preceding Local Task. By contrast, others became more efficient at across-track navigation over the course of the Global Task (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). This variance in when knowledge of the global environment was first evident allowed us to perform an individual differences analysis linking brain activity to behavior that depends on having built global environmental knowledge during Local Navigation (see <bold>Hippocampal representations predict later navigation performance</bold> and Figure 6).</p></sec><sec id="s2-4"><title>fMRI assays of learning-driven representational change</title><p>Our primary objectives were to characterize representational changes in the human memory network driven by local and global environmental learning. Participants underwent fMRI at three timepoints across the study—Pre-Learning, Post Local Navigation, and Post Global Navigation (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). During fMRI, participants viewed images of the landmark buildings and fractals used in their unique virtual environment, performing a low-level perceptual decision-making task. Of interest was how the neural patterns associated with each landmark and fractal changed as a function of learning (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). Accordingly, we extracted voxel-level estimates of neural activity for each stimulus from EC and hippocampus regions-of-interest (ROIs; see Methods) and used pattern similarity analysis to probe whether learning resulted in representational differentiation or integration as a function of (a) the experienced relations between stimuli in the environment (e.g. same vs. different track, see <bold>Hippocampus and entorhinal cortex learn to separate the three tracks</bold>; distance within and across tracks, see <bold>The hippocampus represents both local and global distance</bold>) and (b) behavioral differences in when knowledge of the global environment was evident (i.e., evidence of good vs. poor across-track navigation at the outset of the Global Navigation Task). To test neural hypotheses, we fit linear mixed-effects models against pattern similarity, and included fixed effects of interest, nuisance regressors corresponding to the average univariate activation for each stimulus as well as estimates of perceptual similarity, and a standard set of random effects. Complete details of modeling procedures can be found in the Methods. In the following sections, we report results from both planned and exploratory analyses. For planned analyses, we interpret <italic>a priori</italic> effects when significant at p &lt; 0.05 uncorrected, but for completeness we note whether all reported effects survive FDR correction (see Methods). Given theoretical interest in ventromedial prefrontal cortex (vmPFC) and the representation of structured knowledge, we report exploratory analyses on data from this region in the Appendix.</p></sec><sec id="s2-5"><title>Hippocampus and entorhinal cortex learn to separate the three tracks</title><p>EC and hippocampus encode a spatial map that may underlie the formation of a cognitive map (<xref ref-type="bibr" rid="bib59">O’Keefe and Nadel, 1978</xref>; <xref ref-type="bibr" rid="bib58">O’Keefe and Dostrovsky, 1971</xref>; <xref ref-type="bibr" rid="bib54">McNaughton et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Hafting et al., 2005</xref>). To examine how these regions build structured knowledge, we first asked whether they come to represent the three tracks following learning. We hypothesized that there would be a change in hippocampal and entorhinal pattern similarity for items located on the same track vs. items located on different tracks. An increase in pattern similarity would suggest that within-track item representations are integrated, while a decrease would suggest these representations are differentiated following learning. To test this hypothesis, we ran models predicting pattern similarity within each region, with scan (Pre-Learning/Day 1, Post Local Navigation/Day 2, and Post Global Navigation/Day 3), stimulus type (landmarks and fractals), and context (same track and different tracks) as predictors. We excluded shared landmarks from these models as they are common to multiple tracks, and restricted pattern similarity comparisons to be within stimulus-type (i.e., landmarks to other landmarks, fractals to other fractals).</p><p>In the hippocampus, an initial model that included hemisphere revealed a main effect of hemisphere (<italic>β</italic> = 0.006 ± 0.002; t = 2.756; p = 0.006, survived FDR correction; d = 0.60; <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>), and significant interactions between hemisphere and scan (Day 2 &gt; Day 1 × hemisphere, <italic>β</italic> = –0.011 ± 0.003, t = –3.477, p = 0.0006, survived FDR correction; d = 0.76; Day 3 &gt; Day 1 × hemisphere, <italic>β</italic> = 0.012 ± 0.003, t = 3.795, p = 0.0001, survived FDR correction; d = 0.83). The model also demonstrated a main effect of stimulus type (<italic>β</italic> = 0.007 ± 0.003; t = 2.422; p = 0.015, did not survive FDR correction; d = 0.53) and an interaction between stimulus type and scan (Day 3 &gt; Day 1 × stimulus type, <italic>β</italic> = –0.011 ± 0.004; t = –2.604; p = 0.009, survived FDR correction; d = 0.57). The latter interaction reflected a difference in similarity between landmarks and fractals Pre-Learning (Day 1 fractals &gt; Day 1 landmarks, <italic>β</italic> = –0.009 ± 0.002; z-ratio = –4.269; p = 0.0003 adjusted) which was no longer present following Global Navigation (Day 3 fractals &gt; Day 3 landmarks, <italic>β</italic> = 0.003 ± 0.002; z-ratio = 1.248; p = 0.813 adjusted).</p><p>Given these interactions, we ran models for each hemisphere and stimulus type separately. As <italic>a priori</italic> predicted, we found evidence in right hippocampus that learning differentiates landmarks located on the same track (Day 3 &gt; Day 1 × context, <italic>β</italic> = –0.013 ± 0.007; t = –2.014; p = 0.044, did not survive FDR correction; d = 0.44; <xref ref-type="fig" rid="fig3">Figure 3a</xref>; <xref ref-type="table" rid="app1table2 app1table3">Appendix 1—Tables 2–3</xref>). In right hippocampus, Pre-Learning pattern similarity was comparable for within- vs. across-track landmarks. Post Global Navigation, patterns for landmarks on the same track were less similar (differentiated) than those for landmarks on different tracks. No significant interactions between scan and context were observed for fractals (<xref ref-type="fig" rid="fig3">Figure 3b</xref>; <xref ref-type="table" rid="app1table4 app1table5">Appendix 1—Tables 4–5</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Context representations in hippocampus and entorhinal cortex (EC).</title><p>(<bold>A</bold>) Contrast estimates for models predicting landmark similarity in left and right hippocampus. Right hippocampus differentiates landmarks located on the same track following Global Navigation, such that those experienced within the same track became less similar. A similar pattern of findings was observed in left hippocampus but did not reach statistical significance. (<bold>B</bold>) Contrast estimates for models predicting fractal similarity in left and right hippocampus. Interactions between scan session and context were not significant. (<bold>C</bold>) Contrast estimates for models in left and right EC that include both landmark and fractal stimuli. Left EC differentiates items located on the same track following Local Navigation, such that items experienced within the same track became less similar. Following Global Navigation, pattern similarity remained lower for within-track items, but the interaction between context and scan session did not reach statistical significance. A similar pattern of findings was observed in right EC, but interactions between context and scan session did not reach statistical significance (a <italic>priori</italic> predicted effects: * p &lt; 0.05, uncorrected. Error bars denote SE of the estimates. Hippocampus: Day 2 &gt; Day 1, n = 23; Day 3 &gt; Day 1, n = 21; Left EC: n = 20; Right EC: n = 18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-fig3-v1.tif"/></fig><p>Turning to EC, an initial model with left and right EC revealed a main effect of hemisphere (<italic>β</italic> = –0.022 ± 0.004; t = –6.06; p &lt; 1.38e<sup>–9</sup>, survived FDR correction; d = 1.43; <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>), and interactions between hemisphere and scan (Day 2 &gt; Day 1 × hemisphere, <italic>β</italic> = 0.022 ± 0.005; t = 4.192; p &lt; 2.78e<sup>–5</sup>, survived FDR correction; d = 0.99; Day 3 &gt; Day 1 × hemisphere, <italic>β</italic> = 0.056 ± 0.005; t = 10.552; p &lt; 2e<sup>–16</sup>, survived FDR correction; d = 2.49). This model also revealed an interaction between context and scan (Day 2 &gt; Day 1 × context, <italic>β</italic> = –0.014 ± 0.006; t = –2.05; p = 0.041, did not survive FDR correction; d = 0.48), such that there was no difference in similarity between within-track and across-track items Pre-Learning (Day 1 same track &gt; Day 1 different track, <italic>β</italic> = 0.004 ± 0.004; z-ratio = 1.162; p = 0.855 adjusted), nor were within-track items less similar to each other (i.e. differentiated) following Local Navigation (Day 2 same track &gt; Day 2 different track, <italic>β</italic> = –0.009 ± 0.004; z-ratio = –2.447; p = 0.140 adjusted). No main effect or interactions were observed with stimulus type.</p><p>Given the interaction with hemisphere, we ran separate models to examine left and right EC representations of the three tracks as a function of learning. As predicted, we observed an interaction between context and scan in left EC (Day 2 &gt; Day 1 × context, <italic>β</italic> = –0.014 ± 0.006; t = –2.339; p = 0.019, did not survive FDR correction; d = 0.52; <xref ref-type="fig" rid="fig3">Figure 3c</xref>; <xref ref-type="table" rid="app1table7">Appendix 1—table 7</xref>), such that similarity for same-track vs. across-track items did not differ Pre-Learning, but same-track items were less similar (differentiated) following Local Navigation. Following Global Navigation, pattern similarity remained lower for within-track items, but the interaction between context and scan did not reach statistical significance (Day 3 &gt; Day 1 × region, <italic>β</italic> = –0.009 ± 0.006; t = –1.457; p = 0.145). A similar pattern of findings was observed in right EC, but did not reach statistical significance (<xref ref-type="fig" rid="fig3">Figure 3c</xref>; <xref ref-type="table" rid="app1table8">Appendix 1—table 8</xref>).</p><p>To determine whether the effects of learning on representations of the three tracks differed between EC and hippocampus, we included region in a complete model, testing whether pattern similarity in EC for within- vs. across-track items differed from that in the hippocampus. We found a main effect of region (<italic>β</italic> = –0.015 ± 0.002; t = –7.36; p &lt; 1.86e<sup>–13</sup>, survived FDR correction; d = 1.74; <xref ref-type="table" rid="app1table9">Appendix 1—table 9</xref>), as well as interactions between region and scan (Day 2 &gt; Day 1 × region, <italic>β</italic> = –0.017 ± 0.003; t = –5.757; p &lt; 8.59e<sup>–9</sup>, survived FDR correction; d = 1.36; Day 3 &gt; Day 1 × region, <italic>β</italic> = 0.014 ± 0.003; t = 4.592; p &lt; 4.41e<sup>–6</sup>, survived FDR correction; d = 1.08). Importantly, there was no interaction between region, scan, and context (<xref ref-type="table" rid="app1table9">Appendix 1—table 9</xref>), which suggests statistical support for a functional differentiation between these two regions is absent.</p></sec><sec id="s2-6"><title>The hippocampus represents both local and global distance</title><p>In studies of episodic memory and spatial navigation, the hippocampus is thought to play a crucial role in disambiguating memories for overlapping events (<xref ref-type="bibr" rid="bib98">Yassa and Stark, 2011</xref>; <xref ref-type="bibr" rid="bib48">LaRocque et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">Chanales et al., 2017</xref>). Moreover, prior work indicates that spatial distance is reflected in both hippocampal pattern similarity (<xref ref-type="bibr" rid="bib18">Deuker et al., 2016</xref>) and univariate BOLD activity (<xref ref-type="bibr" rid="bib57">Nyberg et al., 2022</xref>; <xref ref-type="bibr" rid="bib33">Howard et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Patai et al., 2019</xref>). We hypothesized that for participants to navigate successfully, they would need to form distinct representations of local environmental landmarks used to aid navigation. We tested whether learning of local environments led to differentiated hippocampal activity patterns by examining pattern similarity for landmarks located on the same track that were nearest neighbors (link distance 1) vs. slightly further away (link distance 2; <xref ref-type="fig" rid="fig4">Figure 4a</xref>). Here, we predicted pattern similarity would be lower for nearest neighbors following Local and Global Navigation. We ran a model predicting pattern similarity, with scan session and link distance as predictors.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Hippocampal pattern similarity reflects distance in local environments.</title><p>(<bold>A</bold>) Examples of landmarks at link distances 1 and 2 on the same track. (<bold>B</bold>) Hippocampal pattern similarity for within-track landmarks Pre-Learning (left), after the Local Navigation Task (center), and after the Global Navigation Task (right). Interactions between distance and scan session were significant from Pre-Learning to Post Local and Global Navigation. (<bold>C–D</bold>) Pattern similarity for within-track landmarks in the left (<bold>C</bold>) and right (<bold>D</bold>) EC. Interactions between distance and scan session were not significant. (a <italic>priori</italic> predicted effects: ** p &lt; 0.01, * p &lt; 0.05, uncorrected. Error bars denote SE of the estimates. Hippocampus: Day 2 &gt; Day 1, n = 23; Day 3 &gt; Day 1, n = 21; Left EC: n = 20; Right EC: n = 18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-fig4-v1.tif"/></fig><p>An initial model including hemisphere revealed no main effect or interactions with hemisphere (<xref ref-type="table" rid="app1table10">Appendix 1—table 10</xref>). As <italic>a priori</italic> predicted, we found significant interactions between distance and scan (Day 2 &gt; Day 1 × distance, <italic>β</italic> = 0.04 ± 0.015; t = 2.70; p = 0.007, fell near FDR-corrected threshold (p = 0.005), d = 0.56; Day 3 &gt; Day 1 × distance, <italic>β</italic> = 0.034 ± 0.015; t = 2.259; p = 0.024; did not survive FDR correction; d = 0.49; <xref ref-type="table" rid="app1table11">Appendix 1—table 11</xref>), such that following the Local and Global Tasks, hippocampal patterns were less similar (differentiated) for nearby landmarks vs. landmarks located further apart (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). There were no significant interactions between distance and scan when a similar model was run for fractal stimuli (<xref ref-type="table" rid="app1table12">Appendix 1—table 12</xref>).</p><p>In addition to encoding local (within-track) spatial representations, navigation may result in the acquisition of a global map of the virtual environment. Although participants performed the Local Task separately on each track, it is possible that the landmarks shared across tracks trigger integration of the tracks, contributing to the encoding of (at least some dimensions of) a global environmental map. We were particularly interested in whether there would be evidence of global knowledge of the environment (i.e., knowledge of relations that span tracks) after the Local Task, prior to any experience navigating across the connected tracks.</p><p>To this end, we examined pattern similarity for landmarks located on different tracks at increasing distances, predicting it would scale with distance such that pattern similarity would be greatest for proximal vs. distal landmarks (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). To test our prediction, we ran a model predicting pattern similarity between landmarks on different tracks, with scan (Pre-Learning/Day 1, Post Local Navigation/Day 2, and Post Global Navigation/Day 3) and link distance (2, 3, and 4) as predictors. We excluded shared landmarks from this model as they are common to multiple tracks; however, the results do not differ if these landmarks are included in the analysis. An initial model that included hemisphere revealed no main effect or interactions with hemisphere (<xref ref-type="table" rid="app1table13">Appendix 1—table 13</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Distance representations in the Global environment.</title><p>(<bold>A</bold>) Examples of landmarks at different link distances on different tracks. (<bold>B</bold>) Hippocampal pattern similarity for landmarks on different tracks Pre-Learning (left), after the Local Navigation Task (center), and after the Global Navigation Task (right). The interaction between distance and scan session was significant from Pre-Learning to Post Local Navigation, but not to Post Global Navigation. (<bold>C</bold>) A similar pattern of findings was observed in Entorhinal Cortex, but the interaction between distance and scan session was not significant from Pre-Learning to Post Local Navigation (a <italic>priori</italic> predicted effects: ** p &lt; 0.01, uncorrected. Error bars denote SE of the estimates. Hippocampus: Day 2 &gt; Day 1, n = 23; Day 3 &gt; Day 1, n = 21; EC: n = 18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-fig5-v1.tif"/></fig><p>Consistent with our prediction, we found an interaction between link distance and scan following the Local Task (Day 2 &gt; Day 1 × distance, <italic>β</italic> = –0.02 ± 0.007; t = –2.891; p = 0.004, survived FDR correction; d = 0.60), but not the Global Task (Day 3 &gt; Day 1 × distance, <italic>β</italic> = –0.003 ± 0.007; t = –0.413; p = 0.679; <xref ref-type="fig" rid="fig5">Figure 5b</xref>; <xref ref-type="table" rid="app1table14">Appendix 1—table 14</xref>). In contrast to the distance-related differentiation observed between spatial locations within a track, this interaction reflected that hippocampal pattern similarity for locations across tracks did not vary as a function of distance Pre-Learning, but was higher for closer (vs. farther) locations across tracks Post Local Navigation. This finding suggests that some global map knowledge is evident in the hippocampus even though participants had only engaged in within-track navigation. Following Global Navigation and in contrast to our predictions, similarity was comparable at all across-track distances. There were no significant interactions between distance and scan when a similar model was run for fractal stimuli (<xref ref-type="table" rid="app1table15">Appendix 1—table 15</xref>).</p><p>We tested for within- and across-track distance effects within EC as well, given the extensive literature characterizing spatial coding in the region. As with the hippocampus, we tested whether pattern similarity for landmark buildings in EC scaled with local (within-track) and global (across-track) distance. An initial model of local distance that included hemisphere (left and right EC) and link distance (1 and 2) revealed interactions between scan session and hemisphere (Day 2 &gt; Day 1 × hemisphere, <italic>β</italic> = 0.050 ± 0.023, t = 2.093, p = 0.036, did not survive FDR correction; d = 0.47; Day 3 &gt; Day 1 × hemisphere, <italic>β</italic> = 0.069 ± 0.024, t = 2.835, p = 0.005, survived FDR correction; d = 0.67; <xref ref-type="table" rid="app1table16">Appendix 1—table 16</xref>). However, in contrast to hippocampus, we found no interactions between distance and scan session (<xref ref-type="fig" rid="fig4">Figure 4c–d</xref>, <xref ref-type="table" rid="app1table17 app1table18">Appendix 1—Tables 17–18</xref>) when we fit models to data from each hemisphere individually. An initial model of global distance that included hemisphere (left and right EC) and link distance (2, 3, and 4) revealed no main effect of hemisphere or interactions with hemisphere (<xref ref-type="table" rid="app1table17">Appendix 1—table 17</xref>). Unlike the hippocampus, we found no interactions between distance and scan session in the global model (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, <xref ref-type="table" rid="app1table20">Appendix 1—table 20</xref>). No significant interactions between distance and scan were observed when similar models were run for fractal stimuli (<xref ref-type="table" rid="app1table21 app1table22 app1table23">Appendix 1—Tables 21–23</xref>). Our findings in EC were not surprising, as the extent to which EC itself can support structured representations is unclear. Spatial properties of EC neurons are known to be important for path integration and the building of structured knowledge in the hippocampus and neocortex (<xref ref-type="bibr" rid="bib54">McNaughton et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Hafting et al., 2005</xref>). Yet, hippocampal conjunctive coding and interactions between the hippocampus, EC, and neocortex are necessary for cross event-generalization and retrieval-mediated learning (<xref ref-type="bibr" rid="bib101">Zeithamova et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Kumaran and McClelland, 2012</xref>; <xref ref-type="bibr" rid="bib44">Kumaran et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Schapiro et al., 2017</xref>; <xref ref-type="bibr" rid="bib66">Preston and Eichenbaum, 2013</xref>).</p></sec><sec id="s2-7"><title>Hippocampal representations predict later navigation performance</title><p>The observed negative relationship between hippocampal pattern similarity and the distance between across-track landmarks that emerged after Local Navigation (<xref ref-type="fig" rid="fig5">Figure 5b</xref>), but before participants experienced the connected tracks, was quite variable across participants.</p><p>The variability in the observed across-track hippocampal distance effect may reflect that some participants encoded global map knowledge during Local Navigation, whereas others did not (or did so less fully; <xref ref-type="fig" rid="fig6">Figure 6a</xref>). To the extent that this is the case, this would predict that the distance-related hippocampal pattern similarity effect Post Local Navigation should relate to navigational efficiency at the outset of performing the Global Task. Specifically, we predicted that more efficient navigators would have a negative distance function, such that pattern similarity would be greatest for the most proximal across-track landmarks and decrease with distance. To test this hypothesis, we first ran a mixed-effects model predicting neural pattern similarity Post Local Navigation (Day 2), with path inefficiency (median path inefficiency for across-track trials in the first four test runs of Global Navigation on Day 2) and link distance as predictors. Indeed, we observed a significant interaction between path inefficiency and link distance (<italic>β</italic> = –0.001 ± 0.001; t = –1.983; p = 0.048, did not survive FDR correction; d = 0.43; <xref ref-type="table" rid="app1table24">Appendix 1—table 24</xref>), but the direction of the effect was unexpected. Participants who did well from the beginning of the Global Task showed no effect of distance in hippocampal pattern similarity, whereas less efficient navigators showed a negative slope (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). A similar interaction between path inefficiency and link distance was not observed when the model was fit to data from Day 1 (<italic>β</italic> = 0.001 ± 0.001; t = 1.523; p = 0.128; <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3a</xref>; <xref ref-type="table" rid="app1table25">Appendix 1—table 25</xref>) or to data from Day 3 (<italic>β</italic> = 0.001 ± 0.001; t = 1.379; p = 0.168; <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3b</xref>; <xref ref-type="table" rid="app1table26">Appendix 1—table 26</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Path inefficiency on the Global Navigation Task varies with across-track distance-related hippocampal pattern similarity after the Local Navigation Task.</title><p>To qualitatively visualize the relationship between pattern similarity, link distance, and path inefficiency, we split participants into two groups – More Efficient and Less Efficient – based on their median path inefficiency on across-track trials in the first four test runs of the Global Task on Day 2. (<bold>A</bold>) Path inefficiency (%) for each across-track trial during the first four test runs of the Global Navigation Task, plotted for each participant and colored by performance group. (<bold>B</bold>) We used a linear mixed-effects model to formally test this relationship (see main text for details). The linear model revealed a significant interaction between path inefficiency and link distance, with the direction of the effect being unexpected. To qualitatively depict this effect, we plot hippocampal pattern similarity for landmarks on different tracks prior to the Global Navigation Task for the Less Efficient and More Efficient median-split data. Data are split by participants’ subsequent navigation performance as shown in (<bold>A</bold>). (Error bars denote SE of the estimates. More Efficient, n = 11; Less Efficient, n = 10).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-fig6-v1.tif"/></fig><p>Next, we examined single-trial navigation data in relation to pairwise neural similarity in the hippocampus (for each given pair of landmarks and fractals), using mixed-effects models to predict path inefficiency for each trial across the first four test runs of the Global Navigation Task (Day 2). The models included (a) neural similarity (Post Local Navigation) for a given pair of fractals or the nearby landmarks and (b) length of the optimal path for each trial as predictors, and a regressor indicating whether the trial was a within-track or across-track trial. Models were run for landmarks and fractals separately. Here, we observed trend-level evidence that hippocampal pattern similarity (Post Local Navigation) for landmark pairs predicted trial-level subsequent Global Navigation performance (<italic>β</italic> = –41.245 ± 24.162; t = –1.707; p = 0.088; <xref ref-type="table" rid="app1table27">Appendix 1—table 27</xref>), such that greater hippocampal pattern similarity was predictive of a more efficient path (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). The length of the optimal path (<italic>β</italic> = –0.663 ± 0.183; t = –3.631; p = 0.0003; d = 0.79) and trial type (within-track vs. across-track; <italic>β</italic> = 20.551 ± 6.245; t = 3.291; p = 0.001; d = 0.71) significantly predicted navigation performance in the landmark model. There was no interaction between hippocampal pattern similarity for landmark pairs and trial type (<italic>β</italic> = 54.618 ± 43.489; t = 1.256; p = 0.210).</p><p>The length of the optimal path (<italic>β</italic> = –0.646 ± 0.184; t = –3.519; p = 0.0005; d = 0.77) and trial type (within-track vs. across-track; <italic>β</italic> = 27.289 ± 6.196; t = 4.404; p &lt; 1.24e<sup>–5</sup>, survived FDR correction; d = 0.96; <xref ref-type="table" rid="app1table28">Appendix 1—table 28</xref>) were also significant predictors of navigation performance in the model relating pattern similarity for fractal pairs to subsequent performance on the Global Task. However, hippocampal pattern similarity for fractal pairs did not significantly predict subsequent performance (<italic>β</italic> = 3.026 ± 27.687; t = 0.109; p = 0.913).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The episodic memory network builds structured knowledge across experiences to form internal models of the world that enable planning, decision-making, and goal-directed behavior. In this study, we investigated how humans build structured knowledge in immersive, goal-directed tasks. Results revealed that (a) learning restructures representations in the hippocampus and EC, reflecting the structure of the virtual environment; (b) the hippocampus begins to build a representational structure extending beyond directly experienced transitions; and (c) changes in the similarity structure of hippocampal representations relate to subsequent navigation performance.</p><p>We first characterized learning on two behavioral navigation tasks. In the Local Navigation Task, participants quickly learned goal locations along individual tracks and navigated between goals using the most efficient paths (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Prior to the Global Navigation Task, participants were unaware that the tracks would be connected and that they would be required to navigate across tracks to reach cued goals. Nonetheless, some were able to navigate efficiently from the outset of Global Navigation, suggesting they had already formed a global representation of the environment; by contrast, others required extensive experience to do so (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). Our findings are consistent with behavioral research examining individual differences in route integration and cognitive map formation. Prior work characterized navigators into groups of ‘integrators’, ‘nonintegrators’, and ‘imprecise navigators’, suggesting that while some individuals build accurate internal maps, others may rely on fragmented maps or route-based strategies during navigation (<xref ref-type="bibr" rid="bib90">Weisberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib91">Weisberg and Newcombe, 2016</xref>; <xref ref-type="bibr" rid="bib92">Weisberg and Newcombe, 2018</xref>). Importantly, individual differences in navigation inefficiency enabled us to probe variability in neural representations and relate neural pattern similarity to behavior. Our work provides an important extension of prior studies in that it identifies potential neural substrates of individual differences in cognitive map formation.</p><p>At the neural level, we investigated whether mnemonic regions come to represent the three local tracks following navigation by comparing pattern similarity for items located on the same track vs. different tracks. Motivated by recent studies finding lower hippocampal pattern similarity for events with overlapping vs. distinct features (<xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">LaRocque et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">Chanales et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Hulbert and Norman, 2015</xref>), we expected to observe changes in hippocampal pattern similarity for within-track items following local learning. Indeed, we found evidence for differentiation in EC: items on the same track elicited less similar activity patterns when compared to patterns for items on different tracks following local learning (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). We observed a quantitatively similar pattern of effects for landmarks in the hippocampus (<xref ref-type="fig" rid="fig3">Figure 3a</xref>).</p><p>Several factors may explain why we did not find stronger hippocampal differentiation effects. The demands of our navigation tasks differ substantially from those of prior tasks eliciting strong differentiation effects (<xref ref-type="bibr" rid="bib75">Schlichting et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Kyle et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Favila et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">Chanales et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Brown et al., 2010</xref>). In prior studies, participants explicitly learned overlapping associations that had to be distinguished at a later test; for instance, the same item was paired with two different associates, or similar items or routes led to distinct outcomes. In the present study, overlapping features were incidental to task demand; thus individuals may have adopted different navigational strategies with varying consequences for within-track item pattern similarity (for instance, some participants may have learned the sequence of fractals for each track, while others may have formed associations between fractals and nearby landmarks). Tracks had a number of features in common, including shared visual features, a similar spatial layout, and common landmarks. Further, our findings from across-track analyses examining individual differences in navigation suggest that some participants began to build an integrated, global representation of the environment prior to the Global Task, with associated effects in the hippocampus. Given these dynamics, additional variance inherent in our paradigm may explain the nature of the observed effects in hippocampus and EC.</p><p>While we did not observe particularly strong differentiation of hippocampal patterns for all items located on the same track, we hypothesized that participants would need to form distinct representations of local environmental landmarks to perform well. Accordingly, we examined local spatial representations by comparing the similarity between nearest neighboring landmarks to second nearest neighbors on the same track, and found that hippocampal patterns for nearest neighboring landmarks were differentiated following both Local and Global Navigation (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). This finding is consistent with the idea that mnemonic representations – and in particular, representations of large-scale spaces – are hierarchical (<xref ref-type="bibr" rid="bib53">McKenzie et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Milivojevic and Doeller, 2013</xref>; <xref ref-type="bibr" rid="bib13">Chrastil and Warren, 2014</xref>). That is, we found differentiated representations of local space, whereas representations of track context or of the spatial relationships between the tracks were perhaps more coarse.</p><p>We next investigated global representations of map knowledge within the virtual environment. Here, we examined hippocampal pattern similarity for landmarks encountered on different tracks at increasing distances in the virtual environment, expecting to see the emergence of a relationship between hippocampal pattern similarity and distance. We were particularly interested in whether this relationship would be observed after the Local Task, which would suggest that participants were starting to build a global map of the environment prior to direct experience navigating the connected tracks.</p><p>Our results provide support for this prediction – we found a significant interaction between distance and scan (Pre-Learning vs. Post Local Navigation; <xref ref-type="fig" rid="fig5">Figure 5b</xref>, center). Notably, we also observed significant variability in the slope of this function across individuals (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). We hypothesized that this variability in the neural data might relate to the behavioral variability we observed on the subsequent Global Navigation Task, and explored this relationship in two ways. First, we ran a mixed effects model predicting hippocampal pattern similarity Post Local Navigation, with across-track path inefficiency at the start of Global Navigation and link distance as predictors. Here, we observed a significant interaction between path inefficiency and link distance, but the effect was not in the direction we expected. Our results indicate that the interaction between distance and scan (the negative slope in the center panel of <xref ref-type="fig" rid="fig5">Figure 5b</xref>) was driven by participants who were <italic>less</italic> efficient at the outset of Global Navigation, whereas we predicted a negative slope would be more apparent in highly efficient navigators. We initially hypothesized a negative distance-related similarity function would reflect global map learning, and thus expected to observe such a function in efficient navigators at the start of the Global Task on Day 2 and across all participants on Day 3. Such a function was absent following Global Navigation (the right panel of <xref ref-type="fig" rid="fig5">Figure 5b</xref>).</p><p>Why do we observe a flat distance-related similarity function after global map learning? It is possible that individual differences in navigational strategy or the particular learning processes utilized by efficient navigators in the present task organized the map in such a way. It is also possible our findings reflect a change in event boundaries. Evidence from a study examining temporal context found that hippocampal pattern similarity did not differ between items located nearby vs. further apart within a temporal event (<xref ref-type="bibr" rid="bib24">Ezzyat and Davachi, 2014</xref>). Hippocampal pattern similarity differed, however, between items that crossed an event boundary, such that nearby items had increased pattern similarity compared to items located further apart in time. While this study examined temporal events, spatial event boundaries may function similarly in that during retrieval, representations of other within-event items are reinstated. The flat slope observed on Day 3 could thus be a signature of an integrated map.</p><p>Our second approach to relating variability in the neural data to behavioral variability used a model to predict path inefficiency for each trial across the first four test runs of the Global Navigation Task (Day 2). While the present study was not designed to optimize power to detect trial-level effects, here we found trend-level evidence that hippocampal pattern similarity (Post Local Navigation) for landmark pairs predicted trial-level subsequent Global Navigation performance (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>), providing additional suggestive evidence for a relationship between hippocampal pattern similarity and subsequent behavior.</p><p>Replay and recurrence within the hippocampal network is thought to enable the hippocampus to learn relationships between items that have never been directly experienced together (<xref ref-type="bibr" rid="bib101">Zeithamova et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Kumaran and McClelland, 2012</xref>; <xref ref-type="bibr" rid="bib44">Kumaran et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Gupta et al., 2010</xref>; <xref ref-type="bibr" rid="bib97">Wu and Foster, 2014</xref>). The present study provides complementary evidence that the hippocampus begins to build a cognitive map extending beyond directly experienced transitions in a goal-directed task. Overlapping features within the virtual environment (the landmarks shared between tracks) provide the hippocampus with the links it needs to begin building a global map, despite participants not yet experiencing the connected environment. Crucially, not all participants demonstrated integration early in the Global Task, suggesting that cognitive map formation may not be a ‘default’ that incidentally emerges during performance of immersive, goal-directed local navigation. An open question is: How does this variability in whether and how individuals build global knowledge of the environment during local navigation relate to individual differences in spatial and mnemonic processing, in navigational strategy, and/or in other cognitive and contextual factors (<xref ref-type="bibr" rid="bib91">Weisberg and Newcombe, 2016</xref>; <xref ref-type="bibr" rid="bib92">Weisberg and Newcombe, 2018</xref>; <xref ref-type="bibr" rid="bib96">Wolbers and Hegarty, 2010</xref>; <xref ref-type="bibr" rid="bib9">Brunec et al., 2022</xref>)? The current study was limited in its investigation of individual differences due to sample size. Future work is needed to characterize the differences between performance groups and examine how representations of individual tracks and particular spatial positions within tracks covary with performance on subsequent global navigation. Moreover, some of our findings do not survive correction for multiple comparisons, and thus should be interpreted with caution.</p><p>The present findings appear to diverge from work finding hippocampal pattern similarity scaled with perceived spatiotemporal distance in a virtual environment, such that patterns for nearby items were integrated (<xref ref-type="bibr" rid="bib18">Deuker et al., 2016</xref>). In that study, participants were scanned before and after a task with a duration of ~80 min; thus, the findings may reflect hippocampal representations at an earlier stage of learning. In the present paradigm, fMRI took place 21.5–31 hr after extensive navigational practice, raising the possibility that hippocampal differentiation requires extensive learning, potentially including learning that takes place via replay and consolidation processes occurring during sleep. Future studies are necessary to pinpoint when integration occurs across the broader global map.</p><p>While we expected similar results for both fractal and landmark stimuli throughout our analyses, the null findings we observed across ROIs when local and global distance models were run with fractal stimuli were not completely surprising. Fractal and landmark stimuli differ in several key ways, which we believe explain the observed pattern of findings. For example, fractals are only visible in the environment for a minority of trials. During the majority of navigation, participants must rely on the landmark buildings to guide them. Fractals serve as pointers to the goal location to which a participant must navigate on a particular trial, but fractal information may not be used during route planning. Further, fractals are not necessary for participants to learn the layout of the environment; local and global maps can be built from landmarks alone.</p><p>Mnemonic integration depends in part on retrieval and interactions between the hippocampus and vmPFC (<xref ref-type="bibr" rid="bib103">Zheng et al., 2021</xref>; <xref ref-type="bibr" rid="bib66">Preston and Eichenbaum, 2013</xref>; <xref ref-type="bibr" rid="bib41">Kuhl et al., 2012</xref>; <xref ref-type="bibr" rid="bib100">Zeithamova and Preston, 2010</xref>). Our <italic>a priori</italic> analyses focused on the hippocampus and EC, but we conducted exploratory analyses in to determine whether and how vmPFC would come to represent the virtual environment, given its proposed role as a storage site for integrated memory representations (<xref ref-type="bibr" rid="bib28">Frankland and Bontempi, 2005</xref>; <xref ref-type="bibr" rid="bib79">Takashima et al., 2006</xref>; <xref ref-type="bibr" rid="bib80">Takehara-Nishiuchi and McNaughton, 2008</xref>; see Appendix). In the present work, we did not observe robust effects in vmPFC, but it remains possible that this region may represent features of the environment that were not explicit to our modeling approach.</p><p>An emergent body of work is advancing understanding of fundamental mechanisms in the medial temporal lobe and connected cortical structures that build knowledge across events. Part of this work documents pattern separation and pattern differentiation in the hippocampus, which serves to minimize confusion between related memories. At the same time, other theoretical and empirical findings are elucidating the role of these structures in mnemonic integration and cross-event generalization. It has been unclear whether and how these two seemingly opposing mechanisms—differentiation and integration—work together to facilitate the emergence of structured knowledge. Here, we provide complementary evidence that both differentiation and integration occur concurrently within the hippocampus and can emerge at the same points in learning. Our data suggest that hippocampal differentiation provides a structure necessary for participants to distinguish between nearby locations, while integration processes serve to build a global map of the environment. Moreover, integration relates to individual differences in the ability to efficiently navigate between locations in the global map that have not been previously traversed. Future research promises to reveal whether individual differences in global knowledge building are strategic, relate to differences in other neural systems, and/or are differentially sensitive to dysfunction or disease.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Thirty-three participants gave informed written consent, in accordance with procedures approved by the Stanford University Institutional Review Board. Nine dropped out before completing the 3-day study (two felt motion sick, two experienced unrelated illness, and five did not want to continue past Day 1), and data from one participant were excluded for failing to respond during the fMRI task. The final sample consisted of 23 right-handed participants (mean age = 22.91 years, SD = 5.03, range 18–35; 14 females) with normal or corrected-to-normal vision and no self-reported history of psychiatric or neurological disorders. Day 3 data were excluded for two of these participants (one due to scanner-related issues that prevented data acquisition, and one who reported falling asleep during scanning). Thus, Global Navigation Task/Day 3 analyses were conducted with 21 participants. Participants received compensation for their participation ($20/hr).</p></sec><sec id="s4-2"><title>Paradigm overview</title><p>Participants completed an experimental paradigm where they learned to navigate to goal locations within a virtual environment. The 3-day study included two behavioral navigation tasks—the Local Navigation Task and Global Navigation Task—and three fMRI scan sessions (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) that assayed changes in hippocampal and neocortical representations across learning. The virtual environment contained three oval-shaped tracks of distinct texture and color (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Individual tracks contained five goals, initially marked by unique fractal images. Tracks also contained unique landmark buildings that participants could use to guide their navigation, as there were no distal cues in the virtual environment. Landmarks were located near (but not at) goals, with the distance between them varying for each location and participant. Importantly, each track had one landmark in common with each of the other two tracks.</p><p>Participants first completed the Local Navigation Task, where they learned to navigate to goals on each of the three tracks separately, with fog serving to hide others from view (at any given point on a track, only one landmark and goal was visible in the field of view). The next day, participants completed the Global Navigation Task, where the separately learned tracks were connected and participants were required to navigate to goals both within and across tracks (<xref ref-type="fig" rid="fig1">Figure 1b</xref>, right). Participants were not informed and not aware that the tracks would be connected prior to the start of the Global Task, despite the shared landmarks. Efficient navigation on the Global Task required knowledge of the spatial relationships between locations in the global environment both within and across tracks.</p></sec><sec id="s4-3"><title>Stimuli</title><p>Nineteen building facades were selected from <ext-link ext-link-type="uri" xlink:href="https://www.lughertexture.com">https://www.lughertexture.com</ext-link> and used to create landmarks in the virtual environment. Facades were selected to have a variety of features and be of different architectural styles and colors. After resizing, removing signs and reflections, and adding roofs in Adobe Photoshop (Adobe, Inc), buildings were rendered onto 3D models of equal shape and size. Twelve buildings were randomly selected to be incorporated into each participant’s unique environment. Fractal images marked goal locations within the virtual environment. Twenty-three fractals were drawn from the Dryad Digital Repository (<xref ref-type="bibr" rid="bib95">Wilming et al., 2017</xref>; <xref ref-type="bibr" rid="bib94">Wilming, 2017</xref>) and from the stimulus set used in <xref ref-type="bibr" rid="bib7">Brown et al., 2016</xref>. Images were resized and rendered onto 3D models of equal shape and size. Fifteen fractals were randomly selected to be incorporated into each participant’s unique environment. Each participant’s unique set of building and fractal stimuli were used in both behavioral navigation tasks and all fMRI sessions. Buildings and fractals were matched for low-level visual features by using the SHINE toolbox in Matlab (version R2015b) to equate their luminance histograms (<xref ref-type="bibr" rid="bib93">Willenbockel et al., 2010</xref>).</p><p>All 3D models for the virtual environment were created in Maya (Autodesk, Inc), a 3D graphics program. Textures were acquired via a Google search and <ext-link ext-link-type="uri" xlink:href="https://www.lughertexture.com">https://www.lughertexture.com</ext-link>. While the specific stimuli used for landmarks varied between participants, the positions of the landmarks were fixed in the environment. However, goal locations were selected to fall within a 3 arbitrary unit (a.u.) radius of each landmark (1 a.u. = ~3 m/10 ft); thus, goal locations differed slightly across participants.</p></sec><sec id="s4-4"><title>Virtual navigation tasks</title><p>The navigation tasks were developed with Panda3D (1.9.4), a python-based open-source gaming engine, and the PandaEPL library (<xref ref-type="bibr" rid="bib78">Solway et al., 2013</xref>). For all navigation tasks, participants were instructed to navigate to goals using the shortest possible path. Each behavioral run contained 10 navigation trials. At the start of each run, participants were placed at a location on the path and slowly rotated 360 degrees to orient themselves (6 s). Trials then proceeded as follows: (1) a fractal cue appeared onscreen (1 s) indicating the goal to which the participant should navigate; (2) participants chose their heading direction and (3) navigated to the cued goal, pressing the spacebar at arrival; (4) feedback appeared onscreen revealing whether the participant was at the correct location and had navigated via the shortest path (2 s); and (5) the camera panned down and a fixation cross appeared (1 s) before the next trial began (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Participants navigated in the environment by pressing the forward arrow key and adjusted their heading direction using the left and right arrow keys. Movement was fixed along the centerline of the paths and movement speed was held constant. Traveling around one individual path required approximately 30 s.</p></sec><sec id="s4-5"><title>Local Navigation Task</title><p>During the Local Navigation Task, participants learned to navigate to goals on the individual tracks separately, with the other tracks hidden from view. After fMRI scanning on Day 1 (see fMRI task below), the Local Task began with four runs of learning trials on track 1. On learning trials, fractals were visible at goals to allow for learning of their locations within the track environment. The participants then completed two runs of test trials, where fractals were no longer visible and participants had to rely on memory to navigate successfully. This procedure was repeated for track 2 and track 3. After completing two runs of test trials on each track, participants began additional interleaved blocks of test trials, switching between tracks until six test runs (60 trials) were completed for each of the individual tracks. In total, participants navigated from every goal on a track to every other goal on that track, several times. Track order was randomized and counterbalanced across participants. After Day 1, participants returned 24 hr (range = 21–31.5 hr) later for Day 2, which started with two additional runs of Local Navigation Task test trials on each track prior to fMRI on Day 2. These two runs provided an assay of whether knowledge of the individual tracks had been retained overnight.</p></sec><sec id="s4-6"><title>Global Navigation Task</title><p>After fMRI on Day 2, participants began the Global Navigation Task. Here, the three tracks were connected and participants were required to navigate to goals both within and across the different tracks. In this task, common landmarks served as linking points within the larger environment (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Participants first completed three runs of learning trials, where fractals were again visible at goals. While the goal locations did not change between the Local and Global Navigation Tasks, learning trials allowed participants to become accustomed to moving across tracks and gave them an opportunity to orient themselves to the larger environment. Participants then completed nine runs of test trials, where fractals were no longer visible in the environment. For 30 of 90 test trials, participants were cued to navigate to a goal along the track on which they were already located. For the other 60 test trials, participants were required to navigate to a different track. After Day 2, participants returned 24 hr (range = 15.5–28.5 hr) later for Day 3, which started with four additional runs of Global Navigation test trials prior to fMRI on Day 3. These four runs provided an assay of whether spatial knowledge had been retained overnight.</p></sec><sec id="s4-7"><title>Behavioral data analysis</title><p>To assay whether participants successfully learned goal locations, we computed percent correct as the ratio of the number of trials where participants ended navigation within 8 a.u. of the virtual goal location (defined as a correct response) over the number of trials attempted. To assay whether participants had learned spatial relationships between the locations in the environment, we computed a path inefficiency metric by dividing path length by the length of the shortest possible path to the goal, subtracting 1, and multiplying by 100 to express as a percent. Thus, a path inefficiency of 0% would indicate that the participant took the shortest path possible, and a path inefficiency of 100% would indicate that their path was twice the length of the shortest possible path to the goal.</p></sec><sec id="s4-8"><title>fMRI task</title><p>Participants underwent fMRI scanning prior to any learning (Day 1), after the Local Navigation Task (Day 2), and after the Global Navigation Task (Day 3). During each fMRI session, participants viewed images of the 12 landmarks and 15 fractals that were used in their unique virtual environment. Stimuli were presented 12 times per scan session (3 repetitions/run x 4 runs). Within each scan, the stimuli appeared in mini-blocks such that participants saw all 27 images prior to seeing any repeated, with third repeats appearing only after all stimuli had appeared twice. Images were pseudo-randomized within each mini-block such that the same image could not appear within three steps of itself at the block transitions. Moreover, no first order stimulus-to-stimulus transitions were repeated within a scan session.</p><p>On each trial, the stimulus appeared on a gray background for 1.8 s, followed by a fixation cross of 5.4 s (intertrial interval; <xref ref-type="fig" rid="fig1">Figure 1c</xref>). To ensure that participants paid close attention to the images, they performed a visual anomaly detection task in which they were asked to report whether a feature of an image was ‘bleached out’ on trials when a red fixation cross appeared after image offset (<xref ref-type="bibr" rid="bib14">Clarke et al., 2016</xref>). Only a small proportion of stimulus presentations were response trials (~8%). The probability of a response trial having a bleached feature was 0.5. Response trials were excluded from all further analyses.</p></sec><sec id="s4-9"><title>MR data acquisition</title><p>Whole-brain imaging data were acquired on a 3T GE Discovery MR750 MRI scanner (GE Healthcare) using a 32-channel radiofrequency receive-only head coil (Nova Medical). Functional data were acquired using a three-band echo planar imaging (EPI) sequence (acceleration factor = 2) consisting of 63 oblique axial slices parallel to the long axis of the hippocampus (TR = 1.8 s, TE = 30ms, flip angle = 75°, FOV = 220mm × 220mm, voxel size = 2 × 2 × 2 mm<sup>3</sup>). To correct for distortions of the B0 field that may occur with EPI, two B0 field maps were acquired before every functional run, one in each phase encoding direction, with the same slice prescription as the functional runs. Structural images were acquired using a T1-weighted (T1w) spoiled gradient recalled echo structural sequence (186 sagittal slices, TR = 7.26ms, FoV = 230mm × 230mm, voxel size = 0.9 × 0.9 × 0.9 mm<sup>3</sup>). The MR data collection techniques closely mirrored procedures in <xref ref-type="bibr" rid="bib37">Jiang et al., 2020</xref>.</p></sec><sec id="s4-10"><title>Anatomical data preprocessing</title><p>fMRI data preprocessing was performed with fMRIPrep 1.5.3rc2 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_016216">SCR_016216</ext-link>) (<xref ref-type="bibr" rid="bib21">Esteban et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Esteban, 2022</xref>), which is based on <italic>Nipype</italic> 1.3.1 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002502">SCR_002502</ext-link>) (<xref ref-type="bibr" rid="bib29">Gorgolewski et al., 2011</xref>; <xref ref-type="bibr" rid="bib23">Esteban et al., 2022</xref>). The T1-weighted (T1w) structural images were corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (<xref ref-type="bibr" rid="bib86">Tustison et al., 2010</xref>), distributed with ANTs 2.2.0 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_004757">SCR_004757</ext-link>) (<xref ref-type="bibr" rid="bib1">Avants et al., 2008</xref>), and used as the T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a <italic>Nipype</italic> implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as the target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w-reference using FAST (FSL 5.0.9, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002823">SCR_002823;</ext-link> <xref ref-type="bibr" rid="bib102">Zhang et al., 2001</xref>). Brain surfaces were reconstructed using recon-all (FreeSurfer 6.0.1, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001847">SCR_001847;</ext-link> <xref ref-type="bibr" rid="bib17">Dale et al., 1999</xref>), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002438">SCR_002438;</ext-link> <xref ref-type="bibr" rid="bib39">Klein et al., 2017</xref>). Volume-based spatial normalization to standardized space (MNI152NLin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.2.0), using brain-extracted versions of both the T1w-reference and T1w-template. The ICBM 152 Nonlinear Asymmetrical template version 2009 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_008796">SCR_008796</ext-link>; TemplateFlow ID:MNI152NLin2009cAsym; <xref ref-type="bibr" rid="bib27">Fonov et al., 2009</xref>) was selected for spatial normalization.</p></sec><sec id="s4-11"><title>Functional data preprocessing</title><p>For each of the 12 functional runs per participant (4 per scan session), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. A B0-nonuniformity map (or fieldmap) was estimated based on the two EPI references with opposing phase-encoding directions, with 3dQwarp (AFNI) (<xref ref-type="bibr" rid="bib16">Cox and Hyde, 1997</xref>). Based on the estimated susceptibility distortion, a corrected EPI reference was calculated for a more accurate co-registration with the anatomical reference. The BOLD reference was then co-registered to the T1w-reference using bbregister (FreeSurfer) which implements boundary-based registration (<xref ref-type="bibr" rid="bib30">Greve and Fischl, 2009</xref>). Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices and six corresponding rotation and translation parameters) were estimated before any spatiotemporal filtering using MCFLIRT (FSL 5.0.9) (<xref ref-type="bibr" rid="bib36">Jenkinson et al., 2002</xref>). The BOLD time-series were resampled onto their original, native space by applying a single, composite transform to correct for head-motion and susceptibility distortions. These resampled BOLD time-series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD time-series were resampled into standard space, generating a preprocessed BOLD run in MNI space. Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS and a set of low-frequency regressors for temporal high-pass filtering. FD and DVARS were calculated for each functional run, both using their implementations in Nipype (following the definitions by <xref ref-type="bibr" rid="bib65">Power et al., 2014</xref>). The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. All resamplings were performed with a single interpolation step by composing all the pertinent transformations (i.e., head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (<xref ref-type="bibr" rid="bib47">Lanczos, 1964</xref>). The preprocessed fMRI data were smoothed by a 2 mm full-width-half-maximum Gaussian kernel.</p></sec><sec id="s4-12"><title>fMRI analysis</title><p>Prior to fMRI analyses, we removed the first 6 TRs in each run. For each scan session, we built general linear models (GLMs) for even and odd runs, which were regressed against preprocessed fMRI data at the voxel level. To obtain stimulus-level beta estimates for brain activity, each stimulus (i.e., landmark or fractal) was represented by a single regressor, time-locked to when it appeared onscreen in odd or in even runs. Each event was modeled as an epoch lasting for the duration of stimulus presentation (1.8 s) and convolved with a canonical hemodynamic response function. A separate regressor was included for response trials to exclude them from further analysis. Response trials were modeled as epochs lasting for the duration of the stimulus presentation and subsequent response period (7.2 s). GLMs also included nuisance regressors marking outlier TRs (DVARS &gt; 5 or FD &gt; 0.9 mm from previous TR), a run regressor, and regressors generated by fMRIPREP representing TR-level six-dimensional head movement estimates, framewise displacement (mm), and low frequency components for temporal high-pass filtering. GLMs yielded estimates of voxel-level brain activity for each stimulus during odd and even runs of each scan session. Modeling was performed with SPM12 (Wellcome Trust Centre for Neuroimaging) and custom Matlab (vR2017b) routines.</p><p>Primary analyses were performed using an <italic>a priori</italic> region-of-interest (ROI) approach targeting the hippocampus and EC. Bilateral hippocampal, entorhinal, parahippocampal, and perirhinal cortical ROIs were manually delineated on each participant’s high-resolution T1-weighted structural image using established procedures (<xref ref-type="bibr" rid="bib61">Olsen et al., 2009</xref>). Due to low TSNR (&lt;28), selected scans from 3 participants were excluded from analyses in right EC, and 1 scan from 1 participant was excluded from analyses in left EC. For exploratory analyses in vmPFC, we obtained a vmPFC mask from a Neurosynth parcellation (<xref ref-type="bibr" rid="bib11">Chang et al., 2021</xref>), and transformed the mask into native space for each participant. We also created a smaller 8 mm spherical vmPFC mask using the peak voxel reported in <xref ref-type="bibr" rid="bib79">Takashima et al., 2006</xref>. ROIs were resampled, masked to exclude voxels outside the brain, and aligned with functional volumes. Finally, we used a visual ROI defined by FreeSurfer’s automated segmentation procedure as a control for our analyses. The mask for this ROI was defined as a conjunction of FreeSurfer’s pericalcarine and calcarine sulcus regions in both hemispheres.</p><p>The activity pattern for each stimulus was quantified as a vector of multi-voxel normalized betas by dividing the original betas by the square root of the covariance matrix of the error terms from the GLM estimation (<xref ref-type="bibr" rid="bib88">Walther et al., 2016</xref>). Separately for each participant, ROI, and scan session, we computed pattern similarity by correlating activity patterns between even and odd runs. Pattern similarity analyses were conducted in Python (v2.7).</p></sec><sec id="s4-13"><title>Statistical analyses</title><p>No explicit power analysis was conducted to predetermine sample size, but we aimed to collect 30 participants for the multi-day study. The present sample size (n = 23) is comparable to that of prior fMRI studies examining mnemonic representations that did not scan across three consecutive days (e.g. <xref ref-type="bibr" rid="bib10">Chanales et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Deuker et al., 2016</xref>; <xref ref-type="bibr" rid="bib83">Tompary and Davachi, 2017</xref>; <xref ref-type="bibr" rid="bib103">Zheng et al., 2021</xref>).</p><p>Statistical analyses were implemented in R (v3.6.3). For neural analyses described above, we implemented linear mixed-effects models using the lme4 and lmerTest statistical packages (<xref ref-type="bibr" rid="bib4">Bates et al., 2015</xref>; <xref ref-type="bibr" rid="bib45">Kuznetsova et al., 2017</xref>). Correlations, our dependent variable, were Fisher transformed to follow a normal distribution. Models included fixed effects of interest (i.e., scan session and link distance), fixed effects corresponding to the average univariate activation for each of the stimuli, fixed effects estimating perceptual similarity in V1 and IT cortex, and a standard set of random effects, including a random intercept modeling the mean subject-specific outcome value, as well as random slope terms modeling subject-specific effects of independent variables of interest (i.e., scan session and average univariate activation). We included average univariate activation as a regressor to limit the possibility that overall activation differences within an ROI would impact pattern similarity (<xref ref-type="bibr" rid="bib68">Ritchey et al., 2013</xref>).</p><p>To control for perceptual similarity between task stimuli, we input the stimuli into vNet, a deep neural network model. vNet was trained on ecoset, a large-scale image set containing images from 565 basic level categories (<xref ref-type="bibr" rid="bib55">Mehrer et al., 2021</xref>). We extracted hidden layer representations putatively corresponding to V1 and IT cortex (layers 1 and 10, respectively), computed similarity matrices, and included these estimates as regressors in linear mixed effects models. Open-source code is available at <ext-link ext-link-type="uri" xlink:href="https://codeocean.com/capsule/9570390/tree/v1">https://codeocean.com/capsule/9570390/tree/v1</ext-link>.</p><p>For all mixed-effects models, the standard set of random effects was chosen by taking a maximal model with all random effects indicated by the experiment setup, then incrementally removing effects and testing the nested model fits using the likelihood ratio test. This procedure was performed for all ROIs. The final standard set of random effects was composed of the minimum necessary effects to achieve the best fit in all regions, and included a random intercept modeling the mean subject-specific outcome value, as well as random slope terms modeling subject-specific effects of independent variables of interest (i.e., scan session and average univariate activation). Scan session was dummy-coded with Pre-Learning (Day 1) as the baseline. Stimulus type and context were sum-coded with the contrasts of landmark &gt; fractal and same track &gt; different tracks, respectively. For the trial-level model relating performance to hippocampal pattern similarity and distance, separate models were run for landmarks and fractals. Models predicted path inefficiency for each trial in the first four test runs of the Global Navigation Task (Day 2). Fixed effects of interest included the length of the optimal path for that trial, the estimated hippocampal pattern similarity for the pair of fractal goals or nearby landmarks associated with the start and end locations the trial, and a regressor indicating whether the trial was a within- or across-track trial. Models were estimated using a restricted maximum likelihood (REML) approach. Model convergence issues were resolved by changing from the default optimizer to ‘bobyqa’ and not having the model calculate derivatives.</p><p>To correct for multiple comparisons, we first computed <italic>m</italic>, a number representing the number of tests conducted for a given hypothesis multiplied by the number of ROIs we examined (including the visual control). For example, to examine learning-driven changes in pattern similarity for items located at different distances on the same track, we tested for two interactions: distance and scan (Day 2 &gt; Day 1), and distance and scan (Day 3 &gt; Day 1). ROIs tested were hippocampus, left and right EC, vmPFC, and the visual control region (Calcarine). <italic>m</italic> thus represented 2 interactions * 5 ROIs = 10. We then controlled for the false discovery rate (FDR) by ordering the p-values for each hypothesis test from smallest to largest (P(min)...P(max)), and checking if the following was satisfied for each ordered p-value (<xref ref-type="bibr" rid="bib5">Benjamini and Hochberg, 1995</xref>):<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>α</mml:mi><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mfrac><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Unless otherwise specified, all p-values reported were uncorrected and we interpret <italic>a priori</italic> predicted effects at this level. For completeness, we also note whether the reported effects survived FDR correction throughout the text and in the Appendix tables. Cohen’s <italic>d</italic> effect sizes were computed for t-values as <inline-formula><mml:math id="inf1"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> .</p></sec><sec id="s4-14"><title>Data availability</title><p>Raw fMRI data are available on OpenNeuro. Analytical code and behavioral data for reproducing analyses, results, and figures shown in the paper are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/coreyfernandez/RID">https://github.com/coreyfernandez/RID</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:399a7f9a8dba13a428b0e5651a213b34c4f0add9;origin=https://github.com/coreyfernandez/RID;visit=swh:1:snp:a383f5cedac3685c9c93234cd7d7f4b7916c2e88;anchor=swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5">swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5</ext-link>; <xref ref-type="bibr" rid="bib26">Fernandez, 2023</xref>). Further information and requests for resources should be directed to and will be fulfilled by the Lead Contact, Corey Fernandez (coreyf@stanford.edu).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants provided written informed consent in accordance with a protocol approved by the Stanford Institutional Review Board (IRB #13032).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80281-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Raw fMRI data is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds004406.v1.0.0">https://doi.org/10.18112/openneuro.ds004406.v1.0.0</ext-link>. Analytical code and behavioral data for reproducing analyses, results, and figures shown in the paper are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/coreyfernandez/RID">https://github.com/coreyfernandez/RID</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:399a7f9a8dba13a428b0e5651a213b34c4f0add9;origin=https://github.com/coreyfernandez/RID;visit=swh:1:snp:a383f5cedac3685c9c93234cd7d7f4b7916c2e88;anchor=swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5">swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5</ext-link>). Further information and requests for resources should be directed to and will be fulfilled by the Lead Contact, Corey Fernandez (<ext-link ext-link-type="uri" xlink:href="https://memorylab.stanford.edu/people/corey-fernandez">coreyf@stanford.edu</ext-link>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Fernandez</surname><given-names>C</given-names></name><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>S-F</given-names></name><name><surname>Choi</surname><given-names>HL</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Representational integration and differentiation in the human hippocampus following goal-directed navigation</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds004406.v1.0.0</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This project was supported by The Marcus and Amalia Wallenberg Foundation and The Stanford Center for Cognitive and Neurobiological Imaging.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakker</surname><given-names>A</given-names></name><name><surname>Kirwan</surname><given-names>CB</given-names></name><name><surname>Miller</surname><given-names>M</given-names></name><name><surname>Stark</surname><given-names>CEL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Pattern separation in the human hippocampal CA3 and dentate gyrus</article-title><source>Science</source><volume>319</volume><fpage>1640</fpage><lpage>1642</lpage><pub-id pub-id-type="doi">10.1126/science.1152882</pub-id><pub-id pub-id-type="pmid">18356518</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballard</surname><given-names>IC</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name><name><surname>McClure</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hippocampal pattern separation supports reinforcement learning</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1073</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-08998-1</pub-id><pub-id pub-id-type="pmid">30842581</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Mächler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>B</given-names></name><name><surname>Walker</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fitting linear mixed-effects models using lme4</article-title><source>Journal of Statistical Software</source><volume>67</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>TI</given-names></name><name><surname>Ross</surname><given-names>RS</given-names></name><name><surname>Keller</surname><given-names>JB</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Stern</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Which way was I going? contextual retrieval supports the disambiguation of well learned overlapping navigational routes</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>7414</fpage><lpage>7422</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6021-09.2010</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>TI</given-names></name><name><surname>Carr</surname><given-names>VA</given-names></name><name><surname>LaRocque</surname><given-names>KF</given-names></name><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Gordon</surname><given-names>AM</given-names></name><name><surname>Bowles</surname><given-names>B</given-names></name><name><surname>Bailenson</surname><given-names>JN</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prospective representation of navigational goals in the human hippocampus</article-title><source>Science</source><volume>352</volume><fpage>1323</fpage><lpage>1326</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0784</pub-id><pub-id pub-id-type="pmid">27284194</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Robin</surname><given-names>J</given-names></name><name><surname>Olsen</surname><given-names>RK</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Integration and differentiation of hippocampal memory traces</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>118</volume><fpage>196</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.07.024</pub-id><pub-id pub-id-type="pmid">32712280</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Nantais</surname><given-names>MM</given-names></name><name><surname>Sutton</surname><given-names>JE</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Exploration patterns shape cognitive MAP learning</article-title><source>Cognition</source><volume>233</volume><elocation-id>105360</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105360</pub-id><pub-id pub-id-type="pmid">36549130</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chanales</surname><given-names>AJH</given-names></name><name><surname>Oza</surname><given-names>A</given-names></name><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Overlap among spatial memories triggers repulsion of hippocampal representations</article-title><source>Current Biology</source><volume>27</volume><fpage>2307</fpage><lpage>2317</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.057</pub-id><pub-id pub-id-type="pmid">28736170</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>LJ</given-names></name><name><surname>Jolly</surname><given-names>E</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Rapuano</surname><given-names>KM</given-names></name><name><surname>Greenstein</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>P-HA</given-names></name><name><surname>Manning</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects affective experience</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabf7129</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abf7129</pub-id><pub-id pub-id-type="pmid">33893106</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chrastil</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural evidence supports a novel framework for spatial navigation</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>20</volume><fpage>208</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.3758/s13423-012-0351-6</pub-id><pub-id pub-id-type="pmid">23229443</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chrastil</surname><given-names>ER</given-names></name><name><surname>Warren</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>From cognitive maps to cognitive graphs</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e112544</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0112544</pub-id><pub-id pub-id-type="pmid">25389769</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Pell</surname><given-names>PJ</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning warps object representations in the ventral temporal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>1010</fpage><lpage>1023</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00951</pub-id><pub-id pub-id-type="pmid">26967942</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clewett</surname><given-names>D</given-names></name><name><surname>Dunsmoor</surname><given-names>J</given-names></name><name><surname>Bachman</surname><given-names>SL</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Survival of the salient: aversive learning rescues otherwise forgettable memories via neural reactivation and post-encoding hippocampal connectivity</article-title><source>Neurobiology of Learning and Memory</source><volume>187</volume><elocation-id>107572</elocation-id><pub-id pub-id-type="doi">10.1016/j.nlm.2021.107572</pub-id><pub-id pub-id-type="pmid">34871800</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR in Biomedicine</source><volume>10</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1002/(sici)1099-1492(199706/08)10:4/5&lt;171::aid-nbm453&gt;3.0.co;2-l</pub-id><pub-id pub-id-type="pmid">9430344</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deuker</surname><given-names>L</given-names></name><name><surname>Bellmund</surname><given-names>JL</given-names></name><name><surname>Navarro Schröder</surname><given-names>T</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An event map of memory space in the hippocampus</article-title><source>eLife</source><volume>5</volume><elocation-id>e16534</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.16534</pub-id><pub-id pub-id-type="pmid">27710766</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Hippocampus: cognitive processes and neural representations that underlie declarative memory</article-title><source>Neuron</source><volume>44</volume><fpage>109</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.08.028</pub-id><pub-id pub-id-type="pmid">15450164</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>RA</given-names></name><name><surname>Patai</surname><given-names>EZ</given-names></name><name><surname>Julian</surname><given-names>JB</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The cognitive map in humans: spatial navigation and beyond</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1504</fpage><lpage>1513</lpage><pub-id pub-id-type="doi">10.1038/nn.4656</pub-id><pub-id pub-id-type="pmid">29073650</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>FMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>FMRIPrep: a robust preprocessing pipeline for functional MRI</data-title><source>Zenoda</source><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/7430291#.Y9oHsibhVCk">https://zenodo.org/record/7430291#.Y9oHsibhVCk</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Burns</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Nipy/nipype: 1.7.1</data-title><source>Zenoda</source><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6415183#.Y9oIAybhVCk">https://zenodo.org/record/6415183#.Y9oIAybhVCk</ext-link></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ezzyat</surname><given-names>Y</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Similarity breeds proximity: pattern similarity within and across contexts is related to later mnemonic judgments of temporal proximity</article-title><source>Neuron</source><volume>81</volume><fpage>1179</fpage><lpage>1189</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.042</pub-id><pub-id pub-id-type="pmid">24607235</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Chanales</surname><given-names>AJH</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Experience-dependent hippocampal pattern differentiation prevents interference during subsequent learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11066</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11066</pub-id><pub-id pub-id-type="pmid">27925613</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fernandez</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>RID</data-title><version designator="swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5">swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:399a7f9a8dba13a428b0e5651a213b34c4f0add9;origin=https://github.com/coreyfernandez/RID;visit=swh:1:snp:a383f5cedac3685c9c93234cd7d7f4b7916c2e88;anchor=swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5">https://archive.softwareheritage.org/swh:1:dir:399a7f9a8dba13a428b0e5651a213b34c4f0add9;origin=https://github.com/coreyfernandez/RID;visit=swh:1:snp:a383f5cedac3685c9c93234cd7d7f4b7916c2e88;anchor=swh:1:rev:f4a2d4915f1922c8a74f1f1a86469cf13789abb5</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>V</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name><name><surname>McKinstry</surname><given-names>R</given-names></name><name><surname>Almli</surname><given-names>C</given-names></name><name><surname>Collins</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>PW</given-names></name><name><surname>Bontempi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The organization of recent and remote memories</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1038/nrn1607</pub-id><pub-id pub-id-type="pmid">15685217</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>AS</given-names></name><name><surname>van der Meer</surname><given-names>MAA</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hippocampal replay is not a simple function of experience</article-title><source>Neuron</source><volume>65</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id><pub-id pub-id-type="pmid">20223204</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>LR</given-names></name><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Mill</surname><given-names>RD</given-names></name><name><surname>Morrison</surname><given-names>LC</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Loftus</surname><given-names>MM</given-names></name><name><surname>Staskute</surname><given-names>L</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The hippocampus and entorhinal cortex encode the path and Euclidean distances to goals during navigation</article-title><source>Current Biology</source><volume>24</volume><fpage>1331</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.001</pub-id><pub-id pub-id-type="pmid">24909328</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulbert</surname><given-names>JC</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural differentiation tracks improved recall of competing memories following interleaved study and retrieval practice</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3994</fpage><lpage>4008</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu284</pub-id><pub-id pub-id-type="pmid">25477369</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadhav</surname><given-names>SP</given-names></name><name><surname>Rothschild</surname><given-names>G</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Coordinated excitation and inhibition of prefrontal ensembles during awake hippocampal sharp-wave ripple events</article-title><source>Neuron</source><volume>90</volume><fpage>113</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.010</pub-id><pub-id pub-id-type="pmid">26971950</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(02)91132-8</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>SF</given-names></name><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Fernandez</surname><given-names>C</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prefrontal reinstatement of contextual task demand is predicted by separable hippocampal patterns</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>2053</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-15928-z</pub-id><pub-id pub-id-type="pmid">32345979</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>G</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural differentiation of incorrectly predicted memories</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>2022</fpage><lpage>2031</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3272-16.2017</pub-id><pub-id pub-id-type="pmid">28115478</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Bao</surname><given-names>FS</given-names></name><name><surname>Giard</surname><given-names>J</given-names></name><name><surname>Häme</surname><given-names>Y</given-names></name><name><surname>Stavsky</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>N</given-names></name><name><surname>Rossa</surname><given-names>B</given-names></name><name><surname>Reuter</surname><given-names>M</given-names></name><name><surname>Chaibub Neto</surname><given-names>E</given-names></name><name><surname>Keshavan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mindboggling morphometry of human brains</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005350</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005350</pub-id><pub-id pub-id-type="pmid">28231282</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kropff</surname><given-names>E</given-names></name><name><surname>Carmichael</surname><given-names>JE</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Speed cells in the medial entorhinal cortex</article-title><source>Nature</source><volume>523</volume><fpage>419</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1038/nature14622</pub-id><pub-id pub-id-type="pmid">26176924</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>BA</given-names></name><name><surname>Bainbridge</surname><given-names>WA</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural reactivation reveals mechanisms for updating memory</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3453</fpage><lpage>3461</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5846-11.2012</pub-id><pub-id pub-id-type="pmid">22399768</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>JJ</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Tracking the emergence of conceptual knowledge during human decision making</article-title><source>Neuron</source><volume>63</volume><fpage>889</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.030</pub-id><pub-id pub-id-type="pmid">19778516</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Generalization through the recurrent interaction of episodic memories: a model of the hippocampal system</article-title><source>Psychological Review</source><volume>119</volume><fpage>573</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1037/a0028681</pub-id><pub-id pub-id-type="pmid">22775499</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What learning systems do intelligent agents need? complementary learning systems theory updated</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>512</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.05.004</pub-id><pub-id pub-id-type="pmid">27315762</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname><given-names>A</given-names></name><name><surname>Brockhoff</surname><given-names>PB</given-names></name><name><surname>Christensen</surname><given-names>RHB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>lmertest package: tests in linear mixed effects models</article-title><source>Journal of Statistical Software</source><volume>82</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.18637/jss.v082.i13</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kyle</surname><given-names>CT</given-names></name><name><surname>Stokes</surname><given-names>JD</given-names></name><name><surname>Lieberman</surname><given-names>JS</given-names></name><name><surname>Hassan</surname><given-names>AS</given-names></name><name><surname>Ekstrom</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Successful retrieval of competing spatial environments in humans involves hippocampal pattern separation mechanisms</article-title><source>eLife</source><volume>4</volume><elocation-id>e10499</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10499</pub-id><pub-id pub-id-type="pmid">26613414</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanczos</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Evaluation of noisy data</article-title><source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source><volume>1</volume><fpage>76</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1137/0701007</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LaRocque</surname><given-names>KF</given-names></name><name><surname>Smith</surname><given-names>ME</given-names></name><name><surname>Carr</surname><given-names>VA</given-names></name><name><surname>Witthoft</surname><given-names>N</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Global similarity and pattern separation in the human medial temporal lobe predict subsequent memory</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>5466</fpage><lpage>5474</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4293-12.2013</pub-id><pub-id pub-id-type="pmid">23536062</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname><given-names>JK</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Pattern separation in the dentate gyrus and CA3 of the hippocampus</article-title><source>Science</source><volume>315</volume><fpage>961</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1126/science.1135801</pub-id><pub-id pub-id-type="pmid">17303747</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>PA</given-names></name><name><surname>Durrant</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Overlapping memory replay during sleep builds cognitive schemata</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>343</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.06.004</pub-id><pub-id pub-id-type="pmid">21764357</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Libby</surname><given-names>LA</given-names></name><name><surname>Hannula</surname><given-names>DE</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Medial temporal lobe coding of item and spatial information during relational binding in working memory</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>14233</fpage><lpage>14242</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0655-14.2014</pub-id><pub-id pub-id-type="pmid">25339737</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychological Review</source><volume>102</volume><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.102.3.419</pub-id><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKenzie</surname><given-names>S</given-names></name><name><surname>Frank</surname><given-names>AJ</given-names></name><name><surname>Kinsky</surname><given-names>NR</given-names></name><name><surname>Porter</surname><given-names>B</given-names></name><name><surname>Rivière</surname><given-names>PD</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal representation of related and opposing memories develop within distinct, hierarchically organized neural schemas</article-title><source>Neuron</source><volume>83</volume><fpage>202</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.019</pub-id><pub-id pub-id-type="pmid">24910078</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Battaglia</surname><given-names>FP</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Path integration and the neural basis of the “ cognitive map. ”</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>663</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1038/nrn1932</pub-id><pub-id pub-id-type="pmid">16858394</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Jones</surname><given-names>EC</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An ecologically motivated image dataset for deep learning yields better models of human vision</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2011417118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2011417118</pub-id><pub-id pub-id-type="pmid">33593900</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milivojevic</surname><given-names>B</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mnemonic networks in the hippocampal formation: from spatial maps to temporal and conceptual codes</article-title><source>Journal of Experimental Psychology. General</source><volume>142</volume><fpage>1231</fpage><lpage>1241</lpage><pub-id pub-id-type="doi">10.1037/a0033746</pub-id><pub-id pub-id-type="pmid">23875564</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nyberg</surname><given-names>N</given-names></name><name><surname>Duvelle</surname><given-names>É</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Spatial goal coding in the hippocampal formation</article-title><source>Neuron</source><volume>110</volume><fpage>394</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.12.012</pub-id><pub-id pub-id-type="pmid">35032426</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>The Hippocampus as a Cognitive Map</source><publisher-name>Clarendon Press ; Oxford University Press</publisher-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The role of hippocampal replay in memory and planning</article-title><source>Current Biology</source><volume>28</volume><fpage>R37</fpage><lpage>R50</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.10.073</pub-id><pub-id pub-id-type="pmid">29316421</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olsen</surname><given-names>RK</given-names></name><name><surname>Nichols</surname><given-names>EA</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Hunt</surname><given-names>JF</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>Gabrieli</surname><given-names>JDE</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Performance-related sustained and anticipatory activity in human medial temporal lobe during delayed match-to-sample</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>11880</fpage><lpage>11890</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2245-09.2009</pub-id><pub-id pub-id-type="pmid">19776274</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Hippocampal conjunctive encoding, storage, and recall: avoiding a trade-off</article-title><source>Hippocampus</source><volume>4</volume><fpage>661</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1002/hipo.450040605</pub-id><pub-id pub-id-type="pmid">7704110</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patai</surname><given-names>EZ</given-names></name><name><surname>Javadi</surname><given-names>A-H</given-names></name><name><surname>Ozubko</surname><given-names>JD</given-names></name><name><surname>O’Callaghan</surname><given-names>A</given-names></name><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Robin</surname><given-names>J</given-names></name><name><surname>Grady</surname><given-names>C</given-names></name><name><surname>Winocur</surname><given-names>G</given-names></name><name><surname>Rosenbaum</surname><given-names>RS</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hippocampal and retrosplenial goal distance coding after long-term consolidation of a real-world environment</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>2748</fpage><lpage>2758</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz044</pub-id><pub-id pub-id-type="pmid">30916744</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peer</surname><given-names>M</given-names></name><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Structuring knowledge with cognitive maps and cognitive graphs</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>37</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.10.004</pub-id><pub-id pub-id-type="pmid">33248898</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Mitra</surname><given-names>A</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Methods to detect, characterize, and remove motion artifact in resting state fmri</article-title><source>NeuroImage</source><volume>84</volume><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id><pub-id pub-id-type="pmid">23994314</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preston</surname><given-names>AR</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Interplay of hippocampus and prefrontal cortex in memory</article-title><source>Current Biology</source><volume>23</volume><fpage>R764</fpage><lpage>R773</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.041</pub-id><pub-id pub-id-type="pmid">24028960</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Heller</surname><given-names>A</given-names></name><name><surname>Cohen</surname><given-names>MX</given-names></name><name><surname>Brozinsky</surname><given-names>CJ</given-names></name><name><surname>Rissman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional connectivity with the hippocampus during successful memory formation</article-title><source>Hippocampus</source><volume>15</volume><fpage>997</fpage><lpage>1005</lpage><pub-id pub-id-type="doi">10.1002/hipo.20141</pub-id><pub-id pub-id-type="pmid">16281291</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Wing</surname><given-names>EA</given-names></name><name><surname>LaBar</surname><given-names>KS</given-names></name><name><surname>Cabeza</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural similarity between encoding and retrieval is related to memory via hippocampal interactions</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>2818</fpage><lpage>2828</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs258</pub-id><pub-id pub-id-type="pmid">22967731</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritvo</surname><given-names>VJH</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nonmonotonic plasticity: how memory retrieval drives learning</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>726</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.06.007</pub-id><pub-id pub-id-type="pmid">31358438</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Kustner</surname><given-names>LV</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Shaping of object representations in the human medial temporal lobe based on temporal regularities</article-title><source>Current Biology</source><volume>22</volume><fpage>1622</fpage><lpage>1627</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.06.056</pub-id><pub-id pub-id-type="pmid">22885059</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Cordova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural representations of events arise from temporal community structure</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>486</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1038/nn.3331</pub-id><pub-id pub-id-type="pmid">23416451</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Gregory</surname><given-names>E</given-names></name><name><surname>Landau</surname><given-names>B</given-names></name><name><surname>McCloskey</surname><given-names>M</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The necessity of the medial temporal lobe for statistical learning</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>1736</fpage><lpage>1747</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00578</pub-id><pub-id pub-id-type="pmid">24456393</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Statistical learning of temporal community structure in the hippocampus</article-title><source>Hippocampus</source><volume>26</volume><fpage>3</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1002/hipo.22523</pub-id><pub-id pub-id-type="pmid">26332666</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160049</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0049</pub-id><pub-id pub-id-type="pmid">27872368</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlichting</surname><given-names>ML</given-names></name><name><surname>Mumford</surname><given-names>JA</given-names></name><name><surname>Preston</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning-related representational changes reveal dissociable integration and separation signatures in the hippocampus and prefrontal cortex</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8151</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9151</pub-id><pub-id pub-id-type="pmid">26303198</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Integrating memories in the human brain: hippocampal-midbrain encoding of overlapping events</article-title><source>Neuron</source><volume>60</volume><fpage>378</fpage><lpage>389</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.023</pub-id><pub-id pub-id-type="pmid">18957228</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siapas</surname><given-names>AG</given-names></name><name><surname>Lubenov</surname><given-names>EV</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Prefrontal phase locking to hippocampal theta oscillations</article-title><source>Neuron</source><volume>46</volume><fpage>141</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.02.028</pub-id><pub-id pub-id-type="pmid">15820700</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solway</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>JF</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>PandaEPL: a library for programming spatial navigation experiments</article-title><source>Behavior Research Methods</source><volume>45</volume><fpage>1293</fpage><lpage>1312</lpage><pub-id pub-id-type="doi">10.3758/s13428-013-0322-5</pub-id><pub-id pub-id-type="pmid">23549683</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takashima</surname><given-names>A</given-names></name><name><surname>Petersson</surname><given-names>KM</given-names></name><name><surname>Rutters</surname><given-names>F</given-names></name><name><surname>Tendolkar</surname><given-names>I</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Zwarts</surname><given-names>MJ</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Fernández</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Declarative memory consolidation in humans: a prospective functional magnetic resonance imaging study</article-title><source>PNAS</source><volume>103</volume><fpage>756</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507774103</pub-id><pub-id pub-id-type="pmid">16407110</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takehara-Nishiuchi</surname><given-names>K</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spontaneous changes of neocortical code for associative memory during consolidation</article-title><source>Science</source><volume>322</volume><fpage>960</fpage><lpage>963</lpage><pub-id pub-id-type="doi">10.1126/science.1161299</pub-id><pub-id pub-id-type="pmid">18988855</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teyler</surname><given-names>TJ</given-names></name><name><surname>DiScenna</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The hippocampal memory indexing theory</article-title><source>Behavioral Neuroscience</source><volume>100</volume><fpage>147</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1037//0735-7044.100.2.147</pub-id><pub-id pub-id-type="pmid">3008780</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tompary</surname><given-names>A</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Consolidation promotes the emergence of representational overlap in the hippocampus and medial prefrontal cortex</article-title><source>Neuron</source><volume>96</volume><fpage>228</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.005</pub-id><pub-id pub-id-type="pmid">28957671</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tse</surname><given-names>D</given-names></name><name><surname>Langston</surname><given-names>RF</given-names></name><name><surname>Kakeyama</surname><given-names>M</given-names></name><name><surname>Bethus</surname><given-names>I</given-names></name><name><surname>Spooner</surname><given-names>PA</given-names></name><name><surname>Wood</surname><given-names>ER</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Morris</surname><given-names>RGM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Schemas and memory consolidation</article-title><source>Science</source><volume>316</volume><fpage>76</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1126/science.1135935</pub-id><pub-id pub-id-type="pmid">17412951</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tse</surname><given-names>D</given-names></name><name><surname>Takeuchi</surname><given-names>T</given-names></name><name><surname>Kakeyama</surname><given-names>M</given-names></name><name><surname>Kajii</surname><given-names>Y</given-names></name><name><surname>Okuno</surname><given-names>H</given-names></name><name><surname>Tohyama</surname><given-names>C</given-names></name><name><surname>Bito</surname><given-names>H</given-names></name><name><surname>Morris</surname><given-names>RGM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Schema-dependent gene activation and memory encoding in neocortex</article-title><source>Science</source><volume>333</volume><fpage>891</fpage><lpage>895</lpage><pub-id pub-id-type="doi">10.1126/science.1205274</pub-id><pub-id pub-id-type="pmid">21737703</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: improved N3 bias correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><pub-id pub-id-type="pmid">20378467</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaidya</surname><given-names>AR</given-names></name><name><surname>Jones</surname><given-names>HM</given-names></name><name><surname>Castillo</surname><given-names>J</given-names></name><name><surname>Badre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural representation of Abstract task structure during generalization</article-title><source>eLife</source><volume>10</volume><elocation-id>e63226</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63226</pub-id><pub-id pub-id-type="pmid">33729156</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id><pub-id pub-id-type="pmid">26707889</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wammes</surname><given-names>J</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Turk-Browne</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Increasing stimulus similarity drives nonmonotonic representational change in hippocampus</article-title><source>eLife</source><volume>11</volume><elocation-id>e68344</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.68344</pub-id><pub-id pub-id-type="pmid">34989336</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weisberg</surname><given-names>SM</given-names></name><name><surname>Schinazi</surname><given-names>VR</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name><name><surname>Shipley</surname><given-names>TF</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Variations in cognitive maps: understanding individual differences in navigation</article-title><source>Journal of Experimental Psychology</source><volume>40</volume><fpage>669</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1037/a0035261</pub-id><pub-id pub-id-type="pmid">24364725</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weisberg</surname><given-names>SM</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How do (some) people make a cognitive map? routes, places, and working memory</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>42</volume><fpage>768</fpage><lpage>785</lpage><pub-id pub-id-type="doi">10.1037/xlm0000200</pub-id><pub-id pub-id-type="pmid">26595065</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weisberg</surname><given-names>SM</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cognitive maps: some people make them, some people struggle</article-title><source>Current Directions in Psychological Science</source><volume>27</volume><fpage>220</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1177/0963721417744521</pub-id><pub-id pub-id-type="pmid">30122809</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V</given-names></name><name><surname>Sadr</surname><given-names>J</given-names></name><name><surname>Fiset</surname><given-names>D</given-names></name><name><surname>Horne</surname><given-names>GO</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Tanaka</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Controlling low-level image properties: the shine toolbox</article-title><source>Behavior Research Methods</source><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilming</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Data from: an extensive dataset of eye movements during viewing of complex images</article-title><source>Dryad</source><volume>4</volume><elocation-id>9PF75</elocation-id><pub-id pub-id-type="doi">10.5061/DRYAD.9PF75</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilming</surname><given-names>N</given-names></name><name><surname>Onat</surname><given-names>S</given-names></name><name><surname>Ossandón</surname><given-names>JP</given-names></name><name><surname>Açık</surname><given-names>A</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Kaspar</surname><given-names>K</given-names></name><name><surname>Gameiro</surname><given-names>RR</given-names></name><name><surname>Vormberg</surname><given-names>A</given-names></name><name><surname>König</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An extensive dataset of eye movements during viewing of complex images</article-title><source>Scientific Data</source><volume>4</volume><elocation-id>160126</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.126</pub-id><pub-id pub-id-type="pmid">28140391</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolbers</surname><given-names>T</given-names></name><name><surname>Hegarty</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>What determines our navigational abilities?</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>138</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.001</pub-id><pub-id pub-id-type="pmid">20138795</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal replay captures the unique topological structure of a novel environment</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>6459</fpage><lpage>6469</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3414-13.2014</pub-id><pub-id pub-id-type="pmid">24806672</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yassa</surname><given-names>MA</given-names></name><name><surname>Stark</surname><given-names>CEL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pattern separation in the hippocampus</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>515</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2011.06.006</pub-id><pub-id pub-id-type="pmid">21788086</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>JY</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Loback</surname><given-names>A</given-names></name><name><surname>Grossrubatscher</surname><given-names>I</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Specific hippocampal representations are linked to generalized cortical representations in memory</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2209</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04498-w</pub-id><pub-id pub-id-type="pmid">29880860</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeithamova</surname><given-names>D</given-names></name><name><surname>Preston</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Flexible memories: differential roles for medial temporal lobe and prefrontal cortex in cross-episode binding</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>14676</fpage><lpage>14684</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3250-10.2010</pub-id><pub-id pub-id-type="pmid">21048124</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeithamova</surname><given-names>D</given-names></name><name><surname>Dominick</surname><given-names>AL</given-names></name><name><surname>Preston</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Hippocampal and ventral medial prefrontal activation during retrieval-mediated learning supports novel inference</article-title><source>Neuron</source><volume>75</volume><fpage>168</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.05.010</pub-id><pub-id pub-id-type="pmid">22794270</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of brain Mr images through a hidden Markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>McAvan</surname><given-names>AS</given-names></name><name><surname>Isham</surname><given-names>EA</given-names></name><name><surname>Ekstrom</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Partially overlapping spatial environments trigger reinstatement in hippocampus and schema representations in prefrontal cortex</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6231</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26560-w</pub-id><pub-id pub-id-type="pmid">34711830</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Hippocampal-vmPFC interactions</title><p>The building of structured knowledge is not solely the province of the hippocampus. Retrieval-mediated changes in mnemonic representations involve interactions between the hippocampus and ventromedial prefrontal cortex (vmPFC; <xref ref-type="bibr" rid="bib83">Tompary and Davachi, 2017</xref>; <xref ref-type="bibr" rid="bib101">Zeithamova et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Kuhl et al., 2012</xref>). Functional coupling between these regions during learning supports the initial formation of structured knowledge and predicts subsequent memory (<xref ref-type="bibr" rid="bib66">Preston and Eichenbaum, 2013</xref>; <xref ref-type="bibr" rid="bib84">Tse et al., 2007</xref>; <xref ref-type="bibr" rid="bib85">Tse et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">Ranganath et al., 2005</xref>; <xref ref-type="bibr" rid="bib77">Siapas et al., 2005</xref>). Over time, vmPFC abstracts and represents commonalities across episodes (<xref ref-type="bibr" rid="bib83">Tompary and Davachi, 2017</xref>; <xref ref-type="bibr" rid="bib100">Zeithamova and Preston, 2010</xref>; <xref ref-type="bibr" rid="bib42">Kumaran et al., 2009</xref>), leading some to propose that structured representations are stored in this region (<xref ref-type="bibr" rid="bib103">Zheng et al., 2021</xref>; <xref ref-type="bibr" rid="bib28">Frankland and Bontempi, 2005</xref>; <xref ref-type="bibr" rid="bib79">Takashima et al., 2006</xref>; <xref ref-type="bibr" rid="bib80">Takehara-Nishiuchi and McNaughton, 2008</xref>). Given the learning-related changes observed in the hippocampus, we conducted an exploratory analysis in vmPFC, hypothesizing that vmPFC would demonstrate a similar representational similarity structure. To test this, we first asked whether vmPFC pattern similarity for stimuli located on the same vs. different tracks significantly differed from that in the hippocampus by running a complete model that included data from both regions. Here, we found no interactions between region, scan, and context (<xref ref-type="table" rid="app1table29">Appendix 1—table 29</xref>), indicating that the similarity structure was not significantly different between regions. By contrast, pattern similarity in both the hippocampus and vmPFC differed from that of a visual control region (see Control Analyses, <xref ref-type="table" rid="app1table30">Appendix 1—table 30</xref>). However, models predicting pattern similarity for stimuli located on the same vs. different tracks in vmPFC revealed no interactions between context and scan (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>; <xref ref-type="table" rid="app1table31">Appendix 1—table 31</xref>).</p><p>Next, we asked whether vmPFC pattern similarity for landmarks at different distances significantly differed from the hippocampus by running a complete model that included data from both regions. Here, we found no effect of region or interactions between region, scan, and distance (<xref ref-type="table" rid="app1table32">Appendix 1—table 32</xref>), indicating that the similarity structure was not significantly different between regions. By contrast, pattern similarity in both the hippocampus and vmPFC differed from that of a visual control region (see Control Analyses; <xref ref-type="table" rid="app1table33">Appendix 1—table 33</xref>). However, in contrast to hippocampus, models predicting pattern similarity for landmarks at different link distances in vmPFC revealed no interactions between link distance and scan for within-track (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6a</xref>; <xref ref-type="table" rid="app1table34">Appendix 1—table 34</xref>) or across-track landmarks (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6b</xref>; <xref ref-type="table" rid="app1table35">Appendix 1—table 35</xref>). We observed no significant interactions when similar models were run for fractal stimuli (<xref ref-type="table" rid="app1table36 app1table37">Appendix 1—Tables 36 and 37</xref>).</p><p>While context- and distance-related effects on vmPFC and hippocampal pattern similarity did not significantly differ, such effects were significant in hippocampus but not in vmPFC. To further test whether distance-related similarity structures were similar between the regions, we modeled the relationship between vmPFC pattern similarity and hippocampal pattern similarity. Specifically, we ran a linear mixed-effects model predicting pairwise similarity values in vmPFC, with scan session, pairwise similarity values in the hippocampus (averaged across hemispheres) and pairwise similarity values in a visual control region as predictors. Here, we observed significant interactions between scan session and pairwise similarity values in the hippocampus Post Local (Day 2 &gt; Day 1 × hippocampal pattern similarity, <italic>β</italic> = –0.094 ± 0.017; t = –5.713; p &lt; 1.12e<sup>–8</sup>, survived FDR correction; d = 1.25) and Global Navigation (Day 3 &gt; Day 1 × hippocampal pattern similarity, <italic>β</italic> = –0.076 ± 0.017; t = –4.355; p &lt; 1.35e<sup>–5</sup>, survived FDR correction; d = 0.95), but not in the visual control region (Day 2 &gt; Day 1 × calcarine pattern similarity, <italic>β</italic> = 0.019 ± 0.013; t = 1.427; p = 0.154; Day 3 &gt; Day 1 × calcarine pattern similarity, <italic>β</italic> = –0.020 ± 0.014; t = –1.433; p = 0.152; <xref ref-type="table" rid="app1table38">Appendix 1—table 38</xref>). This pattern of findings suggests that functional connectivity between the hippocampus and vmPFC weakens over time.</p></sec><sec sec-type="appendix" id="s9"><title>Control analyses</title><p>All linear mixed-effects models were fit to data from a visual control region defined as a conjunction of FreeSurfer’s pericalcarine and calcarine sulcus regions in both hemispheres.</p><p>We first tested whether context effects differed between the visual control region, the hippocampus, EC, and vmPFC by running a complete model predicting neural pattern similarity, with scan session (Pre-Learning/Day 1, Post Local Navigation/Day 2, and Post Global Navigation/Day 3), context (same path and different paths), and region (calcarine, hippocampus, EC, and vmPFC) as predictors. Region was dummy-coded with the visual control region (calcarine) serving as the baseline. Here we found main effects (hippocampus: <italic>β</italic> = –0.50 ± 0.003, t = –176.735, p &lt; 2e<sup>–16</sup>, survived FDR correction, d = 38.51; EC: <italic>β</italic> = –0.513 ± 0.003, t = –181.929, p &lt; 2e<sup>–16</sup>, survived FDR correction, d = 39.70; vmPFC: <italic>β</italic> = –0.480 ± 0.003, t = –144.553, p &lt; 2e<sup>–16</sup>, survived FDR correction, d = 31.54) and region × scan session interactions for all regions (hippocampus × Day 2 &gt; Day 1, <italic>β</italic> = 0.027 ± 0.004, t = 7.233, p &lt; 4.75e<sup>–13</sup>, survived FDR correction, d = 1.51; hippocampus × Day 3 &gt; Day 1, <italic>β</italic> = 0.030 ± 0.004, t = 7.673, p &lt; 1.69e<sup>–14</sup>, survived FDR correction, d = 1.67; EC × Day 2 &gt; Day 1, <italic>β</italic> = 0.011 ± 0.004, t = 3.024, p = 0.003, survived FDR correction, d = 0.63; EC × Day 3 &gt; Day 1, <italic>β</italic> = 0.039 ± 0.004, t = 9.957, p &lt; 2e<sup>–16</sup>, survived FDR correction, d = 2.17; vmPFC × Day 2 &gt; Day 1, <italic>β</italic> = 0.030 ± 0.004, t = 7.029, p &lt; 2.10e<sup>–12</sup>, survived FDR correction, d = 1.47; vmPFC × Day 3 &gt; Day 1, <italic>β</italic> = 0.012 ± 0.004, t = 2.633, p = 0.008, d = 0.57; <xref ref-type="table" rid="app1table30">Appendix 1—table 30</xref>).</p><p>We then fit a linear mixed-effects model predicting neural pattern similarity to data from the visual control region. Scan session (Pre-Learning/Day 1, Post Local Navigation/Day 2, and Post Global Navigation/Day 3), stimulus type (landmarks and fractals), and context (same track and different tracks) were included as predictors. When the context model was fit to data from the control region, we found no interactions between context and scan session (Day 2 &gt; Day 1 × context, <italic>β</italic> = –0.003 ± 0.005; t = –0.717; p = 0.474; Day 3 &gt; Day 1 × context, <italic>β</italic> = –0.002 ± 0.005; t = –0.416; p = 0.678) or context, stimulus type, and scan session (Day 2 &gt; Day 1 × stimulus type × context, <italic>β</italic> = –0.004 ± 0.009; t = –0.377; p = 0.706; Day 3 &gt; Day 1 × stimulus type × context, <italic>β</italic> = 0.014 ± 0.01; t = 1.509; p = 0.131; <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>; <xref ref-type="table" rid="app1table39">Appendix 1—table 39</xref>).</p><p>We next tested whether distance effects differed between the visual control region, the hippocampus, and vmPFC by running a complete model predicting neural pattern similarity for landmarks, with scan session (Pre-Learning/Day 1, Post Local Navigation/Day 2, and Post Global Navigation/Day 3), link distance, and region (calcarine, hippocampus, and vmPFC) as predictors. Region was dummy-coded with the visual control region (calcarine) serving as the baseline. Here we found main effects (hippocampus: <italic>β</italic> = –0.506 ± 0.011, t = –46.889, p &lt; 2e<sup>–16</sup>, survived FDR correction, d = 9.78; vmPFC: <italic>β</italic> = –0.483 ± 0.013, t = –38.294, p &lt; 2e<sup>–16</sup>, survived FDR correction, d = 7.98) and region × scan session interactions for both regions (hippocampus × Day 2 &gt; Day 1, <italic>β</italic> = 0.077 ± 0.015, t = 5.102, p &lt; 3.38e<sup>–7</sup>, survived FDR correction, d = 1.06, hippocampus × Day 3 &gt; Day 1, <italic>β</italic> = 0.047 ± 0.015, t = 3.079, p = 0.002, survived FDR correction, d = 0.67; vmPFC × Day 2 &gt; Day 1, <italic>β</italic> = 0.089 ± 0.017, t = 5.129, p &lt; 2.92e<sup>–7</sup>, survived FDR correction, d = 1.07; <xref ref-type="table" rid="app1table33">Appendix 1—table 33</xref>).</p><p>We fit a linear mixed-effects model predicting neural pattern similarity between landmarks on the same track to data from the visual control region. Scan session (Pre-Learning/Day 1, Post Local Navigation/Day 2, and Post Global Navigation/Day 3) and link distance (1 and 2) were included as predictors. When the local distance model was fit to data from the control region, we found no interactions between link distance and scan session (Day 2 &gt; Day 1 × distance, <italic>β</italic> = 0.007 ± 0.022; t = 0.304; p = 0.761; Day 3 &gt; Day 1 × distance, <italic>β</italic> = –0.035 ± 0.022; t = –1.57; p = 0.117, <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8a</xref>; <xref ref-type="table" rid="app1table40">Appendix 1—table 40</xref>). Finally, we fit a linear mixed-effects model predicting neural pattern similarity between landmarks on different tracks to data from the visual control region. Scan session (Pre-Learning/Day 1, Post Local Navigation/Day 2, and Post Global Navigation/Day 3) and link distance (2, 3, and 4) were included as predictors. When this distance model was fit to data from the control region, we found no interactions between link distance and scan session (Day 2 &gt; Day 1 × distance, <italic>β</italic> = –0.003 ± 0.01; t = –0.261; p &gt; 0.794; Day 3 &gt; Day 1 × distance, <italic>β</italic> = 0.007 ± 0.01; t = 0.727; p &gt; 0.468; <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8b</xref>; <xref ref-type="table" rid="app1table41">Appendix 1—table 41</xref>). These control analyses indicate that the effects observed in the hippocampus were not observed throughout the brain.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Trial structure for all navigation tasks.</title><p>Each behavioral run contained 10 navigation trials. At the start of each run, participants were placed at a location on the track and rotated 360 degrees (6 s). Trials then proceeded as follows: (1) a fractal cue appeared onscreen (1 s) indicating the goal to which the participant should navigate; (2) participants chose their heading direction and (3) navigated to the cued goal location, pressing the spacebar when they arrived; (4) feedback appeared onscreen revealing whether the participant was at the correct location and whether they had navigated via the shortest path (2 s); and (5) the camera panned down and a fixation cross appeared (1 s) before the next trial began. On learning trials, goal locations were marked by fractal images appearing on the track. On test trials, fractals were not visible on the track and participants had to rely on memory to navigate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Performance improves across the Global Navigation Task.</title><p>Individual participants’ performance on across-track navigation trials at the start (first four test runs; left) and end (last two test runs; center) of the task on Day 2, and at the end (last two test runs; right) of Day 3. (<bold>A</bold>) Percent of across-track trials where participants switched to the correct track. (<bold>B</bold>) Percent of across-track trials where participants navigated in the correct direction once switching (n = 21).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Hippocampal pattern similarity for landmark buildings on different tracks.</title><p>To visualize the relationship between pattern similarity, link distance, and path inefficiency, we split participants into two groups – More Efficient and Less Efficient – based on their median path inefficiency on across-track trials in the first four test runs of the Global Task on Day 2. Pattern similarity relationships did not differ between participants who are more or less efficient on the Global Navigation Task, (<bold>A</bold>) Pre-Learning (Day 1) and (<bold>B</bold>) Post Global Navigation (Day 3). (Error bars denote SE of the estimates. More Efficient, n = 11; Less Efficient, n = 10).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig3-v1.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Hippocampal pattern similarity (Post Local Navigation) relates to trial-level performance on the subsequent Global Navigation Task.</title><p>We observed trend-level evidence that greater hippocampal pattern similarity predicted more efficient paths at the start of Global Navigation for both trial types. (Solid lines = estimated linear fit to the data, gray = 95% CI).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig4-v1.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Contrast estimates from a model in vmPFC that includes both landmark and fractal stimuli.</title><p>Interactions between scan session and context were not significant. (Error bars denote SE of the estimates. Day 2 &gt; Day 1, n = 23; Day 3 &gt; Day 1, n = 21).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig5-v1.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Pattern similarity for landmark buildings at different link distances in vmPFC.</title><p>(<bold>A</bold>) Pattern similarity for landmarks on the same track Pre-Learning (left), after the Local Navigation Task (center), and after the Global Navigation Task (right). Interactions between link distance and scan session were not significant. (<bold>B</bold>) Pattern similarity for landmarks on different tracks. Interactions between link distance and scan session were not significant. (Error bars denote SE of the estimates. Day 2 &gt; Day 1, n = 23; Day 3 &gt; Day 1, n = 21).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig6-v1.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Contrast estimates for context models fit to data in a visual region serving as a control (calcarine).</title><p>Within - across context similarity for landmark buildings. Interactions between scan session and context were not significant. (Error bars denote SE of the estimates. Day 2 &gt; Day 1, n = 23; Day 3 &gt; Day 1, n = 21).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig7-v1.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Pattern similarity for landmark buildings at different link distances in a visual region serving as a control (calcarine).</title><p>(<bold>A</bold>) Pattern similarity for landmarks on the same track Pre-Learning (left), after the Local Navigation Task (center), and after the Global Navigation Task (right). Interactions between link distance and scan session were not significant. (<bold>B</bold>) Pattern similarity for landmarks on different tracks. Interactions between link distance and scan session were not significant (Error bars denote SE of the estimates. Day 2 &gt; Day 1, n = 23; Day 3 &gt; Day 1, n = 21).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80281-app1-fig8-v1.tif"/></fig><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Linear mixed-effects model results for an omnibus context model predicting neural pattern similarity, fit to data in the hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 18 tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.035</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">3.858</td><td align="left" valign="bottom"><bold>8.11e<sup>–4</sup></bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.136</td><td align="left" valign="bottom">0.892</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.006</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">–0.542</td><td align="left" valign="bottom">0.588</td></tr><tr><td align="left" valign="bottom">stimulus type (landmark &gt; fractal)</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">2.422</td><td align="left" valign="bottom"><bold>0.015</bold></td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–0.254</td><td align="left" valign="bottom">0.800</td></tr><tr><td align="left" valign="bottom">hemisphere</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">2.756</td><td align="left" valign="bottom"><bold>0.006*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–0.715</td><td align="left" valign="bottom">0.475</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type</td><td align="char" char="." valign="bottom">–0.011</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–2.604</td><td align="left" valign="bottom"><bold>0.009*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–0.365</td><td align="left" valign="bottom">0.715</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–0.334</td><td align="left" valign="bottom">0.739</td></tr><tr><td align="left" valign="bottom">stimulus type × context</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.246</td><td align="left" valign="bottom">0.806</td></tr><tr><td align="left" valign="bottom">Day 2 × hemisphere</td><td align="char" char="." valign="bottom">–0.011</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–3.477</td><td align="left" valign="bottom"><bold>5.08e<sup>–4</sup>*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × hemisphere</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">3.795</td><td align="left" valign="bottom"><bold>1.48e<sup>–4</sup>*</bold></td></tr><tr><td align="left" valign="bottom">stimulus type × hemisphere</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.866</td><td align="left" valign="bottom">0.387</td></tr><tr><td align="left" valign="bottom">context × hemisphere</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">1.066</td><td align="left" valign="bottom">0.287</td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type × context</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">–0.216</td><td align="left" valign="bottom">0.829</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type × context</td><td align="char" char="." valign="bottom">–0.008</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">–0.920</td><td align="left" valign="bottom">0.357</td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type × hemisphere</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–0.320</td><td align="left" valign="bottom">0.749</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type × hemisphere</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–0.192</td><td align="left" valign="bottom">0.848</td></tr><tr><td align="left" valign="bottom">Day 2 × context × hemisphere</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–0.899</td><td align="left" valign="bottom">0.369</td></tr><tr><td align="left" valign="bottom">Day 3 × context × hemisphere</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–0.767</td><td align="left" valign="bottom">0.443</td></tr><tr><td align="left" valign="bottom">stimulus type × context × hemisphere</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">0.497</td><td align="left" valign="bottom">0.619</td></tr><tr><td align="left" valign="bottom">Day 2×stimulus type×context × hemisphere</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">–0.322</td><td align="left" valign="bottom">0.747</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type × context × hemisphere</td><td align="char" char="." valign="bottom">–0.006</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">–0.511</td><td align="left" valign="bottom">0.610</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity for landmark buildings, fit to data in left hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>6</underline> tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.051</td><td align="char" char="." valign="bottom">0.049</td><td align="char" char="." valign="bottom">1.043</td><td align="char" char="." valign="bottom">0.297</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.058</td><td align="char" char="." valign="bottom">0.954</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.011</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">–0.686</td><td align="char" char="." valign="bottom">0.493</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.998</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–0.400</td><td align="char" char="." valign="bottom">0.689</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.006</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–0.834</td><td align="char" char="." valign="bottom">0.404</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity for landmark buildings, fit to data in right hippocampus.</title><p>SE = standard error. No correction for multiple comparisons was applied. To correct for multiple comparisons, we adjusted the α-value for 16 tests. No findings survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.062</td><td align="char" char="." valign="bottom">0.049</td><td align="char" char="." valign="bottom">1.260</td><td align="char" char="." valign="bottom">0.208</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.011</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–0.747</td><td align="char" char="." valign="bottom">0.455</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">–0.099</td><td align="char" char="." valign="bottom">0.921</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">1.385</td><td align="char" char="." valign="bottom">0.166</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.010</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–1.502</td><td align="char" char="." valign="bottom">0.133</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.013</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–2.014</td><td align="char" char="." valign="bottom"><bold>0.044</bold></td></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity for fractals, fit to data in left hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>6</underline> tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">0.945</td><td align="char" char="." valign="bottom">0.345</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">0.262</td><td align="char" char="." valign="bottom">0.794</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">–0.150</td><td align="char" char="." valign="bottom">0.881</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–0.363</td><td align="char" char="." valign="bottom">0.717</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">–0.111</td><td align="char" char="." valign="bottom">0.911</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.472</td><td align="char" char="." valign="bottom">0.637</td></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity for fractals, fit to data in right hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>6</underline> tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.100</td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">3.398</td><td align="left" valign="bottom"><bold>6.89e<sup>–4</sup></bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">–0.268</td><td align="left" valign="bottom">0.789</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.817</td><td align="left" valign="bottom">0.414</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.326</td><td align="left" valign="bottom">0.745</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">–0.823</td><td align="left" valign="bottom">0.411</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.166</td><td align="left" valign="bottom">0.868</td></tr></tbody></table></table-wrap><table-wrap id="app1table6" position="float"><label>Appendix 1—table 6.</label><caption><title>Linear mixed-effects model results for an omnibus context model predicting neural pattern similarity, fit to data in EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 18 tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.049</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">2.912</td><td align="left" valign="bottom"><bold>0.004</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.037</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">–1.874</td><td align="left" valign="bottom">0.061</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.023</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">–1.178</td><td align="left" valign="bottom">0.239</td></tr><tr><td align="left" valign="bottom">stimulus type (landmark &gt; fractal)</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">1.607</td><td align="left" valign="bottom">0.108</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.461</td><td align="left" valign="bottom">0.645</td></tr><tr><td align="left" valign="bottom">hemisphere</td><td align="char" char="." valign="bottom">–0.022</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–6.060</td><td align="left" valign="bottom"><bold>1.37e<sup>–9</sup>*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">1.457</td><td align="left" valign="bottom">0.145</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–0.358</td><td align="left" valign="bottom">0.720</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.014</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–2.046</td><td align="left" valign="bottom"><bold>0.041</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.010</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–1.394</td><td align="left" valign="bottom">0.163</td></tr><tr><td align="left" valign="bottom">stimulus type × context</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–0.139</td><td align="left" valign="bottom">0.889</td></tr><tr><td align="left" valign="bottom">Day 2 × hemisphere</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">4.192</td><td align="left" valign="bottom"><bold>2.77e<sup>–5</sup>*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × hemisphere</td><td align="char" char="." valign="bottom">0.056</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">10.552</td><td align="left" valign="bottom"><bold>&lt;2e<sup>–16</sup>*</bold></td></tr><tr><td align="left" valign="bottom">stimulus type × hemisphere</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.036</td><td align="left" valign="bottom">0.971</td></tr><tr><td align="left" valign="bottom">context × hemisphere</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.524</td><td align="left" valign="bottom">0.600</td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type × context</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.132</td><td align="left" valign="bottom">0.895</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type × context</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–0.387</td><td align="left" valign="bottom">0.699</td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type × hemisphere</td><td align="char" char="." valign="bottom">–0.011</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–1.132</td><td align="left" valign="bottom">0.258</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type × hemisphere</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–0.319</td><td align="left" valign="bottom">0.749</td></tr><tr><td align="left" valign="bottom">Day 2 × context × hemisphere</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">0.263</td><td align="left" valign="bottom">0.793</td></tr><tr><td align="left" valign="bottom">Day 3 × context × hemisphere</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">0.117</td><td align="left" valign="bottom">0.907</td></tr><tr><td align="left" valign="bottom">stimulus type × context × hemisphere</td><td align="char" char="." valign="bottom">–0.009</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–0.633</td><td align="left" valign="bottom">0.527</td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type × context × hemisphere</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">–0.050</td><td align="left" valign="bottom">0.960</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type×context × hemisphere</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">1.057</td><td align="left" valign="bottom">0.291</td></tr></tbody></table></table-wrap><table-wrap id="app1table7" position="float"><label>Appendix 1—table 7.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity, fit to data in left EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>6</underline> tests. No findings survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.082</td><td align="char" char="." valign="bottom">0.040</td><td align="char" char="." valign="bottom">2.025</td><td align="char" char="." valign="bottom"><bold>0.043</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.036</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">–1.490</td><td align="char" char="." valign="bottom">0.136</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.026</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">–1.223</td><td align="char" char="." valign="bottom">0.221</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.656</td><td align="char" char="." valign="bottom">0.512</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.014</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–2.339</td><td align="char" char="." valign="bottom"><bold>0.019</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.009</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–1.457</td><td align="char" char="." valign="bottom">0.145</td></tr></tbody></table></table-wrap><table-wrap id="app1table8" position="float"><label>Appendix 1—table 8.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity, fit to data in right EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>6</underline> tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.067</td><td align="char" char="." valign="bottom">0.049</td><td align="char" char="." valign="bottom">1.362</td><td align="char" char="." valign="bottom">0.173</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.071</td><td align="char" char="." valign="bottom">0.943</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.044</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">1.836</td><td align="char" char="." valign="bottom">0.066</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">1.455</td><td align="char" char="." valign="bottom">0.146</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.012</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–1.568</td><td align="char" char="." valign="bottom">0.117</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.011</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">–1.404</td><td align="char" char="." valign="bottom">0.160</td></tr></tbody></table></table-wrap><table-wrap id="app1table9" position="float"><label>Appendix 1—table 9.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity that included EC and hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 5 tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom">t</th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.074</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">3.623</td><td align="left" valign="bottom"><bold>3.66e<sup>–4</sup></bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.012</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">–0.883</td><td align="left" valign="bottom">0.377</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.010</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">–0.780</td><td align="left" valign="bottom">0.436</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.473</td><td align="left" valign="bottom">0.636</td></tr><tr><td align="left" valign="bottom">region</td><td align="char" char="." valign="bottom">–0.015</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">–7.360</td><td align="left" valign="bottom"><bold>1.85e<sup>–13</sup>*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–0.956</td><td align="left" valign="bottom">0.339</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–0.594</td><td align="left" valign="bottom">0.553</td></tr><tr><td align="left" valign="bottom">Day 2 × region</td><td align="char" char="." valign="bottom">–0.017</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–5.757</td><td align="left" valign="bottom"><bold>8.58e<sup>–9</sup>*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × region</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">4.592</td><td align="left" valign="bottom"><bold>4.40e<sup>–6</sup>*</bold></td></tr><tr><td align="left" valign="bottom">context × region</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.907</td><td align="left" valign="bottom">0.364</td></tr><tr><td align="left" valign="bottom">Day 2 × context × region</td><td align="char" char="." valign="bottom">–0.009</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–1.635</td><td align="left" valign="bottom">0.102</td></tr><tr><td align="left" valign="bottom">Day 3 × context × region</td><td align="char" char="." valign="bottom">–0.007</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–1.278</td><td align="left" valign="bottom">0.201</td></tr></tbody></table></table-wrap><table-wrap id="app1table10" position="float"><label>Appendix 1—table 10.</label><caption><title>Linear mixed-effects model results for an omnibus model of local (within-track) distance, predicting neural pattern similarity for landmark buildings in the hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 5 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.044</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">3.621</td><td align="left" valign="bottom"><bold>9.12e<sup>–4</sup></bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.016</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–1.041</td><td align="left" valign="bottom">0.298</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.020</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">–1.236</td><td align="left" valign="bottom">0.217</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">–0.014</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–0.974</td><td align="left" valign="bottom">0.330</td></tr><tr><td align="left" valign="bottom">hemisphere</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.528</td><td align="left" valign="bottom">0.598</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.036</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">1.721</td><td align="left" valign="bottom">0.085</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.033</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">1.520</td><td align="left" valign="bottom">0.129</td></tr><tr><td align="left" valign="bottom">Day 2 × hemisphere</td><td align="char" char="." valign="bottom">–0.011</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–0.723</td><td align="left" valign="bottom">0.470</td></tr><tr><td align="left" valign="bottom">Day 3 × hemisphere</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">0.288</td><td align="left" valign="bottom">0.774</td></tr><tr><td align="left" valign="bottom">distance × hemisphere</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">–0.124</td><td align="left" valign="bottom">0.901</td></tr><tr><td align="left" valign="bottom">Day 2 × distance × hemisphere</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">0.280</td><td align="left" valign="bottom">0.780</td></tr><tr><td align="left" valign="bottom">Day 3 × distance × hemisphere</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">0.121</td><td align="left" valign="bottom">0.904</td></tr></tbody></table></table-wrap><table-wrap id="app1table11" position="float"><label>Appendix 1—table 11.</label><caption><title>Linear mixed-effects model results in a model of local (within-track) distance, predicting neural pattern similarity for landmarks buildings in the hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests. No results survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">–0.037</td><td align="char" char="." valign="bottom">0.088</td><td align="char" char="." valign="bottom">–0.419</td><td align="char" char="." valign="bottom">0.675</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.021</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">–1.597</td><td align="char" char="." valign="bottom">0.110</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.018</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–1.247</td><td align="char" char="." valign="bottom">0.213</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">–0.016</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–1.510</td><td align="char" char="." valign="bottom">0.131</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.040</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">2.700</td><td align="char" char="." valign="bottom"><bold>0.007</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.034</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">2.259</td><td align="char" char="." valign="bottom"><bold>0.024</bold></td></tr></tbody></table></table-wrap><table-wrap id="app1table12" position="float"><label>Appendix 1—table 12.</label><caption><title>Linear mixed-effects model results in a model of local (within-track) distance, predicting neural pattern similarity for fractals in the hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.038</td><td align="char" char="." valign="bottom">0.047</td><td align="char" char="." valign="bottom">0.802</td><td align="char" char="." valign="bottom">0.422</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">0.068</td><td align="char" char="." valign="bottom">0.946</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">0.456</td><td align="char" char="." valign="bottom">0.653</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.488</td><td align="char" char="." valign="bottom">0.625</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">–0.368</td><td align="char" char="." valign="bottom">0.713</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">–0.338</td><td align="char" char="." valign="bottom">0.735</td></tr></tbody></table></table-wrap><table-wrap id="app1table13" position="float"><label>Appendix 1—table 13.</label><caption><title>Linear mixed-effects model results in an omnibus model of global (across-track) distance, predicting neural pattern similarity for landmark buildings in the hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 5 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.033</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">2.023</td><td align="char" char="." valign="bottom"><bold>0.043</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.018</td><td align="char" char="." valign="bottom">0.023</td><td align="char" char="." valign="bottom">0.779</td><td align="char" char="." valign="bottom">0.436</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.010</td><td align="char" char="." valign="bottom">0.023</td><td align="char" char="." valign="bottom">–0.445</td><td align="char" char="." valign="bottom">0.657</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.369</td><td align="char" char="." valign="bottom">0.712</td></tr><tr><td align="left" valign="bottom">hemisphere</td><td align="char" char="." valign="bottom">–0.012</td><td align="char" char="." valign="bottom">0.018</td><td align="char" char="." valign="bottom">–0.659</td><td align="char" char="." valign="bottom">0.510</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.012</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–1.297</td><td align="char" char="." valign="bottom">0.195</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">0.044</td><td align="char" char="." valign="bottom">0.965</td></tr><tr><td align="left" valign="bottom">Day 2 × hemisphere</td><td align="char" char="." valign="bottom">0.019</td><td align="char" char="." valign="bottom">0.025</td><td align="char" char="." valign="bottom">0.763</td><td align="char" char="." valign="bottom">0.445</td></tr><tr><td align="left" valign="bottom">Day 3 × hemisphere</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">0.026</td><td align="char" char="." valign="bottom">0.632</td><td align="char" char="." valign="bottom">0.527</td></tr><tr><td align="left" valign="bottom">distance × hemisphere</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">1.159</td><td align="char" char="." valign="bottom">0.246</td></tr><tr><td align="left" valign="bottom">Day 2 × distance × hemisphere</td><td align="char" char="." valign="bottom">–0.015</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">–1.082</td><td align="char" char="." valign="bottom">0.279</td></tr><tr><td align="left" valign="bottom">Day 3 × distance × hemisphere</td><td align="char" char="." valign="bottom">–0.007</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–0.485</td><td align="char" char="." valign="bottom">0.628</td></tr></tbody></table></table-wrap><table-wrap id="app1table14" position="float"><label>Appendix 1—table 14.</label><caption><title>Linear mixed-effects model results in a model of global (across-track) distance, predicting neural pattern similarity for landmark buildings in the hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 8 tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.076</td><td align="char" char="." valign="bottom">0.064</td><td align="char" char="." valign="bottom">1.181</td><td align="char" char="." valign="bottom">0.238</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.057</td><td align="char" char="." valign="bottom">0.027</td><td align="char" char="." valign="bottom">2.126</td><td align="char" char="." valign="bottom"><bold>0.034</bold></td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.027</td><td align="char" char="." valign="bottom">0.077</td><td align="char" char="." valign="bottom">0.939</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">1.567</td><td align="char" char="." valign="bottom">0.117</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.020</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–2.891</td><td align="char" char="." valign="bottom"><bold>0.004*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–0.413</td><td align="char" char="." valign="bottom">0.679</td></tr></tbody></table></table-wrap><table-wrap id="app1table15" position="float"><label>Appendix 1—table 15.</label><caption><title>Linear mixed-effects model results in a model of global (across-track) distance, predicting neural pattern similarity for fractals in the hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 8 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.032</td><td align="char" char="." valign="bottom">0.432</td><td align="char" char="." valign="bottom">0.666</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">–0.412</td><td align="char" char="." valign="bottom">0.682</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–0.192</td><td align="char" char="." valign="bottom">0.849</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">–0.944</td><td align="char" char="." valign="bottom">0.345</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">1.471</td><td align="char" char="." valign="bottom">0.141</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">1.081</td><td align="char" char="." valign="bottom">0.280</td></tr></tbody></table></table-wrap><table-wrap id="app1table16" position="float"><label>Appendix 1—table 16.</label><caption><title>Linear mixed-effects model results in an omnibus model of local (within-track) distance, predicting neural pattern similarity for landmark buildings in EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 5 tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.053</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">2.521</td><td align="char" char="." valign="bottom"><bold>0.012</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.057</td><td align="char" char="." valign="bottom">0.023</td><td align="char" char="." valign="bottom">–2.503</td><td align="char" char="." valign="bottom"><bold>0.012</bold></td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.040</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">–1.662</td><td align="char" char="." valign="bottom">0.097</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">0.475</td><td align="char" char="." valign="bottom">0.635</td></tr><tr><td align="left" valign="bottom">hemisphere</td><td align="char" char="." valign="bottom">–0.030</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">–1.775</td><td align="char" char="." valign="bottom">0.076</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.033</td><td align="char" char="." valign="bottom">0.191</td><td align="char" char="." valign="bottom">0.849</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.034</td><td align="char" char="." valign="bottom">0.226</td><td align="char" char="." valign="bottom">0.821</td></tr><tr><td align="left" valign="bottom">Day 2 × hemisphere</td><td align="char" char="." valign="bottom">0.050</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">2.093</td><td align="char" char="." valign="bottom"><bold>0.036</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × hemisphere</td><td align="char" char="." valign="bottom">0.069</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">2.835</td><td align="char" char="." valign="bottom"><bold>0.005*</bold></td></tr><tr><td align="left" valign="bottom">distance × hemisphere</td><td align="char" char="." valign="bottom">–0.034</td><td align="char" char="." valign="bottom">0.033</td><td align="char" char="." valign="bottom">–1.031</td><td align="char" char="." valign="bottom">0.303</td></tr><tr><td align="left" valign="bottom">Day 2 × distance × hemisphere</td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">0.047</td><td align="char" char="." valign="bottom">0.620</td><td align="char" char="." valign="bottom">0.535</td></tr><tr><td align="left" valign="bottom">Day 3 × distance × hemisphere</td><td align="char" char="." valign="bottom">0.026</td><td align="char" char="." valign="bottom">0.048</td><td align="char" char="." valign="bottom">0.545</td><td align="char" char="." valign="bottom">0.586</td></tr></tbody></table></table-wrap><table-wrap id="app1table17" position="float"><label>Appendix 1—table 17.</label><caption><title>Linear mixed-effects model results in a model of local (within-track) distance, predicting neural pattern similarity for landmark buildings in left EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.109</td><td align="char" char="." valign="bottom">0.180</td><td align="char" char="." valign="bottom">0.604</td><td align="char" char="." valign="bottom">0.546</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.057</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">–2.529</td><td align="char" char="." valign="bottom"><bold>0.012</bold></td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.043</td><td align="char" char="." valign="bottom">0.023</td><td align="char" char="." valign="bottom">–1.920</td><td align="char" char="." valign="bottom">0.055</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">0.508</td><td align="char" char="." valign="bottom">0.611</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">0.238</td><td align="char" char="." valign="bottom">0.812</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.031</td><td align="char" char="." valign="bottom">0.232</td><td align="char" char="." valign="bottom">0.816</td></tr></tbody></table></table-wrap><table-wrap id="app1table18" position="float"><label>Appendix 1—table 18.</label><caption><title>Linear mixed-effects model results in a model of local (within-track) distance, predicting neural pattern similarity for landmark buildings in right EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">–0.410</td><td align="char" char="." valign="bottom">0.213</td><td align="char" char="." valign="bottom">–1.921</td><td align="char" char="." valign="bottom">0.055</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">–0.060</td><td align="char" char="." valign="bottom">0.952</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">0.028</td><td align="char" char="." valign="bottom">1.056</td><td align="char" char="." valign="bottom">0.291</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">–0.025</td><td align="char" char="." valign="bottom">0.025</td><td align="char" char="." valign="bottom">–0.977</td><td align="char" char="." valign="bottom">0.329</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.034</td><td align="char" char="." valign="bottom">0.036</td><td align="char" char="." valign="bottom">0.937</td><td align="char" char="." valign="bottom">0.349</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.038</td><td align="char" char="." valign="bottom">0.037</td><td align="char" char="." valign="bottom">1.043</td><td align="char" char="." valign="bottom">0.297</td></tr></tbody></table></table-wrap><table-wrap id="app1table19" position="float"><label>Appendix 1—table 19.</label><caption><title>Linear mixed-effects model results in an omnibus model of global (across-track) distance, predicting neural pattern similarity for landmark buildings in EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 5 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.062</td><td align="char" char="." valign="bottom">0.026</td><td align="char" char="." valign="bottom">2.393</td><td align="char" char="." valign="bottom"><bold>0.017</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.036</td><td align="char" char="." valign="bottom">0.091</td><td align="char" char="." valign="bottom">0.928</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.036</td><td align="char" char="." valign="bottom">0.035</td><td align="char" char="." valign="bottom">–1.042</td><td align="char" char="." valign="bottom">0.297</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.194</td><td align="char" char="." valign="bottom">0.846</td></tr><tr><td align="left" valign="bottom">hemisphere</td><td align="char" char="." valign="bottom">–0.031</td><td align="char" char="." valign="bottom">0.028</td><td align="char" char="." valign="bottom">–1.096</td><td align="char" char="." valign="bottom">0.273</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.024</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–1.555</td><td align="char" char="." valign="bottom">0.120</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">0.084</td><td align="char" char="." valign="bottom">0.933</td></tr><tr><td align="left" valign="bottom">Day 2 × hemisphere</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.040</td><td align="char" char="." valign="bottom">0.208</td><td align="char" char="." valign="bottom">0.835</td></tr><tr><td align="left" valign="bottom">Day 3 × hemisphere</td><td align="char" char="." valign="bottom">0.055</td><td align="char" char="." valign="bottom">0.041</td><td align="char" char="." valign="bottom">1.336</td><td align="char" char="." valign="bottom">0.182</td></tr><tr><td align="left" valign="bottom">distance × hemisphere</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">0.069</td><td align="char" char="." valign="bottom">0.945</td></tr><tr><td align="left" valign="bottom">Day 2 × distance × hemisphere</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">0.614</td><td align="char" char="." valign="bottom">0.540</td></tr><tr><td align="left" valign="bottom">Day 3 × distance × hemisphere</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.139</td><td align="char" char="." valign="bottom">0.889</td></tr></tbody></table></table-wrap><table-wrap id="app1table20" position="float"><label>Appendix 1—table 20.</label><caption><title>Linear mixed-effects model results in a model of global (across-track) distance, predicting neural pattern similarity for landmark buildings in EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 8 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.139</td><td align="char" char="." valign="bottom">0.102</td><td align="char" char="." valign="bottom">1.361</td><td align="char" char="." valign="bottom">0.174</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.034</td><td align="char" char="." valign="bottom">0.042</td><td align="char" char="." valign="bottom">0.812</td><td align="char" char="." valign="bottom">0.417</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.014</td><td align="char" char="." valign="bottom">0.041</td><td align="char" char="." valign="bottom">–0.328</td><td align="char" char="." valign="bottom">0.743</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.201</td><td align="char" char="." valign="bottom">0.841</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.017</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">–1.607</td><td align="char" char="." valign="bottom">0.108</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">0.260</td><td align="char" char="." valign="bottom">0.795</td></tr></tbody></table></table-wrap><table-wrap id="app1table21" position="float"><label>Appendix 1—table 21.</label><caption><title>Linear mixed-effects model results in a model of local (within-track) distance, predicting neural pattern similarity for fractals in left EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.094</td><td align="char" char="." valign="bottom">0.098</td><td align="char" char="." valign="bottom">0.965</td><td align="char" char="." valign="bottom">0.334</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.041</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">–1.664</td><td align="char" char="." valign="bottom">0.110</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.023</td><td align="char" char="." valign="bottom">0.027</td><td align="char" char="." valign="bottom">–0.851</td><td align="char" char="." valign="bottom">0.404</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">–0.015</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">–1.266</td><td align="char" char="." valign="bottom">0.205</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.007</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">–0.422</td><td align="char" char="." valign="bottom">0.673</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">0.946</td><td align="char" char="." valign="bottom">0.344</td></tr></tbody></table></table-wrap><table-wrap id="app1table22" position="float"><label>Appendix 1—table 22.</label><caption><title>Linear mixed-effects model results in a model of local (within-track) distance, predicting neural pattern similarity for fractals in right EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.116</td><td align="char" char="." valign="bottom">0.114</td><td align="char" char="." valign="bottom">1.021</td><td align="char" char="." valign="bottom">0.307</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.016</td><td align="char" char="." valign="bottom">0.018</td><td align="char" char="." valign="bottom">–0.919</td><td align="char" char="." valign="bottom">0.368</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.906</td><td align="char" char="." valign="bottom">0.374</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–0.253</td><td align="char" char="." valign="bottom">0.800</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.015</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">–0.769</td><td align="char" char="." valign="bottom">0.442</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">0.147</td><td align="char" char="." valign="bottom">0.883</td></tr></tbody></table></table-wrap><table-wrap id="app1table23" position="float"><label>Appendix 1—table 23.</label><caption><title>Linear mixed-effects model results in a model of global (across-track) distance, predicting neural pattern similarity for fractals in EC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 8 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.121</td><td align="char" char="." valign="bottom">0.053</td><td align="char" char="." valign="bottom">2.293</td><td align="char" char="." valign="bottom"><bold>0.022</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.034</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">–1.415</td><td align="char" char="." valign="bottom">0.165</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.026</td><td align="char" char="." valign="bottom">0.284</td><td align="char" char="." valign="bottom">0.778</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–1.102</td><td align="char" char="." valign="bottom">0.271</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">1.655</td><td align="char" char="." valign="bottom">0.098</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.778</td><td align="char" char="." valign="bottom">0.436</td></tr></tbody></table></table-wrap><table-wrap id="app1table24" position="float"><label>Appendix 1—table 24.</label><caption><title>Linear mixed-effects model results from a model predicting hippocampal pattern similarity Post Local Navigation (Day 2), with performance on subsequent Global Navigation trials (median path inefficiency for across-track trials in the first four test runs of Global Navigation on Day 2) and link distance as predictors.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 3 tests. No results survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.038</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">1.264</td><td align="char" char="." valign="bottom">0.208</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.814</td><td align="char" char="." valign="bottom">0.416</td></tr><tr><td align="left" valign="bottom">path inefficiency</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">1.174</td><td align="char" char="." valign="bottom">0.241</td></tr><tr><td align="left" valign="bottom">distance × path inefficiency</td><td align="char" char="." valign="bottom">–0.000</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">–1.983</td><td align="char" char="." valign="bottom"><bold>0.048</bold></td></tr></tbody></table></table-wrap><table-wrap id="app1table25" position="float"><label>Appendix 1—table 25.</label><caption><title>Linear mixed-effects model results from a model predicting hippocampal pattern similarity Pre-Learning (Day 1), with performance on subsequent Global Navigation trials (median path inefficiency for across-track trials in the first four test runs of Global Navigation on Day 2) and link distance as predictors.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 3 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.073</td><td align="char" char="." valign="bottom">0.026</td><td align="char" char="." valign="bottom">2.786</td><td align="char" char="." valign="bottom"><bold>0.006</bold></td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">–0.000</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–0.052</td><td align="char" char="." valign="bottom">0.959</td></tr><tr><td align="left" valign="bottom">path inefficiency</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">–1.825</td><td align="char" char="." valign="bottom">0.070</td></tr><tr><td align="left" valign="bottom">distance × path inefficiency</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">1.523</td><td align="char" char="." valign="bottom">0.128</td></tr></tbody></table></table-wrap><table-wrap id="app1table26" position="float"><label>Appendix 1—table 26.</label><caption><title>Linear mixed-effects model results from a model predicting hippocampal pattern similarity Post Global Navigation (Day 3), with performance on subsequent Global Navigation trials (median path inefficiency for across-track trials in the first four test runs of Global Navigation on Day 2) and link distance as predictors.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 3 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.056</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">1.847</td><td align="char" char="." valign="bottom">0.067</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–0.347</td><td align="char" char="." valign="bottom">0.729</td></tr><tr><td align="left" valign="bottom">path inefficiency</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">–1.068</td><td align="char" char="." valign="bottom">0.288</td></tr><tr><td align="left" valign="bottom">distance × path inefficiency</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">0.000</td><td align="char" char="." valign="bottom">1.379</td><td align="char" char="." valign="bottom">0.168</td></tr></tbody></table></table-wrap><table-wrap id="app1table27" position="float"><label>Appendix 1—table 27.</label><caption><title>Linear mixed-effects model results from a model predicting median path inefficiency across first four test runs of Global Navigation on Day 2, with hippocampal pattern similarity Post Local Navigation (Day 2) for landmark pairs and the length of the optimal path as predictors.</title><p>SE = standard error.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="left" valign="bottom">75.944</td><td align="left" valign="bottom">12.743</td><td align="left" valign="bottom">5.960</td><td align="left" valign="bottom"><bold>3.07e-8</bold></td></tr><tr><td align="left" valign="bottom">hippocampal pattern similarity (LM<sub>A</sub>, LM<sub>B</sub>)</td><td align="left" valign="bottom">–41.245</td><td align="left" valign="bottom">24.163</td><td align="left" valign="bottom">–1.707</td><td align="left" valign="bottom">0.088</td></tr><tr><td align="left" valign="bottom">length of optimal path</td><td align="left" valign="bottom">20.551</td><td align="left" valign="bottom">6.245</td><td align="left" valign="bottom">3.291</td><td align="left" valign="bottom"><bold>0.001</bold></td></tr><tr><td align="left" valign="bottom">trial type (within-track &gt; across-track)</td><td align="left" valign="bottom">–0.663</td><td align="left" valign="bottom">0.183</td><td align="left" valign="bottom">–3.631</td><td align="left" valign="bottom"><bold>0.0003</bold></td></tr><tr><td align="left" valign="bottom">hippocampal pattern similarity × length of optimal path</td><td align="left" valign="bottom">54.618</td><td align="left" valign="bottom">43.489</td><td align="left" valign="bottom">1.256</td><td align="left" valign="bottom">0.210</td></tr></tbody></table></table-wrap><table-wrap id="app1table28" position="float"><label>Appendix 1—table 28.</label><caption><title>Linear mixed-effects model results from a model predicting median path inefficiency across first four test runs of Global Navigation on Day 2, with hippocampal pattern similarity Post Local Navigation (Day 2) for fractal pairs and the length of the optimal path as predictors.</title><p>SE = standard error.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="left" valign="bottom">71.855</td><td align="left" valign="bottom">12.352</td><td align="left" valign="bottom">5.817</td><td align="left" valign="bottom"><bold>3.98e-8</bold></td></tr><tr><td align="left" valign="bottom">hippocampal pattern similarity (FR<sub>A</sub>, FR<sub>B</sub>)</td><td align="left" valign="bottom">3.026</td><td align="left" valign="bottom">27.687</td><td align="left" valign="bottom">0.109</td><td align="left" valign="bottom">0.913</td></tr><tr><td align="left" valign="bottom">length of optimal path</td><td align="left" valign="bottom">27.289</td><td align="left" valign="bottom">6.196</td><td align="left" valign="bottom">4.404</td><td align="left" valign="bottom"><bold>1.23e-5</bold></td></tr><tr><td align="left" valign="bottom">trial type (within-track &gt; across-track)</td><td align="left" valign="bottom">–0.646</td><td align="left" valign="bottom">0.184</td><td align="left" valign="bottom">–3.519</td><td align="left" valign="bottom"><bold>0.0005</bold></td></tr><tr><td align="left" valign="bottom">hippocampal pattern similarity × length of optimal path</td><td align="left" valign="bottom">–68.107</td><td align="left" valign="bottom">48.915</td><td align="left" valign="bottom">–1.392</td><td align="left" valign="bottom">0.164</td></tr></tbody></table></table-wrap><table-wrap id="app1table29" position="float"><label>Appendix 1—table 29.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity that included vmPFC and hippocampus.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 6 tests. No results survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.054</td><td align="char" char="." valign="bottom">0.018</td><td align="char" char="." valign="bottom">3.017</td><td align="char" char="." valign="bottom"><bold>0.003</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–0.528</td><td align="char" char="." valign="bottom">0.603</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">–0.239</td><td align="char" char="." valign="bottom">0.813</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.525</td><td align="char" char="." valign="bottom">0.600</td></tr><tr><td align="left" valign="bottom">region</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">–2.195</td><td align="char" char="." valign="bottom"><bold>0.028</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–1.156</td><td align="char" char="." valign="bottom">0.248</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–0.720</td><td align="char" char="." valign="bottom">0.471</td></tr><tr><td align="left" valign="bottom">Day 2 × region</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">1.761</td><td align="char" char="." valign="bottom">0.078</td></tr><tr><td align="left" valign="bottom">Day 3 × region</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–1.660</td><td align="char" char="." valign="bottom">0.097</td></tr><tr><td align="left" valign="bottom">context × region</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.386</td><td align="char" char="." valign="bottom">0.700</td></tr><tr><td align="left" valign="bottom">Day 2 × context × region</td><td align="char" char="." valign="bottom">–0.000</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–0.008</td><td align="char" char="." valign="bottom">0.993</td></tr><tr><td align="left" valign="bottom">Day 3 × context × region</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.845</td><td align="char" char="." valign="bottom">0.398</td></tr></tbody></table></table-wrap><table-wrap id="app1table30" position="float"><label>Appendix 1—table 30.</label><caption><title>Linear mixed-effects model results from a context model predicting neural pattern similarity that included the hippocampus, EC, vmPFC and a visual control region.</title><p>The visual control region served as a baseline. SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 18 tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.591</td><td align="char" char="." valign="bottom">0.019</td><td align="left" valign="bottom">30.662</td><td align="left" valign="bottom"><bold>&lt;2e-16</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.042</td><td align="char" char="." valign="bottom">0.014</td><td align="left" valign="bottom">–3.053</td><td align="left" valign="bottom"><bold>0.005</bold></td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.039</td><td align="char" char="." valign="bottom">0.016</td><td align="left" valign="bottom">–2.400</td><td align="left" valign="bottom"><bold>0.024</bold></td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">0.425</td><td align="left" valign="bottom">0.670</td></tr><tr><td align="left" valign="bottom">region (hippocampus)</td><td align="char" char="." valign="bottom">–0.499</td><td align="char" char="." valign="bottom">0.003</td><td align="left" valign="bottom">–176.735</td><td align="left" valign="bottom"><bold>&lt;2e-16*</bold></td></tr><tr><td align="left" valign="bottom">region (EC)</td><td align="char" char="." valign="bottom">–0.513</td><td align="char" char="." valign="bottom">0.003</td><td align="left" valign="bottom">–181.929</td><td align="left" valign="bottom"><bold>&lt;2e-16*</bold></td></tr><tr><td align="left" valign="bottom">region (vmPFC)</td><td align="char" char="." valign="bottom">–0.480</td><td align="char" char="." valign="bottom">0.003</td><td align="left" valign="bottom">–144.553</td><td align="left" valign="bottom"><bold>&lt;2e-16*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.006</td><td align="left" valign="bottom">–0.460</td><td align="left" valign="bottom">0.645</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.006</td><td align="left" valign="bottom">–0.587</td><td align="left" valign="bottom">0.557</td></tr><tr><td align="left" valign="bottom">Day 2 × region (hippocampus)</td><td align="char" char="." valign="bottom">0.027</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">7.233</td><td align="left" valign="bottom"><bold>4.74e-13*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × region (hippocampus)</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">7.673</td><td align="left" valign="bottom"><bold>1.68e-14*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × region (EC)</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">3.024</td><td align="left" valign="bottom"><bold>0.002*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × region (EC)</td><td align="char" char="." valign="bottom">0.039</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">9.957</td><td align="left" valign="bottom"><bold>&lt;2e-16*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × region (vmPFC)</td><td align="char" char="." valign="bottom">0.031</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">7.029</td><td align="left" valign="bottom"><bold>2.09e-12*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × region (vmPFC)</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">2.633</td><td align="left" valign="bottom"><bold>0.008*</bold></td></tr><tr><td align="left" valign="bottom">context × region (hippocampus)</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.005</td><td align="left" valign="bottom">–0.116</td><td align="left" valign="bottom">0.908</td></tr><tr><td align="left" valign="bottom">context × region (EC)</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.005</td><td align="left" valign="bottom">0.567</td><td align="left" valign="bottom">0.570</td></tr><tr><td align="left" valign="bottom">context × region (vmPFC)</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.006</td><td align="left" valign="bottom">0.156</td><td align="left" valign="bottom">0.876</td></tr><tr><td align="left" valign="bottom">Day 2 × context × region (hippocampus)</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.008</td><td align="left" valign="bottom">–0.117</td><td align="left" valign="bottom">0.907</td></tr><tr><td align="left" valign="bottom">Day 3 × context × region (hippocampus)</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.008</td><td align="left" valign="bottom">0.182</td><td align="left" valign="bottom">0.855</td></tr><tr><td align="left" valign="bottom">Day 2 × context × region (EC)</td><td align="char" char="." valign="bottom">–0.010</td><td align="char" char="." valign="bottom">0.008</td><td align="left" valign="bottom">–1.349</td><td align="left" valign="bottom">0.177</td></tr><tr><td align="left" valign="bottom">Day 3 × context × region (EC)</td><td align="char" char="." valign="bottom">–0.006</td><td align="char" char="." valign="bottom">0.008</td><td align="left" valign="bottom">–0.780</td><td align="left" valign="bottom">0.435</td></tr><tr><td align="left" valign="bottom">Day 2 × context × region (vmPFC)</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.009</td><td align="left" valign="bottom">–0.109</td><td align="left" valign="bottom">0.913</td></tr><tr><td align="left" valign="bottom">Day 3 × context × region (vmPFC)</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.009</td><td align="left" valign="bottom">0.725</td><td align="left" valign="bottom">0.468</td></tr></tbody></table></table-wrap><table-wrap id="app1table31" position="float"><label>Appendix 1—table 31.</label><caption><title>Linear mixed-effects model results for a context model predicting neural pattern similarity, fit to data in vmPFC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>6</underline> tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.049</td><td align="char" char="." valign="bottom">0.036</td><td align="char" char="." valign="bottom">1.366</td><td align="char" char="." valign="bottom">0.172</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–0.244</td><td align="char" char="." valign="bottom">0.809</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.015</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–0.981</td><td align="char" char="." valign="bottom">0.340</td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.748</td><td align="char" char="." valign="bottom">0.455</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">–0.731</td><td align="char" char="." valign="bottom">0.465</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.463</td><td align="char" char="." valign="bottom">0.643</td></tr></tbody></table></table-wrap><table-wrap id="app1table32" position="float"><label>Appendix 1—table 32.</label><caption><title>Linear mixed-effects model results from a distance model predicting neural pattern similarity for landmark buildings that included the hippocampus and vmPFC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 6 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.036</td><td align="char" char="." valign="bottom">0.037</td><td align="char" char="." valign="bottom">0.982</td><td align="char" char="." valign="bottom">0.326</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.138</td><td align="char" char="." valign="bottom">0.890</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.012</td><td align="char" char="." valign="bottom">–0.205</td><td align="char" char="." valign="bottom">0.837</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">1.853</td><td align="char" char="." valign="bottom">0.064</td></tr><tr><td align="left" valign="bottom">region</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–0.544</td><td align="char" char="." valign="bottom">0.587</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–1.025</td><td align="char" char="." valign="bottom">0.306</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–0.572</td><td align="char" char="." valign="bottom">0.567</td></tr><tr><td align="left" valign="bottom">Day 2 × region</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">1.543</td><td align="char" char="." valign="bottom">0.123</td></tr><tr><td align="left" valign="bottom">Day 3 × region</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–0.126</td><td align="char" char="." valign="bottom">0.900</td></tr><tr><td align="left" valign="bottom">distance × region</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">–0.525</td><td align="char" char="." valign="bottom">0.600</td></tr><tr><td align="left" valign="bottom">Day 2 × distance × region</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">–0.663</td><td align="char" char="." valign="bottom">0.508</td></tr><tr><td align="left" valign="bottom">Day 3 × distance × region</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.156</td><td align="char" char="." valign="bottom">0.876</td></tr></tbody></table></table-wrap><table-wrap id="app1table33" position="float"><label>Appendix 1—table 33.</label><caption><title>Linear mixed-effects model results from a distance model predicting neural pattern similarity for landmark buildings that included the hippocampus, vmPFC, and a visual control region.</title><p>The visual control region served as a baseline. SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>2</underline> tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.537</td><td align="char" char="." valign="bottom">0.037</td><td align="left" valign="bottom">14.413</td><td align="left" valign="bottom"><bold>&lt;2e-16</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.087</td><td align="char" char="." valign="bottom">0.020</td><td align="left" valign="bottom">–4.329</td><td align="left" valign="bottom"><bold>6.55e-5</bold></td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.059</td><td align="char" char="." valign="bottom">0.022</td><td align="left" valign="bottom">–2.712</td><td align="left" valign="bottom"><bold>0.009</bold></td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.003</td><td align="left" valign="bottom">–0.628</td><td align="left" valign="bottom">0.530</td></tr><tr><td align="left" valign="bottom">region (hippocampus)</td><td align="char" char="." valign="bottom">–0.506</td><td align="char" char="." valign="bottom">0.011</td><td align="left" valign="bottom">–46.889</td><td align="left" valign="bottom"><bold>&lt;2e-16*</bold></td></tr><tr><td align="left" valign="bottom">region (vmPFC)</td><td align="char" char="." valign="bottom">–0.483</td><td align="char" char="." valign="bottom">0.013</td><td align="left" valign="bottom">–38.294</td><td align="left" valign="bottom"><bold>&lt;2e-16*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.005</td><td align="left" valign="bottom">1.655</td><td align="left" valign="bottom">0.098</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.005</td><td align="left" valign="bottom">–0.233</td><td align="left" valign="bottom">0.816</td></tr><tr><td align="left" valign="bottom">Day 2 × region (hippocampus)</td><td align="char" char="." valign="bottom">0.077</td><td align="char" char="." valign="bottom">0.015</td><td align="left" valign="bottom">5.102</td><td align="left" valign="bottom"><bold>3.38e-7*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × region (hippocampus)</td><td align="char" char="." valign="bottom">0.047</td><td align="char" char="." valign="bottom">0.015</td><td align="left" valign="bottom">3.079</td><td align="left" valign="bottom"><bold>0.002*</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × region (vmPFC)</td><td align="char" char="." valign="bottom">0.089</td><td align="char" char="." valign="bottom">0.017</td><td align="left" valign="bottom">5.129</td><td align="left" valign="bottom"><bold>2.92e-7*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × region (vmPFC)</td><td align="char" char="." valign="bottom">0.028</td><td align="char" char="." valign="bottom">0.018</td><td align="left" valign="bottom">1.549</td><td align="left" valign="bottom">0.121</td></tr><tr><td align="left" valign="bottom">distance × region (hippocampus)</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">0.004</td><td align="left" valign="bottom">1.387</td><td align="left" valign="bottom">0.165</td></tr><tr><td align="left" valign="bottom">distance × region (vmPFC)</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.005</td><td align="left" valign="bottom">1.073</td><td align="left" valign="bottom">0.283</td></tr><tr><td align="left" valign="bottom">Day 2 × distance × region (hippocampus)</td><td align="char" char="." valign="bottom">–0.010</td><td align="char" char="." valign="bottom">0.006</td><td align="left" valign="bottom">–1.821</td><td align="left" valign="bottom">0.069</td></tr><tr><td align="left" valign="bottom">Day 3 × distance × region (hippocampus)</td><td align="char" char="." valign="bottom">–0.000</td><td align="char" char="." valign="bottom">0.006</td><td align="left" valign="bottom">–0.041</td><td align="left" valign="bottom">0.967</td></tr><tr><td align="left" valign="bottom">Day 2 × distance × region (vmPFC)</td><td align="char" char="." valign="bottom">–0.015</td><td align="char" char="." valign="bottom">0.007</td><td align="left" valign="bottom">–2.296</td><td align="left" valign="bottom"><bold>0.022*</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × distance × region (vmPFC)</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.007</td><td align="left" valign="bottom">–0.086</td><td align="left" valign="bottom">0.931</td></tr></tbody></table></table-wrap><table-wrap id="app1table34" position="float"><label>Appendix 1—table 34.</label><caption><title>Linear mixed-effects model results from a model of local (within-track) distance, predicting neural pattern similarity for landmark buildings in vmPFC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.144</td><td align="char" char="." valign="bottom">0.167</td><td align="char" char="." valign="bottom">0.866</td><td align="char" char="." valign="bottom">0.387</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.027</td><td align="char" char="." valign="bottom">–0.174</td><td align="char" char="." valign="bottom">0.862</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.024</td><td align="char" char="." valign="bottom">0.027</td><td align="char" char="." valign="bottom">–0.867</td><td align="char" char="." valign="bottom">0.386</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">–0.234</td><td align="char" char="." valign="bottom">0.815</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.025</td><td align="char" char="." valign="bottom">0.028</td><td align="char" char="." valign="bottom">0.910</td><td align="char" char="." valign="bottom">0.363</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">0.745</td><td align="char" char="." valign="bottom">0.456</td></tr></tbody></table></table-wrap><table-wrap id="app1table35" position="float"><label>Appendix 1—table 35.</label><caption><title>Linear mixed-effects model results from the model of global (across-track) distance, predicting neural pattern similarity for landmark buildings in vmPFC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 8 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.055</td><td align="char" char="." valign="bottom">0.119</td><td align="char" char="." valign="bottom">0.462</td><td align="char" char="." valign="bottom">0.644</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.043</td><td align="char" char="." valign="bottom">0.044</td><td align="char" char="." valign="bottom">0.993</td><td align="char" char="." valign="bottom">0.321</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.046</td><td align="char" char="." valign="bottom">–0.012</td><td align="char" char="." valign="bottom">0.990</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.008</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">0.955</td><td align="char" char="." valign="bottom">0.340</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.013</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">–1.071</td><td align="char" char="." valign="bottom">0.284</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">–0.320</td><td align="char" char="." valign="bottom">0.749</td></tr></tbody></table></table-wrap><table-wrap id="app1table36" position="float"><label>Appendix 1—table 36.</label><caption><title>Linear mixed-effects model results from the model of local (within-track) distance, predicting neural pattern similarity for fractals in vmPFC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">–0.009</td><td align="char" char="." valign="bottom">0.085</td><td align="char" char="." valign="bottom">–0.107</td><td align="char" char="." valign="bottom">0.914</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">0.172</td><td align="char" char="." valign="bottom">0.864</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">–0.094</td><td align="char" char="." valign="bottom">0.926</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">0.865</td><td align="char" char="." valign="bottom">0.387</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–0.347</td><td align="char" char="." valign="bottom">0.729</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">–0.021</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">–1.403</td><td align="char" char="." valign="bottom">0.161</td></tr></tbody></table></table-wrap><table-wrap id="app1table37" position="float"><label>Appendix 1—table 37.</label><caption><title>Linear mixed-effects model results from the model of global (across-track) distance, predicting neural pattern similarity for fractals in vmPFC.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 8 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">–0.055</td><td align="char" char="." valign="bottom">0.058</td><td align="char" char="." valign="bottom">–0.958</td><td align="char" char="." valign="bottom">0.338</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.023</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.982</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.046</td><td align="char" char="." valign="bottom">0.024</td><td align="char" char="." valign="bottom">–1.974</td><td align="char" char="." valign="bottom">0.053</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">0.002</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.432</td><td align="char" char="." valign="bottom">0.666</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.001</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">–0.136</td><td align="char" char="." valign="bottom">0.892</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">1.632</td><td align="char" char="." valign="bottom">0.103</td></tr></tbody></table></table-wrap><table-wrap id="app1table38" position="float"><label>Appendix 1—table 38.</label><caption><title>Linear mixed-effects model results from a model predicting neural pattern similarity between pairs of items in vmPFC, with scan session, hippocampal pattern similarity, and pattern similarity in a visual control region (calcarine) as predictors.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 2 tests. An asterisk (*) denotes findings that survived FDR correction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">–0.007</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">–0.331</td><td align="left" valign="bottom">0.743</td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.006</td><td align="char" char="." valign="bottom">0.019</td><td align="char" char="." valign="bottom">–0.298</td><td align="left" valign="bottom">0.768</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">0.199</td><td align="left" valign="bottom">0.844</td></tr><tr><td align="left" valign="bottom">hippocampal pattern similarity (Item<sub>A</sub>, Item<sub>B</sub>)</td><td align="char" char="." valign="bottom">0.414</td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">14.387</td><td align="left" valign="bottom"><bold>1.25e-14</bold></td></tr><tr><td align="left" valign="bottom">calcarine pattern similarity (Item<sub>A</sub>, Item<sub>B</sub>)</td><td align="char" char="." valign="bottom">0.108</td><td align="char" char="." valign="bottom">0.020</td><td align="char" char="." valign="bottom">5.465</td><td align="left" valign="bottom"><bold>4.83e-6</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × hippocampal pattern similarity (Item<sub>A</sub>, Item<sub>B</sub>)</td><td align="char" char="." valign="bottom">–0.094</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">–5.713</td><td align="left" valign="bottom"><bold>1.11e-8</bold></td></tr><tr><td align="left" valign="bottom">Day 3 × hippocampal pattern similarity (Item<sub>A</sub>, Item<sub>B</sub>)</td><td align="char" char="." valign="bottom">–0.076</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">–4.355</td><td align="left" valign="bottom"><bold>1.34e-5</bold></td></tr><tr><td align="left" valign="bottom">Day 2 × calcarine pattern similarity (Item<sub>A</sub>, Item<sub>B</sub>)</td><td align="char" char="." valign="bottom">0.019</td><td align="char" char="." valign="bottom">0.013</td><td align="char" char="." valign="bottom">1.427</td><td align="left" valign="bottom">0.154</td></tr><tr><td align="left" valign="bottom">Day 3 × calcarine pattern similarity (Item<sub>A</sub>, Item<sub>B</sub>)</td><td align="char" char="." valign="bottom">–0.020</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">–1.433</td><td align="left" valign="bottom">0.152</td></tr></tbody></table></table-wrap><table-wrap id="app1table39" position="float"><label>Appendix 1—table 39.</label><caption><title>Linear mixed-effects model results from a context model predicting neural pattern similarity, fit to data from a visual control region.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 1<underline>6</underline> tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.554</td><td align="char" char="." valign="bottom">0.043</td><td align="char" char="." valign="bottom">13.030</td><td align="left" valign="bottom"><bold>&lt;2e-16</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.021</td><td align="char" char="." valign="bottom">0.025</td><td align="char" char="." valign="bottom">–0.820</td><td align="left" valign="bottom">0.412</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.039</td><td align="char" char="." valign="bottom">0.041</td><td align="char" char="." valign="bottom">–0.944</td><td align="left" valign="bottom">0.345</td></tr><tr><td align="left" valign="bottom">stimulus type (landmark &gt; fractal)</td><td align="char" char="." valign="bottom">–0.024</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">–7.009</td><td align="left" valign="bottom"><bold>2.45e-12</bold></td></tr><tr><td align="left" valign="bottom">context (same track &gt; different tracks)</td><td align="char" char="." valign="bottom">0.001</td><td align="char" char="." valign="bottom">0.003</td><td align="char" char="." valign="bottom">0.245</td><td align="left" valign="bottom">0.806</td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">1.088</td><td align="left" valign="bottom">0.277</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">–0.407</td><td align="left" valign="bottom">0.684</td></tr><tr><td align="left" valign="bottom">Day 2 × context</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">–0.717</td><td align="left" valign="bottom">0.474</td></tr><tr><td align="left" valign="bottom">Day 3 × context</td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">–0.416</td><td align="left" valign="bottom">0.678</td></tr><tr><td align="left" valign="bottom">stimulus type × context</td><td align="char" char="." valign="bottom">–0.008</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–1.252</td><td align="left" valign="bottom">0.211</td></tr><tr><td align="left" valign="bottom">Day 2 × stimulus type × context</td><td align="char" char="." valign="bottom">–0.004</td><td align="char" char="." valign="bottom">0.009</td><td align="char" char="." valign="bottom">–0.377</td><td align="left" valign="bottom">0.706</td></tr><tr><td align="left" valign="bottom">Day 3 × stimulus type × context</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">1.509</td><td align="left" valign="bottom">0.131</td></tr></tbody></table></table-wrap><table-wrap id="app1table40" position="float"><label>Appendix 1—table 40.</label><caption><title>Linear mixed-effects model results from a model of local (within-track) distance predicting neural pattern similarity, fit to data from a visual control region.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 10 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.317</td><td align="char" char="." valign="bottom">0.135</td><td align="char" char="." valign="bottom">2.343</td><td align="char" char="." valign="bottom"><bold>0.019</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.040</td><td align="char" char="." valign="bottom">0.030</td><td align="char" char="." valign="bottom">–1.351</td><td align="char" char="." valign="bottom">0.177</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.025</td><td align="char" char="." valign="bottom">0.044</td><td align="char" char="." valign="bottom">–0.577</td><td align="char" char="." valign="bottom">0.564</td></tr><tr><td align="left" valign="bottom">distance (link distance 2 &gt; link distance 1)</td><td align="char" char="." valign="bottom">0.014</td><td align="char" char="." valign="bottom">0.016</td><td align="char" char="." valign="bottom">0.870</td><td align="char" char="." valign="bottom">0.384</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">0.304</td><td align="char" char="." valign="bottom">0.761</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">–0.035</td><td align="char" char="." valign="bottom">0.022</td><td align="char" char="." valign="bottom">–1.570</td><td align="char" char="." valign="bottom">0.117</td></tr></tbody></table></table-wrap><table-wrap id="app1table41" position="float"><label>Appendix 1—table 41.</label><caption><title>Linear mixed-effects model results from a model of global (across-track) distance predicting neural pattern similarity, fit to data from a visual control region.</title><p>SE = standard error. To correct for multiple comparisons, we adjusted the α-value for 8 tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom"><italic>β</italic></th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">(Intercept)</td><td align="char" char="." valign="bottom">0.418</td><td align="char" char="." valign="bottom">0.095</td><td align="char" char="." valign="bottom">4.379</td><td align="left" valign="bottom"><bold>1.31e-5</bold></td></tr><tr><td align="left" valign="bottom">Day 2</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.040</td><td align="char" char="." valign="bottom">–0.082</td><td align="left" valign="bottom">0.935</td></tr><tr><td align="left" valign="bottom">Day 3</td><td align="char" char="." valign="bottom">–0.061</td><td align="char" char="." valign="bottom">0.051</td><td align="char" char="." valign="bottom">–1.190</td><td align="left" valign="bottom">0.234</td></tr><tr><td align="left" valign="bottom">distance</td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">–0.783</td><td align="left" valign="bottom">0.434</td></tr><tr><td align="left" valign="bottom">Day 2 × distance</td><td align="char" char="." valign="bottom">–0.003</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">–0.261</td><td align="left" valign="bottom">0.794</td></tr><tr><td align="left" valign="bottom">Day 3 × distance</td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.010</td><td align="char" char="." valign="bottom">0.727</td><td align="left" valign="bottom">0.468</td></tr></tbody></table></table-wrap></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80281.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.04.12.488078" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.12.488078"/></front-stub><body><p>This is a carefully designed and analysed fMRI study investigating how neural representations in the hippocampus, entorhinal cortex, and ventromedial prefrontal cortex change as a function of spatial learning. These important and compelling results provide new insight into how local and global knowledge about our environment is represented. It will be of great interest to researchers studying the differentiation and integration of memories and the formation of cognitive maps.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80281.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Norman</surname><given-names>Kenneth A</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hx57361</institution-id><institution>Princeton University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Bellmund</surname><given-names>Jacob LS</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.12.488078">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.04.12.488078v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Representational integration and differentiation in the human hippocampus following goal-directed navigation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Chris Baker as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Kenneth A Norman (Reviewer #1); Jacob L S Bellmund (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>As you will see from the detailed comments below, the reviewers all think this study addresses an interesting and timely question with careful design and analysis all leading to a high-quality manuscript.</p><p>However, the reviewers also highlight areas where the manuscript could be improved, including additional analyses. Below, I summarize the essential revisions you will need to make, although we would like you to respond in detail to all of the comments and recommendations of the reviewers.</p><p>Essential revisions:</p><p>1) Given the exploratory nature of many of the analyses, you should provide greater clarity on the specific predictions, highlight what alternative predictions/outcomes would mean, and provide a clearer link between the conceptual prediction and analytic outcomes. In this context, you should also provide greater clarity on how multiple comparisons were applied for the different analyses.</p><p>2) Please present the results in a clear and consistent way throughout the manuscript (in particular, see comments from Reviewers 1 and 2 for details).</p><p>3) Consider an alternate analytic analysis to the median split applied to the performance data. We would recommend a mixed effects model (see Reviewer 2).</p><p>4) The reviewers were skeptical of the explanation of the results linking behavior to neural similarity (where you interpret a &quot;flatter&quot; pattern of similarity values across distances as reflecting the integration of landmarks into a global map). The additional behavioral and neural analyses suggested in the reviews might help you arrive at a better-substantiated account of these results.</p><p>5) Fractals and landmarks should be analyzed in a more consistent way (if you did not have an a priori reason to favor one over the other, you should always present results from both).</p><p>6) Present additional behavioral measures to more precisely examine navigational performance (see comments from Reviewer 3).</p><p>7) Reconsider the hippocampus-vmPFC &quot;connectivity&quot; analysis (see comments from Reviewer 2).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>In addition to addressing the recommendations noted in the public review, here are some other points for the authors to consider:</p><p>1. In the Introduction, the authors say, &quot;we predicted that early evidence of global map learning during local navigation would depend on the integration and predict participants' ability to subsequently navigate across the environment&quot;. In the paper, it is not clear what the basis for this prediction was: Was this an a priori prediction arising from existing results and/or theories, or did the prediction arise after the authors observed the slope across the distance 2, 3, 4 conditions post-local-learning? It is also unclear whether the authors predicted the specific direction of this relationship (I actually find it somewhat counterintuitive: I would have thought that participants who show more sensitivity to location – i.e., a sharper slope across the distance 2, 3, and 4 conditions – would show better navigation performance). Lastly, it is unclear whether the authors originally planned to operationalize &quot;global map learning&quot; using the slope across the 2, 3, and 4 conditions, and whether they also considered other ways of relating neural data to navigation behavior. Specifying the answers more clearly in the paper will help readers to evaluate the results.</p><p>2. Issues with results presentation: The way that the results are presented makes it very challenging to do apples-to-apples comparisons of different brain regions. For example, Figure 3 shows the &quot;within – across context similarity&quot; measure for EC and Figure 4 shows the &quot;link distance 1 – link distance 2 similarity&quot; measure for the hippocampus. Featuring different measures for different areas makes it hard to compare them. I know that the corresponding apples-to-apples measures are in various supplements but readers should not have to dig for them. If the authors had a priori reasons to feature these different dependent measures for different regions, that might justify the results presentation strategy used here, but – as things stand – it is unclear why some results were featured in the main text and others were relegated to the supplement. In the absence of an a priori justification for the present way of displaying results, I think it would be useful if the authors showed corresponding results for hippocampus and EC right next to each other in the main paper, regardless of significance.</p><p>3. Additional analyses to connect to other findings in the literature: I think it would be useful for the authors to consider whether there are additional exploratory analyses they could run to connect better to other findings in the literature. For example, many of the extant studies on MTL integration and differentiation separately investigate hippocampal subfields. If there are reasons to expect different results in different subfields, it might be worth doing this (but if the scans are not well suited for this purpose, or the authors don't think that there are good reasons to do so, that is fine). Also, the &quot;hippocampal-vmPFC interaction&quot; analysis is unconventional. Usually, this involves some kind of functional connectivity but here it was a second-order similarity analysis. Is there a reason that the authors took this approach? Do the authors think that their paradigm lends itself to interesting predictions relating to the more standard functional connectivity approach? Lastly, there are other ROIs that are known to be involved in spatial learning, e.g., RSC. Why did the authors focus on vmPFC instead of RSC in their analyses?</p><p>4. I think that it might be useful to show the similarity values for the 1, 2 within-track conditions alongside the 2, 3, and 4 (across-track) conditions. This may not be enough to resolve the question about whether integration is taking place, but looking at these results together (instead of presenting them in separate figures/analyses) might provide a more clear picture of what the regions are doing.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I have a few more suggestions to help with readability/interpretation.</p><p>1. It was difficult to keep track of the results as they related to the different ROIs, within vs. across tracks, distances, and time (related to my comment #5 above). It would be helpful to display all relevant plots in the same way throughout the manuscript, especially as the same mixed-effects models were applied (e.g., the same panels for different regions in the same order). Even if the findings are not significant, it is helpful to see the results in the same figure rather than comparing them with supplementary figures.</p><p>For example, Figure 4B shows data for same-track landmarks collapsed across link distance, and Figure 4C shows data for different-track landmarks split by link distance. This makes it difficult to directly compare these values, even though they were entered into the same model.</p><p>2. More broadly, the reporting of the results was somewhat difficult to follow. While I appreciate the importance of accounting for all factors in the same model, the order in which different factors were reported sometimes varied across models, making it difficult to keep in mind how the different models compared.</p><p>3. As I was reading the results, I made a note to suggest a distance analysis only to realize it was carried out later on (p. 16) – it would be helpful to include more signposting to motivate and help the reader anticipate this analysis.</p><p>4. Figure 5B was difficult to follow. The solid lines represent similarity prior to the global navigation task for both groups, but the dashed line only represents similarity post-global navigation for the less efficient navigators.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. The path inefficiency metric conflates the effects of local and global knowledge of the environment. For example, when participants switch tracks correctly in the global condition but fail to respond precisely on the goal track, this could result in a relatively high error despite efficient global navigation performance. While the comparison to the local condition is helpful of course, the report of additional behavioral measures would be desirable in my view. Relevant measures could quantify how frequently participants (a) manage to switch tracks correctly and (b) how frequently they choose to navigate in the correct direction after switching tracks.</p><p>2. It is unclear to me why the distance-based analyses (Figure 4) are based on the landmark images only. I would expect a similar effect to be present for the fractal stimuli as participants were asked to memorize the fractal locations. Thus, I think demonstrating a similar effect for the fractals as for the landmarks would substantially strengthen the finding of a hippocampal representation of local and global distances.</p><p>3. Again related to the analysis of distance representations in the hippocampus, I am wondering why the authors chose the link distance as their distance measure. I think the paper would benefit from a justification for why this measure was chosen.</p><p>4. With respect to the formation of cognitive maps of space there is also evidence for the representation of (remembered) Euclidean distances in the hippocampus (e.g. Howard et al., J Neurosci, 2011; Deuker et al., <italic>eLife</italic>, 2016). I would be curious to see if the authors can detect similar representations of Euclidean distances in their data in an additional exploratory analysis.</p><p>5. A surprising and somewhat puzzling aspect of the reported results is that the global distance effect can only be detected after the local, but not after the global navigation task. Further, this effect seems to be driven by participants who take less efficient paths. I cannot quite follow the authors' interpretation in the Results section that the &quot;acquisition of a more fully integrated global map&quot; would lead to this pattern of results. Further, the authors state in the Discussion section that a &quot;negative distance-related similarity function reflects restricted learning of the global map, hindering performance on the Global Task&quot;. In my view, a negative relationship between distance and representational similarity could rather point towards the formation of an integrated, map-like representation of the environment, where nearby landmarks share more similar representations than those that are separated by larger distances. I would appreciate some clarification on why the authors think such an effect would hinder task performance and why it might disappear with the &quot;building of a more global cognitive map&quot;.</p><p>6. I am wondering about why the authors chose a median split to analyze interindividual differences. As dichotomizing a continuous variable such as navigation performance is often problematic (as discussed in detail e.g. in MacCallum et al., Psychological Methods, 2000), I would like to know whether the effect also holds when using an approach based on correlation/regression.</p><p>7. I tried to access the analysis code, but the GitHub repository referred to in the Data availability statement is currently empty.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80281.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions (for the authors):</p><p>1) Given the exploratory nature of many of the analyses, you should provide greater clarity on the specific predictions, highlight what alternative predictions/outcomes would mean, and provide a clearer link between the conceptual prediction and analytic outcomes. In this context, you should also provide greater clarity on how multiple comparisons were applied for the different analyses.</p></disp-quote><p>We thank the Editors and Reviewers for these helpful directions. Throughout the revised manuscript, we added text that we believe more clearly links predictions with outcomes. For example:</p><p>“We hypothesized that there would be a change in hippocampal and entorhinal pattern similarity for items located on the same track vs. items located on different tracks. An increase in pattern similarity would suggest that within-track item representations are integrated, while a decrease would suggest these representations are differentiated following learning.”</p><p>Other edits made to address this concern are marked in the manuscript using track changes.</p><p>Regarding corrections for multiple comparisons, we have added the following to the Results:</p><p>“We report results from both planned and exploratory analyses. For planned analyses, we interpret a priori effects when significant at p &lt;.05 uncorrected, but for completeness we note whether all reported effects survive FDR correction (see Methods).”</p><p>Further, we have added the following to the Methods section:</p><p>“To correct for multiple comparisons, we first computed <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, a number representing the number of tests conducted for a given hypothesis multiplied by the number of ROIs we examined (including the visual control). For example, to examine learning-driven changes in pattern similarity for items located at different distances on the same track, we tested for two interactions: distance and scan (Day 2 &gt; Day 1), and distance and scan (Day 3 &gt; Day 1). ROIs tested were hippocampus, left and right EC, vmPFC, and the visual control region (Calcarine). <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> thus represented 2 interactions * 5 ROIs = 10. We then controlled for the false discovery rate (FDR) by ordering the p-values for each hypothesis test from smallest to largest (P(min)…P(max)), and checking if the following was satisfied for each ordered p-value<sup>97</sup>: <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mtext> </mml:mtext><mml:mi>α</mml:mi><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mfrac><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mo>∝</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> Unless otherwise specified, all p-values reported were uncorrected and we interpret a priori predicted effects at this level. For completeness, we also note whether the reported effects survived FDR correction throughout the text and in the Appendix tables.”</p><p>Further, as per Reviewer 1 (General Comments 1 and 2):</p><p>– We now report the denominator <italic>m</italic> for each model in the Appendix tables and clearly indicate which findings survive FDR correction. For example, we added the following text to the caption for Appendix – Table 1:</p><p>“To correct for multiple comparisons, we adjusted the <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> value for 18 tests. An asterisk (*) denotes findings that survived FDR correction.”</p><p>– We updated text throughout the manuscript to further clarify which results survived corrections for multiple comparisons.</p><p>– We emphasize in the Discussion that a number of our findings do not survive FDR correction and encourage the reader to interpret our results with caution:</p><p>“Moreover, some of our findings do not survive correction for multiple comparisons, and thus should be interpreted with caution.”</p><disp-quote content-type="editor-comment"><p>2) Please present the results in a clear and consistent way throughout the manuscript (in particular, see comments from Reviewers 1 and 2 for details).</p></disp-quote><p>We are grateful for the feedback we received from several reviewers with suggestions about how to improve the clarity and consistency of the presentation of our findings. Accordingly, we revised the figures to present findings from hippocampus and EC next to each other (regardless of significance) in the main text. To be consistent in reporting our findings for the local and global distance analyses, we now report similarity values (as opposed to model contrasts) for the link distance 1 and 2 (within-track) conditions in Figure 4.</p><disp-quote content-type="editor-comment"><p>3) Consider an alternate analytic analysis to the median split applied to the performance data. We would recommend a mixed effects model (see Reviewer 2).</p></disp-quote><p>We greatly appreciate the reviewers’ suggestions regarding alternative analytic approaches. We removed this section of the original manuscript and ran two models to address comments related to the median split: (1) a mixed-effects model predicting neural pattern similarity Post Local Navigation, with a continuous metric of task performance (each participant’s median path inefficiency for across-track trials in the first four test runs of Global Navigation) and link distance as predictors (as suggested by Reviewer 3); and (2) a mixed-effects model relating trial-wise navigation data to pairwise similarity values for each given pair of landmarks and fractals (as suggested by Reviewer 2). We report findings for both; the following has been added to the main text:</p><p>“The variability in the observed across-track hippocampal distance effect may reflect that some participants encoded global map knowledge during Local Navigation, whereas others did not (or did so less fully; Figure 6a). To the extent that this is the case, this would predict that the distance-related hippocampal pattern similarity effect Post Local Navigation should relate to navigational efficiency at the outset of performing the Global Task. Specifically, we predicted that more efficient navigators would have a negative distance function, such that pattern similarity would be greatest for the most proximal across-track landmarks and decrease with distance. To test this hypothesis, we first ran a mixed effects model predicting neural pattern similarity Post Local Navigation (Day 2), with path inefficiency (median path inefficiency for across-track trials in the first four test runs of Global Navigation on Day 2) and link distance as predictors. Indeed, we observed a significant interaction between path inefficiency and link distance (β = -0.001 ± 0.001; t = -1.983; p = 0.048, did not survive FDR correction; d = 0.43; Appendix – Table 24), but the direction of the effect was unexpected. Participants who did well from the beginning of the Global Task showed no effect of distance in hippocampal pattern similarity, whereas less efficient navigators showed a negative slope (Figure 6b). A similar interaction between path inefficiency and link distance was not observed when the model was fit to data from Day 1 (β = 0.001 ± 0.001; t = 1.523; p = 0.128; Appendix – Figure 3a; Appendix – Table 25) or to data from Day 3 (β = 0.001 ± 0.001; t = 1.379; p = 0.168; Appendix – Figure 3b; Appendix – Table 26).</p><p>Next, we examined single-trial navigation data in relation to pairwise neural similarity in the hippocampus (for each given pair of landmarks and fractals), using mixed-effects models to predict path inefficiency for each trial across the first four test runs of the Global Navigation Task (Day 2). The models included (a) neural similarity (Post Local Navigation) for a given pair of fractals or the nearby landmarks and (b) length of the optimal path for each trial as predictors, and a regressor indicating whether the trial was a within-track or across-track trial. Models were run for landmarks and fractals separately. Here we observed trend-level evidence that hippocampal pattern similarity (Post Local Navigation) for landmark pairs predicted trial-level subsequent Global Navigation performance (β = -41.245 ± 24.162; t = -1.707; p = 0.088; Appendix – Table 27), such that greater hippocampal pattern similarity was predictive of a more efficient path (Appendix – Figure 4). The length of the optimal path (β = -0.663 ± 0.183; t = -3.631; p = 0.0003; d = 0.79) and trial type (within-track vs. across-track; β = 20.551 ± 6.245; t = 3.291; p = 0.001; d = 0.71) significantly predicted navigation performance in the landmark model. There was no interaction between hippocampal pattern similarity for landmark pairs and trial type (β = 54.618 ± 43.489; t = 1.256; p = 0.210).</p><p>The length of the optimal path (β = -0.646 ± 0.184; t = -3.519; p = 0.0005; d = 0.77) and trial type (within-track vs. across-track; β = 27.289 ± 6.196; t = 4.404; p &lt; 1.24e<sup>-5</sup>, survived FDR correction; d = 0.96; Appendix – Table 28) were also significant predictors of navigation performance in the model relating pattern similarity for fractal pairs to subsequent performance on the Global Task. However, hippocampal pattern similarity for fractal pairs did not significantly predict subsequent performance (β = 3.026 ± 27.687; t = 0.109; p = 0.913).”</p><disp-quote content-type="editor-comment"><p>4) The reviewers were skeptical of the explanation of the results linking behavior to neural similarity (where you interpret a &quot;flatter&quot; pattern of similarity values across distances as reflecting the integration of landmarks into a global map). The additional behavioral and neural analyses suggested in the reviews might help you arrive at a better-substantiated account of these results.</p></disp-quote><p>Based on the additional behavioral and neural analyses suggested by reviewers, we added the following to the Discussion section:</p><p>“We hypothesized that this variability in the neural data might relate to the behavioral variability we observed on the subsequent Global Navigation Task, and explored this relationship in two ways. First, we ran a mixed effects model predicting hippocampal pattern similarity Post Local Navigation, with across-track path inefficiency at the start of Global Navigation and link distance as predictors. Here we observed a significant interaction between path inefficiency and link distance, but the effect was not in the direction we expected. Our results indicate that the interaction between distance and scan (the negative slope in the center panel of Figure 5b) was driven by participants who were <italic>less</italic> efficient at the outset of Global Navigation, whereas we predicted a negative slope would be more apparent in highly efficient navigators. We initially hypothesized a negative distance-related similarity function would reflect global map learning, and thus expected to observe such a function in efficient navigators at the start of the Global Task on Day 2 and across all participants on Day 3. Such a function was absent following Global Navigation (the right panel of Figure 5b).</p><p>Why do we observe a flat distance-related similarity function after global map learning? It is possible that individual differences in navigational strategy or the particular learning processes utilized by efficient navigators in the present task organized the map in such a way. It is also possible our findings reflect a change in event boundaries. Evidence from a study examining temporal context found that hippocampal pattern similarity did not differ between items located nearby vs. further apart within a temporal event<sup>61</sup>. Hippocampal pattern similarity differed, however, between items that crossed an event boundary, such that nearby items had increased pattern similarity compared to items located further apart in time. While this study examined temporal events, spatial event boundaries may function similarly in that during retrieval, representations of other within-event items are reinstated. The flat slope observed on Day 3 could thus be a signature of an integrated map.</p><p>Our second approach to relating variability in the neural data to behavioral variability used a model to predict path inefficiency for each trial across the first four test runs of the Global Navigation Task (Day 2). While the present study was not designed to optimize power to detect trial-level effects, here we found trend-level evidence that hippocampal pattern similarity (Post Local Navigation) for landmark pairs predicted trial-level subsequent Global Navigation performance (Figure 7), providing additional suggestive evidence for a relationship between hippocampal pattern similarity and subsequent behavior.”</p><disp-quote content-type="editor-comment"><p>5) Fractals and landmarks should be analyzed in a more consistent way (if you did not have an a priori reason to favor one over the other, you should always present results from both).</p></disp-quote><p>In our original submission, we included findings with respect to fractals in all analyses except for the distance analyses. In the revision, we updated the relevant sections of the manuscript to include results from fractals. No significant interactions between distance and scan were observed when local and global distance models were run for fractal stimuli in the hippocampus, EC, and vmPFC. These findings have been added throughout the main text and the Appendix with variations of the following language:</p><p>“There were no significant interactions between distance and scan when similar models were run for fractal stimuli.”</p><p>Full model results are reported in the Appendix (Appendix – Tables 12, 15, 21-23, 36-37).</p><p>In addition, based on feedback from Reviewers 2 and 3, we added the following to the discussion:</p><p>“While we expected similar results for both fractal and landmark stimuli throughout our analyses, the null findings we observed across ROIs when local and global distance models were run with fractal stimuli were not completely surprising. Fractal and landmark stimuli differ in several key ways, which we believe explain the observed pattern of findings. For example, fractals are only visible in the environment for a minority of trials. During the majority of navigation, participants must rely on the landmark buildings to guide them. Fractals serve as pointers to the goal location to which a participant must navigate on a particular trial, but fractal information may not be used during route planning. Further, fractals are not necessary for participants to learn the layout of the environment; local and global maps can be built from landmarks alone.”</p><disp-quote content-type="editor-comment"><p>6) Present additional behavioral measures to more precisely examine navigational performance (see comments from Reviewer 3).</p></disp-quote><p>Following the guidance from Reviewer 3, we determined whether participants (a) switched to the correct track and (b) navigated in the correct direction once switching on each Global Navigation trial at the start (first four test runs) and end (last two test runs) of the task on Day 2, and at the end of Day 3 (last two test runs).</p><p>We added the following to the Results section:</p><p>“We also visually examined participants’ routes on each across-track Global Navigation Task trial at the start (first four test runs) and end (last two test runs) of the task on Day 2, and at the end (last two test runs) of Day 3, noting whether participants (a) initially switched to the correct track on the trial and (b) navigated in the correct direction after switching. At the start of the Global Task on Day 2, participants switched to the correct track on 65.78% (SD = 21.35%) of across-track trials. Of those trials, participants navigated in the correct direction after switching 77.75% (SD = 22.52%) of the time. By the end of Day 2, those numbers increased to 75.09% (SD = 20.33%) and 85.64% (SD = 16.91%) respectively. Performance continued to improve on Day 3, with participants switching to the correct track on 78.36% (SD = 21.77%) of across-track trials during the last two runs of the Global Navigation Task and navigating in the correct direction 90.48% (SD = 10.63%) of the time after switching tracks.”</p><disp-quote content-type="editor-comment"><p>7) Reconsider the hippocampus-vmPFC &quot;connectivity&quot; analysis (see comments from Reviewer 2).</p></disp-quote><p>We could not undertake a more traditional univariate functional connectivity analysis because we did not scan during navigation. We agree with the reviewers that the null results reported for the second-order similarity analysis in the initial manuscript are confusing and difficult to interpret. Based on the reviewer comments, we replaced the second-order similarity analysis with the following:</p><p>“While context- and distance-related effects on vmPFC and hippocampal pattern similarity did not significantly differ, such effects were significant in hippocampus but not in vmPFC. To further test whether distance-related similarity structures were similar between the regions, we modeled the relationship between vmPFC pattern similarity and hippocampal pattern similarity. Specifically, we ran a linear mixed-effects model predicting pairwise similarity values in vmPFC, with scan session, pairwise similarity values in the hippocampus (averaged across hemispheres) and pairwise similarity values in a visual control region as predictors. Here, we observed significant interactions between scan session and pairwise similarity values in the hippocampus Post Local (Day 2 &gt; Day 1 × hippocampal pattern similarity, β = -0.094 ± 0.017; t = -5.713; p &lt; 1.12e<sup>-8</sup>, survived FDR correction; d = 1.25) and Global Navigation (Day 3 &gt; Day 1× hippocampal pattern similarity, β = -0.076 ± 0.017; t = -4.355; p &lt; 1.35e<sup>-5</sup>, survived FDR correction; d = 0.95), but not in the visual control region (Day 2 &gt; Day 1 × calcarine pattern similarity, β = 0.019 ± 0.013; t = 1.427; p = 0.154; Day 3 &gt; Day 1 × calcarine pattern similarity, β = -0.020 ± 0.014; t = -1.433; p = 0.152; Appendix – Table 38). This pattern of findings suggests that functional connectivity between the hippocampus and vmPFC weakens over time.”</p><p>Finally, we clarified that the analyses undertaken in vmPFC were exploratory. While we believe this is a legitimate avenue for exploration, we moved the entire treatment of findings from vmPFC to the Appendix. Please note that while we moved the vmPFC findings to the Appendix so as to streamline the manuscript, this move did not change our approach to correction for multiple comparisons; the region was still included in the FDR correction.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Here are some other points for the authors to consider:</p><p>1. In the Introduction, the authors say, &quot;we predicted that early evidence of global map learning during local navigation would depend on the integration and predict participants' ability to subsequently navigate across the environment&quot;. In the paper, it is not clear what the basis for this prediction was: Was this an a priori prediction arising from existing results and/or theories, or did the prediction arise after the authors observed the slope across the distance 2, 3, 4 conditions post-local-learning? It is also unclear whether the authors predicted the specific direction of this relationship (I actually find it somewhat counterintuitive: I would have thought that participants who show more sensitivity to location – i.e., a sharper slope across the distance 2, 3, and 4 conditions – would show better navigation performance). Lastly, it is unclear whether the authors originally planned to operationalize &quot;global map learning&quot; using the slope across the 2, 3, and 4 conditions, and whether they also considered other ways of relating neural data to navigation behavior. Specifying the answers more clearly in the paper will help readers to evaluate the results.</p></disp-quote><p>Thank you for this recommendation. We followed it by adding the following to the Results section so as to clarify our prediction with respect to the brain-behavior analysis:</p><p>“Specifically, we predicted that more efficient navigators would have a negative distance function, such that pattern similarity would be greatest for the most proximal across-track landmarks and decrease with distance. To test this hypothesis, we first ran a mixed effects model predicting neural pattern similarity Post Local Navigation (Day 2), with path inefficiency (median path inefficiency for across-track trials in the first four test runs of Global Navigation on Day 2) and link distance as predictors. Indeed, we observed a significant interaction between path inefficiency and link distance (β = -0.001 ± 0.001; t = -1.983; p = 0.048, did not survive FDR correction; d = 0.43; Appendix – Table 24), but the direction of the effect was unexpected. Participants who did well from the beginning of the Global Task showed no effect of distance in hippocampal pattern similarity, whereas less efficient navigators showed a negative slope (Figure 6b). A similar interaction between path inefficiency and link distance was not observed when the model was fit to data from Day 1 (β = 0.001 ± 0.001; t = 1.523; p = 0.128; Appendix – Figure 3a; Appendix – Table 25) or to data from Day 3 (β = 0.001 ± 0.001; t = 1.379; p = 0.168; Appendix – Figure 3b; Appendix – Table 26).”</p><p>We found the direction of this effect to be counterintuitive as well. Additional follow-up analyses provided further suggestive evidence that across-track path inefficiency relates to trial-level hippocampal pattern similarity (see response to Essential Revisions General comment #3). We offer the following interpretation of these analyses in the Discussion:</p><p>“Our results indicate that the interaction between distance and scan (the negative slope in the center panel of Figure 5b) was driven by participants who were <italic>less</italic> efficient at the outset of Global Navigation, whereas we predicted a negative slope would be more apparent in highly efficient navigators. We initially hypothesized a negative distance-related similarity function would reflect global map learning, and thus expected to observe such a function in efficient navigators at the start of the Global Task on Day 2 and across all participants on Day 3. Such a function was absent following Global Navigation (the right panel of Figure 5b).</p><p>Why do we observe a flat distance-related similarity function after global map learning? It is possible that individual differences in navigational strategy or the particular learning processes utilized by efficient navigators in the present task organized the map in such a way. It is also possible our findings reflect a change in event boundaries. Evidence from a study examining temporal context found that hippocampal pattern similarity did not differ between items located nearby vs. further apart within a temporal event<sup>61</sup>. Hippocampal pattern similarity differed, however, between items that crossed an event boundary, such that nearby items had increased pattern similarity compared to items located further apart in time. While this study examined temporal events, spatial event boundaries may function similarly in that during retrieval, representations of other within-event items are reinstated. The flat slope observed on Day 3 could thus be a signature of an integrated map.”</p><disp-quote content-type="editor-comment"><p>2. Issues with results presentation: The way that the results are presented makes it very challenging to do apples-to-apples comparisons of different brain regions. For example, Figure 3 shows the &quot;within – across context similarity&quot; measure for EC and Figure 4 shows the &quot;link distance 1 – link distance 2 similarity&quot; measure for the hippocampus. Featuring different measures for different areas makes it hard to compare them. I know that the corresponding apples-to-apples measures are in various supplements but readers should not have to dig for them. If the authors had a priori reasons to feature these different dependent measures for different regions, that might justify the results presentation strategy used here, but – as things stand – it is unclear why some results were featured in the main text and others were relegated to the supplement. In the absence of an a priori justification for the present way of displaying results, I think it would be useful if the authors showed corresponding results for hippocampus and EC right next to each other in the main paper, regardless of significance.</p></disp-quote><p>We appreciate this comment and agree that the organization of the paper makes it difficult to compare results between regions. Accordingly, we revised the figures to present findings from the hippocampus and EC next to each other. Please see our above response to Essential Revisions General comment #2 for additions to the revised manuscript.</p><disp-quote content-type="editor-comment"><p>3. Additional analyses to connect to other findings in the literature: I think it would be useful for the authors to consider whether there are additional exploratory analyses they could run to connect better to other findings in the literature. For example, many of the extant studies on MTL integration and differentiation separately investigate hippocampal subfields. If there are reasons to expect different results in different subfields, it might be worth doing this (but if the scans are not well suited for this purpose, or the authors don't think that there are good reasons to do so, that is fine). Also, the &quot;hippocampal-vmPFC interaction&quot; analysis is unconventional. Usually, this involves some kind of functional connectivity but here it was a second-order similarity analysis. Is there a reason that the authors took this approach? Do the authors think that their paradigm lends itself to interesting predictions relating to the more standard functional connectivity approach? Lastly, there are other ROIs that are known to be involved in spatial learning, e.g., RSC. Why did the authors focus on vmPFC instead of RSC in their analyses?</p></disp-quote><p>The exploratory analysis in vmPFC was flagged by multiple reviewers, and we now take an alternative approach in the resubmission. Please see our response above to Essential Revisions General comment #7 for full details. Because we cannot undertake a more traditional univariate functional connectivity analysis (we did not scan during navigation, and it is thus not clear what would be modulating vmPFC activity during the fMRI task), we have moved the vmPFC section to the Appendix.</p><p>Finally, we agree that additional exploratory analyses (for example, in RSC) have merit. We are currently undertaking an analysis in RSC and hope to publish these findings in the future; however, in this work we focus on a priori hypotheses in EC and hippocampus. Unfortunately, our scanning protocol was not optimized for subfield segmentation.</p><disp-quote content-type="editor-comment"><p>4. I think that it might be useful to show the similarity values for the 1, 2 within-track conditions alongside the 2, 3, and 4 (across-track) conditions. This may not be enough to resolve the question about whether integration is taking place, but looking at these results together (instead of presenting them in separate figures/analyses) might provide a more clear picture of what the regions are doing.</p></disp-quote><p>Following this guidance and other related comments, we revised the figures in the main text to show similarity values for the 1 and 2 within-track conditions (Figure 4), in addition to the 2, 3, and 4 across-track conditions (Figure 5). Please see our above response to Essential Revisions General comment #2 for additions to the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I have a few more suggestions to help with readability/interpretation.</p><p>1. It was difficult to keep track of the results as they related to the different ROIs, within vs. across tracks, distances, and time (related to my comment #5 above). It would be helpful to display all relevant plots in the same way throughout the manuscript, especially as the same mixed-effects models were applied (e.g., the same panels for different regions in the same order). Even if the findings are not significant, it is helpful to see the results in the same figure rather than comparing them with supplementary figures.</p><p>For example, Figure 4B shows data for same-track landmarks collapsed across link distance, and Figure 4C shows data for different-track landmarks split by link distance. This makes it difficult to directly compare these values, even though they were entered into the same model.</p></disp-quote><p>We appreciate this comment and agree that the organization of the paper makes it difficult to compare results across regions. Accordingly, we revised the figures to present findings from the hippocampus and EC next to each other (regardless of significance). Please see our above response to Essential Revisions General comment #2 for additions to the revised manuscript.</p><disp-quote content-type="editor-comment"><p>2. More broadly, the reporting of the results was somewhat difficult to follow. While I appreciate the importance of accounting for all factors in the same model, the order in which different factors were reported sometimes varied across models, making it difficult to keep in mind how the different models compared.</p></disp-quote><p>We appreciate this guidance and revised the manuscript with a focus on increasing the clarity of our results, with an emphasis on how the reported results relate to our a priori hypotheses.</p><disp-quote content-type="editor-comment"><p>3. As I was reading the results, I made a note to suggest a distance analysis only to realize it was carried out later on (p. 16) – it would be helpful to include more signposting to motivate and help the reader anticipate this analysis.</p></disp-quote><p>We appreciate this feedback and added additional language to the beginning of the Results, under the section entitled “fMRI assays of learning-driven representational change”:</p><p>“we extracted voxel-level estimates of neural activity for each stimulus from EC and hippocampus regions-of-interest (ROIs; see Methods) and used pattern similarity analysis to probe whether learning resulted in representational differentiation or integration as a function of (a) the experienced relations between stimuli in the environment (e.g., same vs. different track, see Hippocampus and entorhinal cortex learn to separate the three tracks; distance within and across tracks, see The hippocampus represents both local and global distance) and (b)…”</p><disp-quote content-type="editor-comment"><p>4. Figure 5B was difficult to follow. The solid lines represent similarity prior to the global navigation task for both groups, but the dashed line only represents similarity post-global navigation for the less efficient navigators.</p></disp-quote><p>We removed the median-split analysis from the manuscript and the dashed line from this figure (now Figure 6B). As noted above, our analysis of the across-participant brain-behavior relationship was now conducted using a linear mixed-effects model. However, to qualitatively visualize the findings from the linear model, we retained presentation of the split-half data in Figure 6, and we revised the caption as follows:</p><p>“To qualitatively visualize the relationship between pattern similarity, link distance, and path inefficiency, we split participants into two groups – More Efficient and Less Efficient – based on their median path inefficiency on across-track trials in the first four test runs of the Global Task on Day 2. (A) Path inefficiency (%) for each across-track trial during the first four test runs of the Global Navigation Task, plotted for each participant and colored by performance group. (B) We used a linear mixed-effects model to formally test this relationship (see main text for details). The linear model revealed a significant interaction between path inefficiency and link distance, with the direction of the effect being unexpected. To qualitatively depict this effect, we plot hippocampal pattern similarity for landmarks on different tracks prior to the Global Navigation Task for the Less Efficient and More Efficient median-split data. Data are split by participants’ subsequent navigation performance as shown in (A). (Error bars denote SE of the estimates. More Efficient, n = 11; Less Efficient, n = 10).”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. The path inefficiency metric conflates the effects of local and global knowledge of the environment. For example, when participants switch tracks correctly in the global condition but fail to respond precisely on the goal track, this could result in a relatively high error despite efficient global navigation performance. While the comparison to the local condition is helpful of course, the report of additional behavioral measures would be desirable in my view. Relevant measures could quantify how frequently participants (a) manage to switch tracks correctly and (b) how frequently they choose to navigate in the correct direction after switching tracks.</p></disp-quote><p>We thank Dr. Bellmund for the proposed suggestions. In the revised manuscript, we determined whether participants (a) switched to the correct track and (b) navigated in the correct direction once switching on each Global Navigation trial at the start (first four test runs) and end (last two test runs) of the task on Day 2, as well as at the end of Day 3 (last two test runs). We agree that the addition of these data provides a more complete window onto task performance. Please see the above response to Essential Revisions General comment #6 for full details.</p><disp-quote content-type="editor-comment"><p>2. It is unclear to me why the distance-based analyses (Figure 4) are based on the landmark images only. I would expect a similar effect to be present for the fractal stimuli as participants were asked to memorize the fractal locations. Thus, I think demonstrating a similar effect for the fractals as for the landmarks would substantially strengthen the finding of a hippocampal representation of local and global distances.</p></disp-quote><p>Again, we are grateful for the feedback on how to improve the consistency of results reporting. In the revision, we now include results for fractal stimuli in the revised manuscript; please see our above response to Essential Revisions General comment #5 for full details.</p><disp-quote content-type="editor-comment"><p>3. Again related to the analysis of distance representations in the hippocampus, I am wondering why the authors chose the link distance as their distance measure. I think the paper would benefit from a justification for why this measure was chosen.</p></disp-quote><p>Please see our above response to Dr. Bellmund’s General Comment #3.</p><disp-quote content-type="editor-comment"><p>4. With respect to the formation of cognitive maps of space there is also evidence for the representation of (remembered) Euclidean distances in the hippocampus (e.g. Howard et al., J Neurosci, 2011; Deuker et al., eLife, 2016). I would be curious to see if the authors can detect similar representations of Euclidean distances in their data in an additional exploratory analysis.</p></disp-quote><p>We appreciate Dr. Bellmund’s suggestion; however, our task was not designed with hypotheses regarding Euclidean distances in mind (i.e., there are no distal cues in the environment that participants might use to orient themselves). We ran a small behavioral pilot study that asked participants to draw a map of the environment following Global Navigation, with mixed success. While some participants were able to do this, others were unable to conceptualize a top-down view of the environment. Further, a number of participants thought the tracks were circular instead of oval-shaped, which would distort their estimates of Euclidean distances. For these reasons, we ultimately decided against conducting the proposed exploratory analyses.</p><disp-quote content-type="editor-comment"><p>5. A surprising and somewhat puzzling aspect of the reported results is that the global distance effect can only be detected after the local, but not after the global navigation task. Further, this effect seems to be driven by participants who take less efficient paths. I cannot quite follow the authors' interpretation in the Results section that the &quot;acquisition of a more fully integrated global map&quot; would lead to this pattern of results. Further, the authors state in the Discussion section that a &quot;negative distance-related similarity function reflects restricted learning of the global map, hindering performance on the Global Task&quot;. In my view, a negative relationship between distance and representational similarity could rather point towards the formation of an integrated, map-like representation of the environment, where nearby landmarks share more similar representations than those that are separated by larger distances. I would appreciate some clarification on why the authors think such an effect would hinder task performance and why it might disappear with the &quot;building of a more global cognitive map&quot;.</p></disp-quote><p>As noted above, we appreciate Dr. Bellmund’s input here. We revised and clarified the Discussion based on reviewer comments. Please see our above response to Essential Revisions General comment #4.</p><disp-quote content-type="editor-comment"><p>6. I am wondering about why the authors chose a median split to analyze interindividual differences. As dichotomizing a continuous variable such as navigation performance is often problematic (as discussed in detail e.g. in MacCallum et al., Psychological Methods, 2000), I would like to know whether the effect also holds when using an approach based on correlation/regression.</p></disp-quote><p>As noted above, we are grateful for this input. Following this guidance, we replaced the median-split analysis with a mixed-effects model predicting neural pattern similarity Post Local Navigation, with a continuous metric of task performance (each participant’s median path inefficiency for across-track trials in the first four test runs of Global Navigation) and link distance as predictors. Please see our above response to Dr. Bellmund’s General Comment 5.</p><disp-quote content-type="editor-comment"><p>7. I tried to access the analysis code, but the GitHub repository referred to in the Data availability statement is currently empty.</p></disp-quote><p>We remedied this oversight. The analysis code is now available at https://github.com/coreyfernandez/RID. fMRI data will be uploaded to Open Neuro by the time of publication.</p></body></sub-article></article>