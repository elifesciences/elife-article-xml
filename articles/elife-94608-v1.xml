<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">94608</article-id><article-id pub-id-type="doi">10.7554/eLife.94608</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94608.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Perceptual error based on Bayesian cue combination drives implicit motor adaptation</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-158303"><name><surname>Zhang</surname><given-names>Zhaoran</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4192-4088</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-344855"><name><surname>Wang</surname><given-names>Huijun</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-344856"><name><surname>Zhang</surname><given-names>Tianyang</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-344857"><name><surname>Nie</surname><given-names>Zixuan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-41628"><name><surname>Wei</surname><given-names>Kunlin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5098-3808</contrib-id><email>wei.kunlin@pku.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>School of Psychological and Cognitive Sciences, Peking University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Beijing Key Laboratory of Behavior and Mental Health</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking-Tsinghua Center for Life Sciences, Peking University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution>National Key Laboratory of General Artificial Intelligence</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Orban de Xivry</surname><given-names>Jean-Jacques</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f950310</institution-id><institution>KU Leuven</institution></institution-wrap><country>Belgium</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>04</day><month>07</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP94608</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-23"><day>23</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-11-23"><day>23</day><month>11</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.23.568442"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-26"><day>26</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94608.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-17"><day>17</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94608.2"/></event></pub-history><permissions><copyright-statement>© 2024, Zhang, Wang et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Zhang, Wang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-94608-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-94608-figures-v1.pdf"/><abstract><p>The sensorimotor system can recalibrate itself without our conscious awareness, a type of procedural learning whose computational mechanism remains undefined. Recent findings on implicit motor adaptation, such as over-learning from small perturbations and fast saturation for increasing perturbation size, challenge existing theories based on sensory errors. We argue that perceptual error, arising from the optimal combination of movement-related cues, is the primary driver of implicit adaptation. Central to our theory is the increasing sensory uncertainty of visual cues with increasing perturbations, which was validated through perceptual psychophysics (Experiment 1). Our theory predicts the learning dynamics of implicit adaptation across a spectrum of perturbation sizes on a trial-by-trial basis (Experiment 2). It explains proprioception changes and their relation to visual perturbation (Experiment 3). By modulating visual uncertainty in perturbation, we induced unique adaptation responses in line with our model predictions (Experiment 4). Overall, our perceptual error framework outperforms existing models based on sensory errors, suggesting that perceptual error in locating one’s effector, supported by Bayesian cue integration, underpins the sensorimotor system’s implicit adaptation.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>motor learning</kwd><kwd>procedural learning</kwd><kwd>implicit motor learning</kwd><kwd>prediction error</kwd><kwd>bayesian cue combination</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>62061136001</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Kunlin</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32071047</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Kunlin</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31871102</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Kunlin</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32300868</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Zhaoran</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>STI2030-Major Project 2021ZD0202600</institution></institution-wrap></funding-source><award-id>2021ZD0202601</award-id><principal-award-recipient><name><surname>Wei</surname><given-names>Kunlin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Model based on Bayesian cue combination shows that procedural motor learning is driven by perceptual error in localizing one's effector.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To achieve and sustain effective motor performance, humans consistently recalibrate their sensorimotor systems to adapt to both internal and external environmental disturbances (<xref ref-type="bibr" rid="bib6">Berniker and Kording, 2008</xref>; <xref ref-type="bibr" rid="bib51">Shadmehr et al., 2010</xref>; <xref ref-type="bibr" rid="bib70">Wolpert et al., 2011</xref>). For instance, transitioning to a high-sensitivity gaming mouse, which drives cursor movement at an accelerated rate compared to a standard computer mouse, may initially result in decreased performance in computer-related tasks. However, humans are capable of rapidly adapting to this new visuomotor mapping within a short period of time. While conscious corrections can facilitate this adaptation process, our sensorimotor system often times adapts itself implicitly without our conscious efforts (<xref ref-type="bibr" rid="bib2">Albert et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Krakauer et al., 2019</xref>).</p><p>While recent research has intensively examined the interplay between explicit and implicit learning systems (<xref ref-type="bibr" rid="bib3">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Miyamoto et al., 2020</xref>), several characteristics of implicit motor adaptation have emerged that challenge traditional theories. Conventionally, motor adaptation is conceptualized as error-based learning, in which learning accrues in proportion to the motor error experienced (<xref ref-type="bibr" rid="bib10">Cheng and Sabes, 2006</xref>; <xref ref-type="bibr" rid="bib15">Donchin et al., 2003</xref>; <xref ref-type="bibr" rid="bib58">Thoroughman and Shadmehr, 2000</xref>). However, implicit adaptation exhibits an overcompensation phenomenon where the extent of adaptation surpasses the error induced by visual perturbations (<xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Morehead et al., 2017</xref>). Additionally, implicit adaptation manifests a saturation effect; it increases with perturbations but plateaus across a broad range of larger perturbations (<xref ref-type="bibr" rid="bib8">Bond and Taylor, 2015</xref>; <xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Morehead et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Neville and Cressman, 2018</xref>). These observations of overcompensation and saturation are incongruent with prevailing state-space updating models, which presuppose that incremental learning constitutes only a fraction of the motor error (<xref ref-type="bibr" rid="bib33">McDougle et al., 2015</xref>; <xref ref-type="bibr" rid="bib53">Smith et al., 2006</xref>). Another aspect of implicit adaptation that remains mechanistically unexplained pertains to its impact on proprioception. In traditional motor adaptation, proprioception is biased towards visual perturbation, maintaining a stable bias throughout the adaptation process (<xref ref-type="bibr" rid="bib45">Ruttle et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Ruttle et al., 2021</xref>). In contrast, implicit adaptation initially biases proprioceptive localization of the hand towards the visual perturbation, but this bias gradually drifts in the opposite direction over time (<xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref>).</p><p>Causal inference of motor errors has been suggested to explain the discounting of large perturbations (<xref ref-type="bibr" rid="bib68">Wei and Körding, 2009</xref>). However, the causal inference account predicts a decline in adaptation with increasing perturbation, diverging from the observed ramp-like saturation effect. Tsay and colleagues recently synthesized existing evidence to propose that implicit adaptation reaches an upper bound set by cerebellar error correction mechanisms, reflected in a ramp-like influence of vision on proprioception (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>). While this ramp function (instantiated as proprioceptive re-alignment mode, PReMo) could explain the observed saturation, the postulate of an upper bound on visual influence lacks empirical validation. Some research supports the idea of saturation in proprioceptive recalibration (<xref ref-type="bibr" rid="bib36">Modchalingam et al., 2019</xref>), yet other studies suggest a linear increase with visual perturbations (<xref ref-type="bibr" rid="bib44">Rossi et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Salomonczyk et al., 2011</xref>). Additionally, current models fall short of quantitatively capturing the time-dependent shifts in proprioceptive bias associated with implicit adaptation.</p><p>In this study, we put forth a unified model to account for the distinct features of implicit adaptation based on the Bayesian combination of movement-related cues. Prior models have overlooked the fact that visual uncertainty related to the perturbation increases with the size of the perturbation as the cursor moves further from the point of fixation and into the visual periphery (<xref ref-type="bibr" rid="bib24">Klein and Levi, 1987</xref>; <xref ref-type="bibr" rid="bib30">Levi et al., 1987</xref>). This is particularly pertinent for implicit adaptation that is widely investigated by the so-called error-clamp paradigm, in which participants are instructed to fixate on the target and disregard the perturbing cursor. Moreover, conventional theories of motor adaptation define motor error according to the sensory modality of the perturbation, that is, visual errors for visual perturbations (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>; <xref ref-type="bibr" rid="bib68">Wei and Körding, 2009</xref>). We propose an alternative: perceptual error drives implicit adaptation, as the sensory perturbation influences the perception of the effector’s position and, subsequently, motor adaptation. Through a series of experiments, we aim to demonstrate that combining eccentricity-induced visual uncertainty (Experiment 1) with a traditional motor adaptation model (state-space model) and a classical perception model (Bayesian cue combination) can explain both over-compensation and saturation effects (Experiment 2), as well as the time-dependent changes in hand localization (<xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref>). Finally, to provide causal evidence supporting our Perceptual Error Adaptation (PEA) model, we manipulated visual uncertainty and observed that subsequent adaptation was attenuated for large perturbations but not for small ones—a finding that contradicts existing models but aligns well with the PEA model. Across the board, our model outperforms those based on ramp error correction (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>) and causal inference of errors (<xref ref-type="bibr" rid="bib68">Wei and Körding, 2009</xref>), offering a more parsimonious explanation for the salient features of implicit adaptation.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The perceptual error adaptation model with varying visual uncertainty</title><p>We start by acknowledging that the perceptual estimation of effector position is dynamically updated and influenced by sensory perturbations during motor adaptation. For implicit adaptation studied via the error-clamp paradigm, participants are required to bring their hand to the target while ignoring the direction-clamped cursor (<xref ref-type="bibr" rid="bib37">Morehead et al., 2017</xref>). Accordingly, the perceptual estimation of the hand movement direction relies on three noisy sensory cues: the visual cue from the cursor, the proprioceptive cue from the hand, and the sensory prediction of the reaching action (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Without loss of generality, we posit that each cue is governed by an independent Gaussian distribution: the visual cue <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> follows <inline-formula><mml:math id="inf2"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></inline-formula>, where θ is the cursor direction and <inline-formula><mml:math id="inf3"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is visual variance, the proprioceptive cue <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> follows <inline-formula><mml:math id="inf5"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the hand movement direction and <inline-formula><mml:math id="inf7"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is proprioceptive variance, and the sensory prediction cue <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> follows <inline-formula><mml:math id="inf9"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf10"><mml:mi>T</mml:mi></mml:math></inline-formula> is the target direction and <inline-formula><mml:math id="inf11"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is prediction variance. Participants aim for the target, expecting their hand to reach it. Using the Bayesian cue combination framework (<xref ref-type="bibr" rid="bib7">Berniker and Kording, 2011</xref>), the perceived hand location (<inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) on trial <italic>n</italic> can be derived:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The Perceptual Error Adaptation (PEA) model for error-clamp adaptation.</title><p>(<bold>A</bold>) Illustration of involved sensorimotor cues for estimating hand direction <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> . The clamped cursor, the hand, and the sensory prediction of the reaching action provide the visual (<inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), proprioceptive (<inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), and the sensory prediction cue (<inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) of movement direction, respectively. The hand direction estimate is assumed to be based on maximum likelihood cue combination. (<bold>B</bold>) Assuming a linear dependency of visual uncertainty on eccentricity, the PEA model predicts that implicit adaptation extent is a concave function of perturbation size θ, a pattern qualitatively aligning with previous findings (<xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Morehead et al., 2017</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig1-v1.tif"/></fig><p>This estimated hand position is derived using maximum likelihood estimation from the three noisy cues. Given that the clamped cursor deviates the target by θ, the visual cue <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> biases the hand estimate <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> towards the cursor’s direction. This deviation from the target direction <inline-formula><mml:math id="inf19"><mml:mi>T</mml:mi></mml:math></inline-formula> constitutes the perceptual error, which drives adaptation on the subsequent trial n+<italic>1</italic> (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Consisting with existing models (<xref ref-type="bibr" rid="bib3">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Cheng and Sabes, 2006</xref>; <xref ref-type="bibr" rid="bib18">Herzfeld et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">McDougle et al., 2015</xref>), trial-to-trial adaptation is modeled using a state-space equation:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf20"><mml:mi>A</mml:mi></mml:math></inline-formula> is the retention rate capturing inter-movement forgetting and <inline-formula><mml:math id="inf21"><mml:mi>B</mml:mi></mml:math></inline-formula> is the learning rate capturing the proportion of error corrected within a trial. The interplay between forgetting and learning dictates the overall learning extent, that is, the asymptote of <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> :<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, the positive influence of perturbation size θ on the adaptation extent is counterbalanced by the rise in visual uncertainty <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , since sensory uncertainty of various visual stimuli increases linearly with eccentricity (<xref ref-type="bibr" rid="bib24">Klein and Levi, 1987</xref>; <xref ref-type="bibr" rid="bib30">Levi et al., 1987</xref>). As participants are instructed to fixate on the target, an increase in θ leads to increased eccentricity. Hence, we model this linear increase in visual uncertainty by<disp-formula id="equ4">, <label>(4)</label><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>θ</mml:mi></mml:math></disp-formula></p><p>where <italic>a</italic> and <italic>b</italic> are free parameters. We conducted simulations of implicit adaptation with varying error clamp size (<inline-formula><mml:math id="inf24"><mml:mi>θ</mml:mi></mml:math></inline-formula>). The model simulation closely resembles the saturated adaptation in three independent experiments (<xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Morehead et al., 2017</xref>). In fact, our PEA model predicts a concave adaptation pattern, contrasting with the ramp pattern suggested by the PReMo model (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>). In Experiment 1, we aim to validate the assumption of a linear increase in visual uncertainty (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>); in Experiment 2, we seek to verify whether implicit adaptation adheres to a concave pattern as prescribed by the PEA model. Subsequent experiments, namely Experiments 3 and 4, will test the model’s additional novel predictions concerning changes in proprioception and the impact of experimentally manipulated visual uncertainty on adaptation, respectively.</p></sec><sec id="s2-2"><title>Experiment 1: Visual uncertainty increases with perturbation size</title><p>To quantify visual uncertainty in a standard error-clamp adaptation setting, we employed psychometric methods. Occluded from seeing their actual hand, participants (n=18) made repetitive reaches to a target presented 10 cm straight head while an error-clamped cursor moving concurrently with one of three perturbation sizes (i.e. 4°, 16°, and 64°), randomized trial-by-trial. In alignment with the error-clamp paradigm, participants were instructed to fixate on the target and ignore the rotated cursor feedback. Eye-tracking confirmed compliance with these instructions (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Perturbation directions were counterbalanced across trials, with equal probability of clockwise (CW) and counterclockwise (CCW) rotation. Post-movement, participants were required to judge the cursor’s rotation direction (CW or CCW) relative to a briefly displayed reference point (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and Figure 6A). Employing this two-alternative forced-choice (2AFC) task and the Parameter Estimation by Sequential Testing (PEST) procedure (<xref ref-type="bibr" rid="bib31">Lieberman and Pentland, 1982</xref>), we derived psychometric functions for visual discrimination of cursor movement direction (Figure 6 and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Our findings reveal a significant increase in visual uncertainty (<inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) with perturbation size for both CW and CCW rotations (Friedman test, CW direction: χ<sup>2</sup>(2)=34.11, p=4e-8; CCW: χ<sup>2</sup>(2)=26.47, p=2e-6). Given the symmetry for the two directions, we collapsed data from both directions, and confirmed the linear relationship between <inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and θ by a generalized linear model: <inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>θ</mml:mi></mml:math></inline-formula>, with  <inline-formula><mml:math id="inf28"><mml:mi>a</mml:mi></mml:math></inline-formula> = 1.853 and <inline-formula><mml:math id="inf29"><mml:mi>b</mml:mi></mml:math></inline-formula> = 0.309, R<sup>2</sup>=0.255 (<italic>F</italic>=51.6, p=2.53e-9). The 95% confidence intervals (CI) for <inline-formula><mml:math id="inf30"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:mi>b</mml:mi></mml:math></inline-formula> are [0.440, 3.266] and [0.182, 0.435], respectively. The intercept was similar to the visual uncertainty estimated in a previous study (<xref ref-type="bibr" rid="bib60">Tsay et al., 2021a</xref>). We thus observed a striking sevenfold increase in visual uncertainty from a 4° perturbation to a 64° perturbation (22.641±6.024° vs. 3.172±0.453°). In addition, we observed significant correlations of <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between different perturbation sizes (Spearman correlation, 4° and 16°: ñ=0.795, p&lt;0.001; 16° and 64°: ñ=0.527, p=0.026). This finding further confirmed that the relative magnitude of visual uncertainty among individuals was consistent across perturbation sizes.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Experiment 1 measuring the dependency of visual uncertainty on perturbation size.</title><p>(<bold>A</bold>) Illustration of all possible cursor endpoints during the experiment and the procedure of the 2AFC task for judging the cursor motion direction. In an exemplary trial, the participant reaches to a target while a direction-clamped cursor moves concurrently, serving as an error-clamp perturbation. Following a 1000 ms blank masking period, a reference point appears for 150ms, either clockwise or counterclockwise from the clamped cursor. The participant is then asked to make a binary judgment regarding the direction of the clamped cursor relative to the reference point. (<bold>B</bold>) The visual uncertainty, obtained from psychometrical estimation based on the 2AFC, is plotted as a function of perturbation size (n = 18). Both individual estimates (red dots) and group-level statistics (boxplots) are shown. Positive angles correspond to CW rotations, negative angles to CCW rotations. (<bold>C</bold>) Collapsing data from both rotation directions, we observe that visual uncertainty closely follows a linear function of perturbation size. The dark gray line and its shaded region denote the regression line and its 95% confidence interval, respectively. The purple line is generated with the values of <italic>a</italic> and <italic>b</italic> fitted from data in Experiment 2 with <italic>a</italic> and <italic>b</italic> treated as free parameters (see Methods for details).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Heat map of eye fixations during the 2AFC task in Experiment 1.</title><p>The screen is partitioned into 10x10 pixel grids, and the cumulative number of gaze samples in each grid is recorded. Data from all participants, aggregated across each day of practice, are presented. The color map signifies the normalized count of gaze samples in each grid. Data are separately displayed for the three distinct phases of a trial, as delineated by the columns on the left, middle, and right. These correspond to periods during hand movement, the appearance of the visual mask and reference point, and the time allotted for manual response. On average, 95.06%, 89.93%, and 86.55% of gaze samples fall within the ±50-pixel range of the central line during these three phases, respectively. These results corroborate that participant adhered to the instructions and refrained from looking at the cursor during the visual discrimination task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Performance of an exemplary participant in Experiment 1.</title><p>Six panels display the psychometric curves corresponding to different error-clamp sizes. The x-axis denotes the angular deviation between the clamped cursor and the reference point (as depicted in <xref ref-type="fig" rid="fig6">Figure 6A</xref>). A negative value implies that the reference point appears on the counterclockwise (CCW) side of the clamped cursor. The blue dots represent the proportion of trials in which the participant reported that ‘the yellow point is on the clockwise (CW) side of the clamped cursor’ for various angular deviations between these two. Data were aggregated from all trials across three days of the experiment. The gray-shaded region represents the interquartile range (25th to 75th percentile) of the psychometric curve, and the width of this shaded region serves as an indicator of the amplitude of visual uncertainty.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig2-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Experiment 2: Visual uncertainty modulated perceptual error accounts for overcompensation and saturation in implicit adaptation</title><p>The critical test of the PEA model lies in its ability to employ the increase in visual uncertainty obtained from Experiment 1 to precisely explain key features of implicit adaptation. Earlier research mostly scrutinized smaller perturbation angles when reporting saturation effects (<xref ref-type="bibr" rid="bib8">Bond and Taylor, 2015</xref>; <xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>). In contrast, Experiment 2 involved seven participant groups (n=84) in characterizing implicit adaptation across an extensive range of perturbation sizes (i.e. 2°, 4°, 8°, 16°, 32°, 64°, and 95°). After 30 baseline training cycles without perturbations, each group underwent 80 cycles of error-clamped reaching and 10 washout cycles without visual feedback (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We replicated key features of implicit adaptation: it incrementally reached a plateau and then declined during washout. Small perturbations led to overcompensation beyond visual errors: for 2°, 4°, 8°, and 16° clamp sizes, the adaptation was substantially larger than the perturbation itself. Across perturbation sizes, the faster the early adaptation, the larger the final adaptation level (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). We noticed that our adaptation phase might not be long enough for small clamp-size conditions to fully adapt (e.g. 2°, 4°, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Nevertheless, we operationally used the adaptation level achieved at the end of the adaptation phase as the measure of adaptation extent. Critically, the adaptation extent displayed a concave pattern: increasing steeply for smaller perturbations and tapering off for larger ones (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). A one-way ANOVA revealed a significant group difference in adaptation extent (F(6,83) = 12.108, p=1.543e-09). Planned contrasts indicated that 8°, 16°, and 32° perturbations did not differ from each other (all p&gt;0.417, with Tukey-Kramer correction), consistent with earlier evidence of invariant implicit adaptation (<xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>). However, 64° and 95° perturbations led to significantly reduced adaptation extents compared to 8° (p=3.194e-05 and 5.509e-06, respectively), supporting the concave pattern as a more accurate portrayal of implicit adaptation across varying perturbation sizes.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Results and model fitting of Experiment 2.</title><p>(<bold>A</bold>) Implicit adaptation to error clamps of varying sizes is depicted (n = 84); colored dot-lines and colored shading area represent the mean and standard error for each participant group. The light gray area indicates trials with error-clamp perturbations. Adaptation starts after baseline, gradually asymptotes to its final extent, and then decays with null feedback during washout. Different perturbation sizes result in distinct adaptation rates and extents. Group averages and standard error across participants are shown, along with predictions (colored solid lines) from the PEA model. (<bold>B</bold>) The adaptation extent (cycle 100–110) exhibits a nonlinear dependency on perturbation size, conforming to a concave function as prescribed by PEA (purple line). Color dots and error bars denote the mean and standard error across participants in each group. (<bold>C–D</bold>) The same data fitted with the PReMo model and the causal inference model. For more details, refer to <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Model fitting for observed implicit adaptation in Experiment 2.</title><p>This supplementary figure provides a comprehensive evaluation of the three competing models: the PEA model, the PReMo model, and the causal inference model. (<bold>A</bold>) Results of PEA Model Fitting: The layout of these plots mirrors that of <xref ref-type="fig" rid="fig3">Figure 3A and B</xref>, serving as a direct comparison between the empirical data and the predictions made by the PEA model. (<bold>B</bold>) Results of PReMo Model Fitting: The left panel is a duplicate of <xref ref-type="fig" rid="fig3">Figure 3C</xref>, while the right panel presents the trial-by-trial data fitting. This juxtaposition allows for a nuanced evaluation of the PReMo model’s performance at both the aggregate and individual trial levels. (<bold>C</bold>) Results of Causal Inference Model: The arrangement of these plots is consistent with panels (<bold>A</bold>) and (<bold>B</bold>), facilitating a straightforward comparison of all three models. For a detailed assessment of the quality of model fitting and subsequent model comparisons, please refer to <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1b</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Correlation between initial learning rate and adaptation extent in Experiment 2.</title><p>For each participant, the initial learning rate is calculated as the change in hand angle between the 1st and 10th cycle, divided by 10. The adaptation extent is defined as the average hand angle across the last 10 adaptation cycles. When pooling data across all perturbation sizes, a significant correlation is observed between the initial learning rate and the adaptation extent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Model fitting of single-trial learning from Experiment 2 of <xref ref-type="bibr" rid="bib60">Tsay et al., 2021a</xref>.</title><p>Blue dots represent the mean single-trial learning across varying perturbation size, with error bars denoting standard errors across participants. The left, middle and right panel present the fitting results for the PEA, PReMo, and causal inference models, respectively. For additional details, refer to the Methods, Results, and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a and b</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Alternative Model fitting with PEA for Experiment 2; see details in Appendix 2.</title><p>(<bold>A</bold>) Results of PEA Model Fitting. The layout is similar to <xref ref-type="fig" rid="fig3">Figure 3A</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>. (<bold>B</bold>) Fitted ratio between uncertainties of different cues. (<bold>C</bold>) Estimated actual value of uncertainties of different cues.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig3-figsupp4-v1.tif"/></fig></fig-group><p>Importantly, the PEA model, when augmented with visual uncertainty data from Experiment 1, precisely predicts this size-dependent adaptation behavior (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Beyond adaptation extent, the model also accurately predicts the trial-by-trial adaptation across all seven participant groups, employing a single parameter set (R<sup>2</sup>=0.975; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). The model had only four free parameters (<italic>A</italic>=0.974, <italic>B</italic>=0.208, <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 11.119°, <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 5.048°; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref>). Remarkably, both the retention rate <italic>A</italic> and learning rate <italic>B</italic> are consistent with previous studies focusing on visuomotor rotation adaptation (<xref ref-type="bibr" rid="bib3">Albert et al., 2022</xref>). We also quantified proprioceptive uncertainty (<inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) in a subset of participants (n=13) using a similar 2AFC procedure as in Experiment 1. We found that <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was 9.737°±5.598° (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), which did not statistically differ from the <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> value obtained from the model fitting (two-tailed t-test, p=0.391). We also quantified <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as the standard deviation of movement direction from baseline in Experiment 2.The calculated standard deviation is 5.128°±0.190°, also not significantly differ from the estimated <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by model fitting (two-tailed t-test, p=0.693). To further verify the model, a separate data fitting without assuming a linear function in the visual uncertainty was conducted on the data of Experiment 2 (see details in Appendix 2 and <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). The estimated visual uncertainty has a strong linear relationship with perturbation size (<italic>R</italic>=0.991, p&lt;0.001). In addition, the slope of model-fitted visual uncertainty is very close to the values we obtained in Experiment 1. In summary, the perceptual parameters obtained in Experiment 1, when incorporated into the PEA model, effectively explain the implicit adaptation behaviors observed in different participant groups in Experiment 2.</p><p>In comparative analysis, the PReMo model yields a substantially lower R<sup>2</sup> value of 0.749 (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). It tends to underestimate adaptation for medium-size perturbations and overestimate it for large ones (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; see also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref> for trial-by-trial fitting). Another alternative is the causal inference model, previously shown to account for nonlinearity in motor learning (<xref ref-type="bibr" rid="bib34">Mikulasch et al., 2022</xref>; <xref ref-type="bibr" rid="bib68">Wei and Körding, 2009</xref>). Although this model has been suggested for implicit adaptation (<xref ref-type="bibr" rid="bib60">Tsay et al., 2021a</xref>), it fails to reproduce the observed concave adaptation pattern (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3C</xref>). The model aligns well with adaptations to medium-size perturbations (8°, 16°, and 32°) but falls short for small and large ones, yielding an R<sup>2</sup> value of 0.711 (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref> for trial-by-trial fits). Model comparison metrics strongly favor the PEA model over both the PReMo and causal inference models, as evidenced by AIC scores of 2255, 3543, and 3283 for the PEA, PReMo, and causal inference models, respectively (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1b</xref>). In summary, it is the eccentricity-induced visual uncertainty that most accurately accounts for the implicit adaptation profile across a broad spectrum of perturbation sizes, rather than saturated visual influence or causal inference of error.</p></sec><sec id="s2-4"><title>Experiment 3: Cue combination accounts for changes in proprioception measures during implicit adaptation</title><p>Motor adaptation not only recalibrates the motor system but also alters proprioception (<xref ref-type="bibr" rid="bib44">Rossi et al., 2021</xref>) and even vision (<xref ref-type="bibr" rid="bib52">Simani et al., 2007</xref>). For implicit adaptation, the perceived hand location shifted towards the clamped cursor immediately upon the introduction of the error-clamp, but then gradually drifted away from the clamped cursor (<xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref>), shown as a gradual change from negative to positive relative to the target (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This perceived hand location was verbally reported after an active movement with the error-clamp, with the hand staying at the end of the reach. Thus, it is not the so-called proprioceptive recalibration that is typically probed using a passively located hand (<xref ref-type="bibr" rid="bib11">Cressman and Henriques, 2009</xref>; <xref ref-type="bibr" rid="bib38">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="bib57">’t Hart and Henriques, 2016</xref>; <xref ref-type="bibr" rid="bib60">Tsay et al., 2021a</xref>). Nevertheless, this intriguing gradual drift of active localization of the hand is informative of the underlying mechanism of implicit adaptation. The PReMo model proposes that the initial negative drift reflects a misperceived hand location, which gradually reduces to zero, and the late positive drift reflects the influence of visual calibration of the target (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>). However, this assumption lacks empirical validation, and how visual recalibration relates to a proprioceptive measurement without visual cues is not laid out either. In contrast, we suggest that the perceived hand location is based on the same Bayesian cue combination principle we laid out in the PEA model. In this particular setting, the perceived hand location at the end of each reach is determined by the proprioceptive cue (<inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and the estimated hand position under the influence of clamped feedback (<inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), the two relevant cues at the time of reporting relative to the target position.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Proprioceptive data fitting and results from Experiment 3.</title><p>(<bold>A</bold>) The data from <xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref> are presented alongside the fitting of the PEA model. Participants adapting to a 30° error-clamp perturbation were required to report the location of their adapted hand using visual aids of numbers. The report was provided when the hand stayed at the end of movement. Initially, the proprioceptive estimate of the hand is biased toward the visual cursor (negative in the plot) and then gradually shifts toward the hand (positive in the plot). This trend is accurately captured by the PEA model: lines represent model fitting results, with the adapted hand direction in indigo and the reported hand direction in blue. The hand direction estimate (<inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) following a reach movement is shown in red. (<bold>B–C</bold>) Model simulations for proprioceptive bias from the PEA and PReMo models. Color gradients denote the simulations with varying ratio between the weights of <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the two cues available for estimating the hand direction. Note that the two models prescribe distinct profiles for the dependency of proprioception bias on perturbation size. (<bold>D</bold>) Experimental design. A reaching block, either with or without visual perturbations, is followed by a proprioception test block. The size and direction of the visual perturbation vary across blocks. The proprioception test is conducted when the hand is passively moved to a target (red dots) situated near the reaching target (blue dots). (<bold>E</bold>) The observed proprioceptive bias as a function of perturbation size (n = 11). Data from the three proprioception test trials are separately plotted. The first trial reveals proprioception biases that form a concave function of perturbation size.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Proprioception uncertainty estimation results.</title><p>Thirteen participants from Experiment 1 participated in a proprioception discrimination task to measure their proprioceptive uncertainty in the setting of the error-clamp adaptation. The setup paralleled that used for estimating visual uncertainty in Experiment 1. In each trial, participants initially held their hand at the starting position. They were instructed to relax their arm while the experimenter, seated on the other side of the monitor, pulled their hand to a proprioceptive target near the straight-ahead target. The arms of the experimenter and the participant were blocked from the view of the participant. After 0.8 s, a yellow reference point appeared. The angular deviation between the participant’s hand and this reference was determined using the same PEST procedure employed in Experiment 1. Participants indicated, by pressing left or right arrow keys by their left hand, whether the reference point appeared on the CW or CCW side of their actual right-hand position. The maximum deviation allowed was 30°, with an initial step size of 10° and a stop threshold of 0.5°. This task was conducted over six runs across 3 consecutive days. Similar to <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, panel (<bold>A</bold>) to (<bold>M</bold>) show the psychometric curves for each participant with data from the 3 days pooled together. (<bold>N</bold>) and (<bold>O</bold>) present the measured proprioceptive uncertainty and bias for all participants (gray dots) and their mean ± standard deviation (red error bars).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig4-figsupp1-v1.tif"/></fig></fig-group><p>During early adaptation, <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is biased towards the clamped feedback (small negative value, <xref ref-type="fig" rid="fig4">Figure 4A</xref>), while <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> remains near the target as the motor system has yet to adapt (close to 0, <xref ref-type="fig" rid="fig4">Figure 4A</xref>). This results in an initial negative proprioceptive bias. As adaptation progresses, although <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> remains biased with a small negative value, <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> gradually shifts in the positive direction due to adaptation (value changed from near zero to increasingly positive), resulting in an increasingly positive perceived hand location. Remarkably, the PEA model can predict these temporal changes in the localization of an actively located hand with high accuracy (R<sup>2</sup>=0.982; <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><p>We postulate that the same Bayesian cue combination governs not only the active localization of the hand in <xref ref-type="fig" rid="fig4">Figure 4A</xref> but also the proprioceptive recalibration that is probed by the passive localization of the hand. With the biased <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> during adaptation, our PEA model can make specific quantitative predictions about the relationship between proprioception changes, measured by a passively located hand, and visual perturbation size. While traditional visuomotor paradigms suggest either invariant (<xref ref-type="bibr" rid="bib36">Modchalingam et al., 2019</xref>) or linear increases in proprioceptive recalibration with an increase in visual-proprioceptive discrepancy (<xref ref-type="bibr" rid="bib48">Salomonczyk et al., 2011</xref>), the PEA model prescribes a concave function in relation to visual perturbation size simply due to the fact that <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> follows a concave function in relation to the perturbation size (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>).</p><p>To empirically test this prediction, Experiment 3 (n=11) measured participants' proprioceptive recalibration during implicit adaptation using a procedure similar to the error-clamp perturbations in Experiment 2. After each block of six adaptation trials, participants’ right hands were passively moved by a robotic manipulandum, and they indicated the perceived direction of their right hand using a visually represented ‘dial’ controlled by their left hand (Figure 7B). This method quantifies proprioceptive recalibration during adaptation (<xref ref-type="bibr" rid="bib11">Cressman and Henriques, 2009</xref>). Each adaptation block was followed by three such proprioception test trials. The alternating design between adaptation and proprioception test blocks allowed us to assess proprioceptive biases across varying perturbation sizes, which consisted of ± 10°, ± 20°, ± 40°, and ± 80°, to cover a wide range (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><p>Our findings confirmed a typical proprioceptive recalibration effect, as the perceived hand direction was biased towards the visual perturbation (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Importantly, the bias in the initial proprioception test trial exhibited a concave function of perturbation size. A one-way repeated-measures ANOVA revealed a significant effect of perturbation size (F(3,30)=3.603, p=0.036), with the 20° and 40° conditions displaying significantly greater proprioceptive bias compared to the 80° condition (pairwise comparisons: 20<sup>o</sup> v.s. 80<sup>o</sup>, p=0.034; 40<sup>o</sup> v.s. 80<sup>o</sup>, p=0.003). The bias was significantly negative for 20° and 40° conditions (p=0.005 and p=0.007, respectively, with one-tailed t-test) but not for 10° and 80° conditions (p=0.083 and p=0.742, respectively). The concave pattern aligns well with the PEA model’s predictions (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), further consolidating its explanatory power.</p><p>This stands in contrast to the PReMo model, which assumes a saturation for the influence of the visual cue on the hand estimate (<xref ref-type="disp-formula" rid="equ12 equ13">Equations 12 and 13</xref>). As a result, PReMo’s predicted proprioceptive bias follows a ramp function, deviating substantially from our empirical findings (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). The causal inference model, which mainly focuses on the role of visual feedback in error correction, cannot directly predict changes in proprioceptive recalibration.</p><p>Interestingly, we observed that the proprioceptive bias was reduced to insignificance by the third trial in each proprioception test block (one-tailed t-test, all p&gt;0.18; <xref ref-type="fig" rid="fig4">Figure 4E</xref>, yellow line). The observed proprioceptive bias is formally modeled as a result of the biasing effect of the perceived hand estimate <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> . In our mini-block of passive localization, the participants neither actively moved nor received any cursor perturbations for three trials in a row. Thus, the fact that the measured proprioceptive bias is reduced to nearly zero at the third trial suggests that the effect of perceived hand estimate <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> decays rather rapidly.</p></sec><sec id="s2-5"><title>Experiment 4: Differential impact of upregulated visual uncertainty on implicit adaptation across perturbation sizes</title><p>Thus far, we have presented both empirical and computational evidence underscoring the pivotal role of perceptual error and visual uncertainty in implicit adaptation. It is crucial to note, however, that this evidence is arguably correlational, arising from natural variations in visual uncertainty as a function of perturbation size. To transition from correlation to causation, Experiment 4 (n=19) sought to directly manipulate visual uncertainty by blurring the cursor, thereby offering causal support for the role of multimodal perceptual error in implicit adaptation.</p><p>By increasing visual uncertainty via cursor blurring, we hypothesized a corresponding decrease in adaptation across all perturbation sizes. Notably, the PEA model predicts a size-dependent attenuation in adaptation: the reduction is less marked for smaller perturbations and more pronounced for larger ones (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). This prediction diverges significantly from those of competing models. The PReMo model, operating under the assumption of a saturation effect for large visual perturbations, predicts that cursor blurring will only influence adaptation to smaller perturbations, leaving adaptation to larger perturbations unaffected (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The causal inference model makes an even more nuanced prediction: it anticipates that the blurring will lead to a substantial reduction in adaptation for small perturbations, a diminishing effect for medium perturbations, and a potential reversal for large perturbations (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). This prediction results from the model’s core concept that causal attribution of the cursor to self-action—which directly dictates the magnitude of adaptation—decreases for small perturbations but increases for large ones when overall visual uncertainty is elevated.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Results of Experiment 4.</title><p>(<bold>A–C</bold>) Model simulations for single-trial learning under different visual uncertainty levels, shown separately for the PEA, PReMo and causal inference models. Blue curves represent simulated learning based on model parameters estimated from Experiment 2. Curves with red gradient indicate simulations with increasing levels of visual uncertainty, color coded by the ratio of visual uncertainty for the blurred cursor to that of the clear cursor. (<bold>D</bold>) Experimental design. Following 60 baseline trials without perturbations, participants completed 15 mini-blocks of error-clamp adaptation over three successive days. Each mini-block features 12 different types of error-clamp perturbations, distinguished by two cursor presentations (blurred or clear cursor) and six clamp sizes. Each perturbation trial, varied randomly in perturbation type, is flanked by two no-feedback trials. The change in hand direction between these two no-feedback trials serves to quantify singe-trial learning. (<bold>E</bold>) The single-trial learning with the blurred cursor is less than that with the clear cursor, but the difference is non-monotonic across perturbation size (n = 19, *** denote p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig5-v1.tif"/></fig><p>Starting from the above predictions, Experiment 4 was designed to assess the impact of elevated visual uncertainty across small (4°), medium (16°), and large (64°) perturbation sizes. Visual uncertainty was augmented by superimposing a Gaussian blurring mask on the cursor (<xref ref-type="bibr" rid="bib9">Burge et al., 2008</xref>). Each participant performed reaching tasks with either a standard or blurred clamped cursor for a single trial, bracketed by two null trials devoid of cursor feedback (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). These three-trial mini-blocks permitted the quantification of one-trial learning as the directional difference of movements between the two null trials. To preclude the cumulative effect of adaptation, perturbation sizes and directions were randomized across mini-blocks.</p><p>Crucially, our findings corroborated the predictions of the PEA model: visual uncertainty significantly diminished adaptation for medium and large perturbations (16° and 64°), while leaving adaptation for small perturbations (4°) largely unaffected (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). A two-way repeated-measures ANOVA, with two levels of uncertainty and three levels of perturbation size, revealed a significant main effect of increased visual uncertainty in reducing implicit adaptation (F(1,18) = 42.255, p=4.112e-06). Furthermore, this effect interacted with perturbation size (F(2,36) = 5.391, p=0.012). Post-hoc analyses demonstrated that elevated visual uncertainty significantly attenuated adaptation for large perturbations (p=2.877e-04, d=0.804 for 16<sup>o</sup>; p=1.810e-05, d=1.442 for 64<sup>o</sup>) but exerted no such effect on small perturbations (p=0.108, d=0.500). These empirical outcomes are not congruent with the predictions of either the PReMo or the causal inference models (<xref ref-type="fig" rid="fig5">Figure 5B and C</xref>). This lends compelling empirical support to the primacy of perceptual error in driving implicit adaptation, as posited by our PEA model.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we elucidate the central role of perceptual error, derived from multimodal sensorimotor cue integration, in governing implicit motor adaptation. Utilizing the classical error-clamp paradigm, we uncover that the overcompensation observed in response to small perturbations arises from a sustained perceptual error related to hand localization, and the saturation effect commonly reported in implicit adaptation is not an intrinsic characteristic of cerebellum-based learning but is attributable to increasing sensory uncertainty with increasing visual perturbation eccentricity—a factor hitherto neglected in existing models of sensorimotor adaptation. Contrary to conventional theories that describe implicit adaptation as either saturated or invariant (<xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>), our data reveal a concave dependency of implicit adaptation on visual perturbation size, characterized by diminishing adaptation in response to larger perturbations. Notably, our Perceptual Error Adaptation (PEA) model, calibrated using perceptual parameters from one set of participants, provides a robust account of implicit adaptation in separate groups subjected to varying perturbations. The model further successfully captures the perceptual consequences of implicit adaptation, including the continuous shifts in hand localization after active movements during adaptation process (<xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref>) and the proprioceptive recalibration probed by localization of the passively moved hand. Lastly, we manipulated visual uncertainty independently of perturbation size and demonstrated that upregulating visual uncertainty selectively attenuated adaptation in the context of larger perturbations while leaving smaller perturbations unaffected. These empirical results, inconsistent with predictions from existing models, underscore the conceptual and quantitative superiority of our PEA model. Our study advocates for a revised understanding of implicit motor adaptation; that is, it is driven not by sensory prediction errors but by perceptual errors in localizing one’s effector, conforming to Bayesian cue combination.</p><p>The importance of perceptual error was first implied in the PReMo model (<xref ref-type="bibr" rid="bib63">Tsay et al., 2022b</xref>), which also proposed that a misperceived hand location is the driving signal for implicit adaptation. However, this previous work improperly called this perceptual error a proprioceptive error, leading to the impression that it is a type of sensory error (but see their new unpublished <xref ref-type="bibr" rid="bib65">Tsay et al., 2024</xref>). The two models differ fundamentally in their conceptualization of how different cues contribute to the error signal. The PReMo model posits two intermediate perceptual variables with Bayesian cue integration: a visual estimate of the cursor and a proprioceptive estimate of the hand (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>). The final error signal for adaptation is presumed to be a proprioceptive error, not from further Bayesian cue combination, but from a visual-to-proprioceptive bias that is governed by a predetermined, ramp-like visual influence that saturates around a 6–7° visual-proprioceptive discrepancy (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>). These assumptions lack empirical validation. In fact, our findings in Experiment 3 indicate that proprioceptive recalibration follows a concave function with respect to visual perturbation size, contradicting the ramp-like function assumed by PReMo. Moreover, the presupposed ramp-like visual influence generates a rigid prediction for a ramp-like adaptation extent profile, which is at odds with the concave adaptation pattern observed in Experiment 2 and in a similar study involving trial-by-trial learning (<xref ref-type="bibr" rid="bib60">Tsay et al., 2021a</xref>). Furthermore, the PReMo model predicts that upregulating visual uncertainty will selectively reduce adaptation to small perturbations while sparing large ones. This is inconsistent with our findings in Experiment 4, which demonstrated that upregulating visual uncertainty substantially impacted adaptation more to larger perturbations than small ones. Lastly, though PReMo has the potential to explain the temporal shifts in perceived hand location during adaptation, the authors resorted to separate mechanisms of proprioceptive and visual recalibration at different phases of adaptation to explain these shifts (<xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref>). In summary, the PReMo model’s assumptions introduce limitations that make it less consistent with empirical observations, particularly concerning the nonlinearities observed in both motoric and perceptual aspects of implicit adaptation.</p><p>The causal inference framework (<xref ref-type="bibr" rid="bib68">Wei and Körding, 2009</xref>) fails to predict sensorimotor changes in implicit adaptation adequately. For instance, it underestimates the adaptation extent for large perturbations and incorrectly predicts that increasing visual uncertainty would augment, rather than reduce, adaptation to large perturbations. The casual inference model is also based on Bayesian principle, then why does it fail to account for the implicit adaptation? We postulate that the failure of the causal inference model is due to its neglect of visual uncertainty as a function of perturbation size, as we revealed in Experiment 1. In fact, previous studies that advocating the Bayesian principle in motor adaptation have largely focused on experimentally manipulating sensory cue uncertainty to observe its effects on adaptation (<xref ref-type="bibr" rid="bib9">Burge et al., 2008</xref>; <xref ref-type="bibr" rid="bib16">He et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Körding and Wolpert, 2004</xref>; <xref ref-type="bibr" rid="bib69">Wei and Körding, 2010</xref>), similar to our Experiment 4. Our findings suggest that causal inference of perturbation alone, without incorporating visual uncertainty, cannot fully account for the diverse findings in implicit adaptation. The increase in visual uncertainty by perturbation size is substantial: our Experiment 1 yielded an approximate sevenfold increase from a 4° perturbation to a 64° perturbation. We have attributed this to the fact that people fixate in the desired movement direction during movements. Interestingly, even for conventional visuomotor rotation paradigm where people are required to ‘control’ the perturbed cursor, their fixation is also on the desired direction, not on the cursor itself (<xref ref-type="bibr" rid="bib12">de Brouwer et al., 2018a</xref>; <xref ref-type="bibr" rid="bib13">de Brouwer et al., 2018b</xref>). Thus, we postulate that a similar hike in visual uncertainty in other ‘free-viewing’ perturbation paradigms. Future studies are warranted to extend our PEA model to account for implicit adaptation in other perturbation paradigms.</p><p>Our research contributes to an ongoing debate concerning the driving forces behind error-based motor learning, specifically addressing the question of whether implicit adaptation is driven by target error or sensory prediction error (<xref ref-type="bibr" rid="bib3">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="bib21">Izawa and Shadmehr, 2011</xref>; <xref ref-type="bibr" rid="bib29">Leow et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Mazzoni and Krakauer, 2006</xref>; <xref ref-type="bibr" rid="bib33">McDougle et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Miyamoto et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Taylor and Ivry, 2011</xref>; <xref ref-type="bibr" rid="bib66">Tseng et al., 2007</xref>). Most empirical data fueling this debate stem from traditional motor adaptation paradigms where explicit and implicit learning co-occur and interact. In these paradigms—visuomotor rotation being a prime example—target error (TE) is defined as the disparity between the target and the perturbed cursor, while sensory prediction error (SPE) is the disparity between the predicted and actual cursor. These two types of error, sensory (specifically, visual) in nature, differ simply due to the presence of explicit learning: the predicted (desired and aiming) direction deviates from the target direction when explicit learning is present (<xref ref-type="bibr" rid="bib56">Taylor et al., 2014</xref>). Our study suggests that neither target error nor sensory (visual) prediction error drives implicit adaptation in the error-clamp paradigm. The error-clamp paradigm isolates implicit learning and thus eliminates potential confounds from explicit learning. In this paradigm, the predicted and target directions are aligned, and the target error and sensory prediction error effectively refer to the same visual discrepancy. However, eitherTE or SPE, if plugged in the classical state-space models, is able to account for the nuanced features of implicit adaptation (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>). In contrast, our PEA model reframes the perturbing cursor as a visual cue influencing the perceptual estimation of hand location rather than as a source of visual error. The resultant bias in hand estimation from the desired target serves as the actual error signal. This leads us to posit that the error signal driving implicit sensorimotor adaptation is fundamentally perceptual rather than sensory. From a normative standpoint, this perceptual error could be construed either as a predictive or performance error (<xref ref-type="bibr" rid="bib3">Albert et al., 2022</xref>), but importantly, it is not tied to a specific modality (i.e. vision or proprioception). Instead, it directly pertains to the perceptual estimate that is crucial for task execution, that is, bringing the hand in the desired direction.</p><p>The concept of perceptual error-driven learning can be extrapolated to various motor learning paradigms, including other motor adaptation tasks. For instance, in visuomotor rotation tasks, explicit learning manifests as a deviation in the aiming direction from the visual target, whereas implicit learning manifests as a further deviation the actual hand position from this aiming direction (<xref ref-type="bibr" rid="bib56">Taylor et al., 2014</xref>). With the presence of re-aiming, the perturbed cursor still deviates from the re-aiming direction and thus produces the perceptual bias of the hand from the desired direction, which subsequently drives implicit adaptation. In this scenario, the perceptual error is defined as the difference between the perceptual estimate of the hand and the ‘re-aiming’ direction, which serves as the new ‘target’ when explicit learning is in play. Our PEA model would predict similar saturation effects in implicit adaptation for this conventional adaptation paradigm, comparable to for the error-clamp paradigm. Indeed, existing findings from the conventional adaptation support this prediction: the implicit adaptation follows either a saturation effect (<xref ref-type="bibr" rid="bib8">Bond and Taylor, 2015</xref>; <xref ref-type="bibr" rid="bib39">Neville and Cressman, 2018</xref>) or a concave pattern (<xref ref-type="bibr" rid="bib62">Tsay et al., 2022a</xref>) across a range of perturbation sizes. Furthermore, according to the PEA framework, this perceptual error is anchored on the aiming target, thereby naturally predicting that implicit and explicit adaptations should interact in a complementary manner, a notion that aligns with recent theories on their interaction (<xref ref-type="bibr" rid="bib3">Albert et al., 2022</xref>; <xref ref-type="bibr" rid="bib35">Miyamoto et al., 2020</xref>).</p><p>Besides visuomotor rotation, all the primary motor adaptation paradigms, including prism adaptation (e.g. <xref ref-type="bibr" rid="bib43">Petitet et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">von Helmholtz, 1867</xref>), visuomotor gain adaptation (e.g. <xref ref-type="bibr" rid="bib42">Pearson et al., 2010</xref>), and force field adaptation (e.g. <xref ref-type="bibr" rid="bib50">Shadmehr and Mussa-Ivaldi, 1994</xref>), involves locating one’s effector under the influence of perturbed sensory feedback. Thus, we postulate that perceptual error elicited by sensory perturbation might play a similar significant role in driving implicit learning in these diverse task paradigms. It is noteworthy that error-based learning also plays a major role in motor skill learning, including de novo skill learning and motor acuity learning (<xref ref-type="bibr" rid="bib27">Krakauer et al., 2019</xref>). Unlike motor adaptation, motor skill learning currently lacks computational models to account for its error-based learning component, owing to the complexity and diversity of skill learning tasks. We believe that Bayesian cue combination and perceptual error about locating one’s effect might serve as a starting point for theorization of the implicit processes in skill learning.</p><p>Our study also provides a new perspective on explaining proprioceptive changes during motor adaptation, advocating for a Bayesian cue combination framework. Previously, the change in proprioceptive hand localization during motor adaptation has been ascribed to visual-proprioceptive discrepancy-induced recalibration (<xref ref-type="bibr" rid="bib46">Ruttle et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Salomonczyk et al., 2013</xref>) and/or altered sensory prediction caused by the adapted forward internal model (<xref ref-type="bibr" rid="bib38">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="bib57">’t Hart and Henriques, 2016</xref>). To dissect these components, researchers have often compared proprioceptive localization in actively moved (<xref ref-type="bibr" rid="bib47">Ruttle et al., 2021</xref>; <xref ref-type="bibr" rid="bib57">’t Hart and Henriques, 2016</xref>) versus passively placed (passive localization, e.g. Experiment 3) hands during adaptation, attributing the smaller bias in passive localization to recalibration alone. The difference between the two is then considered to reflect altered sensory prediction due to motor adaptation (<xref ref-type="bibr" rid="bib38">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Rossi et al., 2021</xref>). But these conceptual divisions lack computational models for validation. For instance, researchers have shown that proprioceptive recalibration in visuomotor adaptation is either a fixed proportion (e.g. 20%) of the visual-proprioceptive discrepancy (<xref ref-type="bibr" rid="bib17">Henriques and Cressman, 2012</xref>; <xref ref-type="bibr" rid="bib47">Ruttle et al., 2021</xref>) or largely invariant (<xref ref-type="bibr" rid="bib36">Modchalingam et al., 2019</xref>). However, a computational model has yet to be proposed elucidate the underlying mechanism for these diverse findings. In fact, cross-sensory calibration typically follows the Bayesian principle, as shown in task paradigms other than motor adaptation (<xref ref-type="bibr" rid="bib54">Stetson et al., 2006</xref>; <xref ref-type="bibr" rid="bib71">Wozny and Shams, 2011</xref>). We propose that proprioceptive changes elicited by motor adaptation, no matter measured by a passive or active localization method, are governed by the same Bayesian cue combination principle and the same critical cue, that is (mis)perceived hand location (<inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). Given that <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> has a concave dependency to visual perturbation size, we would observe a corresponding concave proprioceptive recalibration profile.</p><p>Besides highlighting the importance of Bayesian cue combination, our study also suggests that proprioceptive measurements should be understood by considering available cues in the specific task setting. For both localization tests, that is the passive (Experiment 3) and active localization (<xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>), the hand stayed at the trial end when the participants reported their perceived hand location. Thus, the reported hand location is determined by the just-experienced <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the actual proprioceptive cue. In the case of active localization, the proprioceptive cue becomes increasingly large (biased to the positive direction in reference to the target), driven by the adaptation process. In this sense, active localization indeed serves as a multifaceted reflection of both the internal model (the adapted hand, supplying <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and proprioceptive recalibration (the perceived hand after a movement, <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>),as proposed by previous researchers (<xref ref-type="bibr" rid="bib38">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Rossi et al., 2021</xref>). During the initial stages of perturbation, the immediate negative bias in active localization is predominantly attributable to rapid proprioceptive recalibration. This is evidenced by a sudden shift in the estimated hand position (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> ; <xref ref-type="fig" rid="fig4">Figure 4A</xref>), occurring before the internal model has had sufficient time to adapt. Then, why does active localization in traditional motor adaptation paradigms yield a largely stable bias (<xref ref-type="bibr" rid="bib45">Ruttle et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Ruttle et al., 2021</xref>)? We note that traditional visuomotor rotation paradigms invokes a rapid initial explicit learning, driving the adaptation to its asymptote quickly. At the same time, previous investigations have predominantly measured active localization when adaptation has asymptoted (<xref ref-type="bibr" rid="bib17">Henriques and Cressman, 2012</xref>; <xref ref-type="bibr" rid="bib36">Modchalingam et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Salomonczyk et al., 2011</xref>; <xref ref-type="bibr" rid="bib49">Salomonczyk et al., 2013</xref>; <xref ref-type="bibr" rid="bib61">Tsay et al., 2021b</xref>). Consequently, these studies overlook the evolving effect of the adaptation. In contrast, the gradual nature of implicit adaptation, shown in the error-clamp paradigm here, provides a unique opportunity to uncover the underlying mechanisms governing changes in active localization during the adaptation process.</p><p>Notably, our model aligns with previous findings that show a positive correlation between proprioceptive recalibration, measured by the passive localization method, and motor adaptation based on individual differences (<xref ref-type="bibr" rid="bib47">Ruttle et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Salomonczyk et al., 2013</xref>; <xref ref-type="bibr" rid="bib61">Tsay et al., 2021b</xref>). Unlike existing theories that posit proprioceptive recalibration either as a component (<xref ref-type="bibr" rid="bib36">Modchalingam et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Mostafa et al., 2019</xref>; <xref ref-type="bibr" rid="bib47">Ruttle et al., 2021</xref>) or a driver for implicit adaptation (<xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>), our PEA model provides a mechanistic and empirically testable framework. It posits that the misestimation of hand position (<inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>)—induced by the recent perturbation—serves as the driving factor for both implicit adaptation and changes in proprioception. In other words, this misestimation is the common cause for both implicit adaptation and proprioceptive recalibration. This is why the concave dependency of <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> on perturbation leads to similar concave perturbation dependencies in both implicit adaptation and proprioceptive recalibration. Updated on a trial-by-trial basis, this misestimation exerts immediate effects, manifesting as an abrupt negative bias (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Additionally, its influence decays rapidly, becoming negligible within three trials (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). These converging lines of evidence strongly suggest that perceptual misestimation of hand position is central to the process of proprioceptive recalibration during adaptation.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Design of Experiment 1.</title><p>(<bold>A</bold>) Top-down view of the setup in visual discrimination task. The reference point (yellow) was presented either CW or CCW relative to the clamped cursor (dashed circle), which has a perturbation size θ. (<bold>B</bold>) Trial structure of the visual discrimination task. Purple rectangles represent error-clamped trials with varying perturbation size, rectangles with yellow edges represent the ensuing visual discrimination test for each perturbation size. (<bold>C–D</bold>) Exemplary sequences of the reference point: These sequences illustrate the deviation of the reference point from the cursor (<bold>C</bold>) and the changing step size across trials (<bold>D</bold>), following the PEST algorithm. Individual trials are represented by blue dots. Yellow and red dots mark the initiation and termination of each round of trials, respectively. In each round, the reference point starts on either the CW or CCW side of the cursor; In the subsequent round, it starts on the opposite side.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig6-v1.tif"/></fig><p>Our findings contribute nuanced perspectives to the modulation of implicit learning rate by factors beyond visual perturbation size. Previous studies have shown that environmental inconsistency—defined as the inconsistency of visual errors—reduced the rate (<xref ref-type="bibr" rid="bib18">Herzfeld et al., 2014</xref>; <xref ref-type="bibr" rid="bib20">Hutter and Taylor, 2018</xref>) or extent (<xref ref-type="bibr" rid="bib2">Albert et al., 2021</xref>) of implicit adaptation. Baseline motor variance in unperturbed conditions has been shown to increase implicit adaptation rate, proposed as a sign of better exploratory learning (<xref ref-type="bibr" rid="bib72">Wu et al., 2014</xref>). These studies interpret such phenomena as parametric changes in the learning rate in relation to visual errors, conceptualized as alterations to the <italic>B</italic> parameter in existing models. However, an apparent change in learning rate due to visual errors does not necessarily signify parametric modification, but may attribute to other factors that influence the use of visual cues (<xref ref-type="bibr" rid="bib16">He et al., 2016</xref>), such as visual uncertainty in our case. Previous research has also pointed to various alternative factors for determining apparent learning rate, including error discounting based on causal inference of error (<xref ref-type="bibr" rid="bib68">Wei and Körding, 2009</xref>), proprioceptive uncertainty (<xref ref-type="bibr" rid="bib47">Ruttle et al., 2021</xref>; <xref ref-type="bibr" rid="bib61">Tsay et al., 2021b</xref>), and state estimation uncertainty (<xref ref-type="bibr" rid="bib16">He et al., 2016</xref>). In line with these previous studies, our work suggests a shift in perspective: the driving error signal for implicit learning should be considered as perceptual, rather than merely visual. Hence, all the aforementioned factors can be considered as contributing to the perceptual estimate. This paradigmatic shift could serve as a cornerstone for future research aimed at understanding how learning rates change under varying conditions.</p><p>Our new framework opens avenues for exploring the memory characteristics of implicit learning. Traditional motor adaptation often exhibits ‘savings’, or accelerated relearning upon re-exposure to a perturbation (<xref ref-type="bibr" rid="bib14">Della-Maggiore and McIntosh, 2005</xref>; <xref ref-type="bibr" rid="bib19">Huberdeau et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Krakauer et al., 2005</xref>; <xref ref-type="bibr" rid="bib28">Landi et al., 2011</xref>). In contrast, implicit adaptation has been found to exhibit a decreased learning rate during re-adaptation (<xref ref-type="bibr" rid="bib4">Avraham et al., 2021</xref>), a phenomenon attributed to conditioning (<xref ref-type="bibr" rid="bib4">Avraham et al., 2021</xref>) or associative learning mechanisms (<xref ref-type="bibr" rid="bib5">Avraham et al., 2022</xref>). Investigating this ‘anti-saving’ effect will yield insights into the unique memory properties of implicit learning. Although our current PEA model is structured around single-epoch learning and does not directly address this question, it does raise new, testable hypotheses. For example, is the reduced adaptation rate during relearning attributable to a down-weighting of perturbed visual feedback in cue combination, or does it reflect a parametric alteration in the learning rate? Another noteworthy aspect of implicit learning is its remarkably slow decay rate. It has been observed that the number of trials required to wash out the implicit adaptation exceeds the number of trials needed to establish it (<xref ref-type="bibr" rid="bib4">Avraham et al., 2021</xref>; <xref ref-type="bibr" rid="bib59">Tsay et al., 2020</xref>). In the context of our perceptual error framework, this raises the possibility that washout phases might be governed by state updating involving a distinct set of sensorimotor cues or an alternative updating mechanism, such as memory formation and selection (<xref ref-type="bibr" rid="bib40">Oh and Schweighofer, 2019</xref>).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>We recruited 115 college students from Peking University (77 females, 38 males, 22.05±2.82 years, mean ± SD). Participants were all right-handed according to the Edinburgh handedness inventory (<xref ref-type="bibr" rid="bib41">Oldfield, 1971</xref>) and had normal or corrected-to-normal vision. Participants were naïve to the purpose of the experiment and provided written informed consent, which was approved by the Institutional Review Board of the School of Psychological and Cognitive Sciences, Peking University. Participants received monetary compensation upon completion of the experiment.</p></sec><sec id="s4-2"><title>Apparatus</title><p>In Experiment 1, 2, and 4, participants were seated in front of a vertically-placed LCD screen (29.6x52.7 cm, Dell, Round Rock, TX, US). They performed the movement task with their right hand, holding a stylus and slide it on a horizontally placed digitizing tablet (48.8x30.5 cm, Intuos 4 PTK-1240, Wacom, Saitama, Japan). In Experiment 1, a keyboard was provided to the participants’ left hand to enable them to report the direction of visual stimuli in the discrimination task. A customized wooden shelter was placed above the tablet to block the peripheral vision of the right arm. In Experiment 1 and 4, participants placed their chin on a chin rest attached to the wooden shelter to stabilize their heads. Their eye movement was recorded by an eye tracker (Tobii pro nano, Tobii, Danderyd Municipality, Sweden) affixed at the lower edge of the screen. The sampling rate was 160–200 Hz for the tablet and 60 Hz for the eye tracker.</p><p>Experiment 3 was conducted using the KINARM planar robotic manipulandum with a virtual-reality system (BKIN Technologies Ltd., Kingston, Canada). Participants, seated in a chair, held the robot handles with their left and right hands (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The movement task was performed with the right handle and the left handle was used to indicate the perceived direction of the right hand in the proprioception test. A semi-silvered mirror was placed below the eye level to block the vision of the hands and the robotic manipulandum; it also served as a display monitor.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Setup for measuring proprioceptive recalibration in Experiment 3.</title><p>(<bold>A</bold>) Reaching movement with error-clamped cursor, performed by the right hand holding a robot handle. (<bold>B</bold>) Passive movement in the proprioception test. The right hand was passively moved to the unseen target (h<sub>r</sub>), depicted here as a small black dot. A red hollow circle with an expanding radius appears on the screen during passive movement, signaling the increasing distance between the hand and the start position. Subsequently, participants used their left hand to report the right-hand location (h<sub>p</sub>) by aligning a red rectangle on the red circle, which is displayed at the target distance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-fig7-v1.tif"/></fig></sec><sec id="s4-3"><title>Experiment 1: Measuring visual uncertainty in error-clamp adaptation</title><p>Eighteen among twenty participants finished the reaching with clamped error feedback and visual discrimination task in 3 consecutive days; two participants withdrew during the experiment. Participants made reaching movement by sliding the stylus from a start position at the center of the workspace to towards a target (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The start position, the target, and the cursor were represented by a gray dot, a blue cross and a white dot on the screen, respectively. All these elements had a diameter of 5 mm. The procedure of the motor and visual discrimination task is illustrated in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. To initiate a trial, participants moved the cursor into the start position. Following an 800ms holding period, a target appeared 10 cm away in the twelve o’clock direction, and participants were instructed to slide through the target rapidly while maintaining a straight hand trajectory. The trial terminated when the distance between the hand and the start position exceeded 10 cm, regardless of whether the target was hit. A warning message, ‘too slow’, would appear on the screen if participants failed to complete the trial within 300ms after initiating the movement. Each practice day began with 60 standard reaching trials, during which veridical feedback about hand location was provided by the cursor. The target would change from blue to green if the cursor successfully passed through it. In subsequent visual clamp trials, the cursor moved along a predetermined direction set by the perturbation angle, while its position was updated in real-time based on the hand’s location. The cursor’s distance from the start position was equal to the distance between the hand and the start position until the end of the trial.</p><p>Following each trial, the cursor remained frozen at its final position for an additional 800ms before disappearing. The visual discrimination task commenced 1000ms thereafter. A yellow reference point, located 10 cm from the start position, was displayed for 150ms near the cursor’s final position (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig6">Figure 6A</xref>). Subsequently, all visual stimuli, except for the blue cross at the start position, were removed from the screen. Participants were then required to judge whether the reference point was situated in a clockwise (CW) or counterclockwise (CCW) direction relative to the cursor’s final position and to report their judgment by pressing a key on the keyboard. Participants were informed that they no longer controlled the direction of cursor movement during the task. They were instructed to fixate their gaze on either the start position or the blue cross during the motor task, while actively ignoring the white cursor. During the discrimination task, they were required to maintain their gaze on the blue cross. Eye movements were monitored in real-time using an eye tracker. Participants received a warning if their gaze was detected outside a 75-pixel-wide band-shaped region centered on the line of gaze four consecutive times during the experiment (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>In each trial, the angular deviation between the error-clamped cursor and the reference point was determined using a PEST procedure (<xref ref-type="bibr" rid="bib31">Lieberman and Pentland, 1982</xref>). <xref ref-type="fig" rid="fig6">Figure 6C–D</xref> illustrates the evolution of the deviation angle and step size for an exemplary participant experiencing a –16° perturbation. In each round, the deviation commenced at 30° (indicated by yellow points in <xref ref-type="fig" rid="fig6">Figure 6C–D</xref>) and was altered by one step size following each trial. The initial step size was set at 10° and was halved whenever the direction judgment changed (i.e. from ‘CW’ to ‘CCW’ or vice versa). For a specific perturbation angle, the initial deviation always started from the CW direction for the first round and flipped the direction at the beginning of the next round. A round terminated either when the step size fell below a predefined criterion (indicated by the red line in <xref ref-type="fig" rid="fig6">Figure 6D</xref>) or when the trial count exceeded 30. Six perturbation angles were randomly interleaved (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), and the experiment concluded when four complete rounds of the PEST procedure had been completed for each perturbation angle. Consequently, the total number of trials varied among participants and across practice days (300–500 trials per day on average, with a maximum of 720 trials). Additionally, for some perturbation angles, more than four complete rounds could be conducted in a single day.</p></sec><sec id="s4-4"><title>Experiment 2: Motor adaptation with different perturbation size</title><p>Eighty-four participants were randomly allocated into seven groups, each comprising 12 individuals. Each group performed a motor adaptation task featuring clamped visual feedback at different perturbation angles: 2°, 4°, 8°, 16°, 32°, 64°, and 95°. As in Experiment 1, participants were instructed to slide rapidly and directly through the target, which was represented by a blue dot rather than a cross. In each trial, the target appeared at one of four possible locations (45°, 135°, 225° or 315° counter-clockwise from the positive x-axis). The sequence of target locations was randomized yet constrained so that all four positions appeared in cycles of four trials. Each group commenced with a baseline session that included 15 cycles of reaching trials with veridical feedback, followed by 15 cycles without visual feedback. Subsequently, during the perturbation session, participants completed 80 cycles of training trials featuring the error-clamped cursor with one perturbation angle (i.e. clamp size), depending on their group assignment. To assess the aftereffect, a session comprising 10 cycles of movement without visual feedback was administered. In summary, each participant practiced a total of 480 trials (120 cycles x 4 target directions) in Experiment 2.</p></sec><sec id="s4-5"><title>Experiment 3: Proprioception test with different perturbation sizes</title><p>Eleven participants were recruited for testing their proprioceptive recalibration. This experiment incorporated two types of trials: reaching trials and proprioception test trials. During the reaching trials, participants were instructed to aim for a target, which could appear at one of three possible locations (25°, 45°, or 65° counter-clockwise from the positive x-axis, as represented by light blue dots in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, right panel). The task was similar to those in Experiments 1 and 2, with the key difference being that participants performed the task using KINARM robots (as depicted in <xref ref-type="fig" rid="fig7">Figure 7A</xref>). The dimensions and relative distances of the visual stimuli remained consistent with those used in Experiments 1 and 2. As in previous experiments, three kinds of visual feedback were provided during different sessions: no visual feedback, veridical feedback, and feedback featuring an error-clamped cursor.</p><p>In the proprioception test, participants were instructed to hold the robot’s right handle and wait to be passively moved by the robot to one of six proprioception targets (small red dots in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, right panel). These targets were spaced at 10° intervals, ranging from 20° to 70° counterclockwise from the positive x-axis, and flanked the three reaching targets. The passive movement lasted for 1000ms and followed a straight-line path at a speed consistent with a minimum jerk velocity profile. During this movement, a ring with a 10 cm radius, centered at the start position, was displayed on the screen (depicted as a red arc in <xref ref-type="fig" rid="fig7">Figure 7B</xref>). The cursor was also replaced by a ring, its radius expanding as the hand moved toward the proprioception target.</p><p>After the right hand reached the proprioception target, participants were instructed to maintain their right hand’s position. Using the left handle, they were then asked to indicate the perceived location of their right hand. The position of the left handle was mapped to the rotation of a ‘dial’, which was constrained to the target arc.</p><p>The position of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was displayed on the target arc as a small red rectangle (a visual ‘dial’, as shown in <xref ref-type="fig" rid="fig7">Figure 7B</xref>). Participants were instructed to indicate the location of their right hand by moving the red rectangle to the position they perceived as accurate. The final position of <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was recorded when its angular velocity remained below 1 degree/second for a duration exceeding 1000ms. The proprioceptive bias was then calculated as the angular deviation between the actual hand position (<inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the perceived hand position (<inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>Reaching trials and proprioception test trials were organized into blocks (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). Each reaching block consisted of six trials, targeting three different locations with two repetitions each. Each reaching block was followed by a proprioception test block consisting of three trials. In these test trials, the robot moved the participant’s right hand toward a target position near one of the three reaching targets. These test targets were randomly chosen from six possible locations (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, right panel). The entire experiment comprised 40 reaching blocks and 40 subsequent proprioception test blocks. The first four reaching blocks provided veridical cursor feedback, the next four offered no cursor feedback, and the remaining 32 featured one of eight possible perturbation sizes ( ± 10°, ± 20°, ± 40°, and ± 80°). The size of the perturbation was randomized between blocks.</p></sec><sec id="s4-6"><title>Experiment 4: Upregulating visual uncertainty affects implicit adaptation</title><p>Nineteen participants from Experiment 1 completed Experiment 4. The reaching task employed the same setup as in Experiment 1. However, instead of performing perceptual judgments of cursor motion direction, participants engaged in movements with one of three types of cursor feedback: veridical feedback, no feedback, and feedback with clamped perturbation. To assess the influence of visual uncertainty on implicit learning, we modified the cursor to appear blurred in half of the clamped trials. The blurring mask had a diameter of 6.8 mm, and the color intensity decreased from the cursor’s center following a two-dimensional Gaussian distribution with σx = σx = 1.4 mm. As depicted in <xref ref-type="fig" rid="fig5">Figure 5D</xref>, participants underwent the same procedures across three consecutive days. Each day consisted of 60 baseline trials, followed by 15 training blocks designed to assess single-trial learning. Within each training block, 12 trials featured an error-clamped cursor, each flanked by a trial without feedback. The difference between two adjacent no-feedback trials served as a measure of single-trial learning at specific perturbation sizes. Each of the 12 perturbation trials was randomly assigned one of 12 possible perturbations, comprising two cursor presentations (blurred or clear) and six clamp sizes ( ± 4°, ± 16°, ± 64°). Each participant practices 180 trials (15 blocks x 12 trials per block) with perturbation per day and 540 trials in total.</p></sec><sec id="s4-7"><title>Data analysis</title><sec id="s4-7-1"><title>Processing of kinematic data</title><p>In Experiments 1, 2, and 4, hand kinematic data were collected online at a sampling rate ranging between 160 and 200 Hz and subsequently resampled offline to 125 Hz. The movement direction of the hand was determined by the vector connecting the start position to the hand position at the point where it crossed 50% of the target distance, that is 5 cm from the start position.</p><p>In Experiment 3, hand positions and velocities were directly acquired from the KINARM robot at a fixed sampling rate of 1 kHz. The raw kinematic data were smoothed using a fifth-order Savitzky-Golay filter with a window length of 50ms. Owing to the high temporal resolution and reliable velocity profiles provided by the KINARM system, the heading direction in Experiment 3 was calculated as the vector connecting the start position to the hand position at the point of peak velocity. For each participant, the baseline hand direction and proprioceptive bias was subtracted from the data.</p></sec><sec id="s4-7-2"><title>Psychometric curve</title><p>For the visual discrimination task, data of all three days were pooled together, the probability of responding that ‘the reference point was in the counter-clockwise direction of the cursor’ was calculated as <italic>p</italic> for all angle differences (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). At each perturbation size, a logistic function was used to fit the probability distribution for individual participants:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where k is the slope, and x<sub>0</sub> is the origin of the logistic function. The visual uncertainty was defined as the angle differences between 25% and 75% of the logistic function:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <italic>p</italic><sub><italic>1</italic></sub>=25% and <italic>p</italic><sub><italic>2</italic></sub>=75%.</p></sec><sec id="s4-7-3"><title>Statistical analysis</title><p>In Experiment 1, since the visual uncertainty <inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> follows a non-negative skewed distribution among participants, it violated the assumption of the ANOVA test. We thus applied Friedman’s nonparametric test to determine whether <inline-formula><mml:math id="inf66"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> changes with the perturbation angle θ. Specifically, <inline-formula><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for both positive and negative θ were subjected to Friedman’s test separately, with θ serving as the factor. Spearman correlation analyses were conducted between <inline-formula><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in different perturbation sizes. Given the symmetry between positive and negative θ, we pool the data to quantify the linear dependency of <inline-formula><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> on the absolute θ (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). Because <inline-formula><mml:math id="inf70"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is expected to be always positive and there are potential positive outliers, we assume that it is generated from a gamma distribution rather than a normal distribution, which is always positive and favors right-skewed distribution that can well capture the positive outliers. Thus, the data was fitted by a generalized linear regression model with the absolute value of θ as independent variable and <inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as a dependent variable.</p><p>In Experiment 2, the adaptation extent was defined as the mean hand angles in the last 10 cycles in the perturbation phase (cycle 101–110). A one-way ANOVA with perturbation size serving as the factor to examine its influence on the adaptation extent. Pairwise post-hoc comparisons were conducted using Tukey-Kramer correction.</p><p>In Experiment 3, proprioceptive biases were quantified as the angular difference between the perceived and actual hand directions. A one-way repeated-measures ANOVA was conducted on the data of the first trial, using perturbation size as the within-subject factor. Greenhouse-Geisser corrections were applied when the assumption of sphericity was violated (<xref ref-type="bibr" rid="bib23">Kirk, 1968</xref>). Multiple pairwise comparisons were conducted among different perturbation sizes for the first proprioception test. To determine if the proprioceptive biases were significantly different from zero, one-tailed (left) <italic>t</italic>-tests were conducted separately for the first and third proprioception test trials at each perturbation size.</p><p>In Experiment 4, the single-trial learning data was subjected to a 2 (visual uncertainty) x 3 (perturbation size) repeated-measures ANOVA. Greenhouse-Geisser corrections were applied as above, and the simple main effect of visual uncertainty was tested for each of the three perturbation sizes.</p></sec></sec><sec id="s4-8"><title>Model fitting and simulations</title><sec id="s4-8-1"><title>Perceptual Error Adaptation (PEA) model</title><sec id="s4-8-1-1"><title>Model fitting for adaptation extent as a function of perturbation size</title><p>To fit the adaptation extent data from three different experiments in previous studies in <xref ref-type="bibr" rid="bib22">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Morehead et al., 2017</xref>, <xref ref-type="disp-formula" rid="equ3 equ4">Equations 3 and 4</xref> were modified for simplification. To avoid overfitting of the small dataset, we reduced the number of model parameters by assuming that <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> asymptote to the target direction in the final adaptation trials that are used for computing adaptation extent, thus the retention rate <inline-formula><mml:math id="inf73"><mml:mi>A</mml:mi><mml:mo>≡</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. Insert <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, the asymptote hand angle with different perturbation size is:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>/</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>θ</mml:mi></mml:math></disp-formula></p><p>Two ratio parameters <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>/</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula> were used in data fitting. Three datasets were fitted separately.</p></sec><sec id="s4-8-1-2"><title>Model fitting for trial-by-trial adaptation and proprioception changes</title><p>The trial-by-trial changes of adaptation (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) and of proprioceptive changes (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) were fitted with <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, and <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> based on the mean performance of all participants. The PEA model only had four free parameters, Θ = [ <inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ,<inline-formula><mml:math id="inf77"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <italic>A</italic>, <italic>B</italic>]. The slope <italic>a</italic> and intercept <italic>b</italic> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> were obtained by psychometric tests from Experiment 1 (see statistical analysis). The reported hand position (<inline-formula><mml:math id="inf78"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , blue dots in <xref ref-type="fig" rid="fig4">Figure 4A</xref>) was based on the proprioceptive cue <inline-formula><mml:math id="inf79"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the estimated hand <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from the reaching trial. With the Bayesian cue combination assumption, the reported hand position was biased by <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with a ratio determined by the variance of <inline-formula><mml:math id="inf82"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> :<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf84"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf85"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are the variance of <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf87"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> respectively (see Appendix 1 for further details of this fitting).</p><p>To verify if the slope <italic>b</italic> and intercept <italic>a</italic> obtained from Experiment 1 are consistent across experiments, they were also estimated by fitting data from Experiment 2 (<xref ref-type="fig" rid="fig3">Figure 3</xref>). In this case, the model fitting was performed with 6 free parameters, Θ = [ <inline-formula><mml:math id="inf88"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ,<inline-formula><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <italic>a</italic>, <italic>b, A</italic>, <italic>B</italic>]. The fitted values of <italic>a</italic> and <italic>b</italic> are fallen into the 95% CI of estimated parameters in Experiment 1 (purple line in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, see details in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref>).</p><p>The dependence of proprioceptive recalibration on perturbation size (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) were simulated by the PEA model with the parameter values estimated from Experiment 2. We assumed that the proprioceptive bias results from the influence of a biased hand estimate (<inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) during adaptation and the influence is quantified as a percentage of its deviation from the true hand location:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where the actual hand location is 0, <inline-formula><mml:math id="inf91"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the percentage of influence, and <inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is determined by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. In simulation, <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> varied from 0.05 to 0.8 to estimate the overall dependence of proprioceptive recalibration on perturbation size.</p></sec><sec id="s4-8-1-3"><title>Model fitting and simulation for single-trial learning</title><p>In the single-trial learning paradigm (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>), the average movement direction across trials aligns with the target direction since the visual perturbations are evenly distributed in both directions. Thus, the sensory cues <inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> have the same mean value. For modeling single-trial learning, instead of having two separate cues, we assume a combined cue of <inline-formula><mml:math id="inf96"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to follow <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></inline-formula>, where <italic>T</italic> is the target direction, <inline-formula><mml:math id="inf99"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></inline-formula> represents the variance of integrated sensory signal of <inline-formula><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Single-trial learning was quantified as the difference between the two null trials before and after the perturbation trial. As the perturbation size in the triplet of trials varied randomly, we assume that the effects of different perturbations are independent. Thus, single-trial learning was modeled as learning from the current perturbation without history effect. It follows the equations modified from <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the visual perturbation, <inline-formula><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the weights of the cues, <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the standard deviation of the visual cue specified by <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>. Parameter set Θ = [ <inline-formula><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <italic>a</italic>, <italic>b, B</italic>] was fitted to the average data from all participants. Model simulations (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) were performed with the same single-trial learning equations. For the clear cursor condition, we used the same parameter values estimated from Experiment 2 (see details in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref>). For the blurred cursor condition, the standard deviation of visual cue was changed to:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>for the simulation of the increase in visual uncertainty, the ratio <inline-formula><mml:math id="inf107"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> varied from 1.1 to 3.</p></sec></sec></sec><sec id="s4-9"><title>PReMo model</title><p>We used the PReMo model to fit the average adaptation extent obtained from Experiment 2 (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). Following the study by <xref ref-type="bibr" rid="bib64">Tsay et al., 2022c</xref>, the hand position at trial n+<italic>1</italic> is:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>In data fitting, we used two parameters to represent the ratio between sensory cues: <inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></inline-formula> . The data were fitted with the parameter set Θ = [ <inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> ,<inline-formula><mml:math id="inf111"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>,<inline-formula><mml:math id="inf112"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>,<inline-formula><mml:math id="inf113"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <italic>A</italic>, <italic>B</italic>], where <inline-formula><mml:math id="inf114"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the saturation angle, <inline-formula><mml:math id="inf115"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a scaling factor, <italic>A</italic> is the retention rate and <italic>B</italic> is the learning rate. For simulating the proprioceptive localization of the hand (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), the parameter values estimated from Experiment 2 were used. The bias of hand estimation in the proprioception trials is determined as: <inline-formula><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , where ratio <inline-formula><mml:math id="inf117"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> varies from 0.05 to 0.8. Thus, similar to the PEA model simulation, the proprioceptive bias is a fraction of the bias in the hand estimation from the adaptation trials. Single-trial learning (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) was simulated with:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is determined by <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> and <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>. For the clear condition, we used the parameter values estimated from Experiment 2 with PReMo. For the blurred cursor condition, the standard deviation of visual signal <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> increases with a ratio <inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , as in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>.</p></sec><sec id="s4-10"><title>Causal inference model</title><p>The causal inference model by <xref ref-type="bibr" rid="bib68">Wei and Körding, 2009</xref> was used to fit the data of Experiment 2 (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). The hand position at trial n+1 is updated by learning from visual error at trial n:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <italic>A</italic> and <italic>B</italic> are the retention and learning rates, respectively; <italic>T</italic> is the target direction. Specifically for this model, the learning from error is modulated by the probability (<italic>p</italic>) of causal attribution of visual error to the action or proprioception:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the visual cue at trial n. <italic>S</italic> and <italic>C</italic> are the scaling factors, and <inline-formula><mml:math id="inf122"><mml:mi>σ</mml:mi></mml:math></inline-formula> is the standard deviation of the integrated cue combining visual and proprioceptive cues, following<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Thus, the data were fitted with five parameters Θ = [<italic>σ</italic>, <italic>S</italic>, <italic>C</italic>, <italic>A</italic>, <italic>B</italic>]. For simulating single-trial learning with cursor blurring (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), the ratio between <inline-formula><mml:math id="inf123"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is fixed as 1⁄2. The single-trial learning was determined as:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <italic>p</italic> is determined by <xref ref-type="disp-formula" rid="equ18">Equation 18</xref>. Put <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> and <xref ref-type="disp-formula" rid="equ19">Equation 19</xref> into <inline-formula><mml:math id="inf125"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></inline-formula> , we can calculate the standard deviation of the integrated sensory signal for the blurred cursor: <inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>5</mml:mn><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:msqrt></mml:math></inline-formula> . Simulation was performed with <italic>R</italic> ranging from 1.1 to 3.</p></sec><sec id="s4-11"><title>Data fitting</title><p>All data were fitted using MATLAB 2022b (MathWorks, Natick, MA, US) built-in function <italic>fmincon</italic> with 100 randomly sampled initial values of parameter sets. See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1b</xref> for the fitted parameter values and comparisons between different models.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Funding acquisition, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Methodology</p></fn><fn fn-type="con" id="con3"><p>Data curation</p></fn><fn fn-type="con" id="con4"><p>Data curation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Participants were provided written informed consent, which was approved by the Institutional Review Board of the School of Psychological and Cognitive Sciences, Peking University.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Model fitting results and model comparisons.</title><p>(<bold>a</bold>) Model fitting and simulation parameters with the PEA model. (<bold>b</bold>) Model comparisons.</p></caption><media xlink:href="elife-94608-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-94608-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and codes presented in this work are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.24503926.v2">figshare</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/ZhaoranZhang/PEAmodel.git">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib73">Zhang, 2024</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data set of the manuscript &quot;Perceptual error based on Bayesian cue combination drives implicit motor adaptation&quot;</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.24503926.v2</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the STI2030-Major Project 2021ZD0202600 (2021ZD0202601) and the National Natural Science Foundation of China (62061136001, 32071047, 31871102) awarded to KW, and the National Natural Science Foundation of China (32300868) awarded to ZZ.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical Bayesian optimization for model fitting with Bayesian adaptive direct search</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname><given-names>ST</given-names></name><name><surname>Jang</surname><given-names>J</given-names></name><name><surname>Sheahan</surname><given-names>HR</given-names></name><name><surname>Teunissen</surname><given-names>L</given-names></name><name><surname>Vandevoorde</surname><given-names>K</given-names></name><name><surname>Herzfeld</surname><given-names>DJ</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An implicit memory of errors limits human sensorimotor adaptation</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>920</fpage><lpage>934</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-01036-x</pub-id><pub-id pub-id-type="pmid">33542527</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname><given-names>ST</given-names></name><name><surname>Jang</surname><given-names>J</given-names></name><name><surname>Modchalingam</surname><given-names>S</given-names></name><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Henriques</surname><given-names>D</given-names></name><name><surname>Lerner</surname><given-names>G</given-names></name><name><surname>Della-Maggiore</surname><given-names>V</given-names></name><name><surname>Haith</surname><given-names>AM</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Competition between parallel sensorimotor learning systems</article-title><source>eLife</source><volume>11</volume><elocation-id>e65361</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65361</pub-id><pub-id pub-id-type="pmid">35225229</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avraham</surname><given-names>G</given-names></name><name><surname>Morehead</surname><given-names>JR</given-names></name><name><surname>Kim</surname><given-names>HE</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Reexposure to a sensorimotor perturbation produces opposite effects on explicit and implicit learning processes</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001147</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001147</pub-id><pub-id pub-id-type="pmid">33667219</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avraham</surname><given-names>G</given-names></name><name><surname>Taylor</surname><given-names>JA</given-names></name><name><surname>Breska</surname><given-names>A</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name><name><surname>McDougle</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Contextual effects in sensorimotor adaptation adhere to associative learning rules</article-title><source>eLife</source><volume>11</volume><elocation-id>e75801</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.75801</pub-id><pub-id pub-id-type="pmid">36197002</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berniker</surname><given-names>M</given-names></name><name><surname>Kording</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Estimating the sources of motor errors for adaptation and generalization</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1454</fpage><lpage>1461</lpage><pub-id pub-id-type="doi">10.1038/nn.2229</pub-id><pub-id pub-id-type="pmid">19011624</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berniker</surname><given-names>M</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Estimating the relevance of world disturbances to explain savings, interference and long-term motor adaptation effects</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002210</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002210</pub-id><pub-id pub-id-type="pmid">21998574</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bond</surname><given-names>KM</given-names></name><name><surname>Taylor</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Flexible explicit but rigid implicit learning in a visuomotor adaptation task</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>3836</fpage><lpage>3849</lpage><pub-id pub-id-type="doi">10.1152/jn.00009.2015</pub-id><pub-id pub-id-type="pmid">25855690</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>J</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The statistical determinants of adaptation rate in human reaching</article-title><source>Journal of Vision</source><volume>8</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1167/8.4.20</pub-id><pub-id pub-id-type="pmid">18484859</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>S</given-names></name><name><surname>Sabes</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Modeling sensorimotor learning with linear dynamical systems</article-title><source>Neural Computation</source><volume>18</volume><fpage>760</fpage><lpage>793</lpage><pub-id pub-id-type="doi">10.1162/089976606775774651</pub-id><pub-id pub-id-type="pmid">16494690</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cressman</surname><given-names>EK</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Sensory recalibration of hand position following visuomotor adaptation</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>3505</fpage><lpage>3518</lpage><pub-id pub-id-type="doi">10.1152/jn.00514.2009</pub-id><pub-id pub-id-type="pmid">19828727</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Brouwer</surname><given-names>AJ</given-names></name><name><surname>Albaghdadi</surname><given-names>M</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Gallivan</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Using gaze behavior to parcellate the explicit and implicit contributions to visuomotor learning</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>1602</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1152/jn.00113.2018</pub-id><pub-id pub-id-type="pmid">29995600</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Brouwer</surname><given-names>AJ</given-names></name><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Visuomotor feedback gains are modulated by gaze position</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>2522</fpage><lpage>2531</lpage><pub-id pub-id-type="doi">10.1152/jn.00182.2018</pub-id><pub-id pub-id-type="pmid">30183472</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Della-Maggiore</surname><given-names>V</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Time course of changes in brain activity and functional connectivity associated with long-term adaptation to a rotational transformation</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>2254</fpage><lpage>2262</lpage><pub-id pub-id-type="doi">10.1152/jn.00984.2004</pub-id><pub-id pub-id-type="pmid">15574799</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donchin</surname><given-names>O</given-names></name><name><surname>Francis</surname><given-names>JT</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Quantifying generalization from trial-by-trial behavior of adaptive systems that learn with basis functions: theory and experiments in human motor control</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>9032</fpage><lpage>9045</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-27-09032.2003</pub-id><pub-id pub-id-type="pmid">14534237</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Liang</surname><given-names>Y</given-names></name><name><surname>Abdollahi</surname><given-names>F</given-names></name><name><surname>Fisher Bittmann</surname><given-names>M</given-names></name><name><surname>Kording</surname><given-names>K</given-names></name><name><surname>Wei</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The statistical determinants of the speed of motor learning</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005023</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005023</pub-id><pub-id pub-id-type="pmid">27606808</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriques</surname><given-names>DYP</given-names></name><name><surname>Cressman</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Visuomotor adaptation and proprioceptive recalibration</article-title><source>Journal of Motor Behavior</source><volume>44</volume><fpage>435</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1080/00222895.2012.659232</pub-id><pub-id pub-id-type="pmid">23237466</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herzfeld</surname><given-names>DJ</given-names></name><name><surname>Vaswani</surname><given-names>PA</given-names></name><name><surname>Marko</surname><given-names>MK</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A memory of errors in sensorimotor learning</article-title><source>Science</source><volume>345</volume><fpage>1349</fpage><lpage>1353</lpage><pub-id pub-id-type="doi">10.1126/science.1253138</pub-id><pub-id pub-id-type="pmid">25123484</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberdeau</surname><given-names>DM</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Haith</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Practice induces a qualitative change in the memory representation for visuomotor learning</article-title><source>Journal of Neurophysiology</source><volume>122</volume><fpage>1050</fpage><lpage>1059</lpage><pub-id pub-id-type="doi">10.1152/jn.00830.2018</pub-id><pub-id pub-id-type="pmid">31389741</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutter</surname><given-names>SA</given-names></name><name><surname>Taylor</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Relative sensitivity of explicit reaiming and implicit motor adaptation</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>2640</fpage><lpage>2648</lpage><pub-id pub-id-type="doi">10.1152/jn.00283.2018</pub-id><pub-id pub-id-type="pmid">30207865</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izawa</surname><given-names>J</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Learning from sensory and reward prediction errors during motor adaptation</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002012</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002012</pub-id><pub-id pub-id-type="pmid">21423711</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HE</given-names></name><name><surname>Morehead</surname><given-names>JR</given-names></name><name><surname>Parvin</surname><given-names>DE</given-names></name><name><surname>Moazzezi</surname><given-names>R</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Invariant errors reveal limitations in motor correction rather than constraints on error sensitivity</article-title><source>Communications Biology</source><volume>1</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-018-0021-y</pub-id><pub-id pub-id-type="pmid">30271906</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kirk</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1968">1968</year><source>Experimental Design: Procedures for the Behavioral Sciences</source><publisher-name>SAGE Publications, Inc</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>SA</given-names></name><name><surname>Levi</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Position sense of the peripheral retina</article-title><source>Journal of the Optical Society of America A</source><volume>4</volume><elocation-id>1543</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.4.001543</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Bayesian integration in sensorimotor learning</article-title><source>Nature</source><volume>427</volume><fpage>244</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1038/nature02169</pub-id><pub-id pub-id-type="pmid">14724638</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghez</surname><given-names>C</given-names></name><name><surname>Ghilardi</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptation to visuomotor transformations: consolidation, interference, and forgetting</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>473</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4218-04.2005</pub-id><pub-id pub-id-type="pmid">15647491</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Hadjiosif</surname><given-names>AM</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Wong</surname><given-names>AL</given-names></name><name><surname>Haith</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Motor Learning</article-title><source>Comprehensive Physiology</source><volume>9</volume><fpage>613</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1002/cphy.c170043</pub-id><pub-id pub-id-type="pmid">30873583</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landi</surname><given-names>SM</given-names></name><name><surname>Baguear</surname><given-names>F</given-names></name><name><surname>Della-Maggiore</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>One week of motor adaptation induces structural changes in primary motor cortex that predict long-term memory one year later</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>11808</fpage><lpage>11813</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2253-11.2011</pub-id><pub-id pub-id-type="pmid">21849541</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leow</surname><given-names>L-A</given-names></name><name><surname>Marinovic</surname><given-names>W</given-names></name><name><surname>de Rugy</surname><given-names>A</given-names></name><name><surname>Carroll</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Task errors drive memories that improve sensorimotor adaptation</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3075</fpage><lpage>3088</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1506-19.2020</pub-id><pub-id pub-id-type="pmid">32029533</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levi</surname><given-names>DM</given-names></name><name><surname>Klein</surname><given-names>SA</given-names></name><name><surname>Yap</surname><given-names>YL</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Positional uncertainty in peripheral and amblyopic vision</article-title><source>Vision Research</source><volume>27</volume><fpage>581</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(87)90044-7</pub-id><pub-id pub-id-type="pmid">3660620</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieberman</surname><given-names>HR</given-names></name><name><surname>Pentland</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Microcomputer-based estimation of psychophysical thresholds: The Best PEST</article-title><source>Behavior Research Methods &amp; Instrumentation</source><volume>14</volume><fpage>21</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.3758/BF03202110</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzoni</surname><given-names>P</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An implicit plan overrides an explicit strategy during visuomotor adaptation</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>3642</fpage><lpage>3645</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5317-05.2006</pub-id><pub-id pub-id-type="pmid">16597717</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDougle</surname><given-names>SD</given-names></name><name><surname>Bond</surname><given-names>KM</given-names></name><name><surname>Taylor</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Explicit and implicit processes constitute the fast and slow processes of sensorimotor learning</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>9568</fpage><lpage>9579</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5061-14.2015</pub-id><pub-id pub-id-type="pmid">26134640</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikulasch</surname><given-names>FA</given-names></name><name><surname>Rudelt</surname><given-names>L</given-names></name><name><surname>Priesemann</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visuomotor mismatch responses as a hallmark of explaining away in causal inference</article-title><source>Neural Computation</source><volume>35</volume><fpage>27</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01546</pub-id><pub-id pub-id-type="pmid">36283047</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miyamoto</surname><given-names>YR</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Implicit adaptation compensates for erratic explicit strategy in human motor learning</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>443</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0600-3</pub-id><pub-id pub-id-type="pmid">32112061</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Modchalingam</surname><given-names>S</given-names></name><name><surname>Vachon</surname><given-names>CM</given-names></name><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The effects of awareness of the perturbation during motor adaptation on hand localization</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0220884</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0220884</pub-id><pub-id pub-id-type="pmid">31398227</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morehead</surname><given-names>JR</given-names></name><name><surname>Taylor</surname><given-names>JA</given-names></name><name><surname>Parvin</surname><given-names>DE</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Characteristics of implicit sensorimotor adaptation revealed by task-irrelevant clamped feedback</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>1061</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01108</pub-id><pub-id pub-id-type="pmid">28195523</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mostafa</surname><given-names>AA</given-names></name><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Motor learning without moving: Proprioceptive and predictive hand localization after passive visuoproprioceptive discrepancy training</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0221861</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0221861</pub-id><pub-id pub-id-type="pmid">31465524</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neville</surname><given-names>KM</given-names></name><name><surname>Cressman</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The influence of awareness on explicit and implicit contributions to visuomotor adaptation over time</article-title><source>Experimental Brain Research</source><volume>236</volume><fpage>2047</fpage><lpage>2059</lpage><pub-id pub-id-type="doi">10.1007/s00221-018-5282-7</pub-id><pub-id pub-id-type="pmid">29744566</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>Y</given-names></name><name><surname>Schweighofer</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Minimizing precision-weighted sensory prediction errors via memory formation and switching in motor adaptation</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>9237</fpage><lpage>9250</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3250-18.2019</pub-id><pub-id pub-id-type="pmid">31582527</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title><source>Neuropsychologia</source><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id><pub-id pub-id-type="pmid">5146491</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>TS</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Mazzoni</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Learning not to generalize: modular adaptation of visuomotor gain</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>2938</fpage><lpage>2952</lpage><pub-id pub-id-type="doi">10.1152/jn.01089.2009</pub-id><pub-id pub-id-type="pmid">20357068</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petitet</surname><given-names>P</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>O’Shea</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Towards a neuro-computational account of prism adaptation</article-title><source>Neuropsychologia</source><volume>115</volume><fpage>188</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.12.021</pub-id><pub-id pub-id-type="pmid">29248498</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossi</surname><given-names>C</given-names></name><name><surname>Bastian</surname><given-names>AJ</given-names></name><name><surname>Therrien</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mechanisms of proprioceptive realignment in human motor learning</article-title><source>Current Opinion in Physiology</source><volume>20</volume><fpage>186</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.cophys.2021.01.011</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruttle</surname><given-names>JE</given-names></name><name><surname>Cressman</surname><given-names>EK</given-names></name><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Time course of reach adaptation and proprioceptive recalibration during visuomotor learning</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0163695</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0163695</pub-id><pub-id pub-id-type="pmid">27732595</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruttle</surname><given-names>JE</given-names></name><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The fast contribution of visual-proprioceptive discrepancy to reach aftereffects and proprioceptive recalibration</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0200621</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0200621</pub-id><pub-id pub-id-type="pmid">30016356</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruttle</surname><given-names>JE</given-names></name><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Implicit motor learning within three trials</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>1627</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-81031-y</pub-id><pub-id pub-id-type="pmid">33452363</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salomonczyk</surname><given-names>D</given-names></name><name><surname>Cressman</surname><given-names>EK</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Proprioceptive recalibration following prolonged training and increasing distortions in visuomotor adaptation</article-title><source>Neuropsychologia</source><volume>49</volume><fpage>3053</fpage><lpage>3062</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.07.006</pub-id><pub-id pub-id-type="pmid">21787794</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salomonczyk</surname><given-names>D</given-names></name><name><surname>Cressman</surname><given-names>EK</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The role of the cross-sensory error signal in visuomotor adaptation</article-title><source>Experimental Brain Research</source><volume>228</volume><fpage>313</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1007/s00221-013-3564-7</pub-id><pub-id pub-id-type="pmid">23708802</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadmehr</surname><given-names>R</given-names></name><name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Adaptive representation of dynamics during learning of a motor task</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>3208</fpage><lpage>3224</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-05-03208.1994</pub-id><pub-id pub-id-type="pmid">8182467</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadmehr</surname><given-names>R</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Error correction, sensory prediction, and adaptation in motor control</article-title><source>Annual Review of Neuroscience</source><volume>33</volume><fpage>89</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-060909-153135</pub-id><pub-id pub-id-type="pmid">20367317</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simani</surname><given-names>MC</given-names></name><name><surname>McGuire</surname><given-names>LMM</given-names></name><name><surname>Sabes</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual-shift adaptation is composed of separable sensory and task-dependent effects</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>2827</fpage><lpage>2841</lpage><pub-id pub-id-type="doi">10.1152/jn.00290.2007</pub-id><pub-id pub-id-type="pmid">17728389</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Ghazizadeh</surname><given-names>A</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Interacting adaptive processes with different timescales underlie short-term motor learning</article-title><source>PLOS Biology</source><volume>4</volume><elocation-id>e179</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0040179</pub-id><pub-id pub-id-type="pmid">16700627</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stetson</surname><given-names>C</given-names></name><name><surname>Cui</surname><given-names>X</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name><name><surname>Eagleman</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Motor-sensory recalibration leads to an illusory reversal of action and sensation</article-title><source>Neuron</source><volume>51</volume><fpage>651</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.08.006</pub-id><pub-id pub-id-type="pmid">16950162</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>JA</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Flexible cognitive strategies during motor learning</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1001096</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001096</pub-id><pub-id pub-id-type="pmid">21390266</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>JA</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Explicit and implicit contributions to learning in a sensorimotor adaptation task</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>3023</fpage><lpage>3032</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3619-13.2014</pub-id><pub-id pub-id-type="pmid">24553942</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>’t Hart</surname><given-names>BM</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Separating predicted and perceived sensory consequences of motor learning</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0163556</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0163556</pub-id><pub-id pub-id-type="pmid">27658214</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thoroughman</surname><given-names>KA</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Learning of action through adaptive combination of motor primitives</article-title><source>Nature</source><volume>407</volume><fpage>742</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1038/35037588</pub-id><pub-id pub-id-type="pmid">11048720</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsay</surname><given-names>JS</given-names></name><name><surname>Parvin</surname><given-names>DE</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Continuous reports of sensed hand position during sensorimotor adaptation</article-title><source>Journal of Neurophysiology</source><volume>124</volume><fpage>1122</fpage><lpage>1130</lpage><pub-id pub-id-type="doi">10.1152/jn.00242.2020</pub-id><pub-id pub-id-type="pmid">32902347</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsay</surname><given-names>JS</given-names></name><name><surname>Avraham</surname><given-names>G</given-names></name><name><surname>Kim</surname><given-names>HE</given-names></name><name><surname>Parvin</surname><given-names>DE</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>The effect of visual uncertainty on implicit motor adaptation</article-title><source>Journal of Neurophysiology</source><volume>125</volume><fpage>12</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1152/jn.00493.2020</pub-id><pub-id pub-id-type="pmid">33236937</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsay</surname><given-names>JS</given-names></name><name><surname>Kim</surname><given-names>HE</given-names></name><name><surname>Parvin</surname><given-names>DE</given-names></name><name><surname>Stover</surname><given-names>AR</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Individual differences in proprioception predict the extent of implicit sensorimotor adaptation</article-title><source>Journal of Neurophysiology</source><volume>125</volume><fpage>1307</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1152/jn.00585.2020</pub-id><pub-id pub-id-type="pmid">33656948</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsay</surname><given-names>JS</given-names></name><name><surname>Haith</surname><given-names>AM</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name><name><surname>Kim</surname><given-names>HE</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Interactions between sensory prediction error and task error during implicit motor learning</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1010005</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010005</pub-id><pub-id pub-id-type="pmid">35320276</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsay</surname><given-names>JS</given-names></name><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Haith</surname><given-names>AM</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Understanding implicit sensorimotor adaptation as a process of proprioceptive re-alignment</article-title><source>eLife</source><volume>11</volume><elocation-id>e76639</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.76639</pub-id><pub-id pub-id-type="pmid">35969491</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsay</surname><given-names>JS</given-names></name><name><surname>Kim</surname><given-names>HE</given-names></name><name><surname>Saxena</surname><given-names>A</given-names></name><name><surname>Parvin</surname><given-names>DE</given-names></name><name><surname>Verstynen</surname><given-names>T</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2022">2022c</year><article-title>Dissociable use-dependent processes for volitional goal-directed reaching</article-title><source>Proceedings. Biological Sciences</source><volume>289</volume><elocation-id>20220415</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2022.0415</pub-id><pub-id pub-id-type="pmid">35473382</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tsay</surname><given-names>JS</given-names></name><name><surname>Chandy</surname><given-names>AM</given-names></name><name><surname>Chua</surname><given-names>R</given-names></name><name><surname>Miall</surname><given-names>RC</given-names></name><name><surname>Cole</surname><given-names>J</given-names></name><name><surname>Farnè</surname><given-names>A</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name><name><surname>Sarlegna</surname><given-names>FR</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Minimal Impact of Proprioceptive Loss on Implicit Sensorimotor Adaptation and Perceived Movement Outcome</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.19.524726</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tseng</surname><given-names>Y-W</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name><name><surname>Bastian</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sensory prediction errors drive cerebellum-dependent adaptation of reaching</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>54</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1152/jn.00266.2007</pub-id><pub-id pub-id-type="pmid">17507504</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>von Helmholtz</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1867">1867</year><source>Handbuch Der Physiologischen Optik</source><publisher-name>Voss</publisher-name></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>K</given-names></name><name><surname>Körding</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Relevance of error: what drives motor adaptation?</article-title><source>Journal of Neurophysiology</source><volume>101</volume><fpage>655</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1152/jn.90545.2008</pub-id><pub-id pub-id-type="pmid">19019979</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>K</given-names></name><name><surname>Körding</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Uncertainty of feedback and state estimation determines the speed of motor adaptation</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00011</pub-id><pub-id pub-id-type="pmid">20485466</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Principles of sensorimotor learning</article-title><source>Nature Reviews. Neuroscience</source><volume>12</volume><fpage>739</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1038/nrn3112</pub-id><pub-id pub-id-type="pmid">22033537</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wozny</surname><given-names>DR</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recalibration of auditory space following milliseconds of cross-modal discrepancy</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>4607</fpage><lpage>4612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6079-10.2011</pub-id><pub-id pub-id-type="pmid">21430160</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>HG</given-names></name><name><surname>Miyamoto</surname><given-names>YR</given-names></name><name><surname>Gonzalez Castro</surname><given-names>LN</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal structure of motor variability is dynamically regulated and predicts motor learning ability</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>312</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1038/nn.3616</pub-id><pub-id pub-id-type="pmid">24413700</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Peamodel</data-title><version designator="swh:1:rev:11576048dcdcb2437ea0b489d2eb4cbea6d7efcf">swh:1:rev:11576048dcdcb2437ea0b489d2eb4cbea6d7efcf</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0fc705f196b63b02f51b2a31afcd3fa51d36afd5;origin=https://github.com/ZhaoranZhang/PEAmodel;visit=swh:1:snp:cd0e350a8175ab479cc51bdf8f197785d36df600;anchor=swh:1:rev:11576048dcdcb2437ea0b489d2eb4cbea6d7efcf">https://archive.softwareheritage.org/swh:1:dir:0fc705f196b63b02f51b2a31afcd3fa51d36afd5;origin=https://github.com/ZhaoranZhang/PEAmodel;visit=swh:1:snp:cd0e350a8175ab479cc51bdf8f197785d36df600;anchor=swh:1:rev:11576048dcdcb2437ea0b489d2eb4cbea6d7efcf</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Data fitting of <xref ref-type="fig" rid="fig4">Figure 4A</xref></title><p>We use <italic>u</italic>, <italic>v</italic>, <italic>p</italic> and <italic>h</italic> to denote the reciprocal of the variances of motor command cue, visual cue, proprioceptive cue and combined cue (hand estimation <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) respectively:<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ22"><mml:math id="m22"><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:math></disp-formula></p><p>According to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in the main text, the estimated hand can be denoted by:<disp-formula id="equ23"><label>(A1)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>v</mml:mi><mml:mi>h</mml:mi></mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mi>u</mml:mi><mml:mi>h</mml:mi></mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> indicates the target position (T). We further define the weight parameters <inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as:<disp-formula id="equ24"><mml:math id="m24"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ25"><mml:math id="m25"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>and <xref ref-type="disp-formula" rid="equ23">Equation A1</xref> can be wrote as:<disp-formula id="equ26"> , <label>(A2)</label><mml:math id="m26"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>insert <xref ref-type="disp-formula" rid="equ26">Equation A2</xref> into <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> from the main text, the hand position at trial n is:<disp-formula id="equ27"><label>(A3)</label><mml:math id="m27"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>Next, with the same Bayesian cue combination assumption, the reported hand position was combined with <inline-formula><mml:math id="inf131"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> :<disp-formula id="equ28"> , <label>(A4)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf134"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> are the weight parameters:<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>h</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><mml:math id="m30"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>put the weight parameters and <xref ref-type="disp-formula" rid="equ26">Equation A2</xref> into <xref ref-type="disp-formula" rid="equ28">Equation A4</xref>, we can easily get:<disp-formula id="equ31"><label>(A5)</label><mml:math id="m31"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>The adaptation epoch of the actual and reported hand position in <xref ref-type="fig" rid="fig4">Figure 4A</xref> was fitted by <xref ref-type="disp-formula" rid="equ27">Equation A3</xref> and <xref ref-type="disp-formula" rid="equ31">Equation A5</xref> respectively with the parameter set Θ = [ <inline-formula><mml:math id="inf135"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf136"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <italic>A</italic>, <italic>B</italic>].</p><p>We simulated the wash out epoch based on the parameters estimated from the adaptation epoch with the method below (dashed line in <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><p>Since the visual cue was no longer available, the weight of cues changed to:<disp-formula id="equ32"><mml:math id="m32"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ33"><mml:math id="m33"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf137"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, the estimated hand in wash out epoch is:<disp-formula id="equ34"><label>(A6)</label><mml:math id="m34"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>put <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula><inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> and <xref ref-type="disp-formula" rid="equ34">Equation A6</xref> into <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> in the main text, the hand position at trial n is:<disp-formula id="equ35"><label>(A7)</label><mml:math id="m35"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>Similar to <xref ref-type="disp-formula" rid="equ28">Equation A4</xref>, the reported hand position is:<disp-formula id="equ36"><label>(A8)</label><mml:math id="m36"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> denote the corresponding weight of each cue:<disp-formula id="equ37"><mml:math id="m37"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ38"><mml:math id="m38"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf142"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> denotes the reciprocal of the variances of estimated hand during washout epoch:<disp-formula id="equ39"><mml:math id="m39"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi></mml:math></disp-formula></p><p>Put the weight parameters and A6 into A8, the reported hand position can be easily derived:<disp-formula id="equ40"><label>(A9)</label><mml:math id="m40"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo>)</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ35 equ40">Equation A7 and A9</xref> were used to simulate the hand position and report position during washout epoch with Θ = [ <inline-formula><mml:math id="inf143"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf144"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <italic>A</italic>, <italic>B</italic>] estimated from the adaptation epoch.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Fitting block designed adaptation data (Experiment 2) without the assumption of linearity in visual uncertainty</title><p>For each perturbation size <italic>i</italic>, the weight of visual and proprioceptive cues are:<disp-formula id="equ41"><label>(A10)</label><mml:math id="m41"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ42"><label>(A11)</label><mml:math id="m42"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⋅</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the uncertainties of sensory cues under different perturbation sizes (2–95 degree). The estimated hand can be calculated by putting <xref ref-type="disp-formula" rid="equ41">Equation A10</xref> and <xref ref-type="disp-formula" rid="equ42">Equation A11</xref> into <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> from the main text:<disp-formula id="equ43"><mml:math id="m43"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf148"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> denotes the target location, thus:<disp-formula id="equ44"><label>(A12)</label><mml:math id="m44"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Insert <xref ref-type="disp-formula" rid="equ44">Equation A12</xref> into <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> from the main text, the trial-by-trail hand position is:<disp-formula id="equ45"><label>(A13)</label><mml:math id="m45"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>The parameter set <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo>…</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo>…</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> were fitted to the data in Experiment 2. Data fitting was done with a Bayesian optimization algorithm (<xref ref-type="bibr" rid="bib1">Acerbi and Ma, 2017</xref>) with 100 runs of different initial values between a set of predetermined preferred boundaries.</p><p>The ratio between uncertainties of different sensory cues are:<disp-formula id="equ46"><label>(A14)</label><mml:math id="m46"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:math></disp-formula><disp-formula id="equ47"><label>(A15)</label><mml:math id="m47"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>The fitting results are presented in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4A</xref>. Assuming <inline-formula><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> remains constant across different perturbation sizes, we convert <inline-formula><mml:math id="inf151"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf152"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> into ratios, <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4B</xref> illustrates the ratio between estimated uncertainties at different perturbation sizes.</p><p>To determine the values shown in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4C</xref>, we proceeded as follows: First, we found no significant correlation between perturbation sizes and the estimated values of <inline-formula><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<italic>R</italic>=–0.476, <italic>P</italic>=0.281). The mean of <inline-formula><mml:math id="inf154"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is thus set to the mean value derived from the proprioception uncertainty experiment (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), with <inline-formula><mml:math id="inf155"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 9.737° for all perturbation sizes, and <inline-formula><mml:math id="inf156"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 3.681° was also calculated as a ratio of <inline-formula><mml:math id="inf157"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Those results are indicated by the two horizontal dashed lines in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4C</xref>. Subsequently, we estimated <inline-formula><mml:math id="inf158"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for different perturbation sizes, represented by dark brown dots in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4C</xref>. To compare the visual uncertainty estimated from Experiment 2 with the results from Experiment 1, we performed a linear fit of the estimated <inline-formula><mml:math id="inf159"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> values and perturbation sizes using first-order polynomials. The fitting yielded an intercept a=3.617 and a slope b=0.261 (R<sup>2</sup>=0.982, <italic>P</italic>&lt;0.001). The slope and intercept are in close agreement with those from Experiment 1 (the gray line in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4C</xref>), and the fitting results also confirmed a strong linear relationship between <inline-formula><mml:math id="inf160"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and perturbation sizes.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94608.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Orban de Xivry</surname><given-names>Jean-Jacques</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>KU Leuven</institution><country>Belgium</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This study presents an <bold>important</bold> finding on the influence of visual uncertainty and Bayesian cue combination on implicit motor adaptation in young healthy participants, hereby linking perception and action during implicit adaptation. The evidence supporting the claims of the authors is <bold>convincing</bold>. The normative approach of the proposed PEA model, which combines ideas from separate lines of research, including vision research and motor learning, opens avenues for future developments. This work will be of interest to researchers in sensory cue integration and motor learning.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94608.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>I appreciate the normative approach of the PEA model and am eager to examine this model in the future. However, two minor issues remain:</p><p>(1) Clarification on the PReMo Model:</p><p>The authors state, &quot;The PReMo model proposes that this drift comprises two phases: initial proprioceptive recalibration and subsequent visual recalibration.&quot; This description could misinterpret the intent of PReMo. According to PReMo, the time course of the reported hand position is merely a read-out of the *perceived hand position* (x_hat in your paper). Early in adaptation, the perceived hand position is biased by the visual cursor (x_hat in the direction of the cursor); towards the end, due to implicit adaptation, x_hat reduces to zero. This is the same as PEA. I recommend that the authors clarify PReMo's intent to avoid confusion.</p><p>Note, however, the observed overshoot of 1 degree in the reported hand position. In the PReMo paper, we hypothesized that this effect is due to the recalibration of the perceived visual target location (inspired by studies showing that vision is also recalibrated by proprioception, but in the opposite direction). If the goal of implicit adaptation is to align the perceived hand position (x_hat) with the perceived target position (t_hat), then there would be an overshoot of x_hat over the actual target position.</p><p>PEA posits a different account for the overshoot. It currently suggests that the reported hand position combines x_hat (which takes x_p as input) with x_p itself. What is reasoning underlying the *double occurrence* of x_p?</p><p>There seem to be three alternatives that seem more plausible (and could lead to the same overshooting): (1) increasing x_p's contribution (assuming visual uncertainty increases when the visual cursor is absent during the hand report phase), (2) decreasing sigma_p (assuming that participants pay more attention to the hand during the report phase), (3) it could be that the perceived target position undergoes recalibration in the opposite direction to proprioceptive recalibration. All these options, at least to me, seem equally plausible and testable in the future.</p><p>(2) Effect of Visual Uncertainty on Error Size:</p><p>I appreciate the authors' response about methodological differences between the cursor cloud used in previous studies and the Gaussian blob used in the current study. However, it is still not clear to me how the authors reconcile previous studies showing that visual uncertainty reduced implicit adaptation for small but not large errors (Tsay et al, 2021; Makino, et al 2023) with the current findings, where visual uncertainty reduced implicit adaptation for large but not small errors.</p><p>Could the authors connect the dots here: I could see that the cursor cloud increases potential overlap with the visual target when the visual error is small, resulting in intrinsic reward-like mechanisms (Kim et al, 2019), which could potentially explain attenuated implicit adaptation for small visual errors. However, why would implicit adaptation in response to large visual errors remain unaffected by the cursor cloud? Note that we did verify that sigma_v is increased in (Tsay et al. 2021), so it is unlikely due to the cloud simply failing as a manipulation of visual uncertainty.</p><p>In addition, we also reasoned that testing individuals with low vision could offer a different test of visual uncertainty (Tsay et al, 2023). The advantage here is that both control and patients with low vision are provided with the same visual input-a single cursor. Our findings suggest that uncertainty due to low vision also shows reduced implicit adaptation in response to small but not large errors, contrary to the findings in the current paper. Missing in the manuscript is a discussion related to why the authors' current findings contradict those of previous results.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94608.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors present the Perceptual Error Adaptation (PEA) model, a computational approach offering a unified explanation for behavioral results that are inconsistent with standard state-space models. Beginning with the conventional state-space framework, the paper introduces two innovative concepts. Firstly, errors are calculated based on the perceived hand position, determined through Bayesian integration of visual, proprioceptive, and predictive cues. Secondly, the model accounts for the eccentricity of vision, proposing that the uncertainty of cursor position increases with distance from the fixation point. This elegantly simple model, with minimal free parameters, effectively explains the observed plateau in motor adaptation under the implicit motor adaptation paradigm using the error-clamp method. Furthermore, the authors experimentally manipulate visual cursor uncertainty, a method established in visuomotor studies, to provide causal evidence. Their results show that the adaptation rate correlates with perturbation sizes and visual noise, uniquely explained by the PEA model and not by previous models. Therefore, the study convincingly demonstrates that implicit motor adaptation is a process of Bayesian cue integration</p><p>Strengths:</p><p>In the past decade, numerous perplexing results in visuomotor rotation tasks have questioned their underlying mechanisms. Prior models have individually addressed aspects like aiming strategies, motor adaptation plateaus, and sensory recalibration effects. However, a unified model encapsulating these phenomena with a simple computational principle was lacking. This paper addresses this gap with a robust Bayesian integration-based model. Its strength lies in two fundamental assumptions: motor adaptation's influence by visual eccentricity, a well-established vision science concept, and sensory estimation through Bayesian integration. By merging these well-founded principles, the authors elucidate previously incongruent and diverse results with an error-based update model. The incorporation of cursor feedback noise manipulation provides causal evidence for their model. The use of eye-tracking in their experimental design, and the analysis of adaptation studies based on estimated eccentricity, are particularly elegant. This paper makes a significant contribution to visuomotor learning research.</p><p>The authors discussed in the revised version that the proposed model can capture the general implicit motor learning process in addition to the visuomotor rotation task. In the discussion, they emphasize two main principles: the automatic tracking of effector position and the combination of movement cues using Bayesian integration. These principles are suggested as key to understanding and modeling various motor adaptations and skill learning. The proposed model could potentially become a basis for creating new computational models for skill acquisition, especially where current models fall short.</p><p>Weaknesses:</p><p>The proposed model is described as elegant. In this paper, the authors test the model within a limited example condition, demonstrating its relevance to the sensorimotor adaptation mechanisms of the human brain. However, the scope of the model's applicability remains unclear. It has shown the capacity to explain prior data, thereby surpassing previous models that rely on elementary mathematics. To solidify its credibility in the field, the authors must gather more supporting evidence.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94608.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>(2.1) Summary</p><p>In this paper, the authors model motor adaptation as a Bayesian process that combines visual uncertainty about the error feedback, uncertainty about proprioceptive sense of hand position, and uncertainty of predicted (=planned) hand movement with a learning and retention rate as used in state space models. The model is built with results from several experiments presented in the paper and is compared with the PReMo model (Tsay, Kim et al., 2022) as well as a cue combination model (Wei &amp; Körding, 2009). The model and experiments demonstrate the role of visual uncertainty about error feedback in implicit adaptation.</p><p>In the introduction, the authors notice that implicit adaptation (as measured in error-clamp based paradigms) does not saturate at larger perturbations, but decreases again (e.g. Moorehead et al., 2017 shows no adaptation at 135{degree sign} and 175{degree sign} perturbations). They hypothesized that visual uncertainty about cursor position increases with larger perturbations since the cursor is further from the fixated target. This could decrease importance assigned to visual feedback which could explain lower asymptotes.</p><p>The authors characterize visual uncertainty for 3 rotation sizes in a first experiment, and while this experiment could be improved, it is probably sufficient for the current purposes. Then the authors present a second experiment where adaptation to 7 clamped errors are tested in different groups of participants. The models' visual uncertainty is set using a linear fit to the results from experiment 1, and the remaining 4 parameters are then fit to this second data set. The 4 parameters are (1) proprioceptive uncertainty, (2) uncertainty about the predicted hand position, (3) a learning rate and (4) a retention rate. The authors' Perceptual Error Adaptation model (&quot;PEA&quot;) predicts asymptotic levels of implicit adaptation much better than both the PReMo model (Tsay, Kim et al., 2022), which predicts saturated asymptotes, or a causal inference model (Wei &amp; Körding, 2007) which predicts no adaptation for larger rotations. In a third experiment, the authors test their model's predictions about proprioceptive recalibration, but unfortunately compare their data with an unsuitable other data set (Tsay et al. 2020, instead of Tsay et al. 2021). Finally, the authors conduct a fourth experiment where they put their model to the test. They measure implicit adaptation with increased visual uncertainty, by adding blur to the cursor, and the results are again better in line with their model (predicting overall lower adaptation), than with the PReMo model (predicting equal saturation but at larger perturbations) or a causal inference model (predicting equal peak adaptation, but shifted to larger rotations). In particular the model fits for experiment 2 and the results from experiment 4 show that the core idea of the model has merit: increased visual uncertainty about errors dampens implicit adaptation.</p><p>(2.2) Strengths</p><p>In this study the authors propose a Perceptual Error Adaptation model (&quot;PEA&quot;) and the work combines various ideas from the field of cue combination, Bayesian methods and new data sets, collected in four experiments using various techniques that test very different components of the model. The central component of visual uncertainty is assessed in a first experiment. The model uses 4 other parameters to explain implicit adaptation. These parameters are: (1) a learning and (2) a retention rate, as used in popular state space models and the uncertainty (variance) of (3) predicted and (4) proprioceptive hand position. In particular, the authors observe that asymptotes for implicit learning do not saturate, as claimed before, but decrease again when rotations are very large and that this may have to do with visual uncertainty (e.g. Tsay et al., 2021, J Neurophysiol 125, 12-22). The final experiment confirms predictions of the fitted model about what happens when visual uncertainty is increased (overall decrease of adaptation). By incorporating visual uncertainty depending on retinal eccentricity, the predictions of the PEA model for very large perturbations are notably different from, and better than, the predictions of the two other models it is compared to. That is, the paper provides strong support for the idea that visual uncertainty of errors matters for implicit adaptation.</p><p>(2.3) Weaknesses</p><p>Although the authors don't say this, the &quot;concave&quot; function that shows that adaptation does not saturate for larger rotations has been shown before, including in papers cited in this manuscript.</p><p>The first experiment, measuring visual uncertainty for several rotation sizes in error-clamped paradigms has several shortcomings, but these might not be so large as to invalidate the model or the findings in the rest of the manuscript. There are two main issues we highlight here. First, the data is not presented in units that allow comparison with vision science literature. Second, the 1 second delay between movement endpoint and disappearance of the cursor, and the presentation of the reference marker, may have led to substantial degradation of the visual memory of the cursor endpoint. That is, the experiment could be overestimating the visual uncertainty during implicit adaptation.</p><p>The paper's third experiment relies to a large degree on reproducing patterns found in one particular paper, where the reported hand positions - as a measure of proprioceptive sense of hand position - are given and plotted relative to an ever present visual target, rather than relative to the actual hand position. That is, (1) since participants actively move to a visual target, the reported hand positions do not reflect proprioception, but mostly the remembered position of the target participants were trying to move to, and (2) if the reports are converted to a difference between the real and reported hand position (rather than the difference between the target and the report), those would be on the order of ~20{degree sign} which is roughly two times larger than any previously reported proprioceptive recalibration, and an order of magnitude larger than what the authors themselves find (1-2{degree sign}) and what their model predicts. Experiment 3 is perhaps not crucial to the paper, but it nicely provides support for the idea that proprioceptive recalibration can occur with error-clamped feedback.</p><p>Perhaps the largest caveat to the study is that it assumes that people do not look at the only error feedback available to them (and can explicitly suppress learning from it). This was probably true in the experiments used in the manuscript, but unlikely to be the case in most of the cited literature. Ignoring errors and suppressing adaptation would also be a disastrous strategy to use in the real world, such that our brains may not be very good at this. So the question remains to what degree - if any - the ideas behind the model generalize to experiments without fixation control, and more importantly, to real life situations.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94608.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Zhaoran</given-names></name><role specific-use="author">Author</role><aff><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Huijun</given-names></name><role specific-use="author">Author</role><aff><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Tianyang</given-names></name><role specific-use="author">Author</role><aff><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Nie</surname><given-names>Zixuan</given-names></name><role specific-use="author">Author</role><aff><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Wei</surname><given-names>Kunlin</given-names></name><role specific-use="author">Author</role><aff><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the current reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>This study presents an important finding on the influence of visual uncertainty and Bayesian cue combination on implicit motor adaptation in young healthy participants, hereby linking perception and action during implicit adaptation. The evidence supporting the claims of the authors is convincing. The normative approach of the proposed PEA model, which combines ideas from separate lines of research, including vision research and motor learning, opens avenues for future developments. This work will be of interest to researchers in sensory cue integration and motor learning.</p></disp-quote><p>Thank you for the updated assessment. We are also grateful for the insightful and constructive comments from the reviewers, which have helped us improve the manuscript again. We made necessary changes following their comments (trimmed tests, new analysis results, etc) and responded to the comments in a point-by-point fashion below. We hope to publish these responses alongside the public review. Thank you again for fostering the fruitful discussion here.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>I appreciate the normative approach of the PEA model and am eager to examine this model in the future. However, two minor issues remain:</p><p>(1) Clarification on the PReMo Model:</p><p>The authors state, &quot;The PReMo model proposes that this drift comprises two phases: initial proprioceptive recalibration and subsequent visual recalibration.&quot; This description could misinterpret the intent of PReMo. According to PReMo, the time course of the reported hand position is merely a read-out of the *perceived hand position* (x_hat in your paper). Early in adaptation, the perceived hand position is biased by the visual cursor (x_hat in the direction of the cursor); towards the end, due to implicit adaptation, x_hat reduces to zero. This is the same as PEA. I recommend that the authors clarify PReMo's intent to avoid confusion.</p><p>Note, however, the observed overshoot of 1 degree in the reported hand position. In the PReMo paper, we hypothesized that this effect is due to the recalibration of the perceived visual target location (inspired by studies showing that vision is also recalibrated by proprioception, but in the opposite direction). If the goal of implicit adaptation is to align the perceived hand position (x_hat) with the perceived target position (t_hat), then there would be an overshoot of x_hat over the actual target position.</p><p>PEA posits a different account for the overshoot. It currently suggests that the reported hand position combines x_hat (which takes x_p as input) with x_p itself. What is reasoning underlying the *double occurrence* of x_p?</p><p>There seem to be three alternatives that seem more plausible (and could lead to the same overshooting): (1) increasing x_p's contribution (assuming visual uncertainty increases when the visual cursor is absent during the hand report phase), (2) decreasing sigma_p (assuming that participants pay more attention to the hand during the report phase), (3) it could be that the perceived target position undergoes recalibration in the opposite direction to proprioceptive recalibration. All these options, at least to me, seem equally plausible and testable in the future.</p></disp-quote><p>For clarification of the PReMo model’s take on Fig4A, we now write:</p><p>“The PReMo model proposes that the initial negative drift reflects a misperceived hand location, which gradually reduces to zero, and the late positive drift reflects the influence of visual calibration of the target (Tsay, Kim, Saxena, et al., 2022). ”</p><p>However, we would like to point out that the PEA model does not predict a zero <inline-formula><mml:math id="sa4m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (perceived hand location) even at the late phase of adaptation: it remains negative, though not as large as during initial adaptation (see Figure 4A, red line). Furthermore, we have not seen any plausible way to use a visually biased target to explain the overshoot of the judged hand location (see below when we address the three alternative hypotheses the reviewer raised).</p><p>We don’t think the “double” use of <italic>xp</italic> is a problem, simply because there are TWO tasks under investigation when the proprioceptive changes are measured along with adaptation. The first is the reaching adaptation task itself: moving under the influence of the clamped cursor. This task is accompanied by a covert estimation of hand location after the movement (<inline-formula><mml:math id="sa4m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). Given the robustness of implicit adaptation, this estimation appears mandatory and automatic. The second task is the hand localization task, during which the subject is explicitly asked to judge where the hand is. Here, the perceived hand is based on the two available cues, one is the actual hand location <italic>xp</italic>, and the other is the influence from the just finished reaching movement (i.e., <inline-formula><mml:math id="sa4m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). For Bayesian modeling from a normative perspective, sensory integration is based on the available cues to fulfill the task. For the second task of reporting the hand location, the two cues are <italic>xp</italic> and <italic>xp</italic> is used sequentially in this sense. Thus, its dual use is well justified.<inline-formula><mml:math id="sa4m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (with a possible effect of the visual target, which is unbiased since it is defined as 0 in model simulation; thus, its presence does not induce any shift effect).</p><p>Our hypothesis is that the reported hand position results from a combination of <inline-formula><mml:math id="sa4m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from the previous movement and the current hand position <italic>xp</italic>. However, specifically for the overshoot of the judged hand location in the late part of the adaptation (Fig4A), the reviewer raised three alternative explanations by assuming that the PReMo model is correct. Under the PReMo model, the estimated hand location is only determined by <inline-formula><mml:math id="sa4m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <italic>xp</italic> is not used in the hand location report phase. In addition, <italic>xp</italic> used once) and a visual recalibration of the target can explain away the gradual shift from negative to positive (overshoot).<inline-formula><mml:math id="sa4m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (with</p><p>We don’t think any of them can parsimoniously explain our findings here, and we go through these three hypotheses one by one:</p><p>(1) increasing <italic>xp</italic>'s contribution (assuming visual uncertainty increases when the visual cursor is absent during the hand report phase)</p><p>(2) decreasing <italic>σp</italic> (assuming that participants pay more attention to the hand during the report phase)</p><p>The first two alternative explanations basically assume that <italic>xp</italic> has a larger contribution (weighting in Bayesian terms) in the hand location report phase than in the adaptation movement phase, no matter due to an increase in visual uncertainty (alternative explanation 1) or a reduction in proprioceptive uncertainty (alternative explanation 2). Thus, we assume that the reviewer suggests that a larger weight for <italic>xp</italic> can explain why the perceived hand location changes gradually from negative to positive. However, per the PReMo model, a larger weight for the <italic>xp</italic> will only affect <inline-formula><mml:math id="sa4m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is already assumed to change from negative to zero. More weight in in the hand report phase (compared to the adaptation movement phase) would not explain away the reported hand location from negative to positive. This is because no matter how much weight the <italic>xp</italic> has, the PReMo model assumes a saturation <inline-formula><mml:math id="sa4m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>sat </mml:mtext></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the influence of <italic>xp</italic> on <inline-formula><mml:math id="sa4m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus <inline-formula><mml:math id="sa4m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> would not exceed zero in the late adaptation. Then, the PReMo model would rely on the so-called visual shift of the target to explain the overshoot. This leads us to the third alternative the reviewer raised:</p><p>(3) it could be that the perceived target position undergoes recalibration in the opposite direction to proprioceptive recalibration.</p><p>The PReMo model originally assumed that the perceived <italic>target</italic> location was biased in order to explain away the positive overshoot of the reported <italic>hand</italic> location. We assume that the reviewer suggests that the perceived target position, which is shifted to the positive direction, also “biases” the perceived hand position. We also assume that the reviewer suggests that the perceived hand location after a clamp trial (<inline-formula><mml:math id="sa4m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) is zero, and somehow the shifted perceived target position “biases” the reported hand location after a clamp trial. Unfortunately, we did not see any mathematical formulation of this biasing effect in the original paper (Tsay, Kim, Haith, et al., 2022). We are not able to come up with any formulation of this hypothesized biasing effect based on Bayesian cue integration principles. Target and hand are two separate perceived items; how one relates to another needs justification from a normative perspective when discussing Bayesian models. Note this is not a problem for our PEA models, in which both cues used are about hand localization, one is <inline-formula><mml:math id="sa4m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the other is <italic>xp</italic>.</p><p>We believe that mathematically formulating the biasing effect (Figure 4A) is non-trivial since the reported hand location changes continuously from negative to positive. Thus, quantitative model predictions, like the ones our PEA model presents here, are needed.</p><p>To rigorously test the possible effect of visual recalibration of the target, there are two things to do: (1) use the psychometric method to measure the biased perception of the target, and (2) re-do Tsay et al. 2020 experiment without the target. For (2), compared to the case with the target, the PEA model would predict a larger overshoot, while the PReMo would predict a smaller overshoot or even zero overshoot. This can be left for future studies.</p><disp-quote content-type="editor-comment"><p>(2) Effect of Visual Uncertainty on Error Size:</p><p>I appreciate the authors' response about methodological differences between the cursor cloud used in previous studies and the Gaussian blob used in the current study. However, it is still not clear to me how the authors reconcile previous studies showing that visual uncertainty reduced implicit adaptation for small but not large errors (Tsay et al, 2021; Makino, et al 2023) with the current findings, where visual uncertainty reduced implicit adaptation for large but not small errors.</p><p>Could the authors connect the dots here: I could see that the cursor cloud increases potential overlap with the visual target when the visual error is small, resulting in intrinsic reward-like mechanisms (Kim et al, 2019), which could potentially explain attenuated implicit adaptation for small visual errors. However, why would implicit adaptation in response to large visual errors remain unaffected by the cursor cloud? Note that we did verify that sigma_v is increased in (Tsay et al. 2021), so it is unlikely due to the cloud simply failing as a manipulation of visual uncertainty.</p><p>In addition, we also reasoned that testing individuals with low vision could offer a different test of visual uncertainty (Tsay et al, 2023). The advantage here is that both control and patients with low vision are provided with the same visual input-a single cursor. Our findings suggest that uncertainty due to low vision also shows reduced implicit adaptation in response to small but not large errors, contrary to the findings in the current paper. Missing in the manuscript is a discussion related to why the authors' current findings contradict those of previous results.</p></disp-quote><p>For connecting the dots for two previous studies (Tsay et al., 2021, 2023); Note Makino et al., 2023 is not in this discussion since it investigated the weights of multiple cursors, as opposed to visual uncertainty associated with a cursor cloud:</p><p>First, we want to re-emphasize that using the cursor cloud to manipulate visual uncertainty brings some confounds, making it not ideal for studying visuomotor adaptation. For example, in the error clamp paradigm, the error is defined as angular deviation. The cursor cloud consists of multiple cursors spanning over a range of angles, which affects both the sensory uncertainty (the intended outcome) and the sensory estimate of angles (the error estimate, the undesired outcome). In Bayesian terms, the cursor cloud aims to modulate the sigma of a distribution (<italic>σv</italic>) in our model, but it additionally affects the mean of the distribution (µ). This unnecessary confound is neatly avoided by using cursor blurring, which is still a cursor with its center (µ) unchanged from a single cursor. Furthermore, as correctly pointed out in the original paper by Tsay et al., 2020, the cursor cloud often overlaps with the visual target; this &quot;target hit&quot; would affect adaptation, possibly via a reward learning mechanism (Kim et al., 2019). This is a second confound that accompanies the cursor cloud. Yes, the cursor cloud was verified as associated with high visual uncertainty (Tsay et al., 2021); this verification was done with a psychophysics method with a clean background, not in the context of a hand reaching a target that is needed. Thus, despite the cursor cloud having a sizeable visual uncertainty, our criticisms for it still hold when used in error-clamp adaptation.</p><p>Second, bearing these confounds of the cursor cloud in mind, we postulate one important factor that has not been considered in any models thus far that might underlie the lack of difference between the single-cursor clamp and the cloud-cursor clamp when the clamp size is large: the cursor cloud might be harder to ignore than a single cursor. For Bayesian sensory integration, the naive model is to consider the relative reliability of cues only. Yes, the cloud is more uncertain in terms of indicating the movement direction than a single cursor. However, given its large spread, it is probably harder to ignore during error-clamp movements. Note that ignoring the clamped cursor is the task instruction, but the large scatter of the cursor cloud is more salient and thus plausible and harder to ignore. This might increase the weighting of the visual cue despite its higher visual uncertainty. This extra confound is arguably minimized by using the blurred cursor as in our Exp4 since the blurred cursor did not increase the visual angle much (Figure 5D; blurred vs single cursor: 3.4mm vs 2.5mm in radius, 3.90o vs 2.87o in spread). In contrast, the visual angle of the dot cloud is at least a magnitude larger (cursor cloud vs. single cursor: at least 25o vs. 2.15o in the spread, given a 10o standard deviation of random sampling).</p><p>Third, for the low-vision study (Tsay et al., 2023), the patients indeed show reduced implicit adaptation for a 3 o clamp (consistent with our PEA model) but an intact adaptation for 30-degree clamp (not consistent). Though this pattern appears similar to what happens for normal people whose visual uncertainty is upregulated by cursor cloud (Tsay et al., 2021), we are not completely convinced that the same underlying mechanism governs these two datasets. Low-vision patients indeed have higher visual uncertainty about color, brightness, and object location, but their visual uncertainty about visual motion is still unknown. Due to the difference in impairment among low vision people (e.g., peripheral or central affected) and the different roles of peripheral and central vision in movement planning and control (Sivak &amp; Mackenzie, 1992), it is unclear about the overall effect of visual uncertainty in low vision people. The direction of cursor movement that matters for visuomotor rotation here is likely related to visual motion perception. Unfortunately, the original study did not measure this uncertainty in low-vision patients. We believe our Exp1 offers a valid method for this purpose for future studies. More importantly, we should not expect low-vision patients to integrate visual cues in the same way as normal people, given their long-term adaptation to their vision difficulties. Thus, we are conservative about interpreting the seemingly similar findings across the two studies (Tsay et al., 2021, 2023) as revealing the same mechanism.</p><p>A side note: these two previous studies proposed a so-called mis-localization hypothesis, i.e., the cursor cloud was mislocated for small clamp size (given its overlapping with the target) but not for large clamp size. They suggested that the lack of uncertainty effect at small clamp sizes is due to mislocalization, while the lack of uncertainty effect at large clamp sizes is because implicit adaptation is not sensitive to uncertainty at large angles. Thus, these two studies admit that cursor cloud not only upregulates uncertainty but also generates an unwanted effect of so-called “mis-localization” (overlapping with the target). Interestingly, their hypothesis about less sensitivity to visual uncertainty for large clamps is not supported by a model or theory but merely a re-wording of the experiment results.</p><p>In sum, our current study cannot offer an easy answer to &quot;connect the dots&quot; in the aforementioned two studies due to methodology issues and the specialty of the population. However, for resolving conflicting findings, our study suggests solutions include using a psychometric test to quantify visual uncertainty for cursor motion (Exp1), a better uncertainty-manipulation method to avoid a couple of confounds (Exp4, blurred cursor), and a falsifiable model. Future endeavors can solve the difference between studies based on the new insights from the current.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors present the Perceptual Error Adaptation (PEA) model, a computational approach offering a unified explanation for behavioral results that are inconsistent with standard state-space models. Beginning with the conventional state-space framework, the paper introduces two innovative concepts. Firstly, errors are calculated based on the perceived hand position, determined through Bayesian integration of visual, proprioceptive, and predictive cues. Secondly, the model accounts for the eccentricity of vision, proposing that the uncertainty of cursor position increases with distance from the fixation point. This elegantly simple model, with minimal free parameters, effectively explains the observed plateau in motor adaptation under the implicit motor adaptation paradigm using the error-clamp method. Furthermore, the authors experimentally manipulate visual cursor uncertainty, a method established in visuomotor studies, to provide causal evidence. Their results show that the adaptation rate correlates with perturbation sizes and visual noise, uniquely explained by the PEA model and not by previous models. Therefore, the study convincingly demonstrates that implicit motor adaptation is a process of Bayesian cue integration</p><p>Strengths:</p><p>In the past decade, numerous perplexing results in visuomotor rotation tasks have questioned their underlying mechanisms. Prior models have individually addressed aspects like aiming strategies, motor adaptation plateaus, and sensory recalibration effects. However, a unified model encapsulating these phenomena with a simple computational principle was lacking. This paper addresses this gap with a robust Bayesian integration-based model. Its strength lies in two fundamental assumptions: motor adaptation's influence by visual eccentricity, a well-established vision science concept, and sensory estimation through Bayesian integration. By merging these well-founded principles, the authors elucidate previously incongruent and diverse results with an error-based update model. The incorporation of cursor feedback noise manipulation provides causal evidence for their model. The use of eye-tracking in their experimental design, and the analysis of adaptation studies based on estimated eccentricity, are particularly elegant. This paper makes a significant contribution to visuomotor learning research.</p><p>The authors discussed in the revised version that the proposed model can capture the general implicit motor learning process in addition to the visuomotor rotation task. In the discussion, they emphasize two main principles: the automatic tracking of effector position and the combination of movement cues using Bayesian integration. These principles are suggested as key to understanding and modeling various motor adaptations and skill learning. The proposed model could potentially become a basis for creating new computational models for skill acquisition, especially where current models fall short.</p><p>Weaknesses:</p><p>The proposed model is described as elegant. In this paper, the authors test the model within a limited example condition, demonstrating its relevance to the sensorimotor adaptation mechanisms of the human brain. However, the scope of the model's applicability remains unclear. It has shown the capacity to explain prior data, thereby surpassing previous models that rely on elementary mathematics. To solidify its credibility in the field, the authors must gather more supporting evidence.</p></disp-quote><p>Indeed, our model here is based on one particular experimental paradigm, i.e., the error-clamp adaptation. We used it simply because (1) this paradigm is one rare example that implicit motor learning can be isolated in a clean way, and (2) there are a few conflicting findings in the literature for us to explain away by using a unified model.</p><p>For our model’s broad impact, we believe that as long as people need to locate their effectors during motor learning, the general principle laid out here will be applicable. In other words, repetitive movements with a Bayesian cue combination of movement-related cues can underlie the implicit process of various motor learning. To showcase its broad impact, in upcoming studies, we will extend this model to other motor learning paradigms, starting from motor adaptation paradigms that involve both explicit and implicit processes.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>(2.1) Summary</p><p>In this paper, the authors model motor adaptation as a Bayesian process that combines visual uncertainty about the error feedback, uncertainty about proprioceptive sense of hand position, and uncertainty of predicted (=planned) hand movement with a learning and retention rate as used in state space models. The model is built with results from several experiments presented in the paper and is compared with the PReMo model (Tsay, Kim et al., 2022) as well as a cue combination model (Wei &amp; Körding, 2009). The model and experiments demonstrate the role of visual uncertainty about error feedback in implicit adaptation.</p><p>In the introduction, the authors notice that implicit adaptation (as measured in error-clamp based paradigms) does not saturate at larger perturbations, but decreases again (e.g. Moorehead et al., 2017 shows no adaptation at 135{degree sign} and 175{degree sign} perturbations). They hypothesized that visual uncertainty about cursor position increases with larger perturbations since the cursor is further from the fixated target. This could decrease importance assigned to visual feedback which could explain lower asymptotes.</p><p>The authors characterize visual uncertainty for 3 rotation sizes in a first experiment, and while this experiment could be improved, it is probably sufficient for the current purposes. Then the authors present a second experiment where adaptation to 7 clamped errors are tested in different groups of participants. The models' visual uncertainty is set using a linear fit to the results from experiment 1, and the remaining 4 parameters are then fit to this second data set. The 4 parameters are (1) proprioceptive uncertainty, (2) uncertainty about the predicted hand position, (3) a learning rate and (4) a retention rate. The authors' Perceptual Error Adaptation model (&quot;PEA&quot;) predicts asymptotic levels of implicit adaptation much better than both the PReMo model (Tsay, Kim et al., 2022), which predicts saturated asymptotes, or a causal inference model (Wei &amp; Körding, 2007) which predicts no adaptation for larger rotations. In a third experiment, the authors test their model's predictions about proprioceptive recalibration, but unfortunately compare their data with an unsuitable other data set (Tsay et al. 2020, instead of Tsay et al. 2021). Finally, the authors conduct a fourth experiment where they put their model to the test. They measure implicit adaptation with increased visual uncertainty, by adding blur to the cursor, and the results are again better in line with their model (predicting overall lower adaptation), than with the PReMo model (predicting equal saturation but at larger perturbations) or a causal inference model (predicting equal peak adaptation, but shifted to larger rotations). In particular the model fits for experiment 2 and the results from experiment 4 show that the core idea of the model has merit: increased visual uncertainty about errors dampens implicit adaptation.</p><p>(2.2) Strengths</p><p>In this study the authors propose a Perceptual Error Adaptation model (&quot;PEA&quot;) and the work combines various ideas from the field of cue combination, Bayesian methods and new data sets, collected in four experiments using various techniques that test very different components of the model. The central component of visual uncertainty is assessed in a first experiment. The model uses 4 other parameters to explain implicit adaptation. These parameters are: (1) a learning and (2) a retention rate, as used in popular state space models and the uncertainty (variance) of (3) predicted and (4) proprioceptive hand position. In particular, the authors observe that asymptotes for implicit learning do not saturate, as claimed before, but decrease again when rotations are very large and that this may have to do with visual uncertainty (e.g. Tsay et al., 2021, J Neurophysiol 125, 12-22). The final experiment confirms predictions of the fitted model about what happens when visual uncertainty is increased (overall decrease of adaptation). By incorporating visual uncertainty depending on retinal eccentricity, the predictions of the PEA model for very large perturbations are notably different from, and better than, the predictions of the two other models it is compared to. That is, the paper provides strong support for the idea that visual uncertainty of errors matters for implicit adaptation.</p><p>(2.3) Weaknesses</p><p>Although the authors don't say this, the &quot;concave&quot; function that shows that adaptation does not saturate for larger rotations has been shown before, including in papers cited in this manuscript.</p></disp-quote><p>For a proper citation of the “concave” adaptation function: we assume the reviewer is referring to the study by Morehead, 2017 which tested large clamp sizes up to 135 o and 175 o. Unsurprisingly, the 135 o and 175 o conditions lead to nearly zero adaptation, possibly due to the trivial fact that people cannot even see the moving cursor. We have quoted this seminar study from the very beginning. All other error-clamp studies with a block design emphasized an invariant or saturated implicit adaptation with large rotations (e.g., Kim, et al., 2019).</p><disp-quote content-type="editor-comment"><p>The first experiment, measuring visual uncertainty for several rotation sizes in error-clamped paradigms has several shortcomings, but these might not be so large as to invalidate the model or the findings in the rest of the manuscript. There are two main issues we highlight here. First, the data is not presented in units that allow comparison with vision science literature. Second, the 1 second delay between movement endpoint and disappearance of the cursor, and the presentation of the reference marker, may have led to substantial degradation of the visual memory of the cursor endpoint. That is, the experiment could be overestimating the visual uncertainty during implicit adaptation.</p></disp-quote><p>For the issues related to visual uncertainty measurement in Exp1:</p><p>First, our visual uncertainty is about cursor motion direction in the display plane, and the measurement in Exp1 has never been done before. Thus, we do not think our data is comparable to any findings in visual science about fovea/peripheral comparison. We quoted Klein and others’ work (Klein &amp; Levi, 1987; Levi et al., 1987) in vision science since their studies showed that the deviation from the fixation is associated with an increase in visual uncertainty. Their study thus inspired us to conduct Exp1 to probe how our concerned visual uncertainty (specifically for visual motion direction) changes with an increasing deviation from the fixation. Any model and its model parameters should be specifically tailored to the task or context it tries to emulate. In our case, motion direction in a center-out-reaching setting is the modeled context, and all the relevant model parameters should be specified in movement angles. This is particularly important since we need to estimate parameters from one experiment to predict behaviors in another experiment.</p><p>Second, the 1s delay of the reference cursor has minimal impact on the estimate of visual uncertainty based on previous vision studies. Our Exp1 used a similar visual paradigm by (White et al., 1992), which shows that delay does not lead to an increase in visual uncertainty over a broad range of values (from 0.2s to &gt;1s, see their Figure 5-6).</p><p>These two problems have been addressed in the revised manuscript, with proper citations listed.</p><disp-quote content-type="editor-comment"><p>The paper's third experiment relies to a large degree on reproducing patterns found in one particular paper, where the reported hand positions - as a measure of proprioceptive sense of hand position - are given and plotted relative to an ever present visual target, rather than relative to the actual hand position. That is, (1) since participants actively move to a visual target, the reported hand positions do not reflect proprioception, but mostly the remembered position of the target participants were trying to move to, and (2) if the reports are converted to a difference between the real and reported hand position (rather than the difference between the target and the report), those would be on the order of ~20° which is roughly two times larger than any previously reported proprioceptive recalibration, and an order of magnitude larger than what the authors themselves find (1-2°) and what their model predicts. Experiment 3 is perhaps not crucial to the paper, but it nicely provides support for the idea that proprioceptive recalibration can occur with error-clamped feedback.</p></disp-quote><p>Reviewer 3 thinks Tsay 2020 dataset is not appropriate for our theorization, but we respectfully disagree. For the three points raised here, we would like to elaborate:</p><p>(1) As we addressed in the previous response, the reported hand location in Figure 4A (Tsay et al., 2020) is not from a test of proprioceptive recalibration as conventionally defined. In the revision, we explicitly state that this dataset is not about proprioceptive recalibration and also delete texts that might mislead people to think so (see Results section). Instead, proprioceptive recalibration is measured by passive movement, as in our Exp3 (Figure 4E). For error-clamp adaptation here, &quot;the remembered position of the target&quot; is the target. Clearly, the participants did not report the target position, which is ever-present. Instead, their reported hand location shows an interestingly continuous change with ongoing adaptation.</p><p>(2) Since the Tsay 2020 dataset is not a so-called proprioceptive recalibration, we need not take the difference between the reported location and the actual hand location. Indeed, the difference would be ~20 degrees, but comparing it to the previously reported proprioceptive recalibration is like comparing apples to oranges. In fact, throughout the paper, we refer to the results in Fig 4A as “reported hand location”, not proprioceptive recalibration. The target direction is defined as zero degree thus its presence will not bias the reported hand in the Bayesian cue combination (as this visual cue has a mean value of 0). Using the target as the reference also simplifies our modeling.</p><p>(3) Exp3 is crucial for our study since it shows our model and its simple Bayesian cue combination principle are applicable not only to implicit adaptation but also to proprioceptive measures during adaptation. Furthermore, it reproduced the so-called proprioceptive recalibration and explained it away with the same Bayesian cue combination as the adaptation. We noticed that this field has accumulated an array of findings on proprioceptive changes induced by visuomotor adaptation. However, currently, there is a lack of a computational model to quantitatively explain them. Our study at least made an initial endeavor to model these changes.</p><disp-quote content-type="editor-comment"><p>Perhaps the largest caveat to the study is that it assumes that people do not look at the only error feedback available to them (and can explicitly suppress learning from it). This was probably true in the experiments used in the manuscript, but unlikely to be the case in most of the cited literature. Ignoring errors and suppressing adaptation would also be a disastrous strategy to use in the real world, such that our brains may not be very good at this. So the question remains to what degree - if any - the ideas behind the model generalize to experiments without fixation control, and more importantly, to real life situations.</p></disp-quote><p>The largest caveat raised by the reviewer appears to be directed to the error-clamp paradigm in general, not only to our particular study. In essence, this paradigm indeed requires participants to ignore the clamped error; thus, its induced adaptive response can be attributed to implicit adaptation. The original paper that proposed this paradigm (Morehead et al., 2017) has been cited 220 times (According to Google Scholar, at the time of this writing, 06/2024), indicating that the field has viewed this paradigm in a favorable way.</p><p>Furthermore, we agree that this kind of instruction and feedback (invariant clamp) differ from daily life experience, but it does not prevent us from gaining theoretical insights by studying human behaviors under this kind of &quot;artificial&quot; task setting. Thinking of the saccadic adaptation (Deubel, 1987; Kojima et al., 2004): jumping the target while the eye moves towards it, and this somewhat artificial manipulation again makes people adapt implicitly, and the adaptation itself is a &quot;disastrous&quot; strategy for real-life situations. However, scientists have gained an enormous understanding of motor adaptation using this seemingly counterproductive adaptation in real life. Also, think of perceptual learning of task-irrelevant stimuli (Seitz &amp; Watanabe, 2005, 2009): when participants are required to learn to discriminate one type of visual stimuli, the background shows another type of stimuli, which people gradually learn even though they do not even notice its presence. This &quot;implicit&quot; learning can be detrimental to our real life, too, but the paradigm itself has advanced our understanding of the inner workings of the cognitive system.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>L101: There is a typo: (Tsay et al., 2020, 2020) should be corrected to (Tsay et al., 2020).</p></disp-quote><p>Thanks for pointing it out, we corrected this typo.</p><disp-quote content-type="editor-comment"><p>L224-228: It would be beneficial to evaluate the validity of the estimated sigma_u and sigma_p based on previous reports.</p></disp-quote><p>We can roughly estimate <italic>σu</italic> by evaluating the variability of reaching angles during the baseline phase when no perturbation is applied. The standard deviation of the reaching angle in Exp 2 is 5.128o±0.190o, which is close to the <italic>σu</italic> estimated by the model (5.048o). We also used a separate perceptual experiment to test the proprioceptive uncertainty (n = 13, See Figure S6), <italic>σp</italic> from this experiment is 9.737o±5.598o, also close to the <italic>σp</italic> extracted by the model (11.119o). We added these new analysis results to the final version of the paper.</p><disp-quote content-type="editor-comment"><p>L289-298: I found it difficult to understand the update equations of the proprioceptive calibration based on the PEA model. Providing references to the equations or better explanations would be helpful.</p></disp-quote><p>We expanded the process of proprioceptive calibration in Supplementary Text 1 with step-by-step equations and more explanations.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>Suggestions (or clarification of previous suggestions) for revisions</p><p>The authors persist on using the Tsay et al 2020 paper despite its many drawbacks which the authors attempt to address in their reply. But the main drawback is that the results in the 2020 paper is NOT relative to the unseen hand but to the visual target the participants were supposed to move their hand to. If the results were converted so to be relative to the unseen hand, the localization biases would be over 20 deg in magnitude.</p><p>The PEA simulations are plotted relative to the unseen hand which makes sense. If the authors want to persist using the Tsay 2020 dataset despite any issues, they at least need to make sure that the simulations are mimicking the same change. That is, the data from Tsay 2020 needs to be converted to the same variable used in the current paper.</p><p>If the main objection for using the Tsay 2021 is that the design would lead to forgetting, we found that active localization (or any intervening active movements like no-cursor reach) does lead to some interference or forgetting (a small reduction in overall magnitude of adaptation) this is not the case for passive localization, see Ruttle et al, 2021 (data on osf). This was also just a suggestion, there may of course also be other, more suitable data sets.</p></disp-quote><p>As stated above, changing the reference system is not necessary, nor does it affect our results. Tsay et al 2020 dataset is unique since it shows the gradual change of reported hand location along with error-clamp adaptation. The forgetting (or reduction in proprioceptive bias), even if it exists, would not affect the fitting quality of our model for the Tsay 2020 dataset: if we assume that forgetting is invariant over the adaptation process, the forgetting would only reduce the proprioceptive bias uniformly across trials. This can be accounted for by a smaller weight on <inline-formula><mml:math id="sa4m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>hand </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The critical fact is that the model can explain the gradual drift of the proprioceptive judgment of the hand location.</p><p>By the way, Ruttle et al.'s 2021 dataset is not for error-clamp adaptation, and thus we will leave it to test our model extension in the future (after incorporating an explicit process in the model).</p><p>References</p><p>Deubel, H. (1987). Adaptivity of gain and direction in oblique saccades. <italic>Eye Movements from Physiology to Cognition</italic>. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/B9780444701138500308">https://www.sciencedirect.com/science/article/pii/B9780444701138500308</ext-link></p><p>Kim, H. E., Parvin, D. E., &amp; Ivry, R. B. (2019). The influence of task outcome on implicit motor learning. <italic>ELife</italic>, <italic>8</italic>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.39882">https://doi.org/10.7554/eLife.39882</ext-link></p><p>Klein, S. A., &amp; Levi, D. M. (1987). Position sense of the peripheral retina. <italic>JOSA A</italic>, <italic>4</italic>(8), 1543–1553.</p><p>Kojima, Y., Iwamoto, Y., &amp; Yoshida, K. (2004). Memory of learning facilitates saccadic adaptation in the monkey. <italic>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</italic>, <italic>24</italic>(34), 7531–7539.</p><p>Levi, D. M., Klein, S. A., &amp; Yap, Y. L. (1987). Positional uncertainty in peripheral and amblyopic vision. <italic>Vision Research</italic>, <italic>27</italic>(4), 581–597.</p><p>Morehead, J. R., Taylor, J. A., Parvin, D. E., &amp; Ivry, R. B. (2017). Characteristics of implicit sensorimotor adaptation revealed by task-irrelevant clamped feedback. <italic>Journal of Cognitive Neuroscience</italic>, <italic>29</italic>(6), 1061–1074.</p><p>Seitz, &amp; Watanabe. (2005). A unified model for perceptual learning. <italic>Trends in Cognitive Sciences</italic>, <italic>9</italic>(7), 329–334.</p><p>Seitz, &amp; Watanabe. (2009). The phenomenon of task-irrelevant perceptual learning. <italic>Vision Research</italic>, <italic>49</italic>(21), 2604–2610.</p><p>Sivak, B., &amp; Mackenzie, C. L. (1992). Chapter 10 The Contributions of Peripheral Vision and Central Vision to Prehension. In L. Proteau &amp; D. Elliott (Eds.), <italic>Advances in Psychology</italic> (Vol. 85, pp. 233–259). North-Holland.</p><p>Tsay, J. S., Avraham, G., Kim, H. E., Parvin, D. E., Wang, Z., &amp; Ivry, R. B. (2021). The effect of visual uncertainty on implicit motor adaptation. <italic>Journal of Neurophysiology</italic>, <italic>125</italic>(1), 12–22.</p><p>Tsay, J. S., Kim, H. E., Saxena, A., Parvin, D. E., Verstynen, T., &amp; Ivry, R. B. (2022). Dissociable use-dependent processes for volitional goal-directed reaching. <italic>Proceedings. Biological Sciences / The Royal Society</italic>, <italic>289</italic>(1973), 20220415.</p><p>Tsay, J. S., Kim, H., Haith, A. M., &amp; Ivry, R. B. (2022). Understanding implicit sensorimotor adaptation as a process of proprioceptive re-alignment. <italic>ELife</italic>, <italic>11</italic>, e76639.</p><p>Tsay, J. S., Parvin, D. E., &amp; Ivry, R. B. (2020). Continuous reports of sensed hand position during sensorimotor adaptation. <italic>Journal of Neurophysiology</italic>, <italic>124</italic>(4), 1122–1130.</p><p>Tsay, J. S., Tan, S., Chu, M. A., Ivry, R. B., &amp; Cooper, E. A. (2023). Low Vision Impairs Implicit Sensorimotor Adaptation in Response to Small Errors, But Not Large Errors. <italic>Journal of Cognitive Neuroscience</italic>, <italic>35</italic>(4), 736–748.</p><p>White, J. M., Levi, D. M., &amp; Aitsebaomo, A. P. (1992). Spatial localization without visual references. <italic>Vision Research</italic>, <italic>32</italic>(3), 513–526.</p><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>This study presents a valuable finding on the influence of visual uncertainty and Bayesian cue combination on implicit motor adaptation in young healthy participants. The evidence supporting the claims of the authors is solid, although a better discussion of the link between the model variables and the outcomes of related behavioral experiments would strengthen the conclusions. The work will be of interest to researchers in sensory cue integration and motor learning.</p><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>This valuable study demonstrates a novel mechanism by which implicit motor adaptation saturates for large visual errors in a principled normative Bayesian manner. Additionally, the study revealed two notable empirical findings: visual uncertainty increases for larger visual errors in the periphery, and proprioceptive shifts/implicit motor adaptation are non-monotonic, rather than ramp-like. This study is highly relevant for researchers in sensory cue integration and motor learning. However, I find some areas where statistical quantification is incomplete, and the contextualization of previous studies to be puzzling.</p></disp-quote><p>Thank you for your feedback and the positive highlights of our study. We appreciate your insights and will address the concerns in our revisions.</p><disp-quote content-type="editor-comment"><p>Issue #1: Contextualization of past studies.</p><p>While I agree that previous studies have focused on how sensory errors drive motor adaptation (e.g., Burge et al., 2008; Wei and Kording, 2009), I don't think the PReMo model was contextualized properly. Indeed, while PReMo should have adopted clearer language - given that proprioception (sensory) and kinaesthesia (perception) have been used interchangeably, something we now make clear in our new study (Tsay, Chandy, et al. 2023) - PReMo's central contribution is that a perceptual error drives implicit adaptation (see Abstract): the mismatch between the felt (perceived) and desired hand position. The current paper overlooks this contribution. I encourage the authors to contextualize PReMo's contribution more clearly throughout. Not mentioned in the current study, for example, PReMo accounts for the continuous changes in perceived hand position in Figure 4 (Figure 7 in the PReMo study).</p><p>There is no doubt that the current study provides important additional constraints on what determines perceived hand position: Firstly, it offers a normative Bayesian perspective in determining perceived hand position. PReMo suggests that perceived hand position is determined by integrating motor predictions with proprioception, then adding a proprioceptive shift; PEA formulates this as the optimal integration of these three inputs. Secondly, PReMo assumed visual uncertainty to remain constant for different visual errors; PEA suggests that visual uncertainty ought to increase (but see Issue #2).</p></disp-quote><p>Thank you for the comments and suggestions. We have now incorporated the citation for (Tsay et al., 2024), to acknowledge their clarification on the terms of perceptual error. We also agree that our model differs in two fundamental ways. One is to ditch the concept of proprioceptive shift and its contribution to the perceived hand location; instead, we resort to a “one-shot” integration of three types of cues with Bayesian rules. This is a more elegant and probably more ecological way of processing hand location per Occam's Razor. The second essential change is to incorporate the dependency of visual uncertainty on perturbation size into the model, as opposed to resorting to a ramp function of proprioceptive changes relative to perturbation size. The ramp function is not well grounded in perception studies. Yes, we acknowledged that PReMo is the first to recognize the importance of perceptual error, but highlighted the model differences in our Discussion.</p><p>We also think the PReMo model has the potential to explain Fig 4A. But the Tsay et al., 2022 paper assumes that “a generic shift in visual space” explains the gradual proprioceptive changes from negative to positive (see page 17 in Tsay et al., 2022). We do not think that evoking this visual mechanism is necessary to explain Fig 4A; instead, the proprioceptive change is a natural result of hand deviations during implicit adaptation. As the hand moves away from the target (in the positive direction) during adaptation, the estimated hand location goes alone with it. We believe this is the correct way of explaining Fig4A results. As we played around with the PReMo model, we found it is hard to use visual shift to explain this part of data without additional assumptions (at least not with the ones published in Tsay et al., 2022). Furthermore, our PEA model also parsimoniously explains away the proprioceptive shift observed in a completely different setting, i,e., the proprioceptive changes measured by the passive method as a function of perturbation size in Exp 3.</p><p>We expanded the discussion about the comparison between the two models, especially about their different views for explaining Fig4A.</p><disp-quote content-type="editor-comment"><p>Issue #2: Failed replication of previous results on the effect of visual uncertainty.</p><p>(2a) A key finding of this paper is that visual uncertainty linearly increases in the periphery; a constraint crucial for explaining the non-monotonicity in implicit adaptation. One notable methodological deviation from previous studies is the requirement to fixate on the target: Notably, in the current experiments, participants were asked to fixate on the target, a constraint not imposed in previous studies. In a free-viewing environment, visual uncertainty may not attenuate as fast, and hence, implicit adaptation does not attenuate as quickly as that revealed in the current design with larger visual errors. Seems like this current fixation design, while important, needs to be properly contextualized considering how it may not represent most implicit adaptation experiments.</p></disp-quote><p>First, we don’t think there is any previous study that examined visual uncertainty as a function of perturbation size. Thus, we do not have a replication problem here. Secondly, our data indicate that even without asking people to fixate on the target, people still predominantly fixate on the target during error-clamp adaptation (when they are “free” viewing). For our Exp 1, the fixation on the straight line between the starting position and the target is 86%-95% (as shown in Figure S1 now， also see below). We also collected eye-tracking data in Exp 4, which is a typical error-clamp experiment. More than 95% fall with +/- 50 pixels around the center of the screen, even slightly higher than Exp 1. This is well understandable: the typical error-clamp adaptation requires people to ignore the cursor and move the hand towards the target. To minimize the interference of the concurrently moving cursor, people depend on the fixation on the target, the sole task-relevant visual marker in the workspace, to achieve the task goal.</p><p>In sum, forcing the participants to fixate on the target is not because we aimed to make up the linear dependency of visual uncertainty; we required them to do so to mimic the eye-tracking pattern in typical error-clamp learning, which has been revealed in our pilot experiment. The visual uncertainty effect is sound, our study is the first to clearly demonstrate it.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-sa4-fig1-v1.tif"/></fig><p>On a side note (but an important one), the high percentage of fixation on the aiming target is also true for conventional visuomotor rotation, which involves strategic re-aiming (shown in Bromberg et al., 2019; de Brouwer et al., 2018, we have an upcoming paper to show this). This is one reason that our new theory would also be applicable to other types of motor adaptation.</p><disp-quote content-type="editor-comment"><p>(2b) Moreover, the current results - visual uncertainty attenuates implicit adaptation in response to large, but not small, visual errors - deviates from several past studies that have shown that visual uncertainty attenuates implicit adaptation to small, but not large, visual errors (Tsay, Avraham, et al. 2021; Makino, Hayashi, and Nozaki, n.d.; Shyr and Joshi 2023). What do the authors attribute this empirical difference to? Would this free-viewing environment also result in the opposite pattern in the effect of visual uncertainty on implicit adaptation for small and large visual errors?</p></disp-quote><p>We don’t think all the mentioned previous studies manipulated the visual uncertainty in a parametric way, and none of them provided quantitative measures of visual uncertainty. As we detailed in our Exp4 and in our Discussion, we don’t think Tsay et al., 2021 paper’s manipulation of visual uncertainty is appropriate (see below for 2d). Makino et al., 2023 study used multiple clamped cursors to perturb people, and its effect is not easily accountable since additional processes might be invoked given this kind of complex visual feedback. More importantly, we do not think this is a direct way of modulating visual uncertainty, nor did they provide any evidence.</p><disp-quote content-type="editor-comment"><p>(2c) In the current study, the measure of visual uncertainty might be inflated by brief presentation times of comparison and referent visual stimuli (only 150 ms; our previous study allowed for a 500 ms viewing time to make sure participants see the comparison stimuli). Relatedly, there are some individuals whose visual uncertainty is greater than 20 degrees standard deviation. This seems very large, and less likely in a free-viewing environment.</p></disp-quote><p>For our 2AFC, the reference stimulus is the actual clamped cursor, which lasts for 800 ms. The comparison stimulus is a 150-ms dot representation appearing near the reference. For measuring perception of visual motion, this duration is sufficient as previous studies used similar durations (Egly &amp; Homa, 1984; Owsley et al., 1995). We think the 20-degree standard deviation is reasonable given that people fixate on the target, with only peripheral vision to process the fast moving cursor. The steep linear increase in visual uncertainty about visual motion is well documented. The last author of this paper has shown that the uncertainty of visual motion speed (though not about angels) follows the same steep trend (Wei et al., 2010). It is noteworthy that without using our measured visual uncertainty in Exp1, if we fit the adaptation data in Exp2 to “estimate” the visual uncertainty, they are in fact well aligned with each other (see Figure S7 and Supplementary Text 2). This is a strong support that our estimation is valid and accurate. We think this high visual uncertainty is an important message to the field. Thus we now highlighted its magnitude in our Discussion.</p><disp-quote content-type="editor-comment"><p>(2d) One important confound between clear and uncertain (blurred) visual conditions is the number of cursors on the screen. The number of cursors may have an attenuating effect on implicit adaptation simply due to task-irrelevant attentional demands (Parvin et al. 2022), rather than that of visual uncertainty. Could the authors provide a figure showing these blurred stimuli (gaussian clouds) in the context of the experimental paradigm? Note that we addressed this confound in the past by comparing participants with and without low vision, where only one visual cursor is provided for both groups (Tsay, Tan, et al. 2023).</p></disp-quote><p>Thank you for raising this important point about types of visual stimuli for manipulating uncertainty. We used Gaussian blur of a single cursor (similar to Burge et al., 2008) instead of a cloud of dots. We now added a figure inset to show how this blur looks.</p><p>Using a cursor cloud Makino et al., 2023; Tsay et al., 2021 to modulate visual uncertainty has inherent drawbacks that make it unsuitable for visuomotor adaptation. For the error clamp paradigm, the error is defined as angular deviation. The cursor cloud consists of multiple cursors spanning over a range of angles, which affects both the sensory uncertainty (the intended outcome) and the sensory estimate of angles (the error estimate, the undesired outcome). In Bayesian terms, the cursor cloud aims to modulate the sigma of a distribution (sigma_v in our model), but it additionally affects the mean of the distribution (mu). This unnecessary confound is avoided by using cursor blurring, which is still a cursor with its center (mu) unchanged from a single cursor. Furthermore, as correctly pointed out in the original paper by Tsay et al., 2021, the cursor cloud often overlaps with the visual target, this “target hit” would affect adaptation, possibly via a reward learning mechanism (See Kim et al., 2019). This is a second confound that accompanies the cursor cloud.</p><disp-quote content-type="editor-comment"><p>Issue #3: More methodological details are needed.</p><p>(3a) It's unclear why, in Figure 4, PEA predicts an overshoot in terms of perceived hand position from the target. In PReMo, we specified a visual shift in the perceived target position, shifted towards the adapted hand position, which may result in overshooting of the perceived hand position with this target position. This visual shift phenomenon has been discovered in previous studies (e.g., (Simani, McGuire, and Sabes 2007)).</p></disp-quote><p>Visual shift, as it is called in Simani et al., 2007, is irrelevant for our task here. The data we are modeling are motor adaptation (hand position changes) and so-called proprioceptive changes (hand localization changes), both are measured and referenced in the extrinsic coordinate, not referenced to a visual target. For instance, the proprioceptive changes are either relative to the actual hand location (Exp 3) or relative to the goal (Fig 4A). We also don’t think visual shift is necessary in explaining the perceptual judgment of an unseen hand (the target shown during the judgment indeed has an effect of reducing the biasing effect of PE, see below for responses to reviewer 3).</p><p>In the PEA model, the reported hand angle is the result of integrating cues from the actual hand position and the estimated hand position (x_hand_hat) from previous movements. This integration process leads to the combined reported hand position potentially overshooting or undershooting, depending on the degree of adaptation. It is the changed proprioceptive cue (because the actively moved hand slowly adapted to the error clamp) leading to the overshoot of the perceived hand position.</p><p>In Results, we now explain these value changes with parentheses. Model details about the mechanisms of cue combination and model predictions can be found in Supplementary Text 1. We believe these detailed explanations can make this apparent.</p><disp-quote content-type="editor-comment"><p>(3b) The extent of implicit adaptation in Experiment 2, especially with smaller errors, is unclear. The implicit adaptation function seems to be still increasing, at least by visual inspection. Can the authors comment on this trend, and relatedly, show individual data points that help the reader appreciate the variability inherent to these data?</p></disp-quote><p>Indeed, the adaptation for small errors appears not completely saturated with our designated number of trials. However, this will not affect our model analysis. Our model fitting for PEA and other competing models is done on the time-series of adaptation, not on the saturated adaptation extent (see Fig 3A). Thus, despite that some conditions might not produce the full range of adaptation, the data is sufficient to constrain the models. We now mention this concern in Results; we also emphasize that the model not only explains the adaptation magnitude (operationally defined as adaptation extent measured at the same time, i.e., the end of the adaptation phase) but also the full learning process.</p><p>In response, we have included individual data points in the revised Figure 3B-D to provide a clear illustration of the extent of implicit adaptation, particularly for small perturbations.</p><disp-quote content-type="editor-comment"><p>(3c) The same participants were asked to return for multiple days/experiments. Given that the authors acknowledge potential session effects, with attenuation upon re-exposure to the same rotation (Avraham et al. 2021), how does re-exposure affect the current results? Could the authors provide clarity, perhaps a table, to show shared participants between experiments and provide evidence showing how session order may not be impacting results?</p></disp-quote><p>Thank you for raising the issue of session and re-exposure effects. First, we don’t think Exp1 has an effect on Exp4. Exp1 is a perceptual task and Exp4 is a motor adaptation task. Furthermore, Exp1 used random visual stimuli on both sides, thus it did not lead to any adaptation effect on its own. Second, Exp4 indeed had three sessions performed on three days, but the session effect does not change our main conclusion about the visual uncertainty. We used a 3-way repeated-measures anova (3 day x 3 perturbation x 2 visual uncertainty) revealed a significant main effect of day (F(2,36) = 17.693, p&lt;0.001), indicating changes in performance across sessions (see Figure below). Importantly, the effects of perturbation and visual uncertainty (including their interactions) remain the same. The day factor did not interact with them. The main effect of day shows that the overall adaptation effect is reduced across days. Post-hoc pairwise comparisons elucidated that single-trial learning (STL) performance on Day 1 was significantly higher than on Day 2 (p = 0.004) and Day 3 (p &lt; 0.001), with no significant difference between Day 2 and Day 3 (p = 0.106). Other ANOVA details: significant main effects for perturbation (F(1,36) = 8.872, p&lt;0.001) and visual uncertainty (F(1,18) = 49.164, p&lt;0.001), as well as a significant interaction between perturbation size and visual uncertainty (F(2,36) = 5.160, p = 0.013). There were no significant interactions involving the day factor with any other factors (all p &gt; 0.182). Thus, the overall adaptation decreases over the days, but the day does not affect our concerned interaction effect of visual uncertainty and perturbation. The fact that their interaction preserved over different sessions strengthened our conclusion about how visual uncertainty systematically affects implicit adaptation.</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-sa4-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(3d) The number of trials per experiment should be detailed more clearly in the Methods section (e.g., Exp 4). Moreover, could the authors please provide relevant code on how they implemented their computational models? This would aid in future implementation of these models in future work. I, for one, am enthusiastic to build on PEA.</p></disp-quote><p>We have clarified the number of trials conducted in each experiment, with detailed information now readily available in the Methods section of the main text. In addition, we have made the code for data analysis and modeling publicly accessible. These resources can be found in the updated &quot;Data Availability&quot; section of our paper.</p><disp-quote content-type="editor-comment"><p>(3f) In addition to predicting a correlation between proprioceptive shift and implicit adaptation on a group level, both PReMo and PEA (but not causal inference) predict a correlation between individual differences in proprioceptive shift and proprioceptive uncertainty with the extent of implicit adaptation (Tsay, Kim, et al. 2021). Interestingly, shift and uncertainty are independent (see Figures 4F and 6C in Tsay et al, 2021). Does PEA also predict independence between shift and uncertainty? It seems like PEA does predict a correlation.</p></disp-quote><p>Thank you for addressing this insightful question. Our PEA model indeed predicts a positive correlation (although not linear) between the proprioceptive uncertainty and the amplitude of the estimated hand position (x_hand_hat). This prediction is consistent with the simulations conducted, using the same parameters that were applied to generate the results depicted in</p><p>Figure 4B of our manuscript (there is a sign flip as x_hand_hat is negative).</p><fig id="sa4fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-sa4-fig3-v1.tif"/></fig><p>Regarding the absence of a correlation observed in Tsay et al., 2021, we offer several potential explanations for this discrepancy. First, the variability observed in passive hand localization during motor adaptation (as in Tsay et al., 2021) does not directly equal proprioceptive uncertainty, which typically requires psychophysical testing to accurately assess. Second, our study showed that the proprioceptive bias attenuates during the repetitive measurements; in our Exp3, it decreased within a block of three trials. We noticed that Tsay et al., 2021 study used 36 measurements in a row without interleaving adaptation trials. Thus, the “averaged” proprioceptive bias in Tsay’s study might not reflect the actual bias during adaptation. We also noticed that that study showed large individual differences in both proprioceptive bias and proprioceptive variability (not uncertainty), thus getting a positive result, if it were really there, would require a large number of participants, probably larger than their n=30ish sample size. These putative explanations are not put in the revision, which already has a long discussion and has no space for discussing about a null result.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors present the Perceptual Error Adaptation (PEA) model, a computational approach offering a unified explanation for behavioral results that are inconsistent with standard state-space models. Beginning with the conventional state-space framework, the paper introduces two innovative concepts. Firstly, errors are calculated based on the perceived hand position, determined through Bayesian integration of visual, proprioceptive, and predictive cues. Secondly, the model accounts for the eccentricity of vision, proposing that the uncertainty of cursor position increases with distance from the fixation point. This elegantly simple model, with minimal free parameters, effectively explains the observed plateau in motor adaptation under the implicit motor adaptation paradigm using the error-clamp method. Furthermore, the authors experimentally manipulate visual cursor uncertainty, a method established in visuomotor studies, to provide causal evidence. Their results show that the adaptation rate correlates with perturbation sizes and visual noise, uniquely explained by the PEA model and not by previous models. Therefore, the study convincingly demonstrates that implicit motor adaptation is a process of Bayesian cue integration</p><p>Strengths:</p><p>In the past decade, numerous perplexing results in visuomotor rotation tasks have questioned their underlying mechanisms. Prior models have individually addressed aspects like aiming strategies, motor adaptation plateaus, and sensory recalibration effects. However, a unified model encapsulating these phenomena with a simple computational principle was lacking. This paper addresses this gap with a robust Bayesian integration-based model. Its strength lies in two fundamental assumptions: motor adaptation's influenced by visual eccentricity, a well-established vision science concept, and sensory estimation through Bayesian integration. By merging these well-founded principles, the authors elucidate previously incongruent and diverse results with an error-based update model. The incorporation of cursor feedback noise manipulation provides causal evidence for their model. The use of eye-tracking in their experimental design, and the analysis of adaptation studies based on estimated eccentricity, are particularly elegant. This paper makes a significant contribution to visuomotor learning research.</p><p>Weaknesses:</p><p>The paper provides a comprehensive account of visuomotor rotation paradigms, addressing incongruent behavioral results with a solid Bayesian integration model. However, its focus is narrowly confined to visuomotor rotation, leaving its applicability to broader motor learning paradigms, such as force field adaptation, saccadic adaptation, and de novo learning paradigms, uncertain. The paper's impact on the broader fields of neuroscience and cognitive science may be limited due to this specificity. While the paper excellently demonstrates that specific behavioral results in visuomotor rotation can be explained by Bayesian integration, a general computational principle, its contributions to other motor learning paradigms remain to be explored. The paper would benefit from a discussion on the model's generality and its limitations, particularly in relation to the undercompensating effects in other motor learning paradigms.</p></disp-quote><p>Thank you for your thoughtful review and recognition of the contributions our work makes towards understanding implicit motor adaptation through the Perceptual Error Adaptation (PEA) model. We appreciate your suggestion to broaden the discussion about the model's applicability beyond the visuomotor rotation paradigm, a point we acknowledge was not sufficiently explored in our initial discussion.</p><p>Our model is not limited to the error-clamp adaptation, where the participants were explicitly told to ignore the rotated cursor. The error-clamp paradigm is one rare example that implicit motor learning can be isolated in a nearly idealistic way. Our findings thus imply two key aspects of implicit adaptation: (1) localizing one’s effector is implicitly processed and continuously used to update the motor plan; (2) Bayesian cue combination is at the core of integrating movement feedback and motor-related cues (motor prediction cue in our model) when forming procedural knowledge for action control.</p><p>We will propose that the same two principles should be applied to various kinds of motor adaptation and motor skill learning, which constitutes motor learning in general. Most of our knowledge about motor adaptation is from visuomotor rotation, prism adaptation, force field adaptation, and saccadic adaptation. The first three types all involve localizing one’s effector under the influence of perturbed sensory feedback, and they also have implicit learning. We believe they can be modeled by variants of our model, or at least should consider using the two principles we laid out above to think of their computational nature. For skill learning, especially for de novo learning, the area still lacks a fundamental computational model that accounts for skill acquisition process on the level of relevant movement cues. Our model suggests a promising route, i.e., repetitive movements with a Bayesian cue combination of movement-related cues might underlie the implicit process of motor skills.</p><p>We added more discussion on the possible broad implications of our model in the revision.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary</p><p>In this paper, the authors model motor adaptation as a Bayesian process that combines visual uncertainty about the error feedback, uncertainty about proprioceptive sense of hand position, and uncertainty of predicted (=planned) hand movement with a learning and retention rate as used in state space models. The model is built with results from several experiments presented in the paper and is compared with the PReMo model (Tsay, Kim, et al., 2022) as well as a cue combination model (Wei &amp; Körding, 2009). The model and experiments demonstrate the role of visual uncertainty about error feedback in implicit adaptation.</p><p>In the introduction, the authors notice that implicit adaptation (as measured in error-clamp-based paradigms) does not saturate at larger perturbations, but decreases again (e.g. Moorehead et al., 2017 shows no adaptation at 135{degree sign} and 175{degree sign} perturbations). They hypothesized that visual uncertainty about cursor position increases with larger perturbations since the cursor is further from the fixated target. This could decrease the importance assigned to visual feedback which could explain lower asymptotes.</p><p>The authors characterize visual uncertainty for 3 rotation sizes in the first experiment, and while this experiment could be improved, it is probably sufficient for the current purposes. Then the authors present a second experiment where adaptation to 7 clamped errors is tested in different groups of participants. The models' visual uncertainty is set using a linear fit to the results from experiment 1, and the remaining 4 parameters are then fit to this second data set. The 4 parameters are (1) proprioceptive uncertainty, (2) uncertainty about the predicted hand position, (3) a learning rate, and (4) a retention rate. The authors' Perceptual Error Adaptation model (&quot;PEA&quot;) predicts asymptotic levels of implicit adaptation much better than both the PReMo model (Tsay, Kim et al., 2022), which predicts saturated asymptotes, or a causal inference model (Wei &amp; Körding, 2007) which predicts no adaptation for larger rotations. In a third experiment, the authors test their model's predictions about proprioceptive recalibration, but unfortunately, compare their data with an unsuitable other data set. Finally, the authors conduct a fourth experiment where they put their model to the test. They measure implicit adaptation with increased visual uncertainty, by adding blur to the cursor, and the results are again better in line with their model (predicting overall lower adaptation) than with the PReMo model (predicting equal saturation but at larger perturbations) or a causal inference model (predicting equal peak adaptation, but shifted to larger rotations). In particular, the model fits experiment 2 and the results from experiment 4 show that the core idea of the model has merit: increased visual uncertainty about errors dampens implicit adaptation.</p><p>Strengths</p><p>In this study, the authors propose a Perceptual Error Adaptation model (&quot;PEA&quot;) and the work combines various ideas from the field of cue combination, Bayesian methods, and new data sets, collected in four experiments using various techniques that test very different components of the model. The central component of visual uncertainty is assessed in the first experiment. The model uses 4 other parameters to explain implicit adaptation. These parameters are (1) learning and (2) retention rate, as used in popular state space models, and the uncertainty (variance) of (3) predicted and (4) proprioceptive hand position. In particular, the authors observe that asymptotes for implicit learning do not saturate, as claimed before, but decrease again when rotations are very large and that this may have to do with visual uncertainty (e.g. Tsay et al., 2021, J Neurophysiol 125, 12-22). The final experiment confirms predictions of the fitted model about what happens when visual uncertainty is increased (overall decrease of adaptation). By incorporating visual uncertainty depending on retinal eccentricity, the predictions of the PEA model for very large perturbations are notably different from and better than, the predictions of the two other models it is compared to. That is, the paper provides strong support for the idea that visual uncertainty of errors matters for implicit adaptation.</p><p>Weaknesses</p><p>Although the authors don't say this, the &quot;concave&quot; function that shows that adaptation does not saturate for larger rotations has been shown before, including in papers cited in this manuscript.</p><p>The first experiment, measuring visual uncertainty for several rotation sizes in error-clamped paradigms has several shortcomings, but these might not be so large as to invalidate the model or the findings in the rest of the manuscript. There are two main issues we highlight here. First, the data is not presented in units that allow comparison with vision science literature. Second, the 1 second delay between the movement endpoint and the disappearance of the cursor, and the presentation of the reference marker, may have led to substantial degradation of the visual memory of the cursor endpoint. That is, the experiment could be overestimating the visual uncertainty during implicit adaptation.</p><p>The paper's third experiment relies to a large degree on reproducing patterns found in one particular paper, where the reported hand positions - as a measure of proprioceptive sense of hand position - are given and plotted relative to an ever-present visual target, rather than relative to the actual hand position. That is, (1) since participants actively move to a visual target, the reported hand positions do not reflect proprioception, but mostly the remembered position of the target participants were trying to move to, and (2) if the reports are converted to a difference between the real and reported hand position (rather than the difference between the target and the report), those would be on the order of ~20{degree sign} which is roughly two times larger than any previously reported proprioceptive recalibration, and an order of magnitude larger than what the authors themselves find (1-2{degree sign}) and what their model predicts. Experiment 3 is perhaps not crucial to the paper, but it nicely provides support for the idea that proprioceptive recalibration can occur with error-clamped feedback.</p><p>Perhaps the largest caveat to the study is that it assumes that people do not look at the only error feedback available to them (and can explicitly suppress learning from it). This was probably true in the experiments used in the manuscript, but unlikely to be the case in most of the cited literature. Ignoring errors and suppressing adaptation would also be a disastrous strategy to use in the real world, such that our brains may not be very good at this. So the question remains to what degree - if any - the ideas behind the model generalize to experiments without fixation control, and more importantly, to real-life situations.</p><p>Specific comments:</p><p>A small part of the manuscript relies on replicating or modeling the proprioceptive recalibration in a study we think does NOT measure proprioceptive recalibration (Tsay, Parvin &amp; Ivry, JNP, 2020). In this study, participants reached for a visual target with a clamped cursor, and at the end of the reach were asked to indicate where they thought their hand was. The responses fell very close to the visual target both before and after the perturbation was introduced. This means that the difference between the actual hand position, and the reported/felt hand position gets very large as soon as the perturbation is introduced. That is, proprioceptive recalibration would necessarily have roughly the same magnitude as the adaptation displayed by participants. That would be several times larger than those found in studies where proprioceptive recalibration is measured without a visual anchor. The data is plotted in a way that makes it seem like the proprioceptive recalibration is very small, as they plot the responses relative to the visual target, and not the discrepancy between the actual and reported hand position. It seems to us that this study mostly measures short-term visual memory (of the target location). What is astounding about this study is that the responses change over time to begin with, even if only by a tiny amount. Perhaps this indicates some malleability of the visual system, but it is hard to say for sure.</p><p>Regardless, the results of that study do not form a solid basis for the current work and they should be removed. We would recommend making use of the dataset from the same authors, who improved their methods for measuring proprioception shifts just a year later (Tsay, Kim, Parvin, Stover, and Ivry, JNP, 2021). Although here the proprioceptive shifts during error-clamp adaptation (Exp 2) were tiny, and not quite significant (p&lt;0.08), the reports are relative to the actual location of the passively placed unseen hand, measured in trials separate from those with reach adaptation and therefore there is no visual target to anchor their estimates to.</p><p>Experiment 1 measures visual uncertainty with increased rotation size. The authors cite relevant work on this topic (Levi &amp; Klein etc) which has found a linear increase in uncertainty of the position of more and more eccentrically displayed stimuli.</p><p>First, this is a question where the reported stimuli and effects could greatly benefit from comparisons with the literature in vision science, and the results might even inform it. In order for that to happen, the units for the reported stimuli and effects should (also) be degrees of visual angle (dva).</p><p>As far as we know, all previous work has investigated static stimuli, where with moving stimuli, position information from several parts of the visual field are likely integrated over time in a final estimate of position at the end of the trajectory (a Kalman filter type process perhaps). As far as we know, there are no studies in vision science on the uncertainty of the endpoint of moving stimuli. So we think that the experiment is necessary for this study, but there are some areas where it could be improved.</p><p>Then, the linear fit is done in the space of the rotation size, but not in the space of eccentricity relative to fixation, and these do not necessarily map onto each other linearly. If we assume that the eye-tracker and the screen were at the closest distance the manufacturer reports it to work accurately at (45 cm), we would get the largest distances the endpoints are away from fixation in dva. Based on that assumed distance between the participant and monitor, we converted the rotation angles to distances between fixation and the cursor endpoint in degrees visual angle: 0.88, 3.5, and 13.25 dva (ignoring screen curvature, or the absence of it). The ratio between the perturbation angle and retinal distance to the endpoint is roughly 0.221, 0.221, and 0.207 if the minimum distance is indeed used - which is probably fine in this case. But still, it would be better to do fit in the relevant perceptual coordinate system.</p><p>The first distance (4 deg rotation; 0.88 dva offset between fixation and stimulus) is so close to fixation (even at the assumed shortest distance between eye and screen) that it can be considered foveal and falls within the range of noise of eye-trackers + that of the eye for fixating. There should be no uncertainty on or that close to the fovea. The variability in the data is likely just measurement noise. This also means that a linear fit will almost always go through this point, somewhat skewing the results toward linearity. The advantage is that the estimate of the intercept (measurement noise) is going to be very good. Unfortunately, there are only 2 other points measured, which (if used without the closest point) will always support a linear fit. Therefore, the experiment does not seem suitable to test linearity, only to characterize it, which might be sufficient for the current purposes. We'd understand if the effort to do a test of linearity using many more rotations requires too much effort. But then it should be made much clearer that the experiment assumes linearity and only serves to characterize the assumed linearity.</p><p>Final comment after the consultation session:</p><p>There were a lot of discussions about the actual interpretation of the behavioral data from this paper with regards to past papers (Tsay et al. 2020 or 2021), and how it matches the different variables of the model. The data from Tsay 2020 combined both proprioceptive information (Xp) and prediction about hand position (Xu) because it involves active movements. On the other hand, Tsay et al. 2021 is based on passive movements and could provide a better measure of Xp alone. We would encourage you to clarify how each of the variables used in the model is mapped onto the outcomes of the cited behavioral experiments.</p><p>The reviewers discussed this point extensively during the consultation process. The results reported in the Tsay 2020 study reflect both proprioception and prediction. However, having a visual target contributes more than just prediction, it is likely an anchor in the workspace that draws the response to it. Such that the report is dominated by short-term visual memory of the target (which is not part of the model). However, in the current Exp 3, as in most other work investigating proprioception, this is calculated relative to the actual direction.</p><p>The solution is fairly simple. In Experiment 3 in the current study, Xp is measured relative to the hand without any visual anchors drawing responses, and this is also consistent with the reference used in the Tsay et al 2021 study and from many studies in the lab of D. Henriques (none of which also have any visual reach target when measuring proprioceptive estimates). So we suggest using a different data set that also measures Xp without any other influences, such as the data from Tsay et al 2021 instead.</p><p>These issues with the data are not superficial and can not be solved within the model. Data with correctly measured biases (relative to the hand) that are not dominated by irrelevant visual attractors would actually be informative about the validity of the PEA model. Dr. Tsay has so much other that we recommend using a more to-the-point data set that could actually validate the PEA model.</p></disp-quote><p>As the comments are repetitive at some places, we summarize them into three questions and address it one by one below:</p><p>(1) Methodological Concerns about visual uncertainty estimation in Experiment 1: (a) the visual uncertainty is measured in movement angles (degrees), while the unit in vision science is in visual angles (vda). This mismatch of unit hinders direct comparison between the found visual uncertainty and those reported in the literature, and (b) a 1-second delay between movement endpoint and the reference marker presentation causes an overestimate of visual uncertainty due to potential degradation of visual memory. (c) The linear function of visual uncertainty is a result of having only three perturbation sizes.</p><p>a) As noted by the reviewer, our visual uncertainty is about cursor motion direction in the display plane, which has never been measured before. We do not think our data is comparable to any findings in visual science about fovea/peripheral comparison. We quoted Klein and others’ work Klein &amp; Levi, 1987; Levi et al., 1987 in vision science since their studies showed that the deviation from the fixation is associated with the increase in visual uncertainty. Their study thus inspired our Exp1 to probe how our concerned visual uncertainty (specifically for visual motion direction) changes with an increasing deviation from the fixation. We believe that any model and its model parameters should be specifically tailored to the task or context it tries to emulate. In our case, motion direction in a center-out reaching setting is the modeled context, and all the relevant model parameters should be specified in movement angles.</p><p>b) The 1s delay of the reference cursor appears to have minimum impact on the estimate of visual uncertainty, based on previous vision studies. Our Exp1 used a similar visual paradigm by White et al., 1992, which shows that delay does not lead to an increase in visual uncertainty over a broad range of values (from 0.2s to &gt;1s, see their Figure 5-6). We will add more methodology justifications in our revision.</p><p>c) We agree that if more angles are tested we can be more confident about the linearity of visual uncertainty. However, the linear function is a good approximation of visual uncertainty (as shown in Figure 2C). More importantly, our model performance does not hinge on a strict linear function. Say, if it is a power function with an increasing slope, our model will still predict the major findings presented in the paper, as correctly pointed out by the reviewer. It is the increasing trend of visual uncertainty, which is completely overlooked by previous studies, that lead to various seemingly puzzling findings in implicit adaptation. Lastly, without assuming a linear function, we fitted the large dataset of motor adaptation from Exp2 to numerically estimate the visual uncertainty. This estimated visual uncertainty has a strong linear relationship with perturbation size (R = 0.991, p&lt;0.001). In fact, the model-fitted visual uncertainty is very close to the values we obtained in Exp1. We now included this analysis in the revision. See details in Supplementary text 2 and Figure S7.</p><p>(2) Experiment 3's: the reviewer argues that the Tsay et al., 2020 data does not accurately measure proprioceptive recalibration, thus it is not suitable for showing our model’s capacity in explaining proprioceptive changes during adaptation.</p><p>Response: We agree that the data from Tsay et al., 2020 is not from passive localization, which is regarded as the widely-accepted method to measure proprioceptive recalibration, a recalibration effect in the sensory domain. The active localization, as used in Tsay et al., 2020, is hypothesized as closely related to people’s forward prediction (where people want to go as the reviewer put it in the comments). However, we want to emphasize that we never equated Tsay’s findings as proprioceptive recalibration: throughout the paper we call them “reported hand location”. We reserved “proprioceptive recalibration” to our own Exp3, which used a passive localization method. Thus, we are not guilty of using this term. Secondly, as far as we know, localization bias or changes, no matter measured by passive or active methods, have not been formally modeled quantitatively. We believe our model can explain both, at least in the error-clamp adaptation setting here. Exp3 is for passive localization, the proprioceptive bias is caused by the biasing effect from the just-perceived hand location (X_hand_hat) from the adaptation trial. Tsay et al. 2020 data is for active localization, whose bias shows a characteristic change from negative to positive. This can be explained by just-perceived hand location (X_hand_hat again) and a gradually-adapting hand (X_p). We think this is a significant advance in the realm of proprioceptive changes in adaptation. Of course, our idea can be further tested in other task conditions, e.g., conventional visuomotor rotation or even gain adaptation, which should be left for future studies.</p><p>For technical concerns, Tsay et al., 2020 data set is not ideal: when reporting hand location, the participants view the reporting wheel as well as the original target. As correctly pointed out by the reviewer, the presence of the target might provide an anchoring cue for perceptual judgment, which acts as an attractor for localization. If it were the case, our cue combination would predict that this extra attractor effect would lead to a smaller proprioceptive effect than that is currently reported in their paper. The initial negative bias will be closer to the target (zero), and the later positive bias will be closer to the target too. However, the main trend will remain, i.e. the reported hand location would still show the characteristic negative-to-positive change. The attractor effect of the target can be readily modeled by giving less weight to the just-perceived hand location (X_hand_hat). Thus, we would like to keep Tsay et al., 2020 data in our paper but add some explanations of the limitations of this dataset as well as how the model would fare with these limitations.</p><p>That being said, our model can explain away both passive and active localization during implicit adaptation elicited by error clamp. The dataset from Tsay et al., 2021 paper is not a good substitute for their 2020 paper in terms of modeling, since that study interleaved some blocks of passive localization trials with adaptation trials. This kind of block design would lead to forgetting of both adaptation (Xp in our model) and the perceived hand (X_hand_hat in our model), the latter is still not considered in our model yet. As our Exp3, which also used passive localization, shows, the influence of the perceived hand on proprioceptive bias is short-lived, up to three trials without adaptation trials. Of course, it would be of great interest to design future studies to study how the proprioceptive bias changes over time, and how its temporal changes relate to the perceptual error. Our model provides a testbed to move forward in this direction.</p><p>(3) The reviewer raises concerns about the study's assumption that participants ignore error feedback, questioning the model's applicability to broader contexts and real-world scenarios where ignoring errors might not be viable or common.</p><p>Reviewer 2 raised the same question above. We moved our responses here. “We appreciate your suggestion to broaden the discussion about the model's applicability beyond the visuomotor rotation paradigm, a point we acknowledge was not sufficiently explored in our initial discussion.</p><p>Our model is not limited to the error-clamp adaptation, where the participants were explicitly told to ignore the rotated cursor. The error-clamp paradigm is one rare example that implicit motor learning can be isolated in a nearly idealistic way. Our findings thus imply two key aspects of implicit adaptation: (1) localizing one’s effector is implicitly processed and continuously used to update the motor plan; (2) Bayesian cue combination is at the core of integrating movement feedback and motor-related cues (motor prediction cue in our model) when forming procedural knowledge for action control.</p><p>We will propose that the same two principles should be applied to various kinds of motor adaptation and motor skill learning, which constitutes motor learning in general. Most of our knowledge about motor adaptation is from visuomotor rotation, prism adaptation, force field adaptation, and saccadic adaptation. The first three types all involve localizing one’s effector under the influence of perturbed sensory feedback, and they also have implicit learning. We believe they can be modeled by variants of our model, or at least should consider using the two principles we laid out above to think of their computational nature. For skill learning, especially for de novo learning, the area still lacks a fundamental computational model that accounts for skill acquisition process on the level of relevant movement cues. Our model suggests a promising route, i.e., repetitive movements with a Bayesian cue combination of movement-related cues might underlie the implicit process of motor skills.”</p><p>We also add one more important implication of our model: as stated above, our model also explains that the proprioceptive changes, revealed by active or passive localization methods, are brought by (mis)perceived hand localization via Bayesian cue combination. This new insight, though only tested here using the error-clamp paradigm, can be further utilized in other domains, e.g., conventional visuomotor rotation or force field adaptation. We hope this serves as an initial endeavor in developing some computational models for proprioception studies. Please see the extended discussion on this matter in the revision.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p>Revisions:</p><p>All three reviewers were positive about the work and have provided a set of concrete and well-aligned suggestions, which the authors should address in a revised version of the article. These are listed below.</p><p>A few points of particular note:</p><p>(1) There are a lot of discussions about the actual interpretation of behavioral data from this paper or past papers (Tsay et al. 2020 or 2021) and how it matches the different variables of the model.</p><p>(2) There are some discussions on the results of the first experiment, both in terms of how it is reported (providing degrees of visual angle) and how it is different than previous results (importance of the point of fixation). We suggest also discussing a few papers on eye movements during motor adaptation from the last years (work of Anouk de Brouwer and Opher Donchin). Could the authors also discuss why they found opposite results to that of previous visual uncertainty studies (i.e., visual uncertainty attenuates learning with large, but not small, visual errors); rather than the other way around as in Burge et al and Tsay et al 2021 and Makino Nozaki 2023 (where visual uncertainty attenuates small, but not large, visual errors).</p><p>(3) It is recommended by several reviewers to discuss the applicability of the model to other areas/perturbations.</p><p>(4) Several reviewers and I believe that the impact of the paper would be much higher if the code to reproduce all the simulations of the model is made available to the readers. In addition, while I am very positive about the fact that the authors shared the data of their experiments, metadata seems to be missing while they are highly important because these data are otherwise useless.</p></disp-quote><p>Thank you for the concise summary of the reviewers’ comments. We have addressed their concerns point by point.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>L142: The linear increase in visual uncertainty should be substantiated by previous research in vision science. Please cite relevant papers and discuss why the linear model is considered reasonable.</p></disp-quote><p>We cited relevant studies in vision science. Their focus is more about eccentricity inflate visual uncertainty, similar to our findings that deviations from the fixation direction inflate visual uncertainty about motion direction.</p><p>We also want to add that our model performance does not hinge on a strict linear function of visual uncertainty. Say, if it is a power function with an increasing slope, our model will still predict the major findings presented in the paper. It is the increasing trend of visual uncertainty, which is completely overlooked by previous studies, that lead to various seemingly puzzling findings in implicit adaptation. Furthermore, without assuming a linear function, we fitted the large dataset of motor adaptation from Exp2 to numerically estimate the visual uncertainty. This estimated visual uncertainty has a strong linear relationship with perturbation size (R = 0.991, p&lt;0.001). In fact, the model-fitted visual uncertainty is very close to the values we obtained in Exp1. We now included this new analysis in the revision. See details in Supplementary text 2 and Figure S7.</p><disp-quote content-type="editor-comment"><p>L300: I found it challenging to understand the basis for this conclusion. Additional explanatory support is required.</p></disp-quote><p>We unpacked this concluding sentence as follows:</p><p>“The observed proprioceptive bias is formally modeled as a result of the biasing effect of the perceived hand estimate x_hand_hat. In our mini-block of passive localization, the participants neither actively moved nor received any cursor perturbations for three trials in a row. Thus, the fact that the measured proprioceptive bias is reduced to nearly zero at the third trial suggests that the effect of perceived hand estimate x_hand_hat decays rather rapidly.”</p><disp-quote content-type="editor-comment"><p>L331: For the general reader, a visual representation of what the blurring mask looks like would be beneficial.</p></disp-quote><p>Thanks for the nice suggestion. We added pictures of a clear and a blurred cursor in Figure 5D.</p><disp-quote content-type="editor-comment"><p>L390: This speculation is intriguing. It would be helpful if the authors explained why they consider causal inference to operate at an explicit process level, as the reasoning is not clear here, although the idea seems plausible.</p></disp-quote><p>Indeed, our tentative conclusion here is only based on the model comparison results here. It is still possible that causal inference also work for implicit adaptation besides explicit adaptation. We make a more modest conclusion in the revision:</p><p>“The casual inference model is also based on Bayesian principle, then why does it fail to account for the implicit adaptation? We postulate that the failure of the causal inference model is due to its neglect of visual uncertainty as a function of perturbation size, as we revealed in Experiment 1. In fact, previous studies that advocating the Bayesian principle in motor adaptation have largely focused on experimentally manipulating sensory cue uncertainty to observe its effects on adaptation (Burge et al., 2008; He et al., 2016; Körding &amp; Wolpert, 2004; Wei &amp; Körding, 2010), similar to our Experiment 4. Our findings suggest that causal inference of perturbation alone, without incorporating visual uncertainty, cannot fully account for the diverse findings in implicit adaptation. The increase in visual uncertainty by perturbation size is substantial: our Experiment 1 yielded an approximate seven-fold increase from a 4° perturbation to a 64° perturbation. We have attributed this to the fact that people fixate in the desired movement direction during movements. Interestingly, even for conventional visuomotor rotation paradigm where people are required to “control” the perturbed cursor, their fixation is also on the desired direction, not on the cursor itself (de Brouwer, Albaghdadi, et al., 2018; de Brouwer, Gallivan, et al., 2018). Thus, we postulate that a similar hike in visual uncertainty in other “free-viewing” perturbation paradigms. Future studies are warranted to extend our PEA model to account for implicit adaptation in other perturbation paradigms.”</p><disp-quote content-type="editor-comment"><p>L789: The method of estimating Sigma_hand in the brain was unclear. Since Bayesian computation relies on the magnitude of noise, the cognitive system must have estimates of this noise. While vision and proprioception noise might be directly inferred from signals, the noise of the hand could be deduced from the integration of these observations or an internal model estimate. This process of estimating noise magnitude is theorized in recursive Bayesian integration models (or Kalman filtering), where the size estimate of the state noise (sigma_hand) is updated concurrently with the state estimate (x_hand hat). The equation in L789 and the subsequent explanation appear to assume a static model of noise estimation. However, in practice, the noise parameters, including Sigma_hand, are likely dynamic and updated with each new observation. A more detailed explanation of how Sigma_hand is estimated and its role in the cognitive process.</p></disp-quote><p>This is a great comment. In fact, if a Kalman filter is used, the learning rate and the state noise all should be dynamically updated on each trial, under the influence of the observed (x_v). In fact, most adaptation models assume a constant learning rate, including our model here. But a dynamic learning rate (B in our model) is something worth trying. However, in our error-clamp setting, x_v is a constant, thus this observation variable cannot dynamically update the Kalman filter; that’s why we opt to use a “static” Bayesian model to explain our datasets. Thus, Sigma_hand can be estimated by using Bayesian principles as a function of three cues available, i.e., the proprioceptive cue, the visual cue, and the motor prediction cue. We added a</p><p>detailed derivation of sigma_hand in the revision in Supplementary text 1.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>We observed values in Fig 2C for the 64-degree perturbation that seem to be outliers, i.e., greater than 50 degrees. It is unclear how a psychometric curve could have a &quot;slope&quot; or JNP of over 60, especially considering that the tested range was only 60. Since the data plotted in panel C is a collapse of the signed data in panel B, it is perplexing how such large data points were derived, particularly when the signed uncertainty values do not appear to exceed 30.</p><p>Related to the previous point, we would also recommend connecting individual data points: if the uncertainty increases (linearly or otherwise), then people with low uncertainty at the middle distance should also have low uncertainty at the high distance, and people with high uncertainty at one point, should also have that at other distances. Or perhaps the best way to go about this is to use the uncertainty at the two smaller perturbations to predict uncertainty at the largest perturbation for each participant individually?</p></disp-quote><p>Thank you for your suggestion to examine the consistency of individual levels of visual uncertainty across perturbation sizes. First, a sigma_v of 60 degrees is well possible, naturally falling out of the experimental data. It shows some individuals indeed have large visual uncertainty. Given these potential outliers (which should not be readily removed as we don’t have any reason to do so), we estimated the linear function of sigma_v with a robust method, i.e., the GLM with a gamma distribution, which favors right-skewed distribution that can well capture positive outliers. Furthermore, we added in our revision a verification test of our estimates of sigma_v: we used Exp2’s adaptation data to estimate sigma_v without assuming its linear dependency. As shown, the model-fitted sigma_v closely matched the estimated ones from Exp1 (see Supplementary text 2 and Figure S7).</p><p>We re-plotted the sigma_v with connected data points provided, and the data clearly indicate that individuals exhibit consistent levels of visual uncertainty across different perturbation sizes, i.e. those with relatively lower uncertainty at middle distances (in fact, angles) tend to exhibit relatively lower uncertainty at higher distances too, and similarly, those with higher uncertainty at one distance maintain that level of uncertainty at other distances. This is confirmed by spearman correlation analysis to assess the consistency of uncertainties across different degrees of perturbation among individuals. Again, we observed significant correlations between perturbation angles, indicating good individual consistency (4 and 16 degrees, rho = 0.759, p&lt;0.001; 16 and 64 degrees, rho = 0.527, p = 0.026).</p><fig id="sa4fig4" position="float"><label>Author response image 4.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94608-sa4-fig4-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>The illustration in Fig 2A does not seem to show a stimulus that is actually used in the experiment (looks like about -30{degree sign} perturbation). It would be good to show all possible endpoints with all other visual elements to scale - including the start-points of the PEST procedure.</p></disp-quote><p>Thanks for the suggestion. We updated Fig 2A to show a stimulus of +16 degree, as well as added an additional panel to show all the possible endpoints.</p><disp-quote content-type="editor-comment"><p>Finally (related to the previous point), in lines 589-591 it says the target is a blue cross. Then in lines 614-616, it says participants are to fixate the blue cross or the start position. The start position was supposed to have disappeared, so perhaps the blue plus moved to the start position (which could be the case, when looking at the bottom panel in Fig 2A, although in the illustration the plus did not move fully to the start position, just toward it to some degree). Perhaps the descriptions need to be clarified, or it should be explained why people had to make an eye movement before giving their judgments. And if people could have made either (1) no eye movement, but stayed at fixation, (2) moved to the blue plus as shown in the last panel in Fig 2A, or (3) fixated on the home position, we'd be curious to know if this affected participants' judgments.</p></disp-quote><p>Thanks for pointing that out. The blue cross serves as the target in the movement task, then disappears with the cursor after 800ms of frozen time. The blue cross then appeared in the discrimination task at the center of the screen, i.e. the start location. Subjects were asked to fixate at the blue cross during the visual discrimination task. Note this return the fixation to the home position is exactly what we will see in typical error-clamp adaptation: once the movement is over, people guided their hand back to the home position. We performed a pilot study to record the typical fixation pattern during error-clamp adaptation, and Exp1 was intentionally designed to mimic its fixation sequence. We have now updated the description of Figure 2A, emphasizing the stimulus sequence. .</p><disp-quote content-type="editor-comment"><p>In Figure 4A, the label &quot;bias&quot; is confusing as that is used for recalibrated proprioceptive sense of hand position as well as other kinds of biases elsewhere in the paper. What seems to be meant is the integrated hand position (x-hat_hand?) where all three signals are apparently combined. The label should be changed and/or it should be clarified in the caption.</p></disp-quote><p>Thanks for pointing that out, it should be x_hand_hat, and we have corrected this in the revised version of Figure 4.</p><disp-quote content-type="editor-comment"><p>In the introduction, it is claimed that larger perturbations have not been tested with &quot;implicit adaptation&quot; paradigms, but in the same sentence, a paper is cited (Moorehead et al., 2017) that tests a rotation on the same order of magnitude as the largest one tested here (95{degree sign}), as well as much larger rotations (135{degree sign} and 175{degree sign}). With error-clamps. Interestingly, there is no adaptation in those conditions, which seems more in line with the sensory cue integration model. Can the PEA model explain these results as well? If so, this should be included in the paper, and if not, it should be discussed as a limitation.</p></disp-quote><p>First, we double checked our manuscript and found that we never claimed that larger perturbations had not been tested.</p><p>We agree that it is always good to have as many conditions as possible. However, the 135 and 175 degree conditions would lead to minimum adaptation, which would not help much in terms of model testing. We postulated that this lack of adaptation is simply due to the fact that people cannot see the moving cursor, or some other unknown reasons. Our simple model is not designed to cover those kinds of extreme cases.</p><disp-quote content-type="editor-comment"><p>Specify the size of the arc used for the proprioceptive tests in Exp 3 and describe the starting location of the indicator (controlled by the left hand). Ideally, the starting location should have varied across trials to avoid systematic bias.</p></disp-quote><p>Thank you for the comments. The size of the arc used during these tests, as detailed in the methods section of our paper, features a ring with a 10 cm radius centered at the start position. This setup is visually represented as a red arc in Figure 7B.</p><p>After completing each proprioceptive test trial, participants were instructed to position the indicator at approximately -180° on the arc and then relax their left arm. Although the starting location for the subsequent trial remained at-180°, it was not identical for every trial, thereby introducing slight variability.</p><disp-quote content-type="editor-comment"><p>Please confirm that the proprioceptive biases plotted in Fig 4E are relative to the baseline.</p></disp-quote><p>Thank you for bringing this to our attention. Yes, the proprioceptive biases illustrated in Figure 4E are indeed calculated relative to the baseline measurements. We have added this in the method part.</p><disp-quote content-type="editor-comment"><p>Data availability: the data are available online, but there are some ways this can be improved. First, it would be better to use an open data format, instead of the closed, proprietary format currently used. Second, there is no explanation for what's in the data, other than the labels. (What are the units? What preprocessing was done?) Third, no code is made available, which would be useful for a computational model. Although rewriting the analyses in a non-proprietary language (to increase accessibility) is not a reasonable request at this point in the project, I'd encourage it for future projects. But perhaps Python, R, or Julia code that implements the model could be made available as a notebook of sorts so that other labs could look at (build on) the model starting with correct code - increasing the potential impact of this work.</p></disp-quote><p>Great suggestions. We are also fully supportive of open data and open science. We now:</p><p>(1) Updated our data and code repository to include the experimental data in an open data format (.csv) for broader accessibility.</p><p>(2) The data are now accompanied by detailed descriptions to clarify their contents.</p><p>(3) We have made the original MATLAB (.m) codes for data analysis, model fitting and simulation available online.</p><p>(4) We also provide the codes in Jupyter Notebook (.ipynb) formats.</p><p>These updates can be found in the revised “Data Availability” section of our manuscript.</p><p>References</p><p>Bromberg, Z., Donchin, O., &amp; Haar, S. (2019). Eye Movements during Visuomotor Adaptation Represent Only Part of the Explicit Learning. <italic>eNeuro</italic>, <italic>6</italic>(6). https://doi.org/10.1523/ENEURO.0308-19.2019</p><p>Burge, J., Ernst, M. O., &amp; Banks, M. S. (2008). The statistical determinants of adaptation rate in human reaching. <italic>Journal of Vision</italic>, <italic>8</italic>(4), 1–19.</p><p>de Brouwer, A. J., Gallivan, J. P., &amp; Flanagan, J. R. (2018). Visuomotor feedback gains are modulated by gaze position. <italic>Journal of Neurophysiology</italic>, <italic>120</italic>(5), 2522–2531.</p><p>Egly, R., &amp; Homa, D. (1984). Sensitization of the visual field. <italic>Journal of Experimental Psychology. Human Perception and Performance</italic>, <italic>10</italic>(6), 778–793.</p><p>Kim, H. E., Parvin, D. E., &amp; Ivry, R. B. (2019). The influence of task outcome on implicit motor learning. <italic>eLife</italic>, <italic>8</italic>. https://doi.org/10.7554/eLife.39882</p><p>Klein, S. A., &amp; Levi, D. M. (1987). Position sense of the peripheral retina. <italic>JOSA A</italic>, <italic>4</italic>(8), 1543–1553.</p><p>Levi, D. M., Klein, S. A., &amp; Yap, Y. L. (1987). Positional uncertainty in peripheral and amblyopic vision. <italic>Vision Research</italic>, <italic>27</italic>(4), 581–597.</p><p>Makino, Y., Hayashi, T., &amp; Nozaki, D. (2023). Divisively normalized neuronal processing of uncertain visual feedback for visuomotor learning. <italic>Communications Biology</italic>, <italic>6</italic>(1), 1286.</p><p>Owsley, C., Ball, K., &amp; Keeton, D. M. (1995). Relationship between visual sensitivity and target localization in older adults. <italic>Vision Research</italic>, <italic>35</italic>(4), 579–587.</p><p>Simani, M. C., McGuire, L. M. M., &amp; Sabes, P. N. (2007). Visual-shift adaptation is composed of separable sensory and task-dependent effects. <italic>Journal of Neurophysiology</italic>, <italic>98</italic>(5), 2827–2841.</p><p>Tsay, J. S., Avraham, G., Kim, H. E., Parvin, D. E., Wang, Z., &amp; Ivry, R. B. (2021). The effect of visual uncertainty on implicit motor adaptation. <italic>Journal of Neurophysiology</italic>, <italic>125</italic>(1), 12–22.</p><p>Tsay, J. S., Chandy, A. M., Chua, R., Miall, R. C., Cole, J., Farnè, A., Ivry, R. B., &amp; Sarlegna, F. R. (2024). Minimal impact of proprioceptive loss on implicit sensorimotor adaptation and perceived movement outcome. <italic>bioRxiv : The Preprint Server for Biology</italic>. https://doi.org/10.1101/2023.01.19.524726</p><p>Tsay, J. S., Kim, H., Haith, A. M., &amp; Ivry, R. B. (2022). Understanding implicit sensorimotor adaptation as a process of proprioceptive re-alignment. <italic>eLife</italic>, <italic>11</italic>, e76639.</p><p>Wei, K., Stevenson, I. H., &amp; Körding, K. P. (2010). The uncertainty associated with visual flow fields and their influence on postural sway: Weber’s law suffices to explain the nonlinearity of vection. <italic>Journal of Vision</italic>, <italic>10</italic>(14), 4.</p><p>White, J. M., Levi, D. M., &amp; Aitsebaomo, A. P. (1992). Spatial localization without visual references. <italic>Vision Research</italic>, <italic>32</italic>(3), 513–526.</p></body></sub-article></article>