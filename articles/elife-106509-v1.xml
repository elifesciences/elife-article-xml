<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106509</article-id><article-id pub-id-type="doi">10.7554/eLife.106509</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106509.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A computational pipeline to track chromatophores and analyze their dynamics</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ukrow</surname><given-names>Johann</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-4933-7871</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Renard</surname><given-names>Mathieu DM</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0000-0801-7438</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Moghimi</surname><given-names>Mahyar</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Laurent</surname><given-names>Gilles</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2296-114X</contrib-id><email>g.laurent@brain.mpg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02h1nk258</institution-id><institution>Max Planck Institute for Brain Research</institution></institution-wrap><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>07</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP106509</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-03-03"><day>03</day><month>03</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-03-07"><day>07</day><month>03</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.03.03.641160"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-05-01"><day>01</day><month>05</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106509.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-07-09"><day>09</day><month>07</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106509.2"/></event></pub-history><permissions><copyright-statement>© 2025, Ukrow, Renard et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Ukrow, Renard et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106509-v1.pdf"/><abstract><p>Cephalopod chromatophores are small dermal neuromuscular organs, each consisting of a pigment-containing cell and 10–20 surrounding radial muscles. Their expansions and contractions, controlled and coordinated by the brain, are used to modify the animal’s appearance during camouflaging and signaling. Building up on tools developed by this lab, we propose a flexible computational pipeline to track and analyze chromatophore dynamics from high-resolution videos of behaving cephalopods. This suite of functions, which we call CHROMAS, segments and classifies individual chromatophores, compensates for animal movements and skin deformations, thus enabling precise and parallel measurements of chromatophore dynamics and long-term tracking over development. A high-resolution tool for the analysis of chromatophore deformations during behavior reveals details of their motor control and thus, their likely innervation. When applied to many chromatophores simultaneously and combined with statistical and clustering tools, this analysis reveals the complex and distributed nature of the chromatophore motor units. We apply CHROMAS to the skins of the bobtail squid <italic>Euprymna berryi</italic> and the European cuttlefish <italic>Sepia officinalis</italic>, illustrating its performance with species with widely different chromatophore densities and patterning behaviors. More generally, CHROMAS offers many flexible and easily reconfigured tools to quantify the dynamics of pixelated biological patterns.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cephalopod</kwd><kwd>chromatophore</kwd><kwd>tracking</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Laurent</surname><given-names>Gilles</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/101141501</award-id><principal-award-recipient><name><surname>Laurent</surname><given-names>Gilles</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>LOEWE Schwerpunkt CMMS</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Laurent</surname><given-names>Gilles</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication. Open access funding provided by Max Planck Society.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Computer code is provided to track the size and shape of deformable pigment cells in the skin of cephalopods from high-resolution video images.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animal skin color can change adaptively thanks to the migration or direct control of specialized pigment-containing cells. In ectothermic animals, these cells are called chromatophores. They often each contain one of several pigments, enabling elaborate patterns, colors (as in fish or reptiles), and in some cases, such as cephalopods, communication and camouflage. Coleoid cephalopods evolved the unique ability to change in an instant the patterning, color, and 3D texture of their skin through the neural control of their chromatophores. Cephalopod chromatophores are elaborate neuromuscular organs composed of a cell containing an elastic sacculus filled with pigment granules, surrounded by a set of radial muscles, themselves controlled by motor neurons located in the brain (<xref ref-type="bibr" rid="bib8">Cloney and Florey, 1968</xref>; <xref ref-type="bibr" rid="bib31">Messenger, 2001</xref>). The skin of some species of cuttlefish and octopus possesses millions of such chromatophores at a density of 230 per mm<sup>2</sup> in adults. As biological analogs of pixels on a screen, chromatophores operate by varying their size (and thus effective reflectance) rather than photon emission. Chromatophores, together with other specialized cells such as iridophores and leucophores, enable these cephalopods to display a variety of patterns and colors that play crucial roles for camouflage and intra- and inter-specific signaling (<xref ref-type="bibr" rid="bib16">Hanlon et al., 1988</xref>).</p><p>The cephalopod camouflage system operates as a perception-processing-projection pipeline, where visual input from the environment is analyzed by the nervous system and reproduced as a statistical approximation on its skin. For this reason, these skin displays offer a quantifiable readout of the animal’s perceptual state and have fascinated scientists for decades. However, studying chromatophore activity over chromatophore populations is challenging because of their small size (ranging from 5 to 100 μm when retracted, depending on the species), their vast numbers (thousands to millions) over large surfaces (hundreds of cm<sup>2</sup>), the inherent flexibility and motion of cephalopod skin (due to respiration, for example), and the absence of tools to track their state in real time. While researchers did, in a few studies, quantify skin patterns from still images of the animals, chromatophore resolution and tracking of patterns over time were not achieved (<xref ref-type="bibr" rid="bib1">Barbosa et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Hanlon et al., 2009</xref>; <xref ref-type="bibr" rid="bib41">Shohet et al., 2007</xref>). Physiologists, however, did study individual or a few chromatophores in action, but this approach rested heavily on non-automated measurements, with limited precision and scalability (<xref ref-type="bibr" rid="bib33">Packard, 1982</xref>), and did not address the issue of camouflage directly.</p><p>This laboratory recently developed automated and quantitative approaches to circumvent these limitations to study cephalopod camouflage and related behaviors (<xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Woo et al., 2023</xref>). The tools developed there have been rewritten and expanded in an easy-to-use software suite, which we call CHROMAS, the object of the present paper. CHROMAS is a high-resolution analysis software that aims to provide an objective quantification of chromatophore-population behavior over periods of time as long as months, using video recordings of live cephalopod skin as an input. While CHROMAS retains the foundational concepts introduced in <xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref> and <xref ref-type="bibr" rid="bib47">Woo et al., 2023</xref>, the tools have been rewritten to improve performance and usability. New functionalities have been added to extend the temporal range and spatial resolution of the analysis. In particular, we introduce the semi-automated tracking of chromatophore identity over development and the segmentation of individual chromatophores by analysis of deformation anisotropy. Although developed to track and analyze cephalopod skin patterning, these tools can be used with the skin of other species, and more generally, with any comparable time series of images.</p><p>CHROMAS reads video files and extracts chromatophore properties based on known biological features, derived from prior experimental work.</p><sec id="s1-1"><title>Identity and position</title><p>The relative spatial arrangement of chromatophores is anatomically fixed in the skin (<xref ref-type="bibr" rid="bib33">Packard, 1982</xref>) because they are anchored to a stable and appropriately stiff extracellular matrix. Their neighboring chromatophores, however, change as the animal grows, because new chromatophores are continuously added between older ones and change color as they age (<xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref>). CHROMAS keeps track of chromatophore identity over time despite these changes.</p></sec><sec id="s1-2"><title>Expansion state</title><p>When radial muscles (attached distally to the extracellular matrix and proximally to a chromatophore) contract, the chromatophore’s cytoplasmic membrane and internal pigment sack expand; inversely, when those muscles relax, the pigment sack retracts passively due to its elasticity. Because the chromatophore muscles are controlled by motor neurons (<xref ref-type="bibr" rid="bib11">Florey and Kriebel, 1969</xref>), chromatophore size variations offer an indirect but objective and quantifiable readout of neural activity (<xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref>) at video sampling rate.</p></sec><sec id="s1-3"><title>Anisotropy of expansion and detailed innervation</title><p>The radial muscles controlling a chromatophore can collectively be innervated by more than one motor neuron (<xref ref-type="bibr" rid="bib10">Florey, 1969</xref>). Chromatophore expansion is then determined by a set of potentially independent forces, which can result in irregular or anisotropic chromatophore deformations. Analysis of this anisotropy can thus inform us of the fine innervation of individual chromatophores. CHROMAS provides this information.</p><p>We hope that these tools will become valuable resources to address complex biological questions regarding chromatophore neural control, development, biomechanics, and more generally, cephalopod perception and fine motor behavior. More broadly, these tools should be useful also for detailed moving-image analysis in other systems.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><p>CHROMAS provides a versatile framework to investigate a wide range of biological questions. Each tool can be used independently of the others to solve specific tasks, while their combined use enables the construction of elaborate workflows. The subsequent sections provide examples of specific tasks that, when combined sequentially, constitute a powerful workflow (see <xref ref-type="fig" rid="fig1">Figure 1</xref>), as demonstrated in <xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref> and <xref ref-type="bibr" rid="bib47">Woo et al., 2023</xref> with <italic>Sepia officinalis</italic>. <xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig8">8</xref> illustrate the main functions with <italic>Euprymna berryi</italic>, because of its small size and limited number of chromatophores. All functions have been developed to work with larger species with greater chromatophore numbers and density, such as <italic>S. officinalis</italic>, as shown later in <xref ref-type="fig" rid="fig9">Figure 9</xref>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Workflow diagram of CHROMAS.</title><p>Dotted lines indicate optional steps in the process, allowing flexibility depending on experimental requirements. Orange dots represent new functions that did not exist in the initial versions of the pipeline (<xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Woo et al., 2023</xref>). Each step of the pipeline can be run independently, providing users with the flexibility to execute specific stages as needed. Additionally, CHROMAS includes options to generate visual output videos, enabling users to assess and validate the accuracy and quality of the result at every step.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig1-v1.tif"/></fig><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Chunking process.</title><p>The video frames are sorted based on their focus score. Images that are too dark or too blurry are removed from the dataset, and chunks made of consecutive sharp frames are created. In the figure above, the score decreases due to a brightness drop at frame 1200 and to the subject going out of focus around frame 3500, resulting in three distinct chunks. Continuity between chunks is reestablished during the ‘Stitching’ process (see <xref ref-type="fig" rid="fig5">Figure 5</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig2-v1.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Segmentation results.</title><p>In each frame, each pixel is classified as either part of a chromatophore or not. (<bold>a1, a2</bold>) Original frame before segmentation. (<bold>b1, b2</bold>) Original frames with edges of segmented chromatophores in green. (<bold>c1, c2</bold>) Binary segmentation with chromatophores in black and background in white. Note that even the smallest chromatophores, though faint and difficult to discern on the original footage, are segmented.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig3-v1.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Registration.</title><p>Overlay of two frames 20 frames apart within the same chunk. Panels (<bold>a1, a2</bold>) illustrate the misalignment of two chromatophores (red and blue) before registration. Panels (<bold>b1, b2</bold>) show the same frames after automated registration. The registration process compensates for small movements or drifts over time, ensuring correct identification and tracking of the chromatophores over time.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig4-v1.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Stitching video chunks.</title><p>This figure illustrates the stepwise refinement process for stitching adjacent video chunks, ensuring precise alignment critical for downstream analyses. (<bold>a</bold>) Masterframe of chunk 1, with an initial ellipse fit shown in red. (<bold>b</bold>) Masterframe of chunk 2, featuring the ellipse fit and a grid of points for fine alignment in blue. (<bold>c</bold>) Chunk 2 aligned on chunk 1 using the ellipse-fitting method. (<bold>d</bold>) First round of fine alignment, maximizing phase correlation between patches surrounding the points. Points prior to fine alignment are shown in blue, and their adjusted positions post-alignment are in pink. (<bold>e</bold>) Second round of fine alignment, from pink to red. (<bold>f</bold>) Final overlay between chunk 1 and the fully aligned chunk 2. In black are the chromatophore areas common to both chunks; in red and blue are chromatophore areas that are unique to chunks 1 and 2, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig5-v1.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Calculation of chromatophore areas.</title><p>(<bold>a</bold>) Masterframe and zoom on the segmentation of one chromatophore’s contractions and expansions. (<bold>b</bold>) Cleanqueen. Each color represents a different chromatophore territory. (<bold>c</bold>) Surface area of each segmented chromatophore over time (colors as in b). The chromatophore in a is highlighted in the thick blue line. This approach measures the total area occupied by a chromatophore within its defined territory and offers no information about its shape.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig6-v1.tif"/></fig><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Calculation of slice surface areas.</title><p>(<bold>a</bold>) A single chromatophore (the same one as in <xref ref-type="fig" rid="fig6">Figure 6</xref>) is divided radially into 36 slices. (<bold>b</bold>) Close-up on one of the 36 slices, indicating how its surface area is estimated. (<bold>c</bold>) Chromatophore ‘epicenters’ (see text) in blue and motion markers (see text) in green, with edges for triangulation. (<bold>d</bold>) Plot representing the surface areas of the 36 slices of this chromatophore over time. Note that the dynamics of a chromatophore are now described by 36 values per frame, rather than one, as in <xref ref-type="fig" rid="fig6">Figure 6c</xref>. Note also that the top half of the traces describing this chromatophore deviates clearly from the others around frames 550 and 960, indicating their differential control. This fine-grain description of chromatophore deformations over time is used to reveal the fine details of their individual and collective motor control.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig7-v1.tif"/></fig><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Tracking of chromatophores over 12 days of development (t<sub>0</sub> = 7 days post hatching (dph) and t<sub>1</sub> = 21 dph).</title><p>During this time, the number of chromatophores nearly doubled, with many changing chromatic properties. Despite these changes and intermittent filming, our pipeline reliably tracks individual chromatophores, as illustrated with two arbitrarily selected chromatophores, in blue and green.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig8-v1.tif"/></fig><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Example of 4K footage of <italic>Sepia officinalis</italic> processed through the CHROMAS pipeline.</title><p>(<bold>a1, a2</bold>) Original video frame. (<bold>b1, b2</bold>) Binary segmentation with chromatophores in black and background in white. (<bold>c1, c2</bold>) Epicenters of chromatophores in blue and motion markers with triangulation in green. (<bold>d1, d2</bold>) Putative motor units calculated using the Affinity Propagation algorithm. In this example, Principal Component Analysis–Independent Component Analysis (PCA–ICA) revealed independent components influencing the activity of each chromatophore. We found 4.68 ± 1.41 (mean ± std) ICs per chromatophore (<italic>n</italic> = 1616). Those independent components were then clustered together based on their covariation and plotted with the same color. For visualization purposes, only clusters of size 8 and 9 are plotted. In this segment, we found 10.37 ± 14.23 (mean ± std) chromatophores per cluster, with 90% of clusters consisting of less than 20 chromatophores. This example highlights CHROMAS’s ability to process complex datasets, such as footage of <italic>Sepia officinalis</italic>, which exhibits a high density of chromatophores and very active dynamics correlated with its camouflage and other behaviors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106509-fig9-v1.tif"/></fig><p>We provide a comprehensive tutorial guide along with sample video files to test the pipeline. Links to these resources are available in the ‘Data availability’ section. The entire pipeline can be run with the command below, and individual task command lines are included as each step of the pipeline is detailed.</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas run /path/to/example.mp4</styled-content></p><sec id="s2-1"><title>Chunking videos into usable segments</title><p>Analyzing long video recordings of behaving-cephalopod skin can be frustrating because of occasional movement blur, de-focusing, and obstruction. To address this, we divide the video into continuous segments which we call ‘chunks’, consisting of consecutive frames where the animal’s mantle, or a part of it, is both visible and in focus. An animal, or a field, is considered to be in focus if the edges of its chromatophores are clearly defined. The identification of these chunks can be accomplished using focus statistics, which are numerical measures indicative of the sharpness of an image (<xref ref-type="fig" rid="fig2">Figure 2</xref>, focus score). A method often used is the difference of Gaussians, which involves subtracting a blurred version of an image (produced by convolving the image with a Gaussian kernel with a large standard deviation) from a less blurred version of the original image (using a smaller standard deviation). The resulting image emphasizes areas of steep intensity change, such as edges, while suppressing low-frequency variations, making it well suited for detecting the sharp boundaries of chromatophores. The standard deviations of the two Gaussian kernels were chosen such that their difference matched the typical size of chromatophores, to increase the visibility of their edges. The input video is then cut into shorter clips—the chunks (<xref ref-type="fig" rid="fig2">Figure 2</xref>, chunking output). This step can be adapted to exclude other types of unwanted frames based on other parameters, such as low brightness, the absence of a fluorescent tag in the frame, or motion blur.</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas chunk /path/to/example.mp4</styled-content></p></sec><sec id="s2-2"><title>Chromatophore segmentation and color classification</title><p>It is essential to identify and isolate individual chromatophores accurately in video frames before a detailed analysis can begin. ‘Segmentation’ refers to the process of generating a binarized image, where each pixel is classified as either part of a chromatophore or part of the background (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Chromatophore classification by color is also supported. Segmentation relies on deep learning models trained on manually annotated data. We provide two trained models: one for binary (yes, no) and one for chromatophore-color segmentation (dark, orange, yellow), trained on annotated data of <italic>S. officinalis</italic> and <italic>E. berryi</italic> under different lighting conditions and camera setups. Furthermore, these models can be fine-tuned on custom data, and various models can be trained from scratch on custom data to enable research on different species. Available models include Fully Convolutional Networks (<xref ref-type="bibr" rid="bib26">Long et al., 2015</xref>), DeepLabV3 (<xref ref-type="bibr" rid="bib7">Chen et al., 2017</xref>), or U-Net (<xref ref-type="bibr" rid="bib39">Ronneberger et al., 2015</xref>), with ResNet50, ResNet101 (<xref ref-type="bibr" rid="bib19">He et al., 2016</xref>), or MobileNetv3-Large (<xref ref-type="bibr" rid="bib20">Howard et al., 2019</xref>) serving as backbones. Alternatively, segmentation can also be achieved by a random forest classifier or a combination of both, followed by ‘majority vote’.</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas segment /path/to/example.dataset</styled-content></p></sec><sec id="s2-3"><title>Compensating for movement: Registration</title><p>To measure a chromatophore’s activity (deformations), its identity must be correctly transferred across frames; this is a difficult task if its coordinates change from frame to frame. Chunk selection ensures that video segments with blur due to rapid motion have been excluded from the data. However, chromatophores often change position from one frame to another due to slower or more subtle movements such as breathing, slow drifting, or skin deformation, while remaining in focus. Therefore, what we call the ‘Registration’ operation compensates for the animal’s movements to maintain a consistent location of individual chromatophores over time. For this, the Lucas–Kanade optical flow algorithm (<xref ref-type="bibr" rid="bib27">Lucas and Kanade, 1981</xref>) tracks points that are initially randomly sampled on the first frame, from frame to frame. Full displacement maps are then interpolated from the displacement of these tracking points using a moving-least-squares algorithm. This results in all the frames being registered with the first frame of the video as reference (<xref ref-type="fig" rid="fig4">Figure 4</xref>). To ensure accurate registration, tracking points that move implausibly far between frames are automatically discarded, and the registration process is halted if too many points are lost, preventing poor-quality mappings from being used in the analysis pipeline.</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas register/path/to/example.dataset</styled-content></p></sec><sec id="s2-4"><title>Stitching across video chunks</title><p>To create a cohesive dataset, the correct identity of each chromatophore must be tracked across chunks. The absence of visual information between chunks (by definition of the chunks), however, renders standard optical flow techniques unsuitable for this task. A new technique was developed for this end involving the following steps. All registered frames within a chunk are averaged to produce a descriptive image of the position and color of each chromatophore. This average is referred to as the ‘masterframe’ of that chunk (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, without the ellipse). If the animal’s body is fully visible, an initial alignment of the masterframes of successive chunks is achieved through automatic ellipse fitting around the animal’s silhouette (<xref ref-type="fig" rid="fig5">Figure 5a–c</xref>). Otherwise, alignment is performed after the user manually selects matching points between masterframes. Subsequently, displacement vectors are calculated for a regular grid of points sampled over the animal’s body by maximizing phase correlation between image patches surrounding these points (<xref ref-type="fig" rid="fig5">Figure 5b–d</xref>). Full displacement maps are then interpolated using a moving-least-squares algorithm. The accuracy of these mappings is quantified using the reprojection error metric, as detailed in the methods section, and chunks with high reprojection error are excluded from further analysis.</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas stitch /path/to/example.dataset</styled-content></p></sec><sec id="s2-5"><title>Tracking chromatophore size</title><p>Now that the chromatophores have been aligned across frames and chunks, their identity can be tracked and their expansion quantified (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). First, a 2D array, termed ‘Cleanqueen’, is generated, that delineates individual chromatophore territories, that is, the space that each chromatophore can occupy (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). Using inverse registration maps, the cleanqueen is mapped onto the unregistered segmentation frames and the identity of each chromatophore can be tracked. The area of each chromatophore is then calculated as the number of pixels forming the largest connected component inside the chromatophore’s corresponding territory (<xref ref-type="fig" rid="fig6">Figure 6c</xref>).</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas area /path/to/example.dataset</styled-content></p></sec><sec id="s2-6"><title>Tracking anisotropic chromatophore activity</title><p>For most uses, analysis of the global chromatophore expansion is sufficient. There exist conditions, however, where a high-resolution analysis of chromatophore deformations is useful. We will show that it can reveal the multiple innervation of individual chromatophores and the fine and distributed nature of motor units.</p><p>Quantifying the kinematics of an ever-changing shape on a surface which is also subject to deformations is not a trivial task. Whereas chromatophores can be viewed as monochromatic objects that change shape over time, they reside on a surface (the skin) that itself undergoes both local and global deformations, such as mantle muscle contractions or movements caused by breathing, locomotion, or external forces such as water currents. Therefore, an accurate description of the kinematics of a chromatophore can be done only once it has been disentangled from the distortions of the skin. We thus addressed the need to accurately stabilize (i.e., compensate for the movements and deformations of) the background. Registration maps were not suitable for this task, because an optic flow approach would try to correct for chromatophore expansion. We thus developed a different type of registration. This step takes advantage of the fact that the 2D configuration of chromatophores in their most contracted state tends to be fixed in space. The center of mass of chromatophores that remain small and constant in size throughout the video (typically, chromatophores at early or intermediate stages of development, <xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref>) can thus be viewed as inert points on the animal’s skin, and their displacement over frames can then be used to derive skin deformation. We call these points ‘motion markers’. Once this operation has been accomplished, the details of a chromatophore’s deformations can be obtained.</p><p>To quantify these deformations, each chromatophore is divided into radial slices, and the area within each slice is calculated independently (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). To slice a chromatophore radially, its ‘epicenter’ (the starting point of its expansion, blue dots in <xref ref-type="fig" rid="fig7">Figure 7a–c</xref>), must be identified and tracked over frames. This point cannot be the center of mass of the chromatophore, because anisotropic deformations would wrongly shift this point between frames. Instead, a chromatophore’s epicenter is computed following the logic of the previous paragraph: it is calculated as the center of mass at the frame where the chromatophore is in its most contracted state. Because the spatial relationships between epicenters are constant in time, their coordinates can be stored relative to the coordinates of their respective motion markers (<xref ref-type="fig" rid="fig7">Figure 7c</xref>, green dots), that is the three nearest motion markers spanning a triangle around the chromatophore in a Delaunay triangulation (<xref ref-type="fig" rid="fig7">Figure 7c</xref>, green triangles represent the Delaunay triangulation). Relative coordinates are stored as barycentric coordinates <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$(\lambda _{1},\lambda _{2},\lambda _{3})\in R^{3}$\end{document}</tex-math></alternatives></inline-formula>, such that the absolute coordinates <inline-formula><alternatives><mml:math id="inf2"><mml:mi>c</mml:mi></mml:math><tex-math id="inft2">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> of the epicenters can be calculated as <inline-formula><alternatives><mml:math id="inf3"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft3">\begin{document}$c=\lambda _{1}\cdot a_{1}+\lambda _{2}\cdot a_{2}+\lambda _{3}\cdot a_{3}$\end{document}</tex-math></alternatives></inline-formula> from the absolute coordinates <inline-formula><alternatives><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft4">\begin{document}$a_{1},a_{2},a_{3}$\end{document}</tex-math></alternatives></inline-formula> of their motion markers. The combination of these local coordinate systems, along with a sufficiently dense distribution of motion markers, enables us to eliminate the global and local deformations of the skin and to successfully track the positions of all epicenters. In addition, the motion markers are used to keep the orientation of the slices constant over frames, which is necessary to track the identity of each slice.</p><p>By default, chromatophores are sliced into 36 sectors. This number was selected based on the Nyquist–Shannon sampling theorem (<xref ref-type="bibr" rid="bib40">Shannon, 1948</xref>), and on histological analysis showing that chromatophores in <italic>E. berryi</italic> and <italic>S. officinalis</italic> are each controlled by approximately 10–15 radial muscles. This number of slices struck an acceptable balance, ensuring a high enough resolution to capture realistic shape changes, while keeping computational demands manageable. To reduce measurement errors induced by pixel discretization, slice areas were computed by averaging the distance from the border pixels of the slice to the epicenter <italic>r</italic> and then computing the respective area as <inline-formula><alternatives><mml:math id="inf5"><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math><tex-math id="inft5">\begin{document}$area=\pi \cdot r^{2}/n$\end{document}</tex-math></alternatives></inline-formula>, see <xref ref-type="fig" rid="fig7">Figure 7b</xref>.</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas slice /path/to/example.dataset</styled-content></p></sec><sec id="s2-7"><title>Long-term chromatophore tracking</title><p>Tracking individual chromatophores over extended periods (days or weeks) is necessary to examine, among others, functional and developmental issues of control. While standard stitching techniques (see ‘Stitching’) suffice for datasets captured in close temporal proximity (hours), they fail for longer time periods, due to significant changes in animal size, chromatophores color, chromatophore numbers, and occasionally the disappearance of existing ones.</p><p>To address this, CHROMAS includes an option for a manual pre-alignment step prior to automated stitching. This process is facilitated by an intuitive graphical interface (based on <italic>matplotlib</italic> and <italic>tkinter</italic>) allowing users to select corresponding points between two frames from videos captured at different stages. The interface has been developed with usability in mind, providing options such as zoom, rotation, and an ‘undo’ function. These selected points are then utilized to generate warping maps using either the moving-least-squares algorithm or thin plate splines (<italic>NumPy</italic> and <italic>SciPy</italic>), providing a precise initial alignment that significantly improves the continuity and accuracy of subsequent stitching and analysis (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas superstitch /path/to/example.dataset /path/to/later_example.dataset</styled-content></p></sec><sec id="s2-8"><title>Built-in analysis tools and additional features</title><p>CHROMAS also offers a series of built-in analysis tools. Clustering algorithms, for example, are particularly useful to study how motor neurons control small groups of chromatophores, or sets of sectors of chromatophores, potentially revealing patterns of co-innervation and synchronized activity across specific regions of the skin. Our software enables users to perform Principal Component Analysis (PCA), Independent Component Analysis (ICA), and clustering analysis, including, but not limited to, Affinity Propagation (AP) and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). PCA and ICA are instrumental in disentangling the influence of motor neurons on individual chromatophores, while clustering algorithms uncover the broader distribution of motor neuron influence across larger scales (<xref ref-type="fig" rid="fig9">Figure 9</xref>). Such tools can be found under ‘Methods—Clustering motion correlation’.</p><p>Furthermore, CHROMAS integrates advanced tools for analyzing full-body patterns, enabling studies on camouflage effectiveness and pattern dynamics. The software employs the Structural Similarity Index to measure perceptual similarity between biological and background patterns, implemented using the <italic>scikit-image</italic> Python package. Patterns are embedded into a lower-dimensional ‘pattern space’ using dimensionality reduction techniques like PCA and t-Distributed Stochastic Neighbor Embedding, facilitated by the <italic>scikit-learn</italic> and <italic>openTSNE</italic> libraries. Temporal dynamics of patterns are analyzed by plotting trajectories in this space using the <italic>pandas</italic> and <italic>matplotlib</italic> packages, revealing the speed, directionality, and stability of pattern transitions, as described in <xref ref-type="bibr" rid="bib47">Woo et al., 2023</xref>.</p><p><styled-content style="color: #366BFB;">Command line:</styled-content></p><p><styled-content style="color: #366BFB;">&gt; chromas </styled-content><styled-content style="color: #366BFB;">analyse</styled-content><styled-content style="color: #366BFB;"> XXX /path/to/example.dataset</styled-content></p></sec><sec id="s2-9"><title>Performance</title><p>Performance was benchmarked using a 5-min-long 4K video (shot at 20 frames per second, totaling 6000 frames and 494.5 MB) featuring a close-up view of live skin of <italic>S. officinalis</italic>. The benchmark also included two modified versions of the same video, cropped to Full-HD (2000 × 2000) and SD (1000 × 1000) resolutions. Cropped videos preserve the original pixel density per chromatophore but include fewer chromatophores for analysis.</p><p><xref ref-type="table" rid="table1 table2">Tables 1 and 2</xref> document the processing time per frame (in milliseconds) for individual tasks, based on different image sizes and hardware configurations. The hardware categories include a workstation (with a 10-core Intel Core i9-10900X with 64 GB RAM and a NVIDIA RTX A4000 GPU), as well as a laptop (with a 4-core Intel Core i5-1135G7 with 16 GB RAM and no NVIDIA GPU).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Performance on a workstation with a 10-core Intel Core i9-10900X with 64 GB RAM and a NVIDIA RTX A4000 GPU.</title><p>Runtime in milliseconds per frame (mean ± s.d.).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Resolution</th><th align="left" valign="top">Number of chromatophores (ms/frame)</th><th align="left" valign="top">Segmentation(neural net) (ms/frame)</th><th align="left" valign="top">Registration (ms/frame)</th><th align="left" valign="top">Tracking chromatophore size (ms/frame)</th><th align="left" valign="top">Tracking anisotropic activity (ms/frame)</th></tr></thead><tbody><tr><td align="left" valign="top">SD</td><td align="left" valign="top">146</td><td align="left" valign="top">5.38 ± 0.1</td><td align="left" valign="top">18.61 ± 0.1</td><td align="left" valign="top">13.01 ± 1.4</td><td align="left" valign="top">4.98 ± 0.9</td></tr><tr><td align="left" valign="top">HD</td><td align="left" valign="top">566</td><td align="left" valign="top">22.31 ± 0.7</td><td align="left" valign="top">62.87 ± 2.3</td><td align="left" valign="top">140.44 ± 1.0</td><td align="left" valign="top">135.24 ± 0.9</td></tr><tr><td align="left" valign="top">k</td><td align="left" valign="top">2842</td><td align="left" valign="top">986.21 ± 10.4</td><td align="left" valign="top">611.74 ± 5.4</td><td align="left" valign="top">744.69 ± 2.5</td><td align="left" valign="top">495.17 ± 2.8</td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Performance on a laptop with a 4-core Intel Core i5-1135G7 with 16 GB RAM and no NVIDIA GPU.</title><p>Runtime in milliseconds per frame (mean ± s.d.).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Resolution</th><th align="left" valign="top">Number of chromatophores (ms/frame)</th><th align="left" valign="top">Segmentation(lookup) (ms/frame)</th><th align="left" valign="top">Registration (ms/frame)</th><th align="left" valign="top">Tracking chromatophore size (ms/frame)</th><th align="left" valign="top">Tracking anisotropic activity (ms/frame)</th></tr></thead><tbody><tr><td align="left" valign="top">SD</td><td align="left" valign="top">146</td><td align="left" valign="top">11.55 ± 1.81</td><td align="left" valign="top">4.67 ± 0.63</td><td align="left" valign="top">14.56 ± 5.1</td><td align="left" valign="top">9.31 ± 0.6</td></tr><tr><td align="left" valign="top">4k</td><td align="left" valign="top">2842</td><td align="left" valign="top">1679.62 ± 251.8</td><td align="left" valign="top">1490.94 ± 249.9</td><td align="left" valign="top">3072.94 ± 121.6</td><td align="left" valign="top">1488.25 ± 84.0</td></tr></tbody></table></table-wrap><p>While the runtime performance of all tasks (except for stitching) is influenced by hardware capabilities, video length, and resolution (in this particular order), the performance of areas and slice-areas calculation and analysis is also influenced by the number of visible chromatophores in the footage. Furthermore, the performance of stitching depends only on the hardware, the resolution, and the number of chunks.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We developed CHROMAS to be an easy-to-use, flexible, and expandable tool for the objective and statistical quantification of the dynamics of chromatophore activity. Most of the steps in CHROMAS are built upon methodologies established in previous work (e.g., <xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref> and <xref ref-type="bibr" rid="bib47">Woo et al., 2023</xref>), in particular chunking, segmentation, registration, stitching, and area calculation (<xref ref-type="fig" rid="fig1">Figure 1</xref>). These processes were entirely restructured and rewritten in this study to prioritize efficiency, modularity, and ease of use. Beyond these changes and improvements, CHROMAS introduces novel features that further refine the scope of achievable chromatophore analysis using computer vision methods.</p><p>First, our results achieve high levels of spatial resolution. Indeed, our anisotropy analysis captures the dynamics of the smallest cellular components of the chromatophore organs—their radial muscles. The numbers of motor neurons innervating single chromatophores and the numbers of chromatophores innervated by a single motor neuron, both estimated via our image analysis methods, match those measured earlier by direct and minimal electrical stimulation of terminal chromatophore nerves (<xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref>), providing confidence that our new results are accurate. This image-based approach should thus enable one to describe the precise innervation patterns of very large fields of chromatophores, and their evolution in time, at chromatophore-motor-neuron resolution. This level of description for a complex motor output and behavior, especially with tools that require no external indicator is, to our knowledge, unique.</p><p>Second, our semi-automatic methods for tracking chromatophore identity overcome key challenges in long-term monitoring, the biggest one of which is the continuous addition of new chromatophores as the animal develops. This development enables the tracking of chromatophores over very long times (weeks to months), thus spanning the entire lifespan of an animal.</p><sec id="s3-1"><title>Scalability</title><p>Tasks within CHROMAS are designed for efficient parallelization. This enables scaling to the full capacity of single machines as well as execution on high-performance supercomputers. In addition, ‘larger-than-memory’ execution is supported, allowing for the processing of datasets whose size exceeds the available RAM (e.g., with 8K-resolution images). CHROMAS uses an approach to parallelization that does not depend on the hardware, ensuring code maintainability and versatility. This design allows the same script to run efficiently on various hardware configurations, from single-core machines to multi-core systems and clusters. Tasks are parallelized along multiple axes (chunk-based, temporal, and spatial). While temporal parallelization is mainly used, some tasks (segmentation, registration, areas, slice areas) are parallelized along the spatial dimensions as well. For cluster-level parallelization, initial setup on the cluster is required. CHROMAS supports most high-performance computing job schedulers found in scientific research environments, such as SLURM, SGE, TORQUE, LSF, DRMAA, and PBS. Hence, the spatial scale and temporal resolution of our analyses are bound less by the capabilities of CHROMAS than by the quality of the image data and the available computational power.</p></sec><sec id="s3-2"><title>Usability</title><p>CHROMAS can operate in four distinct modes, adapted to user needs. The software can be controlled by a graphical user interface (GUI), enabling quick and easy operation without programming knowledge and widening the pool of potential users. It can also be controlled by a command line interface, offering additional flexibility as well as automation and scripting capabilities. We provide an extensive low-level application programming interface, enabling custom workflows, integration with external tools, and the development of extensions. Last, we provide Jupyter notebooks facilitating interactive data analysis for common use cases.</p><p>The software can be installed either via the PyPI package manager, through the executable installer (.exe), or manually, using the Git repository. Extensive documentation is supplied and hosted.</p><p>Both high-level parameters, like algorithmic choices or storage configurations, and lower-level parameters, like concrete parameters of algorithms, are configured for ease of use within a single YAML file, which can also be modified through the GUI.</p></sec><sec id="s3-3"><title>Known limitations</title><p>CHROMAS analyzes the activity of chromatophores by dividing the mantle into small mosaics (in the ‘cleanqueen’), where each chromatophore is assigned its own territory. Because these territories are discrete—they belong to only one chromatophore at a time—there can be species, ages, and conditions in which the chromatophores expand so much that they encroach on each other’s space and overlap partly. Currently, our software is not able to resolve these overlaps.</p><p>Another known limitation concerns the biological assumptions underlying the current version of CHROMAS. The pipeline is designed for surfaces that remain reasonably planar and undergo deformations primarily in two dimensions. In cephalopods such as octopuses, in which the skin can undergo substantial three-dimensional morphological changes, analyzing chromatophore dynamics may require complementary three-dimensional tracking of the skin surface to correct for out-of-plane deformations and maintain accurate measurement of chromatophore activity.</p></sec><sec id="s3-4"><title>Recommended video parameters for optimal use of CHROMAS</title><p>The performance of CHROMAS depends on the quality of the input videos. Although the pipeline analyzes each frame independently and has no frame rate requirement, we recommend recording at 20 frames per second at least, to capture chromatophore dynamics accurately. Sharp, in-focus frames are critical, particularly for moving subjects, where higher shutter speeds help minimize motion blur. For reliable segmentation, each chromatophore should cover at least 10 pixels across its fully expanded diameter. Higher spatial resolution, with chromatophores covering around 50 pixels in diameter, is recommended if sub-chromatophore dynamics are of interest. Recording conditions should minimize background noise, and the water column should be as clear as possible, free of particles or debris. The water surface should be kept as calm and planar as possible to avoid optical artifacts. If wide-angle lenses or other optics that may introduce distortion are used, lens correction algorithms should be applied during preprocessing to compensate for the optical distortions. For long-term tracking applications (e.g., developmental studies), frequent imaging sessions are recommended. Newly differentiated chromatophores are initially light colored (e.g., yellow) and thus visually distinct from mature chromatophores (which are dark); over days to weeks, however, the light chromatophores darken and become increasingly difficult to differentiate from older ones. Recording at appropriate and regular intervals thus helps track individual chromatophores across developmental stages and improves the reliability of long-term analyses. Following these recommendations will help segmentation, tracking, and analysis with CHROMAS.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Implementation details</title><p>CHROMAS is written for GNU/Linux, Windows, and MacOS operating systems in the Python programming language, requiring Python 3.9 or higher. The software relies on several key libraries for various functionalities. <italic>NumPy</italic> (<xref ref-type="bibr" rid="bib18">Harris et al., 2020</xref>), <italic>SciPy</italic> (<xref ref-type="bibr" rid="bib45">Virtanen et al., 2020</xref>), <italic>pandas</italic> (<xref ref-type="bibr" rid="bib30">McKinney, 2010</xref>), and <italic>xarray</italic> (<xref ref-type="bibr" rid="bib21">Hoyer and Hamman, 2017</xref>) are used for most of the numerical computations, utilizing labeled multi-dimensional arrays. <italic>Dask</italic> (<xref ref-type="bibr" rid="bib37">Rocklin, 2015</xref>; <xref ref-type="bibr" rid="bib9">Dask Development Team, 2016</xref>) is used for parallel computing and distributed task scheduling. Image processing and computer vision tasks are performed using <italic>OpenCV</italic> (<xref ref-type="bibr" rid="bib4">Bradski, 2000</xref>), <italic>scikit-image</italic> (<xref ref-type="bibr" rid="bib44">van der Walt et al., 2014</xref>), and <italic>decord</italic> (<xref ref-type="bibr" rid="bib20">Howard et al., 2019</xref>) for efficient video decoding. <italic>scikit-learn</italic> (<xref ref-type="bibr" rid="bib35">Pedregosa et al., 2011</xref>) is used for data preprocessing, classification models and clustering, <italic>PyTorch</italic> (<xref ref-type="bibr" rid="bib34">Paszke et al., 2019</xref>) and <italic>torchvision</italic> (<xref ref-type="bibr" rid="bib29">Marcel and Rodriguez, 2010</xref>) for deep-learning-based segmentation models, and <italic>albumentations</italic> (<xref ref-type="bibr" rid="bib5">Buslaev et al., 2020</xref>) for image augmentation used in the training of these models. Visualization is accomplished using <italic>Matplotlib</italic> (<xref ref-type="bibr" rid="bib22">Hunter, 2007</xref>) for creating static, animated, and interactive visualizations, and <italic>Bokeh</italic> (<xref ref-type="bibr" rid="bib3">Bokeh Development Team, 2014</xref>) for interactive dashboards visualizing computations. Data storage is performed using <italic>Zarr</italic> (<xref ref-type="bibr" rid="bib32">Miles et al., 2020</xref>) for chunked, compressed, <italic>N</italic>-dimensional arrays. This allows for efficient storage and access of large, chunked, and compressed <italic>N</italic>-dimensional arrays. The command-line interface is created using <italic>Click</italic> (<xref ref-type="bibr" rid="bib38">Ronacher, 2014</xref>), while <italic>tqdm</italic> (<xref ref-type="bibr" rid="bib43">tqdm Developers, 2016</xref>) provides progress bar functionality and the TUI is built using <italic>Trogon</italic> (<xref ref-type="bibr" rid="bib12">Freeman, 2022</xref>). Documentation is generated using <italic>Sphinx</italic> (<xref ref-type="bibr" rid="bib42">Sphinx Team, 2007</xref>). <italic>nbsphinx</italic> (<xref ref-type="bibr" rid="bib15">Grünwald, 2017</xref>) is used to include Jupyter notebooks in the documentation, with <italic>jupyter-client</italic> and <italic>ipykernel</italic> providing Jupyter notebook support (<xref ref-type="bibr" rid="bib25">Jupyter Development Team, 2015</xref>). <italic>Pandoc</italic> (<xref ref-type="bibr" rid="bib28">MacFarlane, 2006</xref>) is used for document conversion and <italic>rpy2</italic> (<xref ref-type="bibr" rid="bib14">Gautier, 2008</xref>) is used for exporting data in R formats.</p><p>Selected sample data accompany the tutorials.</p></sec><sec id="s4-2"><title>Training machine learning models to segment chromatophores</title><sec id="s4-2-1"><title>Data augmentation</title><p>To reduce the amount of manually annotated images needed for training segmentation models, data augmentation techniques are employed to expand the size of the training dataset. This approach was also used in the training of the enclosed models. The <italic>albumentations</italic> library (<xref ref-type="bibr" rid="bib5">Buslaev et al., 2020</xref>) is used for this purpose, offering a variety of transformations including resizing, shifting, scaling, rotation, horizontal and vertical flips, RGB-value shifting, brightness and contrast adjustment, downscaling, random shadows, FancyPCA, and perspective transforms.</p></sec><sec id="s4-2-2"><title>Models</title><p>CHROMAS offers three distinct types of segmentation classifiers, each tailored to accommodate varying levels of visual variability in the dataset, processing speed, and available amounts of training data.</p><p>The first and simplest approach is a lookup table for color values in a specified color space. The training of this classifier involves selecting a color space (e.g. RGB, HSV, or Lab) and mapping a small set of representative color values to specific classes (e.g., background, chromatophore). Once the data points are mapped, all possible color values are classified in advance based on these inputs, generating a lookup table that assigns pixel color values to the respective classes. This method is particularly well-suited for videos where the chromatophores have distinctly different color values from the surrounding skin.</p><p>Some of our datasets exhibited significant visual variability, stemming from changes in illumination (LED intensity, blue light, or lighting angle), optical adjustments such as aperture, and the skin’s varying light reflectance and transmittance. For example, in <italic>E. berryi</italic>, internal structures visible through the transparent skin contribute to making certain skin regions darker. To deal with these issues, we developed two other methods.</p><p>The second option uses a random forest classifier, a computationally efficient approach when lacking a Graphical Processing Unit (GPU) that requires relatively little training data and sometimes outperforms other methods when segmenting very small chromatophores (less than 10 pixels in size). However, its performance remains strongly tied to the lighting conditions of the training data.</p><p>The third approach uses deep learning models, specifically Fully Convolutional Networks (<xref ref-type="bibr" rid="bib26">Long et al., 2015</xref>), DeepLabV3 (<xref ref-type="bibr" rid="bib7">Chen et al., 2017</xref>), and U-Net (<xref ref-type="bibr" rid="bib39">Ronneberger et al., 2015</xref>), with either ResNet50, ResNet101 (<xref ref-type="bibr" rid="bib19">He et al., 2016</xref>), or MobileNetV3-Large (<xref ref-type="bibr" rid="bib20">Howard et al., 2019</xref>) as backbones. This neural network-based method requires more training data, especially at the beginning. Our pre-trained models (trained on manually annotated images of <italic>E. berryi</italic> and <italic>S. officinalis</italic> using Cross Entropy loss), however, are designed to function right away, or to serve as robust baselines for fine-tuning, thus reducing the need for large datasets. While this approach requires access to an NVIDIA GPU, it generalizes well across conditions, making it the most flexible and powerful option.</p></sec></sec><sec id="s4-3"><title>Registration details</title><p>The Lucas–Kanade optical flow algorithm, used for video registration, tracks a sparse set of points across the video. These points are initially defined in the first frame as centers of mass of small chromatophores. The process starts by identifying chromatophore regions through segmentation of the first frame, with an offset calculated to exclude points near the edges based on a percentage of the image dimensions. Candidates are filtered based on shape properties such as eccentricity, solidity, and area and are subsampled evenly across a grid to ensure that tracking points are well distributed.</p><p>After selecting and tracking these points throughout the video, displacement maps of a finer equidistant grid are interpolated using the moving-least-squares algorithm. Full displacement maps are then calculated by linearly interpolating between these points, providing comprehensive registration of the video frames.</p></sec><sec id="s4-4"><title>Stitching reprojection error</title><p>The stitching algorithm is used to align each masterframe in a dataset to every other masterframe. The accuracy of these non-symmetric mappings is quantified by calculating the reprojection error, as developed in <xref ref-type="bibr" rid="bib36">Reiter et al., 2018</xref>. Specifically, all points within a mask—either the animal’s body if it is fully visible or a selected region of interest—of masterframe A are mapped to the reference frame of masterframe B using the A-to-B map, and then mapped back to the reference frame of A using the B-to-A map. The reprojection error is defined as the Euclidean distance between the original points and their remapped counterparts.</p></sec><sec id="s4-5"><title>Clustering motion correlation data</title><p><bold>PCA</bold> is a statistical technique used to reduce data dimensionality while retaining as much variance as possible (<xref ref-type="bibr" rid="bib24">Jolliffe and Cadima, 2016</xref>). It identifies principal components—orthogonal directions that capture the most variance in the data. These components represent independent patterns of variation within the dataset. In the context of chromatophore anisotropy, motor neurons influence subsets of radial muscles, creating distinct patterns of contraction and expansion. By applying PCA to the activity data of the slices (and indirectly, the radial muscles), we could uncover the primary directions of variance, corresponding to the independent influence of individual motor neurons (to the extent that they were recruited independently of one another at least some of the time). We used the <bold>elbow method</bold> to set the number of components worth retaining: in a cumulative-explained-variance versus number of components, the value of <italic>x</italic> at the inflection point beyond which additional components captured less and less additional variance—was chosen. Essentially, each retained principal component reflects a distinct source of coordinated muscle activity, likely driven by a separate motor neuron.</p><p>After identifying the number of principal components using PCA, we apply <bold>ICA</bold> to further identify the underlying sources of activity within our dataset. ICA attempts to separate the data into statistically independent sources (<xref ref-type="bibr" rid="bib2">Bell and Sejnowski, 1995</xref>; <xref ref-type="bibr" rid="bib23">Hyvärinen and Oja, 2000</xref>). In the context of our study, each motor neuron can be thought of as an independent source of muscle activity, which ICA is designed to isolate, even if the signals overlap spatially or temporally. This two-step approach—using PCA to find the number of components and then ICA to extract the independent sources—allowed us to capture both the number and activity of motor neurons acting on a single chromatophore’s radial muscles, ultimately providing a clearer picture of the likely neuromuscular control behind their dynamic shape modulation.</p><p>To identify motor units (a motor neuron and the chromatophores it innervates), we first perform ICA on each individual chromatophore to extract the independent components (ICs) which correspond to motor neuron signals influencing that chromatophore’s activity. To group the chromatophores innervated by the same motor neuron, we use clustering algorithms such as <bold>Affinity Propagation</bold> (<xref ref-type="bibr" rid="bib13">Frey and Dueck, 2007</xref>) or <bold>HDBSCAN</bold> (<xref ref-type="bibr" rid="bib6">Campello et al., 2013</xref>) to the ICs across all chromatophores, clustering by covariation. AP or HDBSCAN is particularly well suited to this task because they determine automatically the optimal number of clusters based on the structure of the data, are capable of handling high-dimensional data, accommodate noise, and detect subtle relationships. This method allows us to identify putative common motor neurons and map their influence across multiple chromatophores, giving us an understanding of the overall motor units controlling the dynamic patterns of chromatophore activity.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Validation</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Validation, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Software</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All research and animal care procedures were carried out in accordance with the institutional guidelines that are in compliance with national and international laws and policies (DIRECTIVE 2010/63/EU; German animal welfare act; FELASA guidelines). The study was approved by the appropriate animal welfare authority (Dr. Vet. Med. E. Simon. Regierungspräsidium Darmstadt, Germany) under approval number V54-19c20/15-F126/1025.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-106509-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>CHROMAS is distributed via the pypi package index (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/chromas/">https://pypi.org/project/chromas/</ext-link>) and is publicly released on GitLab (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17617/1.pa38-mh49">https://doi.org/10.17617/1.pa38-mh49</ext-link>) under the 3-Clause BSD License. The documentation is hosted on GitLab. The data used to train the segmentation models, the trained models, and example videos for the tutorial can be found at <ext-link ext-link-type="uri" xlink:href="https://public.brain.mpg.de/Laurent/Chromas2025/">https://public.brain.mpg.de/Laurent/Chromas2025/</ext-link>.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank F Bayer for assistance in building the experimental setup; F Kretschmer for optimizing the camera control, recording software, and Git workflows; F Vollrath and S Junek for help with image acquisition; P Musset for help with high-performance computing; L Jürgens, S Schwind, LE. Reyes de Frey, D Burgard, M Landler, M Minde S, Kranz P, Dominiczak NK, Vogt G, Wexel S, Dizdarevic, and E Northrup for animal care; and T Woo, X Liang, D Evans, M Elmaleh, and other members of the Laurent laboratory for constructive exchanges. We thank Alice Perenzin and Antje Berken for grant management and scientific coordination. This research was funded by the Max Planck Society (GL), by the LOEWE Schwerpunkt CMMS (State of Hesse) (GL), and by the European Union (ERC grant CAMOUFLAGE, 10114150) (GL).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>A</given-names></name><name><surname>Mäthger</surname><given-names>LM</given-names></name><name><surname>Buresch</surname><given-names>KC</given-names></name><name><surname>Kelly</surname><given-names>J</given-names></name><name><surname>Chubb</surname><given-names>C</given-names></name><name><surname>Chiao</surname><given-names>CC</given-names></name><name><surname>Hanlon</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cuttlefish camouflage: the effects of substrate contrast and size in evoking uniform, mottle or disruptive body patterns</article-title><source>Vision Research</source><volume>48</volume><fpage>1242</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.02.011</pub-id><pub-id pub-id-type="pmid">18395241</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>An information-maximization approach to blind separation and blind deconvolution</article-title><source>Neural Computation</source><volume>7</volume><fpage>1129</fpage><lpage>1159</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id><pub-id pub-id-type="pmid">7584893</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Bokeh Development Team</collab></person-group><year iso-8601-date="2014">2014</year><data-title>Bokeh: python library for interactive visualization</data-title><version designator="3.6.2">3.6.2</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/bokeh/bokeh">https://github.com/bokeh/bokeh</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>The Opencv Library</source><publisher-name>Dr. Dobb’s Journal of Software Tools</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buslaev</surname><given-names>A</given-names></name><name><surname>Iglovikov</surname><given-names>VI</given-names></name><name><surname>Khvedchenya</surname><given-names>E</given-names></name><name><surname>Parinov</surname><given-names>A</given-names></name><name><surname>Druzhinin</surname><given-names>M</given-names></name><name><surname>Kalinin</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Albumentations: fast and flexible image augmentations</article-title><source>Information</source><volume>11</volume><elocation-id>125</elocation-id><pub-id pub-id-type="doi">10.3390/info11020125</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Campello</surname><given-names>R</given-names></name><name><surname>Moulavi</surname><given-names>D</given-names></name><name><surname>Sander</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Density-based clustering based on hierarchical density estimates</article-title><conf-name>Advances in Knowledge Discovery and Data Mining</conf-name><fpage>160</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-37456-2_14</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Schroff</surname><given-names>F</given-names></name><name><surname>Adam</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rethinking atrous convolution for semantic image segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1706.05587">https://doi.org/10.48550/arXiv.1706.05587</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cloney</surname><given-names>RA</given-names></name><name><surname>Florey</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Ultrastructure of cephalopod chromatophore organs</article-title><source>Zeitschrift Fur Zellforschung Und Mikroskopische Anatomie</source><volume>89</volume><fpage>250</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1007/BF00347297</pub-id><pub-id pub-id-type="pmid">5700268</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Dask Development Team</collab></person-group><year iso-8601-date="2016">2016</year><data-title>Dask: parallel computation with blocked algorithms and task scheduling</data-title><version designator="2024.12.1">2024.12.1</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/dask/dask">https://github.com/dask/dask</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Florey</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Ultrastructure and function of cephalopod chromatophores</article-title><source>American Zoologist</source><volume>9</volume><fpage>429</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1093/icb/9.2.429</pub-id><pub-id pub-id-type="pmid">5362276</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Florey</surname><given-names>E</given-names></name><name><surname>Kriebel</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Electrical and mechanical responses of chromatophore muscle fibers of the squid, Loligo opalescens, to nerve stimulation and drugs</article-title><source>Zeitschrift Fur Vergleichende Physiologie</source><volume>65</volume><fpage>98</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1007/BF00297991</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Trogon: GUI generator for click CLI commands</data-title><version designator="0.3.0">0.3.0</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Textualize/trogon">https://github.com/Textualize/trogon</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frey</surname><given-names>BJ</given-names></name><name><surname>Dueck</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Clustering by passing messages between data points</article-title><source>Science</source><volume>315</volume><fpage>972</fpage><lpage>976</lpage><pub-id pub-id-type="doi">10.1126/science.1136800</pub-id><pub-id pub-id-type="pmid">17218491</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gautier</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2008">2008</year><data-title>Rpy2: a python-r bridge</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/rpy2/rpy2">https://github.com/rpy2/rpy2</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Grünwald</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Nbsphinx: jupyter notebook tools for sphinx</data-title><version designator="0.9.6">0.9.6</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/spatialaudio/nbsphinx">https://github.com/spatialaudio/nbsphinx</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanlon</surname><given-names>RT</given-names></name><name><surname>Messenger</surname><given-names>JB</given-names></name><name><surname>Zachary</surname><given-names>YJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Adaptive coloration in young cuttlefish (Sepia officinalis L.): The morphology and development of body patterns and their relation to behaviour</article-title><source>Philosophical Transactions of the Royal Society of London. B, Biological Sciences</source><volume>320</volume><fpage>437</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1098/rstb.1988.0087</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanlon</surname><given-names>RT</given-names></name><name><surname>Chiao</surname><given-names>CC</given-names></name><name><surname>Mäthger</surname><given-names>LM</given-names></name><name><surname>Barbosa</surname><given-names>A</given-names></name><name><surname>Buresch</surname><given-names>KC</given-names></name><name><surname>Chubb</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Cephalopod dynamic camouflage: bridging the continuum between background matching and disruptive coloration</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>364</volume><fpage>429</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0270</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>A</given-names></name><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Tan</surname><given-names>M</given-names></name><name><surname>Chu</surname><given-names>G</given-names></name><name><surname>Vasudevan</surname><given-names>V</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Pang</surname><given-names>R</given-names></name><name><surname>Adam</surname><given-names>H</given-names></name><name><surname>Le</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Searching for MobileNetV3</article-title><conf-name>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</conf-name><conf-loc>Seoul, Korea (South)</conf-loc><pub-id pub-id-type="doi">10.1109/ICCV.2019.00140</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>Hamman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>xarray: N-D labeled arrays and datasets in python</article-title><source>Journal of Open Research Software</source><volume>5</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.5334/jors.148</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: A 2D Graphics Environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Oja</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Independent component analysis: algorithms and applications</article-title><source>Neural Networks</source><volume>13</volume><fpage>411</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(00)00026-5</pub-id><pub-id pub-id-type="pmid">10946390</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolliffe</surname><given-names>IT</given-names></name><name><surname>Cadima</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Principal component analysis: a review and recent developments</article-title><source>Philosophical Transactions of the Royal Society A</source><volume>374</volume><elocation-id>20150202</elocation-id><pub-id pub-id-type="doi">10.1098/rsta.2015.0202</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Jupyter Development Team</collab></person-group><year iso-8601-date="2015">2015</year><data-title>Jupyter-client: client libraries for jupyter</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/jupyter/jupyter_client">https://github.com/jupyter/jupyter_client</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Long</surname><given-names>J</given-names></name><name><surname>Shelhamer</surname><given-names>E</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fully convolutional networks for semantic segmentation</article-title><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>3431</fpage><lpage>3440</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298965</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>BD</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An Iterative Image Registration Technique with an Application to Stereo Vision</article-title><conf-name>IJCAI’81: 7th International Joint Conference on Artificial Intelligence</conf-name><fpage>674</fpage><lpage>679</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>MacFarlane</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><data-title>Pandoc: a universal document converter</data-title><version designator="2.4">2.4</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/jgm/pandoc">https://github.com/jgm/pandoc</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Marcel</surname><given-names>S</given-names></name><name><surname>Rodriguez</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Torchvision the machine-vision package of torch</article-title><conf-name>Proceedings of the 18th ACM International Conference on Multimedia</conf-name><fpage>1485</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1145/1873951.1874254</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Data Structures for Statistical Computing in Python</source><publisher-name>Scipy</publisher-name><pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Messenger</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Cephalopod chromatophores: neurobiology and natural history</article-title><source>Biological Reviews of the Cambridge Philosophical Society</source><volume>76</volume><fpage>473</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1017/s1464793101005772</pub-id><pub-id pub-id-type="pmid">11762491</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Miles</surname><given-names>A</given-names></name><name><surname>Kirkham</surname><given-names>J</given-names></name><name><surname>Durant</surname><given-names>M</given-names></name><name><surname>Bourbeau</surname><given-names>J</given-names></name><name><surname>Onalan</surname><given-names>T</given-names></name><name><surname>Hamman</surname><given-names>J</given-names></name><name><surname>Patel</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Zarr-developers/zarr-python</data-title><version designator="4">4</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4069231">https://doi.org/10.5281/zenodo.4069231</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packard</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Morphogenesis of chromatophore patterns in cephalopods: are morphological and physiological “units” the same</article-title><source>Malacologia</source><volume>23</volume><fpage>193</fpage><lpage>201</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Kopf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>PyTorch: An Imperative Style, High-Performance Deep Learning Library</source><publisher-name>Advances in Neural Information Processing Systems</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Duchesnay</surname><given-names>É</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reiter</surname><given-names>S</given-names></name><name><surname>Hülsdunk</surname><given-names>P</given-names></name><name><surname>Woo</surname><given-names>T</given-names></name><name><surname>Lauterbach</surname><given-names>MA</given-names></name><name><surname>Eberle</surname><given-names>JS</given-names></name><name><surname>Akay</surname><given-names>LA</given-names></name><name><surname>Longo</surname><given-names>A</given-names></name><name><surname>Meier-Credo</surname><given-names>J</given-names></name><name><surname>Kretschmer</surname><given-names>F</given-names></name><name><surname>Langer</surname><given-names>JD</given-names></name><name><surname>Kaschube</surname><given-names>M</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Elucidating the control and development of skin patterning in cuttlefish</article-title><source>Nature</source><volume>562</volume><fpage>361</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0591-3</pub-id><pub-id pub-id-type="pmid">30333578</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rocklin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dask: Parallel Computation with Blocked algorithms and Task Scheduling</article-title><conf-name>Python in Science</conf-name><conf-loc>Austin, Texas</conf-loc><fpage>126</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-013</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ronacher</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>A python package for creating beautiful command-line interfaces</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/pallets/click">https://github.com/pallets/click</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-net: convolutional networks for biomedical image segmentation</article-title><conf-name>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</conf-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A mathematical theory of communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shohet</surname><given-names>A</given-names></name><name><surname>Baddeley</surname><given-names>R</given-names></name><name><surname>Anderson</surname><given-names>J</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cuttlefish camouflage: a quantitative study of patterning</article-title><source>Biological Journal of the Linnean Society</source><volume>92</volume><fpage>335</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1111/j.1095-8312.2007.00842.x</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Sphinx Team</collab></person-group><year iso-8601-date="2007">2007</year><data-title>Sphinx: python documentation generator</data-title><version designator="8.2">8.2</version><source>Sphinx</source><ext-link ext-link-type="uri" xlink:href="https://www.sphinx-doc.org/en/master/">https://www.sphinx-doc.org/en/master/</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><collab>tqdm Developers</collab></person-group><year iso-8601-date="2016">2016</year><data-title>Tqdm: a fast, extensible progress bar for python</data-title><version designator="4.67.1">4.67.1</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/tqdm/tqdm">https://github.com/tqdm/tqdm</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Boulogne</surname><given-names>F</given-names></name><name><surname>Warner</surname><given-names>JD</given-names></name><name><surname>Yager</surname><given-names>N</given-names></name><name><surname>Gouillart</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Decord: video loader and processor for machine learning</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/dmlc/decord">https://github.com/dmlc/decord</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woo</surname><given-names>T</given-names></name><name><surname>Liang</surname><given-names>X</given-names></name><name><surname>Evans</surname><given-names>DA</given-names></name><name><surname>Fernandez</surname><given-names>O</given-names></name><name><surname>Kretschmer</surname><given-names>F</given-names></name><name><surname>Reiter</surname><given-names>S</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The dynamics of pattern matching in camouflaging cuttlefish</article-title><source>Nature</source><volume>619</volume><fpage>122</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06259-2</pub-id><pub-id pub-id-type="pmid">37380772</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1keyresource" position="anchor"><label>Appendix 1—key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Reagent type (species) or resource</th><th align="left" valign="top">Designation</th><th align="left" valign="top">Source or reference</th><th align="left" valign="top">Identifiers</th><th align="left" valign="top">Additional information</th></tr></thead><tbody><tr><td align="left" valign="top">Strain, strain background</td><td align="left" valign="top"><italic>Euprymna berryi</italic> (bobtail squid)</td><td align="left" valign="top">Laurent Lab, Max Planck Institute for Brain Research</td><td align="left" valign="top"/><td align="left" valign="top">Lab-reared; used in developmental tracking and anisotropy analysis</td></tr><tr><td align="left" valign="top">Strain, strain background</td><td align="left" valign="top"><italic>Sepia officinalis</italic> (European cuttlefish)</td><td align="left" valign="top">Laurent Lab, Max Planck Institute for Brain Research</td><td align="left" valign="top"/><td align="left" valign="top">Used for high-density chromatophore datasets</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">CHROMAS</td><td align="left" valign="top">This paper; <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17617/1.pa38-mh49">https://doi.org/10.17617/1.pa38-mh49</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Pipeline for chromatophore segmentation and analysis</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Python</td><td align="left" valign="top">Python Software Foundation; <ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Version 3.9+</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">PyTorch</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib34">Paszke et al., 2019</xref>; <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">https://pytorch.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">For segmentation models</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Torchvision</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib29">Marcel and Rodriguez, 2010</xref>; <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/vision/">https://pytorch.org/vision/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Model architectures</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">OpenCV</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib4">Bradski, 2000</xref>; <ext-link ext-link-type="uri" xlink:href="https://opencv.org/">https://opencv.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Image and video processing</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">scikit-learn (1.6.1)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib35">Pedregosa et al., 2011</xref>; <ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/">https://scikit-learn.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Clustering and dimensionality reduction</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">scikit-image (0.24.0)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib44">van der Walt et al., 2014</xref>; <ext-link ext-link-type="uri" xlink:href="https://scikit-image.org/">https://scikit-image.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Image processing</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">albumentations (1.4.24)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib5">Buslaev et al., 2020</xref>; <ext-link ext-link-type="uri" xlink:href="https://albumentations.ai/">https://albumentations.ai/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Data augmentation</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">xarray (2024.11.0)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib21">Hoyer and Hamman, 2017</xref>; <ext-link ext-link-type="uri" xlink:href="http://xarray.pydata.org/">http://xarray.pydata.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Labeled multi-dimensional arrays</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Zarr (2.18.4)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib32">Miles et al., 2020</xref>; <ext-link ext-link-type="uri" xlink:href="https://zarr.readthedocs.io/">https://zarr.readthedocs.io/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Chunked data storage</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Dask (2024.12.1)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib9">Dask Development Team, 2016</xref>; <ext-link ext-link-type="uri" xlink:href="https://www.dask.org/">https://www.dask.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Parallel and distributed processing</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">decord</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib46">Wang, 2019</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/dmlc/decord">https://github.com/dmlc/decord</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Efficient video loading</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Matplotlib (3.10.0)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib22">Hunter, 2007</xref>; <ext-link ext-link-type="uri" xlink:href="https://matplotlib.org/">https://matplotlib.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Visualization</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Bokeh (3.6.2)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib3">Bokeh Development Team, 2014</xref>; <ext-link ext-link-type="uri" xlink:href="https://bokeh.org/">https://bokeh.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Interactive dashboards</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Click (8.1.8)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib38">Ronacher, 2014</xref>; <ext-link ext-link-type="uri" xlink:href="https://click.palletsprojects.com/">https://click.palletsprojects.com/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">CLI for CHROMAS</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">tqdm (4.67.1)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib43">tqdm Developers, 2016</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/tqdm/tqdm">https://github.com/tqdm/tqdm</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Progress bars</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Trogon (0.3.0)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib12">Freeman, 2022</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/Textualize/trogon">https://github.com/Textualize/trogon</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Terminal GUI</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Sphinx</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib42">Sphinx Team, 2007</xref>; <ext-link ext-link-type="uri" xlink:href="https://www.sphinx-doc.org/">https://www.sphinx-doc.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Documentation generation</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">nbsphinx (0.9.6)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib15">Grünwald, 2017</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/spatialaudio/nbsphinx">https://github.com/spatialaudio/nbsphinx</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Integrates Jupyter notebooks in Sphinx</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">jupyter-client</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib25">Jupyter Development Team, 2015</xref>; <ext-link ext-link-type="uri" xlink:href="https://jupyter.org/">https://jupyter.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Jupyter support</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Pandoc (2.4)</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib28">MacFarlane, 2006</xref>; <ext-link ext-link-type="uri" xlink:href="https://pandoc.org/">https://pandoc.org/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Document conversion</td></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">rpy2</td><td align="left" valign="top"><xref ref-type="bibr" rid="bib14">Gautier, 2008</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/rpy2/rpy">https://github.com/rpy2/rpy</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">R–Python bridge</td></tr><tr><td align="left" valign="top">Other</td><td align="left" valign="top">Pre-trained chromatophore segmentation models</td><td align="left" valign="top">This paper; <ext-link ext-link-type="uri" xlink:href="https://public.brain.mpg.de/Laurent/Chromas2025/">https://public.brain.mpg.de/Laurent/Chromas2025/</ext-link></td><td align="left" valign="top"/><td align="left" valign="top">Trained on *E. berryi* and *S. officinalis*</td></tr><tr><td align="left" valign="top">Other</td><td align="left" valign="top">Workstation (Intel i9-10900X+RTX A4000)</td><td align="left" valign="top">Laurent Lab, Max Planck Institute for Brain Research</td><td align="left" valign="top"/><td align="left" valign="top">Used for performance benchmarking</td></tr><tr><td align="left" valign="top">Other</td><td align="left" valign="top">Laptop (Intel i5-1135G7, no GPU)</td><td align="left" valign="top">Laurent Lab, Max Planck Institute for Brain Research</td><td align="left" valign="top"/><td align="left" valign="top">Used to demonstrate pipeline performance on CPU</td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106509.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>The open-source software Chromas tracks and analyses cephalopod chromatophore dynamics. The software features a user-friendly interface alongside detailed instructions for its application, with <bold>compelling</bold> exemplary applications to two widely divergent cephalopod species, a squid and a cuttlefish, over time periods large enough to encompass new chromatophore development among existing ones. It demonstrates accurate tracking of the position and identity of each chromatophore. The software and methods outlined therein will become an <bold>important</bold> tool for scientists tracking dynamic signaling in animals.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106509.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study provides comprehensive instructions for using the chromatophore tracking software, Chromas, to track and analyse the dynamics of large numbers of cephalopod chromatophores across various spatiotemporal scales. This software addresses a long-standing challenge faced by many researchers who study these soft-bodied creatures, known for their remarkable ability to change colour rapidly. The updated software features a user-friendly interface that can be applied to a wide range of applications, making it an essential tool for biologists focused on animal dynamic signalling. It will also be of interest to professionals in the fields of computer vision and image analysis.</p><p>Strengths:</p><p>This work provides detailed instructions for this tool kit along with examples for potential users to try. The Gitlab inventory hosts the software package, installation documentation, and tutorials, further helping potential users with a less steep learning curve.</p><p>Weaknesses:</p><p>The evidence supporting the authors' claims is solid, particularly demonstrated through the use of cuttlefish and squid. However, it may not be applicable to all coleoid cephalopods yet, such as octopuses, which have an incredibly versatile ability to change their body forms.</p><p>Comments on revisions:</p><p>I am pleased to see the more detailed version of this useful tool along with tutorials designed for diverse users who are interested in animal dynamic colouration. This study provides detailed instructions for using the chromatophore tracking software Chromas to track and analyse the dynamics of large numbers of cephalopod chromatophores across various spatiotemporal scales. The software features a user-friendly interface that is highly compelling and can be applied to a wide range of applications.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106509.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors developed a computational pipeline named CHROMAS to track and analyze chromatophore dynamics, which provides a wide range of biological analysis tools without requiring the user to write code.</p><p>Strengths:</p><p>(1) CHROMAS is an integrated toolbox that provides tools for different biological tasks such as: segment, classify, track and measure individual chromatophores, cluster small groups of chromatophores, analyze full-body patterns, etc.</p><p>(2) It could be used to investigate different species. The authors have already applied it to analyze the skin of the bobtail squid Euprymna berryi and the European cuttlefish Sepia officinalis.</p><p>(3) The tool is open-source and easy to install. The paper describes in detail the experiment requirements, command to complete each task and provides relevant sample figures, which are easy to follow.</p><p>Weaknesses:</p><p>(1) There are some known limitations for the current version. The users should read the &quot;Discussion&quot; chapter carefully before preparing their experiments and using CHROMAS.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106509.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ukrow</surname><given-names>Johann</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Brain Research</institution><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Renard</surname><given-names>Mathieu DM</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Brain Research</institution><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Moghimi</surname><given-names>Mahyar</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Brain Research</institution><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Laurent</surname><given-names>Gilles</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Brain Research</institution><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>This study provides comprehensive instructions for using the chromatophore tracking software, Chromas, to track and analyse the dynamics of large numbers of cephalopod chromatophores across various spatiotemporal scales. This software addresses a long-standing challenge faced by many researchers who study these soft-bodied creatures, known for their remarkable ability to change colour rapidly. The updated software features a user-friendly interface that can be applied to a wide range of applications, making it an essential tool for biologists focused on animal dynamic signalling. It will also be of interest to professionals in the fields of computer vision and image analysis.</p><p>Strengths:</p><p>This work provides detailed instructions for this toolkit along with examples for potential users to try. The Gitlab inventory hosts the software package, installation documentation, and tutorials, further helping potential users with a less steep learning curve.</p><p>Weaknesses:</p><p>The evidence supporting the authors' claims is solid, particularly demonstrated through the use of cuttlefish and squid. However, it may not be applicable to all coleoid cephalopods yet, such as octopuses, which have an incredibly versatile ability to change their body forms.</p></disp-quote><p>The reviewer is right to highlight this limitation. We clarified, in the revised manuscript, that CHROMAS relies on the assumption that chromatophore activity occurs primarily in a plane — a condition that is valid most of the time in squid and cuttlefish, where the majority of skin deformations are in-plane (with small occasional papillae). In cephalopods such as octopuses, however, in which the skin may undergo large 3-dimensional deformations through the action of papillary musculature, this assumption may not always hold. Although octopods’ bodies are more spherical (less flat) than those of squid and cuttlefish, CHROMAS should still be usable and useful if applied to smaller skin areas, especially because chromatophore density is often even higher in octopoda than in sepiidae.</p><p>We added the following paragraph in the discussion:</p><p>Another known limitation concerns the biological assumptions underlying the current version of CHROMAS. The pipeline is designed for surfaces that remain reasonably planar and undergo deformations primarily in two dimensions. In cephalopods such as octopuses, in which the skin can undergo substantial three-dimensional morphological changes, analysing chromatophore dynamics may require complementary three-dimensional tracking of the skin surface to correct for out-of-plane deformations and maintain accurate measurement of chromatophore activity.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The authors developed a computational pipeline named CHROMAS to track and analyse chromatophore dynamics, which provides a wide range of biological analysis tools without requiring the user to write code.</p><p>Strengths:</p><p>(1) CHROMAS is an integrated toolbox that provides tools for different biological tasks such as: segment, classify, track and measure individual chromatophores, cluster small groups of chromatophores, analyse full-body patterns, etc.</p><p>(2) It could be used to investigate different species. The authors have already applied it to analyse the skin of the bobtail squid Euprymna berryi and the European cuttlefish Sepia officinalis.</p><p>(3) The tool is open-source and easy to install. The paper describes in detail the command format to complete each task and provides relevant sample figures.</p><p>Weaknesses:</p><p>(1) The generality and robustness of the proposed pipeline need to be verified through more experimental evaluations. For example, the implementation algorithm depends on relatively specific or obvious image features, clean backgrounds, and objects that do not move too fast.</p><p>(2) The pipeline lacks some kind of self-correction mechanism. If at one moment there is a conflicting match with the previous frames, how does the system automatically handle it to ensure that the tracking results are accurate over a long period of time?</p></disp-quote><p>We thank the reviewer for raising this important point. CHROMAS does rely on relatively clean imaging conditions for optimal performance. However, the computational features of the pipeline — segmentation, tracking, and downstream analysis — have been designed to perform reliably as long as the segmentation models are trained on frames that reflect the diversity of the dataset (e.g., variations in lighting or minor background noise). It is correct, however, that acquiring the necessary quality of input data is both important and non-trivial. The pipeline is designed to work best with high-resolution footage of chromatophores under clear imaging conditions — specifically, with minimal water surface distortion, minimal particulate matter in the water column, and stable focus.</p><p>To mitigate issues arising from motion blur or focus loss, CHROMAS includes an automatic frame quality control step that detects and discards frames that are out of focus, including those where the animal moves too fast for reliable tracking.</p><p>To assist future users, we have now added a section under Discussion detailing the recommended recording conditions and video characteristics for effective analysis with CHROMAS. It reads:</p><p>Recommended Video Parameters for Optimal Use of CHROMAS</p><p>The performance of CHROMAS depends on the quality of the input videos. Although the pipeline analyses each frame independently and has no frame rate requirement, we recommend recording at 20 frames per second at least, to capture chromatophore dynamics accurately. Sharp, in-focus frames are critical, particularly for moving subjects, where higher shutter speeds help minimize motion blur. For reliable segmentation, each chromatophore should cover at least 10 pixels across its fully expanded diameter. Higher spatial resolution, with chromatophores covering around 50 pixels in diameter, are recommended if sub-chromatophore dynamics are of interest. Recording conditions should minimize background noise, and the water column should be as clear as possible, free of particles or debris. The water surface should be kept as calm and planar as possible to avoid optical artifacts. If wide-angle lenses or other optics that may introduce distortion are used, lens correction algorithms should be applied during preprocessing to compensate for the optical distortions. For long-term tracking applications (e.g., developmental studies), frequent imaging sessions are recommended. Newly differentiated chromatophores are initially light colored (e.g., yellow) and thus visually distinct from mature chromatophores (which are dark); over days to weeks, however, the light chromatophores darken and become increasingly difficult to differentiate from older ones. Recording at appropriate and regular intervals thus helps track individual chromatophores across developmental stages and improves the reliability of long-term analyses. Following these recommendations will help segmentation, tracking, and analysis with CHROMAS.</p><p>CHROMAS does not implement an active self-correction mechanism in the sense of real-time error recovery. Yet, several steps are in place to ensure the reliability of registration and tracking over time. During registration, a set of points is tracked across frames using optical flow. If the displacement of a point between two frames exceeds a biologically plausible threshold, that point is automatically discarded from the registration calculation to prevent error propagation. If too many points are discarded, the registration step fails, preventing the acceptance of a poor alignment.</p><p>In addition, masterframes (the averages of all aligned frames in a chunk) are generated at the end of the registration process to enable the visual verification of the quality of the mapping.</p><p>During stitching, CHROMAS calculates reprojection errors between chunks, providing a quantitative measure of stitching validity and allowing users to detect and correct potential mismatches.</p><p>We have revised the Results section to explicitly highlight the error-checking mechanisms implemented during registration and stitching to maintain tracking accuracy over time.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>(1) Figures 2, 3, 5, 6, 8 showed the bobtail squid, however, all command lines for these figures were referred to &quot;sepia_example.dataset&quot;.</p></disp-quote><p>We thank the reviewer for noticing this inconsistency. We have corrected the labeling of the dataset name in the command line examples from &quot;sepia_example.dataset&quot; to the neutral term &quot;example.dataset&quot; to avoid any confusion regarding the species used in the figures.</p><disp-quote content-type="editor-comment"><p>(2) It's excellent that Chromas includes a manual pre-alignment function. However, it's unclear how the authors determined the registration of selected chromatophores across different ages in the long-term tracking session. Given the rapid growth of cephalopods and presumably skin expansion with increased chromatophores, it would be helpful to provide more details or examples on this process.</p></disp-quote><p>The manual pre-alignment function provides an interactive interface allowing the user to select a set of matching chromatophores across frames from different developmental stages. The accuracy of this process depends on the user's ability to recognize individual chromatophores reliably over time. Critically, it is not necessary to identify all those chromatophores; a representative subset is sufficient to interpolate the spatial mapping and align the surrounding chromatophores.</p><p>To limit the potential challenges associated with chromatophore development, frequent imaging sessions (every few days) are recommended initially. Excessive intervals between recordings can result in relative displacements among existing chromatophores and the sudden appearance of newly matured chromatophores, both of which complicate manual matching.</p><p>It should be noted that these challenges are not limitations of the CHROMAS pipeline itself, but rather relate to experimental design choices that affect the quality and traceability of the dataset. The exact parameters (e.g., size/duration of the datasets, spatial resolution, frame rate and intervals between recording sessions) to be used must be adapted to each experimental animal, each age, and ultimately, each question.</p><p>Recommended video acquisition parameters, including guidance on recording frequency for long-term chromatophore tracking, have been added to the Discussion section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) More detailed information should be given, such as operating system requirements, camera frame rate requirements, target size and speed limitations, when chunking videos into usable segments, the minimum length of each segment, etc.</p></disp-quote><p>CHROMAS is platform-independent and requires only a functioning Python 3.9+ environment, regardless of the operating system or OS version, as described in “Methods – Implementation details”.</p><p>Although CHROMAS does not require specific frame rates and because it analyses each frame independently, the quality of each image—and thus of imaging parameters—is critical to enable reliable chromatophore segmentation. If an animal remains relatively calm during recording, low shutter speeds will be adequate for image sharpness. Conversely, if the animal moves frequently or rapidly, it will be preferable to use a higher frame rate and a higher shutter speed to minimize motion blur. Recording parameters should therefore be adjusted accordingly, primarily to optimize image clarity and maintain frames in sharp focus.</p><p>The frame rate should be sufficiently high also to capture the fast dynamics of chromatophore expansions and contractions. Although the pipeline has no specific frame rate requirement, we recommend image rates of at least 20 frames per second to sample the temporal patterns of chromatophore activity adequately, based on biological considerations.</p><p>Each chromatophore should be represented by a sufficiently large number of pixels in each recorded image to enable the reliable estimation of its size, shape, and dynamics. If the spatial resolution is too low, individual chromatophores may appear as small pixel clusters, reducing the accuracy of area and shape measurements and introducing quantization artifacts. Based on our experience, we recommend recording conditions that result in each chromatophore covering at least 10 pixels across its diameter when fully expanded to ensure accurate segmentation and quantitative whole-chromatophore analysis. For sub-chromatophore motion analysis, we recommend a minimum of 50 pixels across the fully expanded diameter.</p><p>These considerations relate to optimizing biological sampling and image quality for analysis, and are not technical requirements imposed by CHROMAS itself.</p><p>We added a Discussion section outlining the recommended recording conditions and video parameters to facilitate effective use of CHROMAS.</p><disp-quote content-type="editor-comment"><p>(2) This pipeline does not include functionality to correct for lens distortion, which may affect the results when accurate measurement of single chromatophore morphology is required.</p></disp-quote><p>We thank the reviewer for this observation. We agree that lens distortion can affect the accurate measurement of chromatophore morphology if present. However, the current datasets analysed with CHROMAS were recorded using a long macro lens with minimal distortion, and visual inspections as well as quantitative assessments of chromatophore geometry did not indicate measurable optical deformation. We acknowledge that for other imaging setups —particularly those relying on the use of wide-angle lenses— lens distortion could introduce artifacts. In such cases, we recommend applying standard lens distortion correction during preprocessing, prior to analysis with CHROMAS.</p><p>We have also addressed this point in the newly added section under the Discussion.</p><disp-quote content-type="editor-comment"><p>(3) How to perform expansion for single chromatophores shown in Figure 6, and how to keep the expansion area consistent?</p></disp-quote><p>The graph in Figure 6 illustrates the expansion of a single chromatophore over time and was generated entirely using the &quot;areas&quot; command and visualization tools available within CHROMAS.</p><p>Spatial consistency is maintained because CHROMAS, through its registration and area extraction steps, tracks the identity of each chromatophore across the video, allowing the same individual to be followed reliably over time.</p><disp-quote content-type="editor-comment"><p>(4) Tables 1 and 2: it's better to add the units of the values in each column.</p></disp-quote><p>We thank the reviewer for the suggestion. We have added the appropriate units to each column in Tables 1 and 2 to improve clarity.</p></body></sub-article></article>