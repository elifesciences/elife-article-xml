<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">92712</article-id><article-id pub-id-type="doi">10.7554/eLife.92712</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92712.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Predictive learning rules generate a cortical-like replay of probabilistic sensory experiences</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Asabuki</surname><given-names>Toshitake</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0951-5791</contrib-id><email>toshitake.asabuki@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Fukai</surname><given-names>Tomoki</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6977-5638</contrib-id><email>tomoki.fukai@oist.jp</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qg15b79</institution-id><institution>Okinawa Institute of Science and Technology Graduate University</institution></institution-wrap><addr-line><named-content content-type="city">Okinawa</named-content></addr-line><country>Japan</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04j1n1c04</institution-id><institution>RIKEN Center for Brain Science, RIKEN ECL Research Unit</institution></institution-wrap><addr-line><named-content content-type="city">Wako</named-content></addr-line><country>Japan</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sjwvz98</institution-id><institution>RIKEN Pioneering Research Institute (PRI)</institution></institution-wrap><addr-line><named-content content-type="city">Wako</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>06</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP92712</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-03-11"><day>11</day><month>03</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-07-29"><day>29</day><month>07</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.02.17.528958"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-07"><day>07</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92712.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-14"><day>14</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92712.2"/></event></pub-history><permissions><copyright-statement>© 2024, Asabuki and Fukai</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Asabuki and Fukai</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-92712-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-92712-figures-v1.pdf"/><abstract><p>The brain is thought to construct an optimal internal model representing the probabilistic structure of the environment accurately. Evidence suggests that spontaneous brain activity gives such a model by cycling through activity patterns evoked by previous sensory experiences with the experienced probabilities. The brain’s spontaneous activity emerges from internally driven neural population dynamics. However, how cortical neural networks encode internal models into spontaneous activity is poorly understood. Recent computational and experimental studies suggest that a cortical neuron can implement complex computations, including predictive responses, through soma–dendrite interactions. Here, we show that a recurrent network of spiking neurons subject to the same predictive learning principle provides a novel mechanism to learn the spontaneous replay of probabilistic sensory experiences. In this network, the learning rules minimize probability mismatches between stimulus-evoked and internally driven activities in all excitatory and inhibitory neurons. This learning paradigm generates stimulus-specific cell assemblies that internally remember their activation probabilities using within-assembly recurrent connections. Our model contrasts previous models that encode the statistical structure of sensory experiences into Markovian transition patterns among cell assemblies. We demonstrate that the spontaneous activity of our model well replicates the behavioral biases of monkeys performing perceptual decision making. Our results suggest that interactions between intracellular processes and recurrent network dynamics are more crucial for learning cognitive behaviors than previously thought.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>synaptic plasticity</kwd><kwd>spontaneous activity</kwd><kwd>statistical learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>KAKENHI</institution></institution-wrap></funding-source><award-id>18H05213</award-id><principal-award-recipient><name><surname>Fukai</surname><given-names>Tomoki</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>KAKENHI</institution></institution-wrap></funding-source><award-id>19H04994</award-id><principal-award-recipient><name><surname>Fukai</surname><given-names>Tomoki</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Biologically plausible synaptic plasticity rules enable recurrent neural networks to spontaneously replay sensory experiences with appropriate probabilistic structure.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The brain is believed to construct an internal statistical model of an uncertain environment from sensory information streams for predicting the external events that are likely to occur. Evidence suggests that spontaneous brain activity learns the representation of such a model through repeated experiences of sensory events. In the cat visual cortex, spontaneously emerging activity patterns cycle through cortical states that include neural response patterns to oriented bars (<xref ref-type="bibr" rid="bib36">Kenet et al., 2003</xref>). In the ferret visual cortex, spontaneous activity gradually resembles a superposition of activity patterns evoked by natural scenes, eventually giving an optimal model of the visual experience (<xref ref-type="bibr" rid="bib8">Berkes et al., 2011</xref>). As replay activities can provide prior information for hierarchical Bayesian computation by the brain (<xref ref-type="bibr" rid="bib14">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib38">Körding and Wolpert, 2004</xref>; <xref ref-type="bibr" rid="bib16">Friston, 2010</xref>; <xref ref-type="bibr" rid="bib15">Fiser et al., 2010</xref>; <xref ref-type="bibr" rid="bib7">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Orbán et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Legaspi and Toyoizumi, 2019</xref>), clarifying how the brain learns the spontaneous replay of optimal internal models is crucial for understanding whole-brain computing. However, the neural mechanisms underlying this modeling process are only poorly understood.</p><p>Several mechanisms of the brain’s probabilistic computation have been explored (<xref ref-type="bibr" rid="bib30">Jimenez Rezende and Gerstner, 2014</xref>; <xref ref-type="bibr" rid="bib40">Li et al., 2022</xref>). Models with reverberating activity are particularly interesting owing to their potential ability to generate spontaneous activity. For instance, spiking neuron networks with symmetric recurrent connections were proposed for Markov Chain Monte Carlo sampling of stochastic events (<xref ref-type="bibr" rid="bib10">Buesing et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Bill et al., 2015</xref>). Spike-timing-dependent plasticity (STDP) was used to organize spontaneous sequential activity patterns, providing a predictive model of sequence input (<xref ref-type="bibr" rid="bib22">Hartmann et al., 2015</xref>). However, previous models did not clarify how recurrent neural networks learn the spontaneous replay of the probabilistic structure of sensory experiences, for which these networks should learn the accurate probabilities of sensory stimuli and an appropriate excitation–inhibition balance simultaneously. Moreover, previous models assumed that each statistically salient stimulus in temporal input is already segregated and is delivered to a pre-assigned assembly of coding neurons, implying that the recurrent network, at least partly, knows the stochastic events to be modeled before learning. How the brain extracts salient events for statistical modeling has not been addressed.</p><p>Here, we present a learning principle to encode the experiences’ probability structure into spontaneous network activity. To this end, we extensively use the synaptic plasticity rule proposed previously based on the hypothesis that the dendrites of a cortical neuron learn to predict its somatic responses (<xref ref-type="bibr" rid="bib63">Urbanczik and Senn, 2014</xref>; <xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>). We generalize the hypothetical predictive learning to a learning principle at the entire network level. Namely, in a recurrent network driven by external input, we ask all synapses on the dendrites of each excitatory or inhibitory neuron to learn to predict its somatic responses (although the dendrites will not be explicitly modeled). This enables the network model to simultaneously learn the events’ probabilistic structure and the excitation–inhibition balance required to replay this structure. Further, our network model requires no pre-assigned cell assemblies since the model neuron can automatically segment statistically salient events in temporal input (<xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>) – a cognitive process known as ‘chunking’ (<xref ref-type="bibr" rid="bib17">Fujii and Graybiel, 2003</xref>; <xref ref-type="bibr" rid="bib31">Jin and Costa, 2010</xref>; <xref ref-type="bibr" rid="bib32">Jin et al., 2014</xref>; <xref ref-type="bibr" rid="bib55">Schapiro et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">Zacks et al., 2001</xref>). Intriguingly, the cell assemblies generated by our model store their replay probabilities primarily in the within-assembly network structure, and intrinsic dynamical properties of membership neurons also contribute to this coding. This is in striking contrast to other network models that encode probabilities into the Markovian transition dynamics among cell assemblies (<xref ref-type="bibr" rid="bib10">Buesing et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Hartmann et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Asabuki and Clopath, 2024</xref>).</p><p>Our model trained on a perceptual decision-making task can replicate both unbiased and biased decision behaviors of monkeys without fine-tuning of parameters (<xref ref-type="bibr" rid="bib21">Hanks et al., 2011</xref>). In addition, in a network model consisting of distinct excitatory and inhibitory neural populations, our learning rule predicts the emergence of two types of inhibitory connections with different computational roles. We show that the emergence of the two inhibitory connection types is crucial for robust learning of an optimal internal model.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Replay of probabilistic sensory experiences – a toy example</title><p>We first explain the task our model solves with a toy example. Consider a task in which the animal should decide whether a given stimulus coincides with or resembles any of two previously learned stimuli. Whether the animal learned these stimuli with a 50–50 chance or a 30–70 chance should affect the animal’s anticipation of their occurrence and hence affect its decision.</p><p>It has been suggested that spontaneous activity expresses an optimal internal model of the sensory environment (<xref ref-type="bibr" rid="bib8">Berkes et al., 2011</xref>). In our toy example, the evoked activity patterns of the two stimuli should be spontaneously replayed with the same probabilities as these stimuli were experienced during learning:<disp-formula id="equ1"><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle \left\langle P\left (\rm features|input,model\right)\right\rangle _{P\left (\rm input\right)}=P\left (\rm features|model\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where features = {stimulus 1, stimulus 2} and the right-hand side expresses the probabilities of replayed activities. The angular brackets indicate averaging over the stimuli. According to Hebb’s hypothesis, two cell assemblies should be formed to memorize the two stimuli in the toy example. Moreover, the spontaneous replay of these cell assemblies should represent the probabilities given in the right-hand side of the above equation. Below, we propose a mathematical principle of learning to achieve these requirements.</p></sec><sec id="s2-2"><title>Prediction-driven synaptic plasticity for encoding an internal model</title><p>We previously proposed a learning rule for a single two-compartment neuron (<xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>). Briefly, our previous model learns statistically salient features repeated in input sequences by minimizing the error between somatic and dendritic response probabilities without external supervision to identify the temporal locations of these features. In this study, we extend this plasticity rule to recurrent networks by asking all neurons in a network to minimize the error in response probabilities between the internally generated and stimulus-evoked activities (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Our central interest is whether this learning principle generates spontaneous activity representing the statistical model of previous experiences.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Unsupervised prior learning in a recurrent neural network.</title><p>(<bold>a</bold>) A schematic of a network model is shown. The interconnected circles denote the model neurons, of which the activities are controlled by two types of inputs: feedforward (FF) and recurrent (REC) inputs. Colored circles indicate active neurons. Here, v<sup>W</sup> denote FF, and v<sup>M</sup> denote REC connections. We considered two modes of activity (i.e., evoked and spontaneous activity). In the evoked mode, the membrane potential u of a network neuron was calculated as a linear combination of inputs across all different connections (v<sup>W</sup>, v<sup>M</sup>, and v<sup>G</sup>). This evoked mode is considered during the learning phase, when all synapses attempt to predict the network activity, as we will explain in the main text. Once all synapses are sufficiently learned, all FF inputs are removed, and the network is driven spontaneously (spontaneous mode). Our interest lies in the statistical similarity of the network activity in these two modes. (<bold>b</bold>) The gain and threshold of output response function was controlled by a dynamic variable, h, which tracks the history of the membrane potential. (<bold>c</bold>) A schematic of the learning rule for a network neuron is shown (top). During learning, for each type of connection on a postsynaptic neuron, synaptic plasticity minimizes the error between output (gray diamond) and synaptic prediction (colored diamonds). Note that all types of synapses share the common plasticity rule, where weight updates are calculated as the multiplication of the error term and the presynaptic activities (bottom). Our hypothesis is that such plasticity rule allows a recurrent neural network to spontaneously replay the learned stochastic activity patterns without external input.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig1-v1.tif"/></fig><p>We first introduce our learning principle using a recurrent network model (nDL model) that does not obey Dale’s law for distinguishing between excitatory and inhibitory neurons (Materials and methods). A more realistic model with distinct excitatory and inhibitory neuron pools will be shown later. The nDL model consists of Poisson spiking neurons, each receiving Poisson spike trains from all input neurons via a modifiable all-to-all afferent feedforward connection matrix <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$\rm {\boldsymbol W}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). These input neurons may be grouped into multiple input neuron groups responding to different sensory features. Due to the all-to-all connectivity, the afferent input has no specific predefined structure. Two types of all-to-all modifiable recurrent connections, <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$ {\boldsymbol M}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$\boldsymbol G$\end{document}</tex-math></alternatives></inline-formula>, exist among the neurons. Matrix <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$\boldsymbol M$\end{document}</tex-math></alternatives></inline-formula> is a mixture of excitatory and inhibitory connections, and matrix <inline-formula><alternatives><mml:math id="inf5"><mml:mi>G</mml:mi></mml:math><tex-math id="inft5">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> represents inhibitory-only connections. Due to a minus sign for <inline-formula><alternatives><mml:math id="inf6"><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft6">\begin{document}$v^{G}$\end{document}</tex-math></alternatives></inline-formula>, all components of <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$\boldsymbol G$\end{document}</tex-math></alternatives></inline-formula> are positive. The firing rate of neurons is defined as a modifiable sigmoidal function of the membrane potential (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), which we will explain later in detail. All types of connections, both afferent and recurrent ones, are modifiable by unsupervised learning rules derived from a common principle: on each neuron, all synapses learn to predict the neuron’s response optimally (<xref ref-type="fig" rid="fig1">Figure 1c</xref>: see Materials and methods). In reality, all synaptic inputs may be terminated on the dendrites, although they are not modeled explicitly.</p><p>Without a teaching signal, predictive learning may suffer a trivial solution problem in which all synapses vanish, and hence all neurons become silent (<xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>). To avoid it, we homeostatically regulate the dynamic range of each neuron (i.e., the slope and threshold of the response function) according to the history <inline-formula><alternatives><mml:math id="inf8"><mml:mi>h</mml:mi></mml:math><tex-math id="inft8">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> of its subthreshold activity (see <xref ref-type="disp-formula" rid="equ8 equ9 equ10">Equations 6–8</xref>). When the value of <inline-formula><alternatives><mml:math id="inf9"><mml:mi>h</mml:mi></mml:math><tex-math id="inft9">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> is increased, the neuron’s excitability is lowered (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The input–output curves of neurons are known to undergo homeostatic regulations through various mechanisms (<xref ref-type="bibr" rid="bib11">Chance et al., 2002</xref>; <xref ref-type="bibr" rid="bib49">Mitchell and Silver, 2003</xref>; <xref ref-type="bibr" rid="bib60">Torres-Torrelo et al., 2014</xref>). Though no direct experimental evidence is available for our homeostatic process via <inline-formula><alternatives><mml:math id="inf10"><mml:mi>h</mml:mi></mml:math><tex-math id="inft10">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula>, it mathematically avoids saturating neuronal activity.</p><p>Note that the present homeostatic regulation of intrinsic excitability differs from the homeostatic synaptic scaling mechanism. The role of homeostatic synaptic scaling in generating irregular cell-assembly activity patterns was previously studied computationally (<xref ref-type="bibr" rid="bib24">Hiratani and Fukai, 2014</xref>; <xref ref-type="bibr" rid="bib41">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="bib67">Zenke et al., 2015</xref>). However, unlike the present model, the previous models did not address whether and how synaptic scaling contributes to statistical modeling by recurrent neural networks. Furthermore, unlike our model, in which neurons in the recurrent layer and input neurons are initially connected in an all-to-all manner, most previous models assumed preconfigured receptive fields for recurrent-layer neurons, implying that these models had predefined stimulus-specific cell assemblies.</p></sec><sec id="s2-3"><title>Cell-assembly formation for learning statistically salient stimuli</title><p>We first explain how our network segments salient stimuli and forms stimulus-specific cell assemblies via network-wide predictive learning rules. To this end, we tested a simple case in which two non-overlapping input groups are intermittently and repeatedly activated with equal probabilities. The two input patterns were separated by irregular, low-frequency, unrepeated spike trains of all input neurons (Materials and methods). We will consider input patterns with unequal occurrence probabilities later. After several presentations of individual input patterns, each network neuron responded selectively to one of the repeated patterns (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). This result is consistent with our previous results (<xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>) that the plasticity of feedforward connections segments input patterns. Indeed, feedforward synapses W on each neuron were strengthened or weakened when they mediated its preferred or non-preferred stimulus, respectively (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, left; <xref ref-type="fig" rid="fig2">Figure 2c</xref>). Inhibitory connections G grew between neurons within the same assembly but not between assemblies (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, right; <xref ref-type="fig" rid="fig2">Figure 2c</xref>, bottom), enhancing the decorrelation of within-assembly neural activities (<xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>). Recurrent connections M were modified to form stimulus-specific cell assemblies, as evidenced by the self-organization of excitatory (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, top) and inhibitory (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, bottom) recurrent connections within and between cell assemblies, respectively. The inhibitory components are necessary for suppressing the simultaneous replay of different cell assemblies, as shown later.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Formation of stimulus-selective assemblies in a recurrent network.</title><p>(<bold>a</bold>) Example dynamics of neuronal output and synaptic predictions are shown before (left) and after (right) learning. Colored bars at the top of the figures represent periods of stimulus presentations. (<bold>b</bold>) Example dynamics of feedforward connection W and inhibitory connection G are shown. W-connections onto neurons organizing to encode the same or different input patterns are shown in red and blue, respectively. Similarly, the same colors are used to represent G connections within and between assemblies. (<bold>c</bold>) Dynamics of the mean connection strengths are shown on neuron in cell assembly 1. Shaded areas represent SDs over 10 samples. In the schematic, triangles indicate input neurons and circles indicate network neurons. The color of each neuron indicates the stimulus preference of each neuron. (<bold>d</bold>) Example dynamics of the averaged dynamical variable <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$\bar{h} $\end{document}</tex-math></alternatives></inline-formula> (top) and the learned network activity (bottom) are shown. The dynamical variables are averaged over the entire network. Neurons are sorted according to their preferred stimuli. During the spontaneous activity, afferent inputs to the network were removed. The inset shows the firing rate distribution of the evoked and the spontaneous activity. (<bold>e</bold>) Correlation coefficients of spontaneous activities of every pair of neurons are shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig2-v1.tif"/></fig><p>We then investigated whether and how spontaneous activity preserves and replays these cell assemblies in the absence of afferent input. To demonstrate this in a more complex task, we trained the network with afferent input involving five repeated patterns and then removed the input and observed post-training spontaneous network activity (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). The termination of afferent input initially lowered the activities of neurons, but their dynamic ranges gradually recovered with the excitability of the neural population (indicated by the population-averaged <inline-formula><alternatives><mml:math id="inf12"><mml:mi>h</mml:mi></mml:math><tex-math id="inft12">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> value), and the network eventually started spontaneously replaying the learned cell assemblies. All plasticity rules were turned off during the recovery period (about 20 s from the input termination), after which the network settled in a stable spontaneous firing state (plasticity off), with firing rates lower than those of the evoked activity (inset). Then, the plasticity rules could be turned on (plasticity on) without drastically destroying the structure of spontaneous replay. Intriguingly, spontaneous neuronal activities were highly correlated within each cell assembly but were uncorrelated between different cell assemblies (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). This was because self-organized recurrent connections <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$\boldsymbol M$\end{document}</tex-math></alternatives></inline-formula> were excitatory within each cell assembly, whereas the between-assembly recurrent connections were inhibitory, as in <xref ref-type="fig" rid="fig2">Figure 2c</xref>.</p><p>Thus, the network model successfully segregates, remembers, and replays stimulus-evoked activity patterns in temporal input. The loss of between-assembly excitatory connections is interesting as it indicates that the present spontaneous reactivation is not due to the sequential activation of cell assemblies. This can also be seen from the relatively long intervals between consecutive cell-assembly activations: spontaneous neural activity does not propagate directly from one cell assembly to another (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). Indeed, within-assembly excitation is the major cause of spontaneous replay in this model, which we will study later in detail.</p><p>In summary, we have proposed the predictive learning rules as a novel plasticity mechanism for all types of synapses (i.e., feedforward and recurrent connections). We have shown that the plasticity rules in our model learn the segmentation of salient patterns in input sequences and form pattern-specific cell assemblies without preconfigured structures. We also showed that our model replays the learned assemblies even when external inputs were removed.</p></sec><sec id="s2-4"><title>Replays of cell assemblies reflect a learned statistical model</title><p>We now turn to the central question of this study. We asked whether internally generated network dynamics through recurrent synapses (i.e., spontaneous replay of cell assemblies) can represent an optimal model of previous sensory experiences. Specifically, we examined whether the network spontaneously reactivates learned cell assemblies with relative frequencies proportional to the probabilities with which external stimuli activated these cell assemblies during learning. We addressed these questions in slightly more complex cases with increased numbers of external stimuli.</p><p>We first examined a case with five stimuli in which stimulus 1 was presented twice as often as the other four stimuli (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). Hereafter, the probability ratio refers to the relative number of times stimulus 1 is presented during learning. For instance, the case shown in <xref ref-type="fig" rid="fig2">Figure 2d</xref> represents the probability ratio one. As in <xref ref-type="fig" rid="fig2">Figure 2d</xref>, the network self-organized five cell assemblies to encode stimuli 1–5 and replayed all of them in subsequent spontaneous activity (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). We found that output neurons were activated more frequently and strongly in cell assembly 1 than in other cell assemblies. Therefore, we assessed quantitative differences in neuronal activity between different cell assemblies by varying the probability ratio. The neuronal firing rate of cell assembly 1 relative to other cell assemblies increased approximately linearly with an increase in the probability ratio (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). Similarly, the size of cell assembly 1 relative to other cell assemblies also increased with the probability ratio (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). However, neither the relative firing rate nor the relative assembly size faithfully reflects changes in the probability ratio: scaling the probability ratio with a multiplicative factor does not scale these quantities with this factor. Therefore, we further investigated whether the assembly activity ratio, the ratio in the total firing rate of cell assembly 1 to other cell assemblies (Materials and methods), scales faithfully with the probability ratio of cell assembly 1. This was the case: the scaling was surprisingly accurate (<xref ref-type="fig" rid="fig3">Figure 3e</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Priors coded in spontaneous activity.</title><p>An nDL network was trained with five probabilistic inputs. (<bold>a</bold>) Stimulus 1 appeared twice as often as the other four stimuli during learning. The example empirical probabilities of the stimuli used for learning are shown. (<bold>b</bold>) The spontaneous activity of the trained network shows distinct assembly structures. (<bold>c</bold>) The mean ratio of the population-averaged firing rate of assembly 1 to those of the other assemblies is shown for different values of the occurrence probability of stimulus 1. Vertical bars show SDs over five trials. A diagonal dashed line is a ground truth. (<bold>d</bold>) Similarly, the mean ratios of the size of assembly 1 to those of the other assemblies are shown. (<bold>e</bold>) The mean ratios of the total activities of neurons in assembly 1 to those of the other assemblies are shown. (<bold>f</bold>) Five stimuli occurring with different probabilities were used for training the nDL model. (<bold>g</bold>) The population firing rates are shown for five self-organized cell assemblies encoding the stimulus probabilities shown in (<bold>f</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Prior encoding by the nDL model.</title><p>As in <xref ref-type="fig" rid="fig3">Figure 3f and g</xref>, the nDL model was trained with a set of stimuli. (<bold>a</bold>) The five stimuli occurred with different probabilities during training. (<bold>b</bold>) The spontaneously replayed cell assemblies exhibited population firing rates proportional to the occurrence probabilities of the corresponding stimuli. (<bold>c</bold>) Similar to a, but with seven stimuli. (<bold>d</bold>) The spontaneous population activities of seven assemblies are shown. The activities were proportional to the occurrence probabilities of stimuli shown in c. Error bars show SDs over five independent simulations. A dashed line is a regression line.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Learning occurrence probabilities of overlapped input patterns.</title><p>(<bold>a</bold>) Two input patterns were presented with 30% (blue) and 70% (red) probabilities of which 50% of input neurons were shared (purple horizontal area). (<bold>b</bold>) The spontaneous activity of the trained network shows distinct assembly structures. (<bold>c</bold>) The ratio of the activities of two learned assemblies in spontaneous activity showed a strong similarity to the stimulus probabilities.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig3-figsupp2-v1.tif"/></fig></fig-group><p>To examine the ability of the nDL network further, we trained it with five stimuli occurring with various probabilities (<xref ref-type="fig" rid="fig3">Figure 3f</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>). After learning, the spontaneous activity of the model replayed the learned cell assemblies at the desired ratios of population firing rates (<xref ref-type="fig" rid="fig3">Figure 3g</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1b</xref>).</p><p>We then asked whether our model would learn a prior distribution for more stimuli. To this end, we presented seven stimulus patterns to the same network with graded probabilities (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1c</xref>). The self-organized spontaneous activity exhibited cell assemblies that well learned the graded probability distribution of these stimuli (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1d</xref>). These results demonstrate that the trained network remembers the probabilities of repetitively experienced stimuli by the spontaneous firing rates of the encoding cell assemblies and that this dynamical coding scheme has a certain degree of scalability.</p><p>So far, we have represented external stimuli with non-overlapping subgroups of input neurons. However, in biologically realistic situations, input neuron groups may share part of their membership neurons. We tested whether the proposed model could learn the probability structure of overlapping input patterns in a case where two input neuron groups shared half of their members. The two patterns were presented with probabilities of 30% and 70%, respectively (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2a</xref>). After sufficient learning, the network model generated two assemblies that encoded the two stimuli without sharing the coding neurons (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2b</xref>) and replayed these assemblies with frequencies proportional to the stimulus presentation probabilities (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2c</xref>). The results look reasonable because each neuron in the network segments one of the stimulus patterns, and recurrent connections within each non-overlapping assembly can encode the probability of its replay.</p><p>Altogether, these results suggest that our model spontaneously replays learned cell assemblies with relative frequencies proportional to the probability that each cell assembly was activated during the learning phase. We have shown that the population activities of assemblies, rather than the firing rates of individual neurons, encode the occurrence probabilities of stimulus patterns.</p></sec><sec id="s2-5"><title>Within-assembly recurrent connections encode probabilistic sensory experiences</title><p>To understand the mechanism underlying the statistical similarity between the evoked patterns and spontaneous activity, we then investigated whether and how biases in probabilistic sensory experiences influence the strengths of recurrent connections. To this end, we compared two cases in which two input patterns (stim 1 and stim 2) occurred with equal (50% vs. 50%) and different (30% vs. 70%) probabilities during learning (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). From the results shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, we hypothesized that within-assembly learned connections should reflect the stimulus occurrence probabilities and hence the activation probabilities of the corresponding cell assemblies during spontaneous activity. Therefore, we calculated the total strengths of incoming recurrent synapses on each neuron within the individual cell assemblies (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). While the distributions of incoming synaptic strengths are similar between cell assemblies coding stimulus 1 and stimulus 2 in the 50-vs-50 case, they look different in the 30-vs-70 case (<xref ref-type="fig" rid="fig4">Figure 4c</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Probability encoding by learned within-assembly synapses.</title><p>(<bold>a</bold>) Two input stimuli were presented in two protocols: uniform (50% vs. 50%) or biased (30% vs. 70%). (<bold>b</bold>) The total incoming synaptic strength on each neuron was calculated within each cell assembly. (<bold>c</bold>) <italic>left</italic>, The distributions of incoming synaptic strength are shown for the learned assemblies in the 50-vs-50 case. <italic>right</italic>, Same as in the left figure, but in the 30-vs-70 case. (<bold>d</bold>) <italic>left</italic>, The empirical probabilities of stimuli 1 and 2 and the normalized excitatory incoming weights within assemblies are compared in the 50-vs-50 case. <italic>right</italic>, Same as in the left figure, but in the 30-vs-70 case.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Within-assembly connections encode the probability structures.</title><p>The mean ratios of spontaneous population firing rates without between-assembly connections are shown. The connections were removed after the network learned to encode the stimulus probabilities shown in <xref ref-type="fig" rid="fig3">Figure 3f</xref>. Error bars indicate the SDs over five trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Inhibitory plasticity during learning is necessary to stabilize spontaneous activity.</title><p>(<bold>a</bold>) Spontaneous activity of learned network with non-plastic inhibitory connections during learning. (<bold>b</bold>) Same as in a, but with plastic inhibitory connections.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Crucial roles of inhibitory plasticity in prior learning.</title><p>We first trained the network models with five external stimuli. (<bold>a</bold>) Then, we terminated the stimuli at −50 s and waited until 0 s for the recovery of network activity through the renormalization process (<xref ref-type="disp-formula" rid="equ12">Equation 10</xref>) with all plasticity rules turned off. We turned on the plasticity of M at time 0 s. We kept the plasticity of G turned off in the truncated model (blue), while we turned on the G-plasticity in the control model (magenta). (<bold>b</bold>) Left, the time evolution of the difference between the average within-assembly coherence and the average between-assembly coherence was plotted for the control (magenta) and truncated (blue) models. Larger differences imply more robust cell assemblies. Error bars indicate the SDs over five trials. Right, activity coherences between neurons are shown at the indicated times. (<bold>c</bold>) The time-varying normalized firing rate of a neuron (gray) and the values predicted by recurrent synaptic inputs (top) and lateral inhibition (bottom) are shown for the control model. (<bold>d</bold>) Similar plots are shown for the truncated model. (<bold>e</bold>) Changes in prediction errors in the control (magenta) and truncated (blue) models are shown for recurrent synaptic inputs (top) and lateral inhibition (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Distinct assembly replay after sequence.</title><p>(<bold>a</bold>) The network was trained repetitively with a fixed sequence (i.e., 1–2–3–4–5). (<bold>b</bold>) An example of spontaneous activity after learning. The assemblies are reactivated almost independently. (<bold>c</bold>) The learned recurrent connection matrix shows stronger intra-assembly and weaker inter-assembly connections.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig4-figsupp4-v1.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Role of dynamical variable <italic>h</italic> in spontaneous replay of assemblies.</title><p>(<bold>a</bold>) An example of assembly dynamics (solid) and dynamical variables <italic>h</italic> (dashed) are shown. Each color refers to one of the five assemblies, and red curves are highlighted for visualization purposes. The dynamical variables show an abrupt increase when the corresponding assembly has peak activity, and a slow decrease otherwise. Note that the dynamics of <italic>h</italic> corresponding to assemblies that do not show large activity peaks decay slowly almost everywhere without showing a significant increase (e.g., the green dashed line). Curves show the averaged values over the individual assemblies, and shaded areas show the standard error. (<bold>b</bold>) The mean ratios of spontaneous population firing rates calculated with fixed <italic>h</italic> variables are shown. The network was first trained to encode the stimulus probabilities shown in <xref ref-type="fig" rid="fig3">Figure 3f</xref> and the <italic>h</italic> values were then fixed during spontaneous activity. The ratios capture the increasing tendency of the true probability distribution in <xref ref-type="fig" rid="fig3">Figure 3f</xref> with degraded accuracies, especially in assemblies 4 and 5. Error bars indicate the SDs over five trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig4-figsupp5-v1.tif"/></fig><fig id="fig4s6" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 6.</label><caption><title>Learning of multivariate priors with assemblies.</title><p>(<bold>a</bold>) Network neurons were separated into two populations receiving different groups of feedforward inputs (left). Subnetwork A received stimuli 1 (S1) and 2 (S2), each presented one at a time with probability 1/2. Subnetwork B received stimuli 3 (S3) or 4 (S4) exclusively when subnetwork A was also stimulated. S3 or S4 was sampled at each presentation according to the probability distribution conditioned on the stimulus presented to subnetwork A (middle and right). (<bold>b</bold>) Raster plot of evoked activity after training. Each subnetwork formed two assemblies responding to different preferred stimuli. Shaded areas with four colors indicate the duration of stimuli given to the two subnetworks. (<bold>c</bold>) The connection matrix self-organized among the cell assemblies is shown. (<bold>d</bold>) The activities of the four assemblies in the presence of S1 and S2 but not S3 and S4 are shown. Despite the absence of stimuli, subnetwork B replayed the assemblies encoding S3 and S4 when subnetwork A was activated by S1 or S2. (<bold>e</bold>) Activities of assemblies 3 and 4 in subnetwork B varied with the stimulus presented to subnetwork A. Each data point corresponds to one of 20 independent stimulus presentations. Error bars represent SDs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig4-figsupp6-v1.tif"/></fig></fig-group><p>Since incoming weights increased more significantly in the cell assembly activated by a more frequent stimulus (i.e., the assembly encoding stimulus 2 in the 30-vs-70 case), we expect that the degree of positive shifts in incoming weight distributions will reflect stimulus probabilities. To examine whether this is indeed the case, we computed the sum of total excitatory incoming weights (i.e., the sum of positive elements of M) over neurons belonging to each assembly after training. We then normalized these excitatory incoming weights over the two assemblies. Interestingly, we found that the normalized excitatory incoming weights for the two assemblies well approximate the empirical probabilities of the two stimuli in both the 50-vs-50 and 30-vs-70 cases (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). These analyses revealed that recurrent connections learned within assemblies encode biases in probabilistic sensory experiences. Indeed, the elimination of between-assembly excitatory connections did not significantly affect the replay probabilities, as the sampling is driven by strong within-assembly recurrent inputs after learning (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p></sec><sec id="s2-6"><title>Roles of inhibitory plasticity for stabilizing cell assemblies</title><p>Experimental and computational results suggest that inhibitory synapses are more robust to spontaneous activity than excitatory synapses and are crucial for maintaining cortical circuit function (<xref ref-type="bibr" rid="bib50">Mongillo et al., 2018</xref>). To see the crucial role of the inhibitory plasticity of G for cell-assembly formation, we compared the spontaneously driven activities in the learned network between two cases, plastic inhibitory connection G versus fixed G, in the 30-vs-70 case. The results show that only a single, highly active assembly self-organizes for fixed inhibitory synapses (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2a</xref>). In contrast, such unstable dynamics do not emerge from plastic inhibitory synapses (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2b</xref>), suggesting the crucial role of inhibitory plasticity in stabilizing spontaneous activity.</p><p>To further clarify the functional role of inhibitory plasticity in regulating spontaneous activity, we compared how the self-organized assembly structure of recurrent connections <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$\boldsymbol M$\end{document}</tex-math></alternatives></inline-formula> evolves in the two simulation settings shown in <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3a</xref>. In the control model, we turned off the plasticity of <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$\boldsymbol G$\end{document}</tex-math></alternatives></inline-formula> for a while after the cessation of external stimuli but again switched it on, as was previously in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The cell-assembly structure initially dissipated but eventually reached a well-defined equilibrium structure (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3b</xref>, magenta). Consistent with this, the postsynaptic potentials mediated by connections <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$\boldsymbol M$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$\boldsymbol G$\end{document}</tex-math></alternatives></inline-formula> predicted the normalized firing rate of a postsynaptic excitatory neuron in the control model (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3c</xref>). In striking contrast, the cell-assembly structure rapidly dissipated in the truncated model in which the G-plasticity was kept turned off after the cessation of external stimuli (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3b</xref>, blue). Accordingly, the postsynaptic potentials induced by <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$\boldsymbol M$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$\boldsymbol G$\end{document}</tex-math></alternatives></inline-formula>, so was the normalized firing rate, evolved into trivial solutions and almost vanished in the truncated model (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3d</xref>). Only the control model, but not the truncated model, could maintain prediction errors small and nearly constant after the termination of the stimuli (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3e</xref>). These results indicate that maintaining the learned representations requires the continuous tuning of within-assembly inhibition.</p></sec><sec id="s2-7"><title>The role of homeostatic regulation of neural activities</title><p>As indicated by the weak couplings between cell assemblies, the present mechanism of probability learning differs from the conventional sequence learning mechanisms. Consistent with this, the network trained repetitively by a fixed sequence of patterned inputs does not exhibit stereotyped sequential transitions among cell assemblies (due to the lack of strong inter-assembly excitatory connections; <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). Indeed, the probability-encoding spontaneous activity emerges in the present model mainly from the within-assembly dynamics driven by strong within-assembly reverberating synaptic input. However, homeostatic variable <inline-formula><alternatives><mml:math id="inf20"><mml:mi>h</mml:mi></mml:math><tex-math id="inft20">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> also plays a role in maintaining a stable spontaneous network activity after learning (see <xref ref-type="fig" rid="fig2">Figure 2d</xref>; activity pattern from 5 to 10 s). This is achieved by the time evolution of <inline-formula><alternatives><mml:math id="inf21"><mml:mi>h</mml:mi></mml:math><tex-math id="inft21">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula>, which maintains the firing rate of each neuron in a suitable range by adjusting the threshold and gain of the somatic sigmoidal response function (<xref ref-type="fig" rid="fig1">Figure 1b</xref>).</p><p>Therefore, we explored the role of the homeostatic variable in learning an accurate internal model of the sensory environment. In each neuron, the variable <inline-formula><alternatives><mml:math id="inf22"><mml:mi>h</mml:mi></mml:math><tex-math id="inft22">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> is updated whenever the membrane potential undergoes an abrupt increase (<xref ref-type="disp-formula" rid="equ8">Equation 6</xref>). Therefore, the time evolution of <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> monitors the activity of each neuron over the timescale of seconds, which in turn regulates the neural activity by controlling the activation function (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5a</xref>; <xref ref-type="disp-formula" rid="equ6 equ7">Equations 4 and 5</xref>). When the instantaneous value of <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> is high, the neuron’s excitability is lowered (namely, the gain and threshold of the response function are decreased or increased, respectively: see <xref ref-type="disp-formula" rid="equ8 equ9 equ10">Equations 6–8</xref>). This activity regulation is crucial to avoid the trivial solution of the plasticity rules (<xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>) but not critical for sampling with appropriate probabilities. Actually, a model with a fixed value of <inline-formula><alternatives><mml:math id="inf25"><mml:mi>h</mml:mi></mml:math><tex-math id="inft25">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> still showed spontaneous replay, although the true probability distribution was estimated less accurately (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5b</xref>: <xref ref-type="fig" rid="fig3">Figure 3f</xref>).</p></sec><sec id="s2-8"><title>Learning conditioned prior distributions</title><p>The predictive coding hypothesizes that top–down input from higher cortical areas provides prior knowledge about computations in lower cortical areas. This implies in the brain’s hierarchical computation that the top–down input conditions the prior distributions in local cortical areas to those relevant to the given context. The proposed learning rules can account for how a conditioned input from other cortical areas conditions the prior distribution in a local cortical circuit.</p><p>The neural network consists of two mutually interacting non-overlapping subnetworks of equal sizes, where the subnetworks may represent different cortical areas (<xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6a</xref>). Subnetwork A was randomly exposed to stimuli 1 and 2 (S1 and S2) with equal probabilities 1/2, whereas subnetwork B was to stimuli 3 and 4 (S3 and S4) with the conditional probabilities 1/3 and 2/3 if S1 was presented to subnetwork A and the conditional probabilities 2/3 and 1/3 if S2 was presented to subnetwork A. After learning, the network model self-organized four cell assemblies, each of which responded preferentially to one of the four stimuli (<xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6b</xref>). Consistent with this, the self-organized connection matrix represented strong within-assembly connections within each cell assembly and weak between-assembly connections (<xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6c</xref>). Note that between-assembly connections were inhibitory between assemblies encoding mutually exclusive stimuli, i.e., S1 and S2 and S3 and S4, as they should be. Now, we turned off S3 and S4 to subnetwork B and only applied S1 or S2 to subnetwork A, each at one time. Applying the same stimulus (i.e., S1 or S2) to subnetwork A activated either S3- or S4-coding cell assembly in subnetwork B in a probabilistic manner (<xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6d</xref>). The cell assemblies evoked in subnetwork B by S1 or S2 to subnetwork A varied the total firing rates approximately in proportion to the conditional probabilities (e.g., P(S3|S1) = 1/3 vs. P(S4|S1) = 2/3) used during learning (<xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6e</xref>). Note that S3- and S4-coding cell assemblies could become simultaneously active to represent the desired activation probabilities (e.g., a vertical arrow in <xref ref-type="fig" rid="fig4s6">Figure 4—figure supplement 6d</xref>). Together, these results indicate that our network can learn prior distributions conditioned by additional inputs through different pathways.</p></sec><sec id="s2-9"><title>Replication of biased perceptual decision making in monkeys</title><p>Prior knowledge about the environment often biases our percept of the external world. For instance, if we know that two possible stimuli exist and that stimulus A appears more often than stimulus B, we tend to feel that a given stimulus is more likely to be stimulus A than stimulus B. Previously, a similar bias was quantitatively studied in monkeys performing a perceptual decision-making task (<xref ref-type="bibr" rid="bib21">Hanks et al., 2011</xref>). In the experiment, monkeys had to judge the direction (right or left) of the coherent motion of moving dots on a display. When both directions of coherent motion appeared randomly during learning, the monkey showed unbiased choice behaviors. However, if the frequencies of the two motion directions were different, the monkey’s choice was biased toward the direction of a more frequent motion stimulus.</p><p>We constructed a network model shown in <xref ref-type="fig" rid="fig5">Figure 5a</xref> to examine whether the present mechanism of spontaneous replay could account for the behavioral bias. The model comprises a recurrent network similar to that used in <xref ref-type="fig" rid="fig2">Figure 2</xref> and two input neuron groups, L and R, encoding leftward or rightward coherent dot movements, respectively. We modulated the firing rates of these input neurons in proportion to the coherence of moving dots (Materials and methods). During learning, we trained this model with external stimuli having input coherence Coh of either –0.5 or +0.5 (Materials and methods), where all dots move leftward in the former or rightward in the latter. In so doing, we mimicked the two protocols used in the behavioral experiment of monkeys: in the 50:50 protocol, two stimuli with Coh = ±0.5 were presented randomly with equal probabilities, while in the 80:20 protocol, stimuli with Coh = +0.5 and –0.5 were delivered with probabilities of 80% and 20%, respectively. In the 80:20 protocol, stimuli were highly biased toward a coherent rightward motion.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Simulations of biased perception of visual motion coherence.</title><p>(<bold>a</bold>) The network model simulated perceptual decision-making of coherence in random dot motion patterns. In the network shown here, network neurons have already learned two assemblies encoding leftward or rightward movements from input neuron groups L and R. The firing rates of input neuron groups were modulated according to the coherence level Coh of random dot motion patterns (Materials and Methods). (<bold>b</bold>) The choice probabilities of monkeys (circles) and the network model (solid lines) are plotted against the motion coherence in two learning protocols with different prior probabilities. The experimental data were taken from <xref ref-type="bibr" rid="bib21">Hanks et al., 2011</xref>. In the 50:50 protocol, moving dots in the “R” (Coh = 0.5) and “L” (Coh = -0.5) directions were presented randomly with equal probabilities, while in the 80:20 protocol, the “R” and “L” directions were trained with 80% and 20% probabilities, respectively. Shaded areas represent SDs over 20 independent simulations. The computational and experimental results show surprising coincidence without curve fitting. (<bold>c</bold>) Spontaneous and evoked activities of the trained networks are shown for the 50:50 (left) and 80:20 (right) protocols. Evoked responses were calculated for three levels of coherence: Coh = -50%, 0%, and 50%. In both protocols, the activity ratio in spontaneous activity matches the prior probability and gives the baseline for evoked responses. In the 80:20 protocol, the biased priors of “R” and “L” motion stimuli shift the activity ratio in spontaneous activity to an “R”-dominant regime.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig5-v1.tif"/></fig><p>The network model could explain the biased choices of monkeys surprisingly well. In either training protocol, the recurrent network self-organized two cell assemblies responding selectively to one of the R and L input neuron groups. Then, we examined whether the responses of the self-organized network are consistent with experimental observations by stimulating it with external inputs having various degrees of input coherence. The resultant psychometric curves almost perfectly coincide with those obtained in the experiment (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). We note that the psychometric curves of the model do not significantly depend on the specific choices of parameter values as far as the network learned stable spontaneous activity. We did not perform any curve fitting to experimental data, implying that the psychometric curves are free from parameter finetuning.</p><p>Biases in the psychometric curves emerged from biased firing rates of spontaneous activity of the self-organized cell assemblies. To show this, we investigated how the activities of the two self-organized cell assemblies change before and after the onset of test stimuli in three relatively simple cases, i.e., Coh = –0.5, 0, and +0.5. <xref ref-type="fig" rid="fig5">Figure 5c</xref> shows the activity ratio AR between the R-encoding cell assembly and the entire network (Materials and methods) in pre-stimulus spontaneous and post-stimulus-evoked activity. When the network was trained in a non-biased fashion (i.e., in the 50:50 protocol), the activity ratio was close to 0.5 in spontaneous activity, implying that the two cell assemblies had similar activity levels. In contrast, when the network was trained in a biased fashion (i.e., in the 80:20 protocol), the activity ratio in spontaneous activity was close to 0.8, implying that the total spontaneous firing rate of R-encoding cell assembly was four times higher than that of L-encoding cell assembly. Our results show that the spontaneous activity generated by the proposed mechanism can account for the precise relationship between motion coherence and perceptual biases in decision making by monkeys.</p></sec><sec id="s2-10"><title>Crucial roles of distinct inhibitory pathways</title><p>The model presented so far lacked biological plausibility in several key aspects. Specifically, we assumed that the recurrent connections <inline-formula><alternatives><mml:math id="inf26"><mml:mi>M</mml:mi></mml:math><tex-math id="inft26">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> could change its sign through plasticity and be either excitatory or inhibitory, while the inhibitory connection <inline-formula><alternatives><mml:math id="inf27"><mml:mi>G</mml:mi></mml:math><tex-math id="inft27">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> was restricted to being inhibitory only. This setting does not reflect the biological constraint that synapses maintain a consistent excitatory or inhibitory type. Furthermore, due to this unconstrained recurrent connectivity <inline-formula><alternatives><mml:math id="inf28"><mml:mi>M</mml:mi></mml:math><tex-math id="inft28">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula>, the original model had two types of inhibitory connections (i.e., the negative part of <inline-formula><alternatives><mml:math id="inf29"><mml:mi>M</mml:mi></mml:math><tex-math id="inft29">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> and the inhibitory connection <inline-formula><alternatives><mml:math id="inf30"><mml:mi>G</mml:mi></mml:math><tex-math id="inft30">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula>) without providing a clear computational role for each type of inhibition.</p><p>To address these limitations and to understand the role of the two types of inhibition, we considered a novel architecture in which all recurrent connections are constrained to be either exclusively excitatory or inhibitory, maintaining their sign throughout the learning process. The refined model includes two different types of inhibitory connections (i.e., <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$\rm M_{inh}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf32"><mml:mi>G</mml:mi></mml:math><tex-math id="inft32">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula>), each serving a specific computational purpose: minimizing prediction error and maintaining the excitatory–inhibitory balance. In combination with the excitatory connection <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$\rm M_{exc}$\end{document}</tex-math></alternatives></inline-formula>, the <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$\rm M_{inh}$\end{document}</tex-math></alternatives></inline-formula> connections are trained to minimize the prediction error between somatic and dendritic activity, as considered in the original M connection in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We found that the trained <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$\rm M_{inh}$\end{document}</tex-math></alternatives></inline-formula> connections introduce competition among cell-assembly activities by forming strong connections between assemblies (<xref ref-type="fig" rid="fig6">Figure 6b</xref>), allowing the network to effectively sample and replay the activities of individual assemblies. In contrast, inhibitory connections <inline-formula><alternatives><mml:math id="inf36"><mml:mi>G</mml:mi></mml:math><tex-math id="inft36">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> were trained to balance network dynamics, as in the original setting. We found that the inhibitory <inline-formula><alternatives><mml:math id="inf37"><mml:mi>G</mml:mi></mml:math><tex-math id="inft37">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> connections form strong intra-assembly inhibition (<xref ref-type="fig" rid="fig6">Figure 6c</xref>), which balances the strong excitatory connections that arise within cell assemblies through plasticity (<xref ref-type="fig" rid="fig6">Figure 6a</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>A network model with distinct excitatory and inhibitory connections.</title><p>(a) Strong excitatory connections were formed within assemblies. (b) The first type of recurrent inhibitory connections, M<sub>inh</sub>, became stronger between assemblies, enhancing assembly competition. (c) The second type of inhibitory connections G were strengthened within assemblies to balance the strong excitatory inputs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig6-v1.tif"/></fig><p>In summary, the dual inhibitory mechanism allows the network to perform the reactivation of different cell assemblies while regulating their internal dynamics. The prediction-error-minimizing inhibitory connections <inline-formula><alternatives><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft38">\begin{document}$M_{inh}$\end{document}</tex-math></alternatives></inline-formula> facilitate selecting and activating specific assemblies through competition such that the learned probabilities are replayed. In contrast, the network-balancing inhibitory connections <inline-formula><alternatives><mml:math id="inf39"><mml:mi>G</mml:mi></mml:math><tex-math id="inft39">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> prevent runaway excitation within active assemblies.</p></sec><sec id="s2-11"><title>An elaborate network model with distinct excitatory and inhibitory neuron pools</title><p>The predictive learning rule performed well in training the nDL model to learn the probabilistic structure of the stimulus-evoked activity patterns. However, whether the same learning rule works in a more realistic neural network is yet to be investigated. To examine this, we constructed an elaborate network model (DL model) consisting of distinct excitatory and inhibitory neuron pools, obeying Dale’s law (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). The nDL model suggested the essential roles of inhibitory plasticity in maintaining excitation–inhibition balance and generating an appropriate number of cell assemblies. To achieve these functions, inhibitory neurons in the DL model project to excitatory and other inhibitory neurons via two synaptic paths (<xref ref-type="fig" rid="fig7">Figure 7b</xref>), motivated by the results shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. In path 1, inhibitory connections alone predict the postsynaptic activity, whereas inhibitory and excitatory connections jointly predict the activity of the postsynaptic neuron in path 2 (Materials and methods). All synapses in the DL model are subject to the predictive learning rule. We trained the DL model with three input neuron groups while varying their activation probabilities. As in the nDL model, the DL model self-organized three cell assemblies activated selectively by the three input neuron groups (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1a</xref>). Furthermore, in the absence of external stimuli, the DL model spontaneously replayed these assemblies with the assembly activity ratios in proportion to the occurrence probabilities of the corresponding stimuli during learning (<xref ref-type="fig" rid="fig7">Figure 7c</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The DL model of excitatory and inhibitory cell assemblies.</title><p>(a) This model consists of distinct excitatory and inhibitory neuron pools, obeying Dale’s law. (b) Each inhibitory neuron projects to another neuron X through two inhibitory paths, path 1 and path 2, where the index X refers to an excitatory or an inhibitory postsynaptic neuron. Hexagons represent minimal units for prediction and learning in the neuron model and may correspond to dendrites, which were not modeled explicitly. (c) The probability ratios estimated by numerical simulations are plotted for the assembly activity ratios (purple), firing rate ratios (cyan), and assembly size ratios (green) as functions of the true probability ratio of external stimuli. Error bars indicate SEs calculated over five simulation trials with different initial states of neurons and synaptic weights in each parameter setting. (d) Inhibitory connection matrices are shown for path 1 and path 2. (e) The mean weights of self-organized synapses on excitatory and inhibitory postsynaptic neurons are shown. (f) Within-assembly and between-assembly connectivity patterns of excitatory and inhibitory neurons are shown. Colors indicate three cell assemblies self-organized. (g) The strengths of lateral inhibitions within-(W/N) and between-assemblies (B/N) are shown for paths 1 and 2. Horizontal bars show the medians and quartiles. (h) The resultant connectivity pattern suggests an effective competitive network between excitatory assemblies with self-(within-assembly) and lateral (between-assembly) inhibition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>The coexistence of the two inhibitory paths is crucial for learning.</title><p>(<bold>a</bold>) A typical spike raster of stimulus-evoked responses is presented for the elaborate network model shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> . (<bold>b</bold>) A spike raster of stimulus-evoked responses is shown for simulations of the elaborate model without inhibitory path 2. Inhibitory connections were modifiable in path 1. (<bold>c</bold>) A similar spike raster is presented for simulations of the elaborate model without inhibitory path 1. Inhibitory connections were modifiable in path 2. The results shown in b and c demonstrate that the network model fails to self-organize the cell assemblies encoding the different stimuli when it lacks one of the two inhibitory paths.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92712-fig7-figsupp1-v1.tif"/></fig></fig-group><p>The two inhibitory paths divided their labors consistent with the results shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. To see this, we investigated the connectivity structures learned by these paths. In path 1, inhibitory connections were primarily found on excitatory neurons in the same assemblies (<xref ref-type="fig" rid="fig7">Figure 7d</xref>, top). In contrast, in path 2, inhibitory connections were stronger on excitatory neurons in different assemblies than those in the same assemblies (<xref ref-type="fig" rid="fig7">Figure 7d</xref>, bottom). On both excitatory and inhibitory neurons, the total inhibition (i.e., path 1 + path 2) was balanced with excitation (<xref ref-type="fig" rid="fig7">Figure 7e</xref>). <xref ref-type="fig" rid="fig7">Figure 7f</xref> summarizes the connectivity structure of the DL model. Excitatory neurons in a cell-assembly project to inhibitory neurons in the same assembly. Then, these inhibitory neurons project back to excitatory neurons in the same or different assemblies via paths 1 and 2. Interestingly, lateral inhibition through path 1 is more potent between excitatory neurons within each cell assembly than between different assemblies (<xref ref-type="fig" rid="fig7">Figure 7g</xref>). In contrast, path 2 mediates equally strong within- and between-assembly inhibition.</p><p>We can understand the necessity of the two inhibitory paths based on the dynamical properties of competitive neural networks. <xref ref-type="fig" rid="fig7">Figure 7h</xref> displays the effective competitive network of excitatory cell assemblies suggested by the above results. Both paths 1 and 2 contribute to within-assembly inhibition among excitatory neurons, whereas between-assembly inhibition (i.e., lateral inhibition) mainly comes from path 2. In a competitive network, the lateral inhibition to self-inhibition strength ratio determines the number of winners having non-vanishing activities: the higher the ratio is, the smaller the number of winners is (<xref ref-type="bibr" rid="bib18">Fukai and Tanaka, 1997</xref>). Therefore, self-organizing the same number of excitatory cell assemblies as that of external stimuli requires tuning the balance between the within- and between-assembly inhibitions. This tuning during learning is likely easier when the network has two independently learnable inhibitory circuits. Indeed, a network model with only one inhibitory path rarely succeeded in encoding and replaying all stimuli used in learning (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1b, c</xref>).</p><p>In summary, we have shown the roles of distinct recurrent inhibitory connections. Using a network consisting of excitatory and inhibitory populations, we have shown that distinct inhibitory circuits are necessary to generate within- and between-assembly competition crucial to maintain the stability of learned multiple assemblies.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Having proper generative models is crucial for accurately predicting statistical events. The brain is thought to improve the prediction accuracy of inference by learning internal generative models of the environment. These models are presumably generated through multiple mechanisms. For instance, the predictive coding hypothesizes that top–down cortical inputs provide lower sensory areas with prior information about sensory experiences (<xref ref-type="bibr" rid="bib16">Friston, 2010</xref>; <xref ref-type="bibr" rid="bib7">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Keller and Mrsic-Flogel, 2018</xref>). However, experimental evidence also suggests that spontaneous activity represents an optimal model of the environment in sensory cortices. This study proposed a biologically plausible mechanism to learn such a model, or priors for experiences, with the brain’s internal dynamics.</p><p>Our model adopted a single predictive learning principle for the plasticity of excitatory and inhibitory synapses to learn the replay of probabilistic experiences. On each neuron, excitatory and inhibitory synaptic weights undergo plastic changes to improve their independent predictions on the cell’s firing. This was done by minimizing the mismatch between the output firing rate and the network predictions (<xref ref-type="disp-formula" rid="equ11 equ19">Equations 9 and 17</xref>). This simple learning rule showed excellent performance in a simplified network model and in a more realistic model obeying Dale’s law. The latter model predicts a division of labor between two inhibitory paths. Intriguingly, the inhibitory path 2 of this model resembles interpyramidal inhibitory connections driven directly by nearby pyramidal cells (<xref ref-type="bibr" rid="bib54">Ren et al., 2007</xref>). In both models, inhibitory synaptic plasticity plays a crucial role in learning an accurate internal model by maintaining excitation–inhibition balance and decorrelating cell-assembly activities (<xref ref-type="bibr" rid="bib65">Vogels et al., 2013</xref>; <xref ref-type="bibr" rid="bib57">Sprekeler, 2017</xref>). It should be noted that while several network models that perform error-based computations like ours exploit only inhibitory recurrent plasticity (<xref ref-type="bibr" rid="bib47">Mikulasch et al., 2021</xref>; <xref ref-type="bibr" rid="bib44">Mackwood et al., 2021</xref>; <xref ref-type="bibr" rid="bib23">Hertäg and Clopath, 2022</xref>; <xref ref-type="bibr" rid="bib48">Mikulasch et al., 2023</xref>), our model learns to reproduce appropriate statistics by modifying both excitatory and inhibitory recurrent connections.</p><p>Various models have been proposed to account for neural mechanisms of Bayesian computation by the brain (<xref ref-type="bibr" rid="bib62">Tully et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Kappel et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Hiratani and Fukai, 2018</xref>; <xref ref-type="bibr" rid="bib27">Hiratani and Latham, 2020</xref>; <xref ref-type="bibr" rid="bib2">Aitchison et al., 2021</xref>; <xref ref-type="bibr" rid="bib43">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib13">Deneve, 2008</xref>; <xref ref-type="bibr" rid="bib52">Nessler et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Hiratani and Fukai, 2016</xref>; <xref ref-type="bibr" rid="bib28">Huang and Rao, 2016</xref>; <xref ref-type="bibr" rid="bib29">Isomura et al., 2022</xref>; <xref ref-type="bibr" rid="bib16">Friston, 2010</xref>; <xref ref-type="bibr" rid="bib7">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Keller and Mrsic-Flogel, 2018</xref>). Typically, these models embed prior knowledge on sensory experiences into the wiring patterns of afferent (and sometimes also recurrent) synaptic inputs such that these inputs can evoke the learned activity patterns associated with the prior knowledge. The present model differs from the previous models in several aspects: (1) The model segments repeated stimuli to be remembered in an unsupervised fashion; (2) Then it generates cell assemblies encoding the segmented stimuli; (3) Finally, it replays these cell assemblies spontaneously with learned probabilities. Note that the same learning rules enable the network to perform all necessary computations for (1)–(3). To our knowledge, our model is the first to perform all these steps for encoding an optimal model of the environment into spontaneous network activity.</p><p>The present mechanism of memory formation differs from the previous ones that self-organize cell assemblies through Hebbian learning rules (<xref ref-type="bibr" rid="bib64">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="bib24">Hiratani and Fukai, 2014</xref>; <xref ref-type="bibr" rid="bib41">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="bib67">Zenke et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Triplett et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Montangie et al., 2020</xref>). First, these mechanisms did not aim for explicit statistical modeling of the environment. Second, the previous studies suggested that the orchestration of multiple plasticity rules, including inhibitory plasticity and homeostatic synaptic scaling, enables the maintenance of cell assemblies (however, see <xref ref-type="bibr" rid="bib45">Manz and Memmesheimer, 2023</xref>). For instance, in STDP, slight changes in the relative times of pre- and postsynaptic spikes can change the polarity of synaptic modifications, implying that STDP requires a mechanism to keep synaptic weights finite (<xref ref-type="bibr" rid="bib35">Kempter et al., 1999</xref>; <xref ref-type="bibr" rid="bib56">Song et al., 2000</xref>; <xref ref-type="bibr" rid="bib46">Masquelier et al., 2008</xref>). In contrast, our learning rule, which induces either long-term potentiation or depression according to the sign of the prediction error calculated independently within each postsynaptic neuron, does not suffer such instability.</p><p>Our model predicts a novel intracellular process that regulates the neuron’s dynamic range according to the history of its subthreshold dynamics. This process plays two important roles in the statistical modeling of our model. First, it avoids the trivial solution (i.e., the zero-weight solution) of our unsupervised predictive learning by homeostatically regulating neurons' intrinsic excitability. Second, the intracellular process cooperates with reverberating synaptic inputs within each cell assembly to generate spontaneous replay activity. We have shown that intracellular homeostasis enhances the sampling from learned distribution without relying on the recurrences among assemblies. This mechanism contrasts with the previous sampling-based models that rely on the transition dynamics between cell assemblies (<xref ref-type="bibr" rid="bib10">Buesing et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Bill et al., 2015</xref>). How neural systems implement the proposed homeostasis is an open question.</p><p>Previous computational models have demonstrated that recurrent networks with Hebbian-like plasticity can learn appropriate Markovian statistics (<xref ref-type="bibr" rid="bib33">Kappel et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Asabuki and Clopath, 2024</xref>). However, our model differs conceptually from these previous models. Kappel et al. showed that STDP in winner-take-all circuits can approximate online learning of hidden Markov models. A key difference with our model is that their neural representations acquire sequences using Markovian sampling dynamics, whereas our model does not depend on such ordered sampling. Specifically, in their model, sequential sampling arises from the off-diagonal elements learned in the recurrent connections (i.e., between-assembly connections). In contrast, our network learns to generate a stochastic reactivation of cell assemblies solely by within-assembly connections. A similar argument can be made for the Asabuki and Clopath paper as well. Further, while our model introduced plasticity at all types of synaptic connections, the previous model assumed plasticity only at recurrent synapses projecting onto the excitatory neurons. In addition, unlike our model, the cell-assembly memberships need to be preconfigured in the previous model.</p><p>The proposed mechanism can account for the behavioral biases observed in perceptual decision making (<xref ref-type="bibr" rid="bib21">Hanks et al., 2011</xref>). This behavioral experiment quantitatively clarified how the difference in the probability between sensory experiences during learning biases the alternative choice behavior of monkeys. In our model, two cell assemblies encoding the different stimuli are replayed at the total firing rates proportional to the corresponding occurrence probabilities. Our results suggest that the difference in spontaneous firing rates of cell assemblies is sufficient to explain the behavioral biases of monkeys. However, other mechanisms, such as biased top–down input, cannot be excluded.</p><p>What could be the advantages of coding prior distributions into spontaneous activity over other ways of probability coding? First, spontaneous replay activities in lower cortical areas may provide training data for modeling by higher cortical areas, promoting hierarchical statistical modeling in predictive coding. This is analogous to the situation where hippocampal engram cells are replayed to reinforce the activity patterns of cortical engrams for memory consolidation during sleep (<xref ref-type="bibr" rid="bib59">Tonegawa et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Ghandour et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Klinzing et al., 2019</xref>; <xref ref-type="bibr" rid="bib58">Takehara-Nishiuchi, 2021</xref>). Memory reinforcement by activity replay has also been studied in machine intelligence (<xref ref-type="bibr" rid="bib12">Dayan et al., 1995</xref>; <xref ref-type="bibr" rid="bib20">Goodfellow, 2014</xref>; <xref ref-type="bibr" rid="bib42">Luczak et al., 2022</xref>). Second, spontaneous replay of internal models may support knowledge generalization during sleep. It was recently reported that a transitive inference task requires post-learning sleep (<xref ref-type="bibr" rid="bib1">Abdou et al., 2021</xref>). In this task, mice had to infer a correct reward delivery rule in a novel behavioral situation from the outcomes of past experiences. The mice failed to generalize the learned rules if the activity of the anterior cingulate cortex was suppressed during post-learning sleep, suggesting that dynamic interactions among rule-coding cortical neurons in spontaneous activity are crucial for rule generalization. Clarifying how spontaneous brain activity generalizes the learned internal models is an intriguing open question.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Neural network model</title><p>Below, we first describe the model architecture and learning rule for the nDL model (i.e., single population violating Dale’s law). Details of the simulation of distinct excitatory and inhibitory populations will be explained later. Unless otherwise stated, recurrent neural networks used in this study consist of <inline-formula><alternatives><mml:math id="inf40"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mo>=</mml:mo><mml:mn>500</mml:mn><mml:mo>)</mml:mo></mml:math><tex-math id="inft40">\begin{document}$N\left (=500\right)$\end{document}</tex-math></alternatives></inline-formula> Poisson neurons, which generate spikes according to a non-stationary Poisson process with rate <inline-formula><alternatives><mml:math id="inf41"><mml:mover accent="true"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft41">\begin{document}$\overset{\hat }{\varphi }\left (u\right)$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf42"><mml:mover accent="true"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mo>∙</mml:mo><mml:mo>)</mml:mo></mml:math><tex-math id="inft42">\begin{document}$\overset{\hat }{\varphi }\left (\cdot \right)$\end{document}</tex-math></alternatives></inline-formula> is a dynamics sigmoidal function, which we will explain later. The membrane potential <inline-formula><alternatives><mml:math id="inf43"><mml:mi>u</mml:mi></mml:math><tex-math id="inft43">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula> of neuron <inline-formula><alternatives><mml:math id="inf44"><mml:mi>i</mml:mi></mml:math><tex-math id="inft44">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> at time <inline-formula><alternatives><mml:math id="inf45"><mml:mi>t</mml:mi></mml:math><tex-math id="inft45">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> is given as follows:<disp-formula id="equ2"><label>(1)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle u_{i}\left (t\right)=\sum _{k=1}^{K}{\rm W}_{ik}x_{k}\left (t\right)+\sum _{k=1}^{N}\left ({\rm M}_{ik}- {\rm G}_{ik}\right)y_{k}\left (t\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf46"><mml:mi>K</mml:mi></mml:math><tex-math id="inft46">\begin{document}$K$\end{document}</tex-math></alternatives></inline-formula> is the number of input neurons. In some simulations, the network model had more than one input neuron group, although the number of input neuron groups is not explicitly shown in <xref ref-type="disp-formula" rid="equ2">Equation 1</xref>. Three matrices <inline-formula><alternatives><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$W\in \mathbb{R} ^{N\times K}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft48">\begin{document}$M\in \mathbb R^{N\times N}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$G\in \mathbb R^{N\times N}$\end{document}</tex-math></alternatives></inline-formula> represent the weights of afferent synaptic connections, recurrent synaptic connections, and inhibitory-only connections, respectively, on neurons in the recurrent network. These synaptic connections are all-to-all. In terms of the kernel function<disp-formula id="equ3"><label>(2)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle \varepsilon \left (s\right)={\rm exp}\left (- s/\tau \right)\cdot \Theta \left (s\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>recurrent input and afferent input to neuron <inline-formula><alternatives><mml:math id="inf50"><mml:mi>i</mml:mi></mml:math><tex-math id="inft50">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> are calculated as<disp-formula id="equ4"><label>(3a)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle x_{i}\left (t\right)=\sum _{t^{'}\in t_{\rm aff}^{\rm f}}\varepsilon \left (t- t^{'}\right),$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ5"><label>(3b)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle y_{i}\left (t\right)=\sum _{t^{'}\in t_{\rm rec}^{\rm f}}\varepsilon \left (t- t^{'}\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf51"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft51">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> stands for the membrane time constant, <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$t_{\rm aff}^{\rm f}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$t_{\rm rec}^{\rm f}$\end{document}</tex-math></alternatives></inline-formula> for the time sets of afferent and recurrent presynaptic spikes, and <inline-formula><alternatives><mml:math id="inf54"><mml:mi>Θ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>∙</mml:mo></mml:mrow></mml:mfenced></mml:math><tex-math id="inft54">\begin{document}$\Theta \left (\cdot \right)$\end{document}</tex-math></alternatives></inline-formula> for the Heaviside function. Throughout this study, <inline-formula><alternatives><mml:math id="inf55"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft55">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> = 15 ms.</p><p>The instantaneous firing rate <inline-formula><alternatives><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft56">\begin{document}$f_{i}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> of each neuron is given as<disp-formula id="equ6"><label>(4)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>φ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle f_{i}\left (t\right)={\hat \varphi }\left (u_{i}\left (t\right);h_{i}\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>in terms of a dynamical sigmoidal response function <inline-formula><alternatives><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>φ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}${\hat \varphi }$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ7"><label>(5)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>φ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle {\hat \varphi }\left (u_{i};h_{i}\right)=\varphi _{0}\left [1+\exp \left [g\beta \left (h_{i}\right)\left (- u_{i}+g\theta \left (h_{i}\right)\right)\right ]\right ]^{- 1},$$\end{document}</tex-math></alternatives></disp-formula></p><p>with a constant value of <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$g=3$\end{document}</tex-math></alternatives></inline-formula> and we have dropped the explicit time dependence in our notation for the sake of simplicity. Here, the dynamical variable <inline-formula><alternatives><mml:math id="inf59"><mml:mi>h</mml:mi></mml:math><tex-math id="inft59">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> is determined by the history of the membrane potential:<disp-formula id="equ8"><label>(6)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mover><mml:mi>h</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle \begin{array}{ll}\overset{˙}{h}_{i}=- \tau _{h}^{- 1}h_{i}, &amp; {\rm if}\,h_{i}\gt u_{i},\\h_{i}\leftarrow u_{i}, &amp; \rm otherwise.\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>The maximum instantaneous firing rate <inline-formula><alternatives><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft60">\begin{document}$\varphi _{0}$\end{document}</tex-math></alternatives></inline-formula> is 50 Hz and <inline-formula><alternatives><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math><tex-math id="inft61">\begin{document}$\tau _{h}=10$\end{document}</tex-math></alternatives></inline-formula> s. Through <xref ref-type="disp-formula" rid="equ8">Equation 6</xref>, <inline-formula><alternatives><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft62">\begin{document}$h_{i}$\end{document}</tex-math></alternatives></inline-formula> tracks the maximum value of the membrane potential <inline-formula><alternatives><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft63">\begin{document}$u_{i}$\end{document}</tex-math></alternatives></inline-formula> in a time window of approximately the length <inline-formula><alternatives><mml:math id="inf64"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft64">\begin{document}$\tau _{h}$\end{document}</tex-math></alternatives></inline-formula> in the immediate past. The value of <inline-formula><alternatives><mml:math id="inf65"><mml:mi>h</mml:mi></mml:math><tex-math id="inft65">\begin{document}$h$\end{document}</tex-math></alternatives></inline-formula> is utilized to regulate the gain <inline-formula><alternatives><mml:math id="inf66"><mml:mi>β</mml:mi></mml:math><tex-math id="inft66">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula> and threshold <inline-formula><alternatives><mml:math id="inf67"><mml:mi>θ</mml:mi></mml:math><tex-math id="inft67">\begin{document}$\theta $\end{document}</tex-math></alternatives></inline-formula> of the sigmoidal response function as follows:<disp-formula id="equ9"><label>(7)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle \beta \left (h_{i}\right)=h_{i}^{- 1}\beta _{0},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ10"><label>(8)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle \theta \left (h_{i}\right)=h_{i}\theta _{0},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the values of constant parameters are <inline-formula><alternatives><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math><tex-math id="inft68">\begin{document}$\beta _{0}=5$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft69">\begin{document}$\theta _{0}=1$\end{document}</tex-math></alternatives></inline-formula>. Neuron <inline-formula><alternatives><mml:math id="inf70"><mml:mi>i</mml:mi></mml:math><tex-math id="inft70">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> generates a Poisson spike train at the instantaneous firing rate of <inline-formula><alternatives><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft71">\begin{document}$f_{i}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>. While a small value of h leads to a steep slope of our activation function (<xref ref-type="disp-formula" rid="equ9">Equation 7</xref>), we have shown numerically that this does not lead to a problem in neural dynamics. Further, the saturation part of the sigmoidal function is crucial for stable formation of assemblies.</p></sec><sec id="s4-2"><title>Learning rules</title><p>We first explain the plasticity rule for feedforward connections. Synaptic connections were modified to minimize the Kullback–Leibler divergence (KL-divergence) between two Poisson distributions associated with the neuron’s output and the feedforward activity over a sufficiently long period <inline-formula><alternatives><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ11"><label>(9)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \mathcal L_{W}=\int _{0}^{T}dt\sum _{i=1}^{N}D_{KL}\left [f_{i}\left (t\right)\| \varphi \left (v_{i}^{W}\left (t\right)\right)\right ],$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}${\rm v}_{i}^{W}$\end{document}</tex-math></alternatives></inline-formula> is a feedforward prediction of a firing rate, defined as:<disp-formula id="equ12"><label>(10)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle {\rm v}_{i}^{W}=\sum _{j=1}^{K}W_{ij}\cdot x_{j},$$\end{document}</tex-math></alternatives></disp-formula></p><p>and <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$f_{i}$\end{document}</tex-math></alternatives></inline-formula> is the firing rate of <inline-formula><alternatives><mml:math id="inf75"><mml:mi>i</mml:mi></mml:math><tex-math id="inft75">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>th neuron. The function <inline-formula><alternatives><mml:math id="inf76"><mml:mi>φ</mml:mi></mml:math><tex-math id="inft76">\begin{document}$\varphi $\end{document}</tex-math></alternatives></inline-formula> is a static sigmoidal function, defined as<disp-formula id="equ13"><label>(11)</label><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle \varphi \left (v_{i}\right)=\varphi _{0}\left [1+\exp \left [g\beta _{0}\left (- v_{i}+g\theta _{0}\right)\right ]\right ]^{- 1}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The above cost function evaluates to what extent the feedforward potential predicts the activity of postsynaptic neurons (<xref ref-type="bibr" rid="bib3">Asabuki and Fukai, 2020</xref>). We have previously shown that taking the gradient of the cost function in <xref ref-type="disp-formula" rid="equ11">Equation 9</xref> derives the online plasticity rule for the feedforward connections as<disp-formula id="equ14"><label>(12)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle \Delta W_{ij}=\eta \varphi _{0}^{- 1}\left (1- \frac{\varphi \left (v_{i}^{W}\right)}{\varphi _{0}}\right)\left [f_{i}- \varphi \left (v_{i}^{W}\right)\right ],$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$\epsilon $\end{document}</tex-math></alternatives></inline-formula> is a learning rate and was set to <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$\epsilon =10^{- 4}$\end{document}</tex-math></alternatives></inline-formula>, unless otherwise specified. Here, we have dropped the explicit time dependence in our notation for the sake of simplicity.</p><p>Similarly, the recurrent connections were modified to minimize the following cost function:<disp-formula id="equ15"><label>(13)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle \mathcal L_{M}=\int _{0}^{T}dt\sum _{i=1}^{N}D_{KL}\left [f_{i}\left (t\right)\| \varphi \left (v_{i}^{\rm M}\left (t\right)\right)\right ],$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft79">\begin{document}${\rm v}_{i}^{\rm M}=\sum _{j=1}^{N}{\rm M}_{ij}\cdot y_{j}$\end{document}</tex-math></alternatives></inline-formula> is a recurrent prediction. Similar to the feedforward plasticity, the gradient descent of the above cost function leads to the following plasticity rule:<disp-formula id="equ16"><label>(14)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle \Delta {\rm M}_{ij}=\eta \varphi _{0}^{- 1}\left (1- \frac{\varphi \left (v_{i}^{\rm M}\right)}{\varphi _{0}}\right)\left [f_{i}- \varphi \left ({\rm v}_{i}^{\rm M}\right)\right ].$$\end{document}</tex-math></alternatives></disp-formula></p><p>The derived recurrent plasticity rule suggests that the recurrent prediction learns the statistical model of the evoked activity, which in turn allows the network to replay the learned internal model.</p><p>In addition to the above plasticity rules, we defined the cost function for the inhibitory plasticity as<disp-formula id="equ17"><label>(15)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle \mathcal L_{G}=\sum _{i=1}^{N}\left [f_{i}\left (t\right)- \varphi \left (v_{i}^{\rm G}\left (t\right)\right)\right ]^{2},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$v_{i}^{G}$\end{document}</tex-math></alternatives></inline-formula> is the inhibitory input onto postsynaptic neuron via inhibitory connection <inline-formula><alternatives><mml:math id="inf81"><mml:mi>G</mml:mi></mml:math><tex-math id="inft81">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ18"><label>(16)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle v_{i}^{\rm G}=\sum _{j=1}^{N}{\rm G}_{ij}\cdot y_{j}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Again, by taking the gradient of <inline-formula><alternatives><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft82">\begin{document}$\mathcal L_{G}$\end{document}</tex-math></alternatives></inline-formula> with respect to <inline-formula><alternatives><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft83">\begin{document}${\rm G}_{ij}$\end{document}</tex-math></alternatives></inline-formula> derive the following inhibitory plasticity rule to keep the network dynamics balanced:<disp-formula id="equ19"><label>(17)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∝</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∝</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle \begin{array}{ll}\Delta \mathrm{G}_{i j} \propto-\frac{\partial \mathcal{L}_G}{\partial \mathrm{G}_{i j}} \\ \propto\left[f_i-\varphi\left(\mathrm{v}_i^{\mathrm{G}}\right)\right] \times \frac{\partial \varphi\left(\mathrm{v}_i^{\mathrm{G}}\right)}{\partial \mathrm{G}_{i j}}\\\propto \varphi \left (v_{i}^{\rm G}\right)\left (1- \frac{\varphi \left (v_{i}^{\rm G}\right)}{\varphi _{0}}\right)\cdot \left [f_{i}- \varphi \left (v_{i}^{\rm G}\right)\right ]\cdot y_{j}.\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>While the resultant rule is not the same as feedforward and recurrent plasticity rules, all of these rules are similar in a sense that the weight updates are proportional to the prediction error and the presynaptic activity. We therefore assumed the following rule for the inhibitory plasticity, which has the same structure as the rest of the plasticity rules that we have already explained:<disp-formula id="equ20"><label>(18)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle \Delta {\rm G}_{ij}=\eta \varphi _{0}^{- 1}\left (1- \frac{\varphi \left (v_{i}^{\rm G}\right)}{\varphi _{0}}\right)\left [f_{i}- \varphi \left (v_{i}^{\rm G}\right)\right ].$$\end{document}</tex-math></alternatives></disp-formula></p><p>We have shown by numerical simulation that the rule keeps the network dynamics balanced.</p><p>Initial values of <inline-formula><alternatives><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft84">\begin{document}$\boldsymbol W$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft85">\begin{document}$\boldsymbol M$\end{document}</tex-math></alternatives></inline-formula> are sampled from Gaussian distributions with the mean 0 and variances <inline-formula><alternatives><mml:math id="inf86"><mml:mrow><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msqrt><mml:mi>K</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math><tex-math id="inft86">\begin{document}$0.1/\sqrt{K}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf87"><mml:mrow><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math><tex-math id="inft87">\begin{document}$0.1/\sqrt{N}$\end{document}</tex-math></alternatives></inline-formula>, respectively. During learning, the elements of <inline-formula><alternatives><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft88">\begin{document}$\boldsymbol W$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft89">\begin{document}$\boldsymbol M$\end{document}</tex-math></alternatives></inline-formula> can take both positive and negative values. After sufficient learning, the postsynaptic potentials <inline-formula><alternatives><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft90">\begin{document}${\rm v}_{i}^{\rm W}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft91">\begin{document}${\rm v}_{i}^{\rm M}$\end{document}</tex-math></alternatives></inline-formula> on neuron on neuron <inline-formula><alternatives><mml:math id="inf92"><mml:mi>i</mml:mi></mml:math><tex-math id="inft92">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> converge to a common value of <inline-formula><alternatives><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft93">\begin{document}$v_{i}$\end{document}</tex-math></alternatives></inline-formula>. Therefore, <inline-formula><alternatives><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft94">\begin{document}$\varphi \left (v_{i}^{\rm W}\right)\approx \varphi \left ({\rm v}_{i}^{\rm M}\right)\approx \varphi \left ({\rm v}_{i}\right)\approx f_{i}$\end{document}</tex-math></alternatives></inline-formula>, implying that the postsynaptic potentials of afferent and recurrent synaptic inputs to neuron <inline-formula><alternatives><mml:math id="inf95"><mml:mi>i</mml:mi></mml:math><tex-math id="inft95">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> can both predict its output <inline-formula><alternatives><mml:math id="inf96"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft96">\begin{document}$f_{i}$\end{document}</tex-math></alternatives></inline-formula> after learning. The initial values of <inline-formula><alternatives><mml:math id="inf97"><mml:mi>G</mml:mi></mml:math><tex-math id="inft97">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula> are uniformly set to <inline-formula><alternatives><mml:math id="inf98"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math><tex-math id="inft98">\begin{document}$1/\sqrt{N}$\end{document}</tex-math></alternatives></inline-formula>, and its elements are truncated to non-negative values during learning. This implies that <inline-formula><alternatives><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft99">\begin{document}${\rm v}_{i}^{\rm G}$\end{document}</tex-math></alternatives></inline-formula> does not become negative. After learning, <inline-formula><alternatives><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft100">\begin{document}$\varphi \left ({\rm v}_{i}^{\rm G}\right)\approx f_{i}$\end{document}</tex-math></alternatives></inline-formula> is satisfied. Although some elements of <inline-formula><alternatives><mml:math id="inf101"><mml:mi>M</mml:mi></mml:math><tex-math id="inft101">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> may give recurrent inhibitory connections, modifiable connections in <inline-formula><alternatives><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft102">\begin{document}$\boldsymbol G$\end{document}</tex-math></alternatives></inline-formula> are necessary to encode all external inputs into specific cell assemblies.</p></sec><sec id="s4-3"><title>Stimulation protocols</title><p>Feedforward input to the recurrent network consisted of <inline-formula><alternatives><mml:math id="inf103"><mml:mi>K</mml:mi></mml:math><tex-math id="inft103">\begin{document}$K$\end{document}</tex-math></alternatives></inline-formula> Poisson spike trains with a background firing rate of 2 Hz. The input randomly presented <inline-formula><alternatives><mml:math id="inf104"><mml:mi>n</mml:mi></mml:math><tex-math id="inft104">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula> non-overlapping patterns of 100 spike trains (the duration 100 ms and the mean frequency 50 Hz), one at a time, with pattern-to-pattern intervals of 100 ms. Therefore, the number of input neurons and patterns satisfies the relationship of <inline-formula><alternatives><mml:math id="inf105"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:math><tex-math id="inft105">\begin{document}$K=100\times n$\end{document}</tex-math></alternatives></inline-formula>. For simplicity, we simulated the constant-interval case, but using irregular intervals does not change the essential results. The value of <inline-formula><alternatives><mml:math id="inf106"><mml:mi>n</mml:mi></mml:math><tex-math id="inft106">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula> varies from task to task, and the values for each figure are as follows: <inline-formula><alternatives><mml:math id="inf107"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math><tex-math id="inft107">\begin{document}$n=5$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2c-e</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>); <inline-formula><alternatives><mml:math id="inf108"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft108">\begin{document}$n=2$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2a-b</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref>-<xref ref-type="fig" rid="fig5">Figure 5</xref>); <inline-formula><alternatives><mml:math id="inf109"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math><tex-math id="inft109">\begin{document}$n=3$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The typical time length required for the convergence of learning is 1000 s.</p></sec><sec id="s4-4"><title>Measures for cell-assembly activities</title><p>Here, we explain the measures used in <xref ref-type="fig" rid="fig3">Figure 3</xref>. We calculated the firing rate ratio of cell assembly 1 in <xref ref-type="fig" rid="fig3">Figure 3c</xref> as follows:<disp-formula id="equ21"><label>(19)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle {\rm Firing\,rate\,ratio}=\frac{r_{i}^{\left (1\right)}}{\left (\sum _{j=2}^{5}\frac{1}{N_{j}}\sum _{i=1}^{N_{j}}r_{i}^{\left (j\right)}\right)/4},$$\end{document}</tex-math></alternatives></disp-formula></p><p>using the average firing rate <inline-formula><alternatives><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft110">\begin{document}$r_{i}^{\left (j\right)}$\end{document}</tex-math></alternatives></inline-formula> of the <inline-formula><alternatives><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft111">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>th neuron in cell assembly <inline-formula><alternatives><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft112">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula> and the number <inline-formula><alternatives><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft113">\begin{document}$N_{j}$\end{document}</tex-math></alternatives></inline-formula> of neurons belonging to the cell assembly. Similarly, we defined the assembly size ratio of cell assembly 1 as<disp-formula id="equ22"><label>(20)</label><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">z</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle {\rm Assembly\, size\, ratio}=\frac{N_{1}}{\left (\sum _{j=2}^{5}N_{j}\right)/4}$$\end{document}</tex-math></alternatives></disp-formula></p><p>in <xref ref-type="fig" rid="fig3">Figure 3d</xref> and assembly activity ratio of cell assembly 1 as<disp-formula id="equ23"><label>(21)</label><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle {\rm Assembly\,activity\,ratio}=\frac{r_{\rm pop}^{\left (1\right)}}{\left (\sum _{i=2}^{5}r_{\rm pop}^{\left (i\right)}\right)/4},$$\end{document}</tex-math></alternatives></disp-formula></p><p>in <xref ref-type="fig" rid="fig3">Figure 3e</xref>. Here, <inline-formula><alternatives><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft114">\begin{document}$r_{\rm pop}^{\left (i\right)}$\end{document}</tex-math></alternatives></inline-formula> represents the population neural activity of cell assembly <inline-formula><alternatives><mml:math id="inf115"><mml:mi>i</mml:mi></mml:math><tex-math id="inft115">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ24"><label>(22)</label><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>≡</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle r_{\rm pop}^{\left (j\right)}\equiv \sum _{i=1}^{N_{j}}r_{i}^{\left (j\right)}$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-5"><title>Simulations of perceptual decision making</title><p>In each learning trial, we trained the network with either leftward or rightward dot movement represented by the corresponding input neurons firing at <inline-formula><alternatives><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">z</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft116">\begin{document}$r_{\rm max}=50\, {\rm Hz}$\end{document}</tex-math></alternatives></inline-formula> In test trials, we defined input coherence as <inline-formula><alternatives><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft117">\begin{document}${\rm Coh}=\rho _{R}- 0.5$\end{document}</tex-math></alternatives></inline-formula> according to <xref ref-type="bibr" rid="bib21">Hanks et al., 2011</xref>, where <inline-formula><alternatives><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft118">\begin{document}$\rho _{R}$\end{document}</tex-math></alternatives></inline-formula> is the ratio of R input neurons to the sum of R and L input neurons in firing rate. The value of Coh ranges between –0.5 (all dots moving leftward) and +0.5 (all dots moving rightward). Then, in test trials for input coherence Coh, we generated Poisson spike trains of R and L input neurons at the rates <inline-formula><alternatives><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$\left (\rm Coh+0.5\right)r_{\rm max}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$\left (\rm - Coh+0.5\right)r_{\rm max}$\end{document}</tex-math></alternatives></inline-formula>, respectively.</p><p>In <xref ref-type="fig" rid="fig5">Figure 5c</xref>, we calculated the activity ratio (AR) as<disp-formula id="equ25"><label>(23)</label><alternatives><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t25">\begin{document}$$\displaystyle {\rm AR}=\frac{r_{\rm R}^{\rm pop}}{r_{\rm R}^{\rm pop}+r_{\rm L}^{\rm pop}},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft121">\begin{document}$r_{\rm R}^{\rm pop}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft122">\begin{document}$r_{\rm L}^{\rm pop}$\end{document}</tex-math></alternatives></inline-formula> represent the average population firing rates of R- and L-encoding cell assemblies, respectively. In <xref ref-type="fig" rid="fig5">Figure 5b</xref>, we defined ‘choices to right’ as<disp-formula id="equ26"><label>(24)</label><alternatives><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">%</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t26">\begin{document}$$\displaystyle \rm Choices\, to\, right=AR\times 100\left (\%\right).$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-6"><title>A network model with distinct excitatory and inhibitory synapses</title><p>Here, we explain the network model and the plasticity rules used in <xref ref-type="fig" rid="fig6">Figure 6</xref>. The network consists of 500 neurons, and the membrane potential of a neuron <inline-formula><alternatives><mml:math id="inf123"><mml:mi>i</mml:mi></mml:math><tex-math id="inft123">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> at time <inline-formula><alternatives><mml:math id="inf124"><mml:mi>t</mml:mi></mml:math><tex-math id="inft124">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> is given as follows:<disp-formula id="equ27"><label>(25)</label><alternatives><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=:</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=:</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=:</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=:</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t27">\begin{document}$$\displaystyle \begin{array}{ll}u_i(t)=\underbrace{\sum_{k=1}^K \mathrm{~W}_{i k} x_k(t)}_{=: \mathrm{v}_i^{\mathrm{W}}}+\underbrace{\left[\sum_{n=1}^N \mathrm{M}_{i n}^{\mathrm{exc}} y_n(t)\right]}_{=: \mathrm{v}_i^{\mathrm{M}}(\mathrm{exc})}-\underbrace{\left[\sum_{n=1}^N \mathrm{M}_{i n}^{\mathrm{inh}} y_n(t)\right]}_{=: \mathrm{v}_i^{\mathrm{M}}(\mathrm{inh})} \\-\underbrace{\sum_{n=1}^N \mathrm{G}_{i n} y_n(t)}_{=: \mathrm{v}_i^{\mathrm{G}}},\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft125">\begin{document}$\left\{{\rm W}_{ik}\right\} $\end{document}</tex-math></alternatives></inline-formula> is afferent synaptic weights, which are a mixture of excitatory and inhibitory connections as in the nDL model. The weights of recurrent excitatory synapses are {<inline-formula><alternatives><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft126">\begin{document}$\rm M^{exc}_{in}$\end{document}</tex-math></alternatives></inline-formula>}. Here, we considered two types of recurrent inhibitory connections, denoted by <inline-formula><alternatives><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft127">\begin{document}$\rm M^{inh}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf128"><mml:mi>G</mml:mi></mml:math><tex-math id="inft128">\begin{document}$G$\end{document}</tex-math></alternatives></inline-formula>, respectively. Here, we assumed that half of the recurrent connections were assumed to be excitatory and the remaining connections were all inhibitory, half of which were <inline-formula><alternatives><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft129">\begin{document}$\rm M^{inh}$\end{document}</tex-math></alternatives></inline-formula> and the other half were <inline-formula><alternatives><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft130">\begin{document}$\rm G$\end{document}</tex-math></alternatives></inline-formula>. We modified these weights according to the following equations:<disp-formula id="equ28"><label>(26a)</label><alternatives><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t28">\begin{document}$$\displaystyle \Delta {\rm W}_{ij}=\eta \mathcal E(f_{i},v_{i}^{\rm W})x_{j},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ29"><label>(26b)</label><alternatives><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t29">\begin{document}$$\displaystyle \Delta {\rm M}_{ij}^{\rm exc}=\eta \mathcal E\,\left (f_{i},v_{i}^{\rm M\left (exc\right)}- v_{i}^{\rm M\left (inh\right)}\right)y_{j},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ30"><label>(26c)</label><alternatives><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t30">\begin{document}$$\displaystyle \Delta {\rm M}_{ij}^{\rm exc}=- \eta \mathcal E \, \left (f_{i},{\rm v}_{i}^{\rm M\left (exc\right)}- {\rm v}_{i}^{\rm M\left (inh\right)}\right)y_{j},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ31"><label>(26d)</label><alternatives><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t31">\begin{document}$$\displaystyle \Delta G_{ij}=\eta \mathcal E\, \left (f_{i},v_{i}^{G}\right)y_{j},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft131">\begin{document}$\mathcal E\left (f_{i},v_{i}\right)$\end{document}</tex-math></alternatives></inline-formula> is the error term defined as<disp-formula id="equ32"><label>(27)</label><alternatives><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t32">\begin{document}$$\displaystyle \mathcal E \left (f_{i},v_{i}\right)\varphi _{0}^{- 1}\left (1- \frac{\varphi \left (v_{i}\right)}{\varphi _{0}}\right)\left [f_{i}- \varphi \left (v_{i}\right)\right ].$$\end{document}</tex-math></alternatives></disp-formula></p><p>At each time step during learning, we truncated all weights of recurrent connections to non-negative values during learning.</p></sec><sec id="s4-7"><title>A network model with distinct excitatory and inhibitory neuron populations</title><p>Here, we explain the architecture of the model used in <xref ref-type="fig" rid="fig7">Figure 7</xref>. The network consists of <inline-formula><alternatives><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">N</mml:mi><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft132">\begin{document}$\rm N_{E}\left (=500\right)$\end{document}</tex-math></alternatives></inline-formula> excitatory and <inline-formula><alternatives><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">N</mml:mi><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft133">\begin{document}$\rm N_{I}\left (=500\right)$\end{document}</tex-math></alternatives></inline-formula> inhibitory neurons. The membrane potential of a neuron <inline-formula><alternatives><mml:math id="inf134"><mml:mi>i</mml:mi></mml:math><tex-math id="inft134">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> of a population <inline-formula><alternatives><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft135">\begin{document}$\rm X$\end{document}</tex-math></alternatives></inline-formula> (=E or I) at time <inline-formula><alternatives><mml:math id="inf136"><mml:mi>t</mml:mi></mml:math><tex-math id="inft136">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> is given as follows:<disp-formula id="equ33"><label>(28)</label><alternatives><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=:</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>E</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>I</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=:</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mspace width="2em"/><mml:mspace width="1em"/><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>I</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=:</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t33">\begin{document}$$\displaystyle \begin{array}{ll}u_i^{\mathrm{X}}(t)=\underbrace{\sum_{k=1}^K \mathrm{~W}_{i k}^{\mathrm{X}} x_k(t)}_{=: \mathrm{v}_i^{\mathrm{W}}}+\underbrace{\left[\sum_{l=1}^{N_E} \mathrm{M}_{i l}^{\mathrm{XE}} y_l^E(t)-\sum_{m=1}^{N_I} \mathrm{G}_{i m}^{\mathrm{XI}(\mathrm{path} 2)} y_m^I(t)\right]}_{=: \mathrm{v}_i^{\mathrm{M}(2)}} \\\quad \quad \quad \qquad \quad-\underbrace{\sum_{m=1}^{N_I} \mathrm{G}_{i m}^{\mathrm{XI}(\mathrm{path} 1)} y_m^I(t)}_{=: \mathrm{v}_i^{\mathrm{M}(1)}},\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf137"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:math><tex-math id="inft137">\begin{document}$\left \{W_{ik}^{X}\right \}$\end{document}</tex-math></alternatives></inline-formula> is afferent synaptic weights, which are a mixture of excitatory and inhibitory connections as in the nDL model. The weights of recurrent excitatory synapses are {<inline-formula><alternatives><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft138">\begin{document}${\rm M}_{il}^{\rm XE}$\end{document}</tex-math></alternatives></inline-formula>}. Here, we considered two types of recurrent inhibitory connections (i.e., path 1 and path 2), denoted by <inline-formula><alternatives><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">G</mml:mi><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft139">\begin{document}$\rm G_{Im }^{XI\left (path1\right)}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">G</mml:mi><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft140">\begin{document}$\rm G_{Im }^{XI\left (path2\right)}$\end{document}</tex-math></alternatives></inline-formula>, respectively. Using the same definitions of the error term as in <xref ref-type="disp-formula" rid="equ32">Equation 27</xref>, we modified these weights according to the following equations:<disp-formula id="equ34"><label>(29a)</label><alternatives><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t34">\begin{document}$$\displaystyle \Delta {\rm W}_{ij}^{\rm X}=\eta \mathcal E \left (f_{i},{\rm v}_{i}^{\rm W}\right)x_{j},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ35"><label>(29b)</label><alternatives><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t35">\begin{document}$$\displaystyle \Delta {\rm M}_{ij}^{\rm XE}=\eta \mathcal E \left (f_{i},{\rm v}_{i}^{\rm M\left (2\right)}\right)y_{j}^{E},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ36"><label>(29c)</label><alternatives><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t36">\begin{document}$$\displaystyle \Delta {\rm G}_{ij}^{\rm XI\left (path2\right)}=- \eta \mathcal E \left (f_{i},{\rm v}_{i}^{\rm M\left (2\right)}\right)y_{j}^{I},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ37"><label>(29d)</label><alternatives><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t37">\begin{document}$$\displaystyle \Delta {\rm G}_{ij}^{\rm XI\left (path1\right)}=\eta \mathcal E \left (f_{i},{\rm v}_{i}^{\rm M\left (1\right)}\right)y_{j}^{I}$$\end{document}</tex-math></alternatives></disp-formula></p><p>To satisfy Dale’s law, we truncated all weights of recurrent connections to non-negative values during learning.</p><p>In <xref ref-type="fig" rid="fig7">Figure 7g</xref>, we measured the lateral inhibition between excitatory neurons via path 1 by calculating:<disp-formula id="equ38"><label>(30)</label><alternatives><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t38">\begin{document}$$\displaystyle \left [{\rm W}_{i,j}^{\rm LI}\right ]=\sum _{k=1}^{N_{E}}{\rm G}_{ik}^{\rm EI\left (path1\right)}{\rm M}_{kj}^{\rm IE}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Lateral inhibition via path 2 was calculated in a similar fashion.</p></sec><sec id="s4-8"><title>Simulation details</title><p>All simulations were performed in customized Python3 code written by TA with numpy 1.17.3 and scipy 0.18. Differential equations were numerically integrated using an Euler method with integration time steps of 1 ms.</p></sec><sec id="s4-9"><title>Data availability</title><p>Code is provided on the GitHub repository (<xref ref-type="bibr" rid="bib4">Asabuki, 2024</xref>). <ext-link ext-link-type="uri" xlink:href="https://github.com/TAsabuki/PriorNet_codes">https://github.com/TAsabuki/PriorNet_codes</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib6">Asabuki, 2025</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-92712-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modeling code is uploaded as Source code.</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors express their sincere thanks to Yukiko Goda for her valuable comments on our manuscript. This work was supported by KAKENHI (nos. 18H05213 and 19H04994) to TF.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abdou</surname><given-names>K</given-names></name><name><surname>Choko</surname><given-names>K</given-names></name><name><surname>Aly</surname><given-names>MH</given-names></name><name><surname>Okubo-Suzuki</surname><given-names>R</given-names></name><name><surname>Inokuchi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inspiring Cognitive Inference in a Cortical Network during REM Sleep</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.04.08.439095</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Jegminat</surname><given-names>J</given-names></name><name><surname>Menendez</surname><given-names>JA</given-names></name><name><surname>Pfister</surname><given-names>JP</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Synaptic plasticity as Bayesian inference</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00809-5</pub-id><pub-id pub-id-type="pmid">33707754</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asabuki</surname><given-names>T</given-names></name><name><surname>Fukai</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Somatodendritic consistency check for temporal feature segmentation</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>1554</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-15367-w</pub-id><pub-id pub-id-type="pmid">32214100</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Asabuki</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>PriorNet</data-title><version designator="v1.0">v1.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/TAsabuki/PriorNet_codes">https://github.com/TAsabuki/PriorNet_codes</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asabuki</surname><given-names>T</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Embedding stochastic dynamics of the environment in spontaneous activity by prediction-based plasticity</article-title><source>eLife</source><volume>13</volume><elocation-id>RP95243</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.95243.2</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Asabuki</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>PriorNet_codes</data-title><version designator="swh:1:rev:6ded6b1f0a505c9cd461317ffee547ca84641bf0">swh:1:rev:6ded6b1f0a505c9cd461317ffee547ca84641bf0</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0a2a5076210650e4f0b83ca0b41e8399392d7908;origin=https://github.com/TAsabuki/PriorNet_codes;visit=swh:1:snp:f67fafaf42c4b09f59bab64e6e5012481dece0bb;anchor=swh:1:rev:6ded6b1f0a505c9cd461317ffee547ca84641bf0">https://archive.softwareheritage.org/swh:1:dir:0a2a5076210650e4f0b83ca0b41e8399392d7908;origin=https://github.com/TAsabuki/PriorNet_codes;visit=swh:1:snp:f67fafaf42c4b09f59bab64e6e5012481dece0bb;anchor=swh:1:rev:6ded6b1f0a505c9cd461317ffee547ca84641bf0</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Usrey</surname><given-names>WM</given-names></name><name><surname>Adams</surname><given-names>RA</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id><pub-id pub-id-type="pmid">23177956</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title><source>Science</source><volume>331</volume><fpage>83</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1126/science.1195870</pub-id><pub-id pub-id-type="pmid">21212356</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bill</surname><given-names>J</given-names></name><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Habenschuss</surname><given-names>S</given-names></name><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distributed bayesian computation and self-organized learning in sheets of spiking neurons with local lateral inhibition</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0134356</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0134356</pub-id><pub-id pub-id-type="pmid">26284370</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Bill</surname><given-names>J</given-names></name><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002211</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002211</pub-id><pub-id pub-id-type="pmid">22096452</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chance</surname><given-names>FS</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Reyes</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Gain modulation from background synaptic input</article-title><source>Neuron</source><volume>35</volume><fpage>773</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00820-6</pub-id><pub-id pub-id-type="pmid">12194875</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Neal</surname><given-names>RM</given-names></name><name><surname>Zemel</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The Helmholtz machine</article-title><source>Neural Computation</source><volume>7</volume><fpage>889</fpage><lpage>904</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.5.889</pub-id><pub-id pub-id-type="pmid">7584891</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Bayesian spiking neurons I: inference</article-title><source>Neural Computation</source><volume>20</volume><fpage>91</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.20.1.91</pub-id><pub-id pub-id-type="pmid">18045002</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.003</pub-id><pub-id pub-id-type="pmid">20153683</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujii</surname><given-names>N</given-names></name><name><surname>Graybiel</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Representation of action sequence boundaries by macaque prefrontal cortical neurons</article-title><source>Science</source><volume>301</volume><fpage>1246</fpage><lpage>1249</lpage><pub-id pub-id-type="doi">10.1126/science.1086872</pub-id><pub-id pub-id-type="pmid">12947203</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukai</surname><given-names>T</given-names></name><name><surname>Tanaka</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A simple neural network exhibiting selective activation of neuronal ensembles: from winner-take-all to winners-share-all</article-title><source>Neural Computation</source><volume>9</volume><fpage>77</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.1.77</pub-id><pub-id pub-id-type="pmid">9117902</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghandour</surname><given-names>K</given-names></name><name><surname>Ohkawa</surname><given-names>N</given-names></name><name><surname>Fung</surname><given-names>CCA</given-names></name><name><surname>Asai</surname><given-names>H</given-names></name><name><surname>Saitoh</surname><given-names>Y</given-names></name><name><surname>Takekawa</surname><given-names>T</given-names></name><name><surname>Okubo-Suzuki</surname><given-names>R</given-names></name><name><surname>Soya</surname><given-names>S</given-names></name><name><surname>Nishizono</surname><given-names>H</given-names></name><name><surname>Matsuo</surname><given-names>M</given-names></name><name><surname>Osanai</surname><given-names>M</given-names></name><name><surname>Sato</surname><given-names>M</given-names></name><name><surname>Ohkura</surname><given-names>M</given-names></name><name><surname>Nakai</surname><given-names>J</given-names></name><name><surname>Hayashi</surname><given-names>Y</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Kitamura</surname><given-names>T</given-names></name><name><surname>Fukai</surname><given-names>T</given-names></name><name><surname>Inokuchi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Orchestrated ensemble activities constitute a hippocampal memory engram</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2637</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10683-2</pub-id><pub-id pub-id-type="pmid">31201332</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generative Adversarial Nets</article-title><conf-name>Proceedings of the In-989 ternational Conference on Neural Information Processing Systems (NIPS 990 2014)</conf-name><fpage>2672</fpage><lpage>2680</lpage></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Mazurek</surname><given-names>ME</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Hopp</surname><given-names>E</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Elapsed decision time affects the weighting of prior probability in a perceptual decision task</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>6339</fpage><lpage>6352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5613-10.2011</pub-id><pub-id pub-id-type="pmid">21525274</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartmann</surname><given-names>C</given-names></name><name><surname>Lazar</surname><given-names>A</given-names></name><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Triesch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Where’s the noise? key features of spontaneous activity and neural variability arise through learning in a deterministic network</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004640</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004640</pub-id><pub-id pub-id-type="pmid">26714277</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertäg</surname><given-names>L</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Prediction-error neurons in circuits with multiple neuron types: Formation, refinement, and functional implications</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2115699119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2115699119</pub-id><pub-id pub-id-type="pmid">35320037</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hiratani</surname><given-names>N</given-names></name><name><surname>Fukai</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interplay between short- and long-term plasticity in cell-assembly formation</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e101535</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0101535</pub-id><pub-id pub-id-type="pmid">25007209</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hiratani</surname><given-names>N</given-names></name><name><surname>Fukai</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hebbian wiring plasticity generates efficient network structures for robust inference with synaptic weight plasticity</article-title><source>Frontiers in Neural Circuits</source><volume>10</volume><elocation-id>41</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2016.00041</pub-id><pub-id pub-id-type="pmid">27303271</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hiratani</surname><given-names>N</given-names></name><name><surname>Fukai</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Redundancy in synaptic connections enables neurons to learn optimally</article-title><source>PNAS</source><volume>115</volume><fpage>E6871</fpage><lpage>E6879</lpage><pub-id pub-id-type="doi">10.1073/pnas.1803274115</pub-id><pub-id pub-id-type="pmid">29967182</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hiratani</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rapid Bayesian learning in the mammalian olfactory system</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3845</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17490-0</pub-id><pub-id pub-id-type="pmid">32737295</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Rao</surname><given-names>RPN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Bayesian inference and online learning in poisson neuronal networks</article-title><source>Neural Computation</source><volume>28</volume><fpage>1503</fpage><lpage>1526</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00851</pub-id><pub-id pub-id-type="pmid">27348304</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isomura</surname><given-names>T</given-names></name><name><surname>Shimazaki</surname><given-names>H</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Canonical neural networks perform active inference</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>55</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02994-2</pub-id><pub-id pub-id-type="pmid">35031656</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jimenez Rezende</surname><given-names>D</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Stochastic variational learning in recurrent spiking networks</article-title><source>Frontiers in Computational Neuroscience</source><volume>8</volume><elocation-id>38</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00038</pub-id><pub-id pub-id-type="pmid">24772078</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>X</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Start/stop signals emerge in nigrostriatal circuits during sequence learning</article-title><source>Nature</source><volume>466</volume><fpage>457</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nature09263</pub-id><pub-id pub-id-type="pmid">20651684</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>X</given-names></name><name><surname>Tecuapetla</surname><given-names>F</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Basal ganglia subcircuits distinctively encode the parsing and concatenation of action sequences</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>423</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1038/nn.3632</pub-id><pub-id pub-id-type="pmid">24464039</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kappel</surname><given-names>D</given-names></name><name><surname>Habenschuss</surname><given-names>S</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Network plasticity as bayesian inference</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004485</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004485</pub-id><pub-id pub-id-type="pmid">26545099</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predictive processing: a canonical cortical computation</article-title><source>Neuron</source><volume>100</volume><fpage>424</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.003</pub-id><pub-id pub-id-type="pmid">30359606</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kempter</surname><given-names>R</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>van Hemmen</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hebbian learning and spiking neurons</article-title><source>Physical Review E</source><volume>59</volume><fpage>4498</fpage><lpage>4514</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.59.4498</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kenet</surname><given-names>T</given-names></name><name><surname>Bibitchkov</surname><given-names>D</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Spontaneously emerging cortical representations of visual attributes</article-title><source>Nature</source><volume>425</volume><fpage>954</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1038/nature02078</pub-id><pub-id pub-id-type="pmid">14586468</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinzing</surname><given-names>JG</given-names></name><name><surname>Niethard</surname><given-names>N</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mechanisms of systems memory consolidation during sleep</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1598</fpage><lpage>1610</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0467-3</pub-id><pub-id pub-id-type="pmid">31451802</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Bayesian integration in sensorimotor learning</article-title><source>Nature</source><volume>427</volume><fpage>244</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1038/nature02169</pub-id><pub-id pub-id-type="pmid">14724638</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legaspi</surname><given-names>R</given-names></name><name><surname>Toyoizumi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A bayesian psychophysics model of sense of agency</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4250</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12170-0</pub-id><pub-id pub-id-type="pmid">31534122</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>AA</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Emergence of probabilistic representation in the neural network of primary visual cortex</article-title><source>iScience</source><volume>25</volume><elocation-id>103975</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2022.103975</pub-id><pub-id pub-id-type="pmid">35310336</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Formation and maintenance of neuronal assemblies through synaptic plasticity</article-title><source>Nature Communications</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/ncomms6319</pub-id><pub-id pub-id-type="pmid">25395015</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luczak</surname><given-names>A</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Kubo</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neurons learn by predicting future activity</article-title><source>Nature Machine Intelligence</source><volume>4</volume><fpage>62</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1038/s42256-021-00430-y</pub-id><pub-id pub-id-type="pmid">35814496</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id><pub-id pub-id-type="pmid">17057707</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackwood</surname><given-names>O</given-names></name><name><surname>Naumann</surname><given-names>LB</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning excitatory-inhibitory neuronal assemblies in recurrent networks</article-title><source>eLife</source><volume>10</volume><elocation-id>e59715</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59715</pub-id><pub-id pub-id-type="pmid">33900199</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manz</surname><given-names>P</given-names></name><name><surname>Memmesheimer</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Purely STDP-based assembly dynamics: Stability, learning, overlaps, drift and aging</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011006</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011006</pub-id><pub-id pub-id-type="pmid">37043481</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masquelier</surname><given-names>T</given-names></name><name><surname>Guyonneau</surname><given-names>R</given-names></name><name><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e1377</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0001377</pub-id><pub-id pub-id-type="pmid">18167538</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikulasch</surname><given-names>FA</given-names></name><name><surname>Rudelt</surname><given-names>L</given-names></name><name><surname>Priesemann</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Local dendritic balance enables learning of efficient representations in networks of spiking neurons</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2021925118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2021925118</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikulasch</surname><given-names>FA</given-names></name><name><surname>Rudelt</surname><given-names>L</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Priesemann</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Where is the error? hierarchical predictive coding through dendritic error computation</article-title><source>Trends in Neurosciences</source><volume>46</volume><fpage>45</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2022.09.007</pub-id><pub-id pub-id-type="pmid">36577388</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>SJ</given-names></name><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Shunting inhibition modulates neuronal gain during synaptic excitation</article-title><source>Neuron</source><volume>38</volume><fpage>433</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00200-9</pub-id><pub-id pub-id-type="pmid">12741990</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Rumpel</surname><given-names>S</given-names></name><name><surname>Loewenstein</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inhibitory connectivity defines the realm of excitatory plasticity</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1463</fpage><lpage>1470</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0226-x</pub-id><pub-id pub-id-type="pmid">30224809</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montangie</surname><given-names>L</given-names></name><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Autonomous emergence of connectivity assemblies via spike triplet interactions</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007835</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007835</pub-id><pub-id pub-id-type="pmid">32384081</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Pfeiffer</surname><given-names>M</given-names></name><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003037</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003037</pub-id><pub-id pub-id-type="pmid">23633941</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural variability and sampling-based probabilistic representations in the visual cortex</article-title><source>Neuron</source><volume>92</volume><fpage>530</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.038</pub-id><pub-id pub-id-type="pmid">27764674</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>M</given-names></name><name><surname>Yoshimura</surname><given-names>Y</given-names></name><name><surname>Takada</surname><given-names>N</given-names></name><name><surname>Horibe</surname><given-names>S</given-names></name><name><surname>Komatsu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Specialized inhibitory synaptic actions between nearby neocortical pyramidal neurons</article-title><source>Science</source><volume>316</volume><fpage>758</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1126/science.1135468</pub-id><pub-id pub-id-type="pmid">17478724</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Cordova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural representations of events arise from temporal community structure</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>486</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1038/nn.3331</pub-id><pub-id pub-id-type="pmid">23416451</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Competitive hebbian learning through spike-timing-dependent synaptic plasticity</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>919</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1038/78829</pub-id><pub-id pub-id-type="pmid">10966623</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprekeler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Functional consequences of inhibitory plasticity: homeostasis, the excitation-inhibition balance and beyond</article-title><source>Current Opinion in Neurobiology</source><volume>43</volume><fpage>198</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.03.014</pub-id><pub-id pub-id-type="pmid">28500933</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takehara-Nishiuchi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neurobiology of systems memory consolidation</article-title><source>The European Journal of Neuroscience</source><volume>54</volume><fpage>6850</fpage><lpage>6863</lpage><pub-id pub-id-type="doi">10.1111/ejn.14694</pub-id><pub-id pub-id-type="pmid">32027423</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tonegawa</surname><given-names>S</given-names></name><name><surname>Morrissey</surname><given-names>MD</given-names></name><name><surname>Kitamura</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The role of engram cells in the systems consolidation of memory</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>485</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0031-2</pub-id><pub-id pub-id-type="pmid">29970909</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torres-Torrelo</surname><given-names>J</given-names></name><name><surname>Torres</surname><given-names>B</given-names></name><name><surname>Carrascal</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Modulation of the input-output function by GABAA receptor-mediated currents in rat oculomotor nucleus motoneurons</article-title><source>The Journal of Physiology</source><volume>592</volume><fpage>5047</fpage><lpage>5064</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2014.276576</pub-id><pub-id pub-id-type="pmid">25194049</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Triplett</surname><given-names>MA</given-names></name><name><surname>Avitan</surname><given-names>L</given-names></name><name><surname>Goodhill</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emergence of spontaneous assembly activity in developing neural networks without afferent input</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006421</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006421</pub-id><pub-id pub-id-type="pmid">30265665</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tully</surname><given-names>PJ</given-names></name><name><surname>Hennig</surname><given-names>MH</given-names></name><name><surname>Lansner</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Synaptic and nonsynaptic plasticity approximating probabilistic inference</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>6</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.3389/fnsyn.2014.00008</pub-id><pub-id pub-id-type="pmid">24782758</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning by the dendritic prediction of somatic spiking</article-title><source>Neuron</source><volume>81</volume><fpage>521</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.030</pub-id><pub-id pub-id-type="pmid">24507189</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title><source>Science</source><volume>334</volume><fpage>1569</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1126/science.1211095</pub-id><pub-id pub-id-type="pmid">22075724</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name><name><surname>Doyon</surname><given-names>N</given-names></name><name><surname>Gilson</surname><given-names>M</given-names></name><name><surname>Haas</surname><given-names>JS</given-names></name><name><surname>Liu</surname><given-names>R</given-names></name><name><surname>Maffei</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name><name><surname>Wierenga</surname><given-names>CJ</given-names></name><name><surname>Woodin</surname><given-names>MA</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibitory synaptic plasticity: spike timing-dependence and putative network function</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>119</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00119</pub-id><pub-id pub-id-type="pmid">23882186</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zacks</surname><given-names>JM</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name><name><surname>Sheridan</surname><given-names>MA</given-names></name><name><surname>Donaldson</surname><given-names>DI</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Ollinger</surname><given-names>JM</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human brain activity time-locked to perceptual event boundaries</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>651</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1038/88486</pub-id><pub-id pub-id-type="pmid">11369948</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6922</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7922</pub-id><pub-id pub-id-type="pmid">25897632</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92712.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study investigates how biologically plausible learning mechanisms can support assembly formation that encodes statistics of the environment, by enabling neural sampling that is based on within-assembly connectivity strength. It <bold>convincingly</bold> shows that assembly formation can emerge from predictive plasticity in excitatory synapses, while two types of plasticity in inhibitory synapses are required: inhibitory homeostatic (predictive) plasticity and inhibitory competitive (anti-predictive) plasticity.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92712.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors have successfully addressed most of the issues raised in the first review. Nevertheless, some of the mentioned problems require further attention, mostly regarding the formal derivation of the learning rules, as well as connections to previous research.</p><p>Regarding the derivations of learning rules: The authors have provided Goal functions for each of the plastic neural connections to give some insight into what these connections do. However, as I understand, this does not address the main concern raised in the previous review: Why do these rules lead to overall network dynamics that sample from the input distribution? Virtually all other work on neural sampling that I am aware of (e.g., from Maass Lab, Lengyel Lab, etc.) start from a single goal function for all connections that somehow quantifies the difference of network dynamics from the target distribution. In the presented work the authors specify different goal functions for the different weights, which does not make clear how the desired network dynamics are ultimately achieved.</p><p>This becomes especially evident looking at the two different recurrent connections (M and G). M minimizes the difference between network activity f and recurrent prediction DKL[f|phi(My)], but why is this alone not enough to ensure a good sampling? G minimizes the squared error [f-phi(Gy)]^2, but what does that mean? The problem is that the goal functions are self-consistent in the sense that both f and phi(Gy) depend on G, which makes an interpretation very difficult. Ultimately it's easier to interpret this by looking at the plasticity rule and see that it leads to a balance. For G the authors furthermore actually ignore the derived plasticity rule and switch to a rule similar to the one for M, meaning that the actual goal function for G is also something like DKL[f|phi(Gy)]. Overall, an overarching optimization goal for the entire network is missing, which makes the interpretation very difficult. I understand that this might be very difficult to provide at this stage, but the authors should at least point out this shortcoming as an open question for the proposed framework.</p><p>Regarding the relation to previous work the authors have provided a lot more detailed discussion, which very much clears up the contributions and novel ideas in their work. Still, there are some claims that are not consistent with the literature. Especially, in lines 767 ff. the authors state that Kappel et al &quot;assumed plasticity only at recurrent synapses projecting onto the excitatory neurons. In addition, unlike our model, the cell assembly memberships need to be preconfigured in the [...] model.&quot; This is not correct, as Kappel et al learn both the feed-forward and recurrent connections, hence the main difference is that in Kappel et al sampling is sequential and not random. This is why I mentioned this work in the first review, as it speaks against the authors claims of novelty (719 ff.), which should be adjusted accordingly.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92712.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The paper reconsiders the formation of Hebbian-type assemblies, with their spontaneous reactivation representing the statistics of the sensory inputs, in the light of predictive synaptic plasticity. It convincingly shows that not all plasticity rules can be predictive in the narrow sense. While plasticity for the excitatory synapses (the forward projecting and recurrent ones) are predictive, two types of plasticity in the recurrent inhibition is required: a homeostatic and competitive one.</p><p>Details:</p><p>Besides the excitatory forward and recurrent connections that are learned based on predictive synaptic plasticity, two types of inhibitory plasticity are considered. A first type of inhibition is homeostatic and roughly balances excitation within the cell assemblies. Plasticity in this type 1 inhibition is also predictive, analogous to the plasticity of the excitatory synapses. However, plasticity in type 2 inhibition is competitive and has a switched sign. Both types of inhibitory plasticity, the predictive (homeostatic) and the anti-predictive (competitive) one, work together with the predictive excitatory plasticity to form cell assemblies representing sensory stimuli. Only if the two types of homeostatic and competitive inhibitory plasticity are present, will the spontaneous replay of the assemblies reflect the statistics of the stimulus presentation.</p><p>Critical review:</p><p>The simulations include Dale's law, making them more biologically realistic. The paper emphasizes predictive plasticity and introduces type 1 inhibitory plasticity that, by construction, tries to fully explain away the excitatory input. In the absence of external inputs, however, due to the symmetry between the excitatory and inhibitory-type-1 plasticity rules, excitation and inhibition tend to fully cancel each other. Multiple options may solve the dilemma:</p><p>(1) As other predictive dendritic plasticity models assume, the presynaptic source for recurrent inhibition is typically less informative than the presynaptic source of excitation, so that inhibition is not able to fully explain away excitation.</p><p>(2) Beside the inhibitory predictive plasticity that mirrors the analogous excitatory predictive plasticity, and additional competitive plasticity can be introduced.</p><p>The paper chooses solution (2) and suggests and additional inhibitory recurrent pathway that is not predictive, but instead anti-predictive with a reversed sign. The combination of the two types of inhibitory plasticities lead to a stable formation of cell assemblies. The stable target activity of the plasticity rules in a memory recall is not anymore 0, as it would be with only type-1-inhibitory plasticity.</p><p>Instead, the target activity of plasticity is now enhanced within a winning assembly, and also positive but reduced in the loosing assemblies.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92712.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The work shows how learned assembly structure and its influence on replay during spontaneous activity can reflect the statistics of stimulus input. In particular, stimuli that are more frequent during training elicit stronger wiring and more frequent activation during replay. Past works (Litwin-Kumar and Doiron, 2014; Zenke et al., 2015) have not addressed this specific question, as classic homeostatic mechanisms forced activity to be similar across all assemblies. Here, the authors use a dynamic gain and threshold mechanism to circumnavigate this issue and link this mechanism to a cellular monitoring of membrane potential history.</p><p>Strengths:</p><p>(1) This is an interesting advance, and the authors link this to experimental work in sensory learning in environments with non-uniform stimulus probabilities.</p><p>(2) The authors consider their mechanism in a variety of models of increasing complexity (simple stimuli, complex stimuli; ignoring Dale's law, incorporating Dale's law).</p><p>(3) Links a cellular mechanism of internal gain control (their variable h) to assembly formation and the non-uniformity of spontaneous replay activity. Offers a promise of relating cellular and synaptic plasticity mechanisms under a common goal of assembly formation.</p><p>Weaknesses:</p><p>(1) However, while the manuscript does show that assembly wiring does follow stimulus likelihood, it is not clear how the assembly specific statistics of h reflect these likelihoods. I find this to be a key issue.</p><p>(2) The authors model does take advantage of the sigmoidal transfer function, and after learning an assembly is either fully active or near fully silent (Fig. 2a). This somewhat artificial saturation may be the reason that classic homeostasis is not required, since runaway activity is not as damaging to network activity.</p><p>(3) Classic mechanisms of homeostatic regulation (synaptic scaling, inhibitory plasticity) try to ensure that firing rates match a target rate (on average). If the target rate is the same for all neurons then having elevated firing rates for one assembly compared to others during spontaneous activity would be difficult. If these homeostatic mechanisms were incorporated, how would they permit the elevated firing rates for assemblies that represent more likely stimuli?</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92712.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Asabuki</surname><given-names>Toshitake</given-names></name><role specific-use="author">Author</role><aff><institution>RIKEN</institution><addr-line><named-content content-type="city">Saitama</named-content></addr-line><country>Japan</country></aff></contrib><contrib contrib-type="author"><name><surname>Fukai</surname><given-names>Tomoki</given-names></name><role specific-use="author">Author</role><aff><institution>Okinawa Institute of Science and Technology</institution><addr-line><named-content content-type="city">Onna-son, Kunigami-gun</named-content></addr-line><country>Japan</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>In their manuscript, the authors propose a learning scheme to enable spiking neurons to learn the appearance probability of inputs to the network. To this end, the neurons rely on error-based plasticity rules for feedforward and recurrent connections. The authors show that this enables the networks to spontaneously sample assembly activations according to the occurrence probability of the input patterns they respond to. They also show that the learning scheme could explain biases in decision-making, as observed in monkey experiments. While the task of neural sampling has been solved before in other models, the novelty here is the proposal that the main drivers of sampling are within-assembly connections, and not between-assembly (Markov chains) connections as in previous models. This could provide a new understanding of how spontaneous activity in the cortex is shaped by synaptic plasticity.</p><p>The manuscript is well written and the results are presented in a clear and understandable way. The main results are convincing, concerning the spontaneous firing rate dependence of assemblies on input probability, as well as the replication of biases in the decision-making experiment. Nevertheless, the manuscript and model leave open several important questions. The main problem is the unclarity, both in theory and intuitively, of how the sampling exactly works. This also makes it difficult to assess the claims of novelty the authors make, as it is not clear how their work relates to previous models of neural sampling.</p></disp-quote><p>We agree with the reviewer that our previous manuscript was not clear regarding the mechanism of the model. We have performed additional simulations and included a derivation of the learning rule to address this, which we explain below.</p><disp-quote content-type="editor-comment"><p>Regarding the unclarity of the sampling mechanism, the authors state that withinassembly excitatory connections are responsible for activating the neurons according to stimulus probability. However, the intuition for this process is not made clear anywhere in the manuscript. How do the recurrent connections lead to the observed effect of sampling? How exactly do assemblies form from feedforward plasticity? This intuitive unclarity is accompanied by a lack of formal justification for the plasticity rules. The authors refer to a previous publication from the same lab, but it is difficult to connect these previous results and derivations to the current manuscript. The manuscript should include a clear derivation of the learning rules, as well as an (ideally formal) intuition of how this leads to the sampling dynamics in the simulation.</p></disp-quote><p>We have included a derivation of our plasticity rules in lines 871-919 in the revised manuscript. Consistent with our claim that predictive plasticity updates the feedforward and the recurrent synapses to predict output firing rates, we have shown that the corresponding cost function measures the discrepancy among the recurrent prediction, feedforward prediction, and the output firing rate. The resultant feedforward plasticity is the same with our previous rule (Asabuki and Fukai, 2020), which segments the salient patterns embedded in the input sequence. The recurrent plasticity rule suggests that the recurrent prediction learns the statistical model of the evoked activity, enabling the network to replay the learned internal model.</p><p>Similarly, for the inhibitory plasticity, we defined a cost function that evaluates the difference between the firing rate and inhibitory potential within each neuron. This rule is crucial for maintaining balanced network dynamics. See our response below for more details on the role of inhibitory plasticity.</p><disp-quote content-type="editor-comment"><p>Some of the model details should furthermore be cleared up. First, recurrent connections transmit signals instantaneously, which is implausible. Is this required, would the network dynamics change significantly if, e.g., excitation arrives slightly delayed? Second, why is the homeostasis on h required for replay? The authors show that without it the probabilities of sampling are not matched, but it is not clear why, nor how homeostasis prevents this. Third, G and M have the same plasticity rule except for G being confined to positive values, but there is no formal justification given for this quite unusual rule. The authors should clearly justify (ideally formally) the introduction of these inhibitory weights G, which is also where the manuscript deviates from their previous 2020 work. My feeling is that inhibitory weights have to be constrained in the current model because they have a different goal (decorrelation, not prediction) and thus should operate with a completely different plasticity mechanism. The current manuscript doesn't address this, as there is no overall formal justification for the learning algorithm.</p></disp-quote><p>First, while the reviewer's suggestion to test with delayed excitation is intriguing and crucial for a more biologically detailed spiking neuron model, we have chosen to maintain the current model configuration. Our use of Poisson spiking neurons, which generate spikes based on instantaneous firing rates, does not heavily depend on precise spike timing information. Therefore, to preserve the simplicity of our results, we kept the model unchanged.</p><p>Second, we agree that our previous claim regarding the importance of the memory trace h for sampling may have been confusing. As shown in Supplementary Figure 7b in the revised manuscript, when we eliminated the dynamics of the memory trace, sampling performance did indeed decrease. However, we also observed that the assembly activity ratio continued to show a linear relationship with stimulus probabilities. Based on these findings, we have revised our claim in the manuscript to clarify that the memory trace is primarily critical for firing rate homeostasis, rather than directly influencing sampling within the learned network. We have explained this in ll. 446-448 in the revised manuscript.</p><p>Third, we explored a new architecture where all recurrent connections are either exclusively excitatory or inhibitory, keeping their sign throughout the learning process. This change addresses the reviewer's concern about our initial assumption that only the inhibitory connection G was constrained to non-negative values. We found that inhibition plays a crucial role in decorrelation and prediction, helping activate specific assemblies through competition while preventing runaway excitation within active assemblies. We have explained this in ll.560-593 in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Finally, the authors should make the relation to previous models of sampling and error-based plasticity more clear. Since there is no formal derivation of the sampling dynamics, it is difficult to assess how they differ exactly from previous (Markov-based) approaches, which should be made more precise. Especially, it would be important to have concrete (ideally experimentally testable) predictions on how these two ideas differ. As a side note, especially in the introduction (line 90), this unclarity about the sampling made it difficult to understand the contrast to Markovian transition models.</p></disp-quote><p>As the reviewer pointed out, previous computational models have demonstrated that recurrent networks with Hebbian-like plasticity can learn appropriate Markovian statistics (Kappel et al., 2014; Asabuki and Clopath, 2024). However, our model differs conceptually from these previous models. While Kappel et al. showed that STDP in winner-take-all circuits can approximate online learning of hidden Markov models (HMMs), a key difference with our model is that their neural representations acquire sequences using Markovian sampling dynamics, whereas our model does not depend on such ordered sampling. Specifically, in their model, sequential sampling arises from learned structures in the off-diagonal elements of the recurrent connections (i.e., between-assembly connections). In contrast, our network learns to stochastically generate recurrent cell assemblies by relying solely on within-assembly connections. A similar argument can be made for Asabuki and Clopath paper as well. Further, while our model introduced plasticity rule for all types of connections, Asabuki and Clopath paper introduced plasticity for recurrent synapses projecting on the excitatory neurons only and the cell assembly memberships were preconfigured unlike our model. We have added additional clarifying sentences in ll. 757-772 of the revised manuscript to elaborate on this point.</p><disp-quote content-type="editor-comment"><p>There are also several related models that have not been mentioned and should be discussed. In 663 ff. the authors discuss the contributions of their model which they claim are novel, but in Kappel et al (STDP Installs in Winner-Take-All Circuits an Online Approximation to Hidden Markov Model Learning) similar elements seem to exist as well, and the difference should be clarified. There is also a range of other models with lateral inhibition that make use of error-based plasticity (most recently reviewed in Mikulasch et al, Where is the error? Hierarchical predictive coding through dendritic error computation), and it should be discussed how the proposed model differs from these.</p></disp-quote><p>We have clarified the difference from previously proposed recurrent network model to perform Markovian sampling. Please see our reply above.</p><p>We have also included additional sentence in ll. 704-709 in the revised manuscript to discuss how our model differs from similar predictive learning models: “It should be noted that while several network models that perform errorbased computations like ours exploit only inhibitory recurrent plasticity (Mikulasch et al., 2021; Mackwood et al., 2021; Hertäg and Clopath., 2022; Mikulasch et al., 2023), our model learns the structured spontaneous activity to reproduce the evoked statistics by modifying both excitatory and inhibitory recurrent connections.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The paper considers a recurrent network with neurons driven by external input. During the external stimulation predictive synaptic plasticity adapts the forward and recurrent weights. It is shown that after the presentation of constant stimuli, the network spontaneously samples the states imposed by these stimuli. The probability of sampling stimulus x^(i) is proportional to the relative frequency of presenting stimulus x^(i) among all stimuli i=1,..., 5.</p><p>Methods:</p><p>Neuronal dynamics:</p><p>For the main simulation (Figure 3), the network had 500 neurons, and 5 nonoverlapping stimuli with each activating 100 different neurons where presented. The voltage u of the neurons is driven by the forward weights W via input rates x, the inhibitory recurrent weights G, are restricted to have non-negative weights (Dale's law), and the other recurrent weights M had no sign-restrictions. Neurons were spiking with an instantaneous Poisson firing rate, and each spike-triggered an exponentially decaying postsynaptic voltage deflection. Neglecting time constants of the postsynaptic responses, the expected postsynaptic voltage reads (in vectorial form) as</p><p>u = W x + (M - G) f (Eq. 5)</p><p>where f = ; phi(u) represents the instantaneous Poisson rate, and phi a sigmoidal nonlinearity. The rate f is only an approximation (symbolized by = ;) of phi(u) since an additional regularization variable h enters (taken up in Point 4 below). The initialisation of W and M is Gaussian with mean 0 and variance 1/sqrt(N), N the number of neurons in the network. The initial entries of G are all set to 1/sqrt(N).</p><p>Predictive synaptic plasticity:</p><p>The 3 types of synapses were each adapted so that they individually predict the postsynaptic firing rate f, in matrix form</p><p>ΔW ≈ (f - phi(W x)) x^T</p><p>ΔM ≈ (f - phi(M f)) f^T</p><p>ΔG ≈ (f - phi(M f)) f^T but confined to non-negative values of G (Dale's law).</p><p>The ^T tells us to take the transpose, and the ≈ again refers to the fact that the ϕ entering in the learning rule is not exactly the ϕ determining the rate, only up to the regularization (see Point 4).</p><p>Main formal result:</p><p>As the authors explain, the forward weight W and the unconstrained weight M develop such that, in expectations,</p><p>f = ; phi(W x) = ; phi(M f) = ; phi(G f) ,</p><p>consistent with the above plasticity rules. Some elements of M remain negative. In this final state, the network displays the behaviour as explained in the summary.</p><p>Major issues:</p><p>Point 1: Conceptual inconsistency</p><p>The main results seem to arise from unilaterally applying Dale's law only to the inhibitory recurrent synapses G, but not to the excitatory recurrent synapses M.</p><p>In fact, if the same non-negativity restriction were also imposed on M (as it is on G), then their learning rules would become identical, likely leading to M=G. But in this case, the network becomes purely forward, u = W x, and no spontaneous recall would arise. Of course, this should be checked in simulations.</p><p>Because Dale's law was only applied to G, however, M and G cannot become equal, and the remaining differences seem to cause the effect.</p><p>Predictive learning rules are certainly powerful, and it is reasonable to consider the same type of error-correcting predictive learning rule, for instance for different dendritic branches that both should predict the somatic activity. Or one may postulate the same type of error-correcting predictive plasticity for inhibitory and excitatory synapses, but then the presynaptic neurons should not be identical, as it is assumed here. Both these types of error-correcting and error-forming learning rules for same-branches and inhibitory/excitatory inputs have been considered already (but with inhibitory input being itself restricted to local input, for instance).</p></disp-quote><p>The model presented above lacked biological plausibility in several key aspects. Specifically, we assumed that the recurrent connection M could change sign through plasticity and be either excitatory or inhibitory, while the inhibitory connection G was restricted to being inhibitory only. This initial setting does not reflect the biological constraint that synapses typically maintain a consistent excitatory or inhibitory type. Furthermore, due to this unconstrained recurrent connectivity M, the original model had two types of inhibitory connections (i.e., the negative part of M and the inhibitory connection G) without providing a clear computational role for each type of inhibition.</p><p>To address these limitations and to understand the role of the two types of inhibition, we explored a new architecture where all recurrent connections are either exclusively excitatory or inhibitory, keeping their sign throughout the learning process. This change addresses the reviewer's concern about our initial assumption that only the inhibitory connection G was constrained to non-negative values. We found that inhibition plays a crucial role in prediction and decorrelation, helping activate specific assemblies through competition while preventing runaway excitation within active assemblies. We have explained this in ll. 561593 in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Point 2: Main result as an artefact of an inconsistently applied Dale's law?</p><p>The main result shows that the probability of a spontaneous recall for the 5 nonoverlapping stimuli is proportional to the relative time the stimulus was presented. This is roughly explained as follows: each stimulus pushes the activity from 0 up towards f = ; phi(W x) by the learning rule (roughly). Because the mean weights W are initialized to 0, a stimulus that is presented longer will have more time to push W up so that positive firing rates are reached (assuming x is non-negative). The recurrent weights M learn to reproduce these firing rates too, while the plasticity in G tries to prevent that (by its negative sign, but with the restriction to non-negative values). Stimuli that are presented more often, on average, will have more time to reach the positive target and hence will form a stronger and wider attractor. In spontaneous recall, the size of the attractor reflects the time of the stimulus presentation. This mechanism so far is fine, but the only problem is that it is based on restricting G, but not M, to non-negative values.</p></disp-quote><p>As mentioned above, we have included an additional simulation where all weights are non-negative. We have demonstrated the new results in Figure 6 before presenting the two-population model in the revised manuscript (Figure 7), so that readers can follow the importance of two pathways of inhibitory connections.</p><disp-quote content-type="editor-comment"><p>Point 3: Comparison of rates between stimulation and recall.</p><p>The firing rates with external stimulations will be considerably larger than during replay (unless the rates are saturated).</p><p>This is a prediction that should be tested in simulations. In fact, since the voltage roughly reads as u = W x + (M - G) f, and the learning rules are such that eventually M = ; G, the recurrences roughly cancel and the voltage is mainly driven by the external input x. In the state of spontaneous activity without external drive, one has u = (M - G) f , and this should generate considerably smaller instantaneous rates f = ; phi(u) than in the case of the feedforward drive (unless f is in both cases at the upper or lower ceiling of phi). This is a prediction that can also be tested.</p><p>Because the figures mostly show activity ratios or normalized activities, it was not possible for me to check this hypothesis with the current figures. So please show non-normalized activities for comparing stimulation and recall for the same patterns.</p></disp-quote><p>We agree with the reviewer that the activity levels of spontaneous and induced activity should be compared. We have shown the distributions of activity level of these activities in our new Figure 2d. As expected, we found that the evoked activity showed stronger activity compared to the spontaneous activity.</p><disp-quote content-type="editor-comment"><p>Point 4: Unclear definition of the variable h.</p><p>The formal definition of h = hi is given by (suppressing here the neuron index i and the h-index of tau)</p><p>tau dh/dt = -h if h&gt;u, (Eq. 10) h = u otherwise.</p><p>But if it is only Equation 10 (nothing else is said), h will always become equal to u, or will vanish, i.e. either h=u or h=0 after some initial transient. In fact, as soon as h&gt;u, h is decaying to 0 according to the first line. If u is &gt;0, then it stops at u=h according to the second line. No reason to change h=u further. If u&lt;=0 while h&gt;u, then h is converging to 0 according to the first line and will stay there. I guess the authors had issues with the recurrent spiking simulations and tried to fix this with some regularization. However as presented, it does not become clear how their regulation works.</p></disp-quote><p>We apologize for the reviewer that our definition of h was unclear. As the reviewer pointed out, since the memory trace is always positive and larger than (or equal to) the membrane potential, it is possible that the membrane potential becomes always negative and the memory trace reach to 0 constantly. However, since the network is always balanced between excitatory and inhibitory inputs, and it does not happen that the membrane potential always diverges negatively. In fact, we trained without any manipulations other than the memory trace described in the manuscript, and the network was able to learn the assembly structure stably.</p><disp-quote content-type="editor-comment"><p>BTW: In Eq. 11 the authors set the gain beta to beta = beta0/h which could become infinite and, putatively more problematic, negative, depending on the value of h. Maybe some remark would convince a reader that no issues emerge from this.</p></disp-quote><p>We have mentioned in ll. 864-866 in the revised manuscript that no issues emerge from the slope parameter.</p><disp-quote content-type="editor-comment"><p>Added from discussions with the editor and the other reviewers:</p><p>Thanks for alerting me to this Supplementary Figure 8. Yes, it looks like the authors did apply there Dale's law for both the excitatory and inhibitory synapses. Yet, they also introduced two types of inhibitory pathways converging both to the excitatory and inhibitory neurons. For me, this is a confirmation that applying Dale's law to both excitatory and inhibitory synapses, with identical learning rules as explained in the main part of the paper, does not work.</p><p>Adding such two pathways is a strong change from the original model as introduced before, and based on which all the Figures in the main text are based. Supplementary Figure 8 should come with an analysis of why a single inhibitory pathway does not work. I guess I gave the reason in my Points 1-3. Some form of symmetry breaking between the recurrent excitation and recurrent inhibition is required so that, eventually, the recurrent excitatory connection will dominate.</p><p>Making the inhibitory plasticity less expressive by applying Dale's law to only those inhibitory synapses seems to be the answer chosen in the Figures of the main text (but then the criticism of unilaterally applying Dale's law).</p><p>Applying Dale's law to both types of synapses, but dividing the labor of inhibition into two strictly separate and asymmetric pathways, and hence asymmetric development of excitatory and inhibitory weights, seems to be another option. However, introducing such two separate inhibitory pathways, just to rescue the fact that Dale's law is applied to both types of synapses, is a bold assumption. Is there some biological evidence of such two pathways in the inhibitory, but not the excitatory connections? And what is the computational reasoning to have such a separation, apart from some form of symmetry breaking between excitation and inhibition? I guess, simpler solutions could be found, for instance by breaking the symmetry between the plasticity rules for the excitatory and inhibitory neurons. All these questions, in my view, need to be addressed to give some insights into why the simulations do work.</p></disp-quote><p>The reviewer’s intuition is correct. To effectively learn cell assembly structures and replay their activities, our model indeed requires two types of inhibitory connections. Please refer to our response above for further details.</p><disp-quote content-type="editor-comment"><p>Overall, Supplementary Figure 8 seems to me too important to be deferred to the Supplement. The reasoning behind the two inhibitory pathways should appear more prominently in the main text. Without this, important questions remain. For instance, when thinking in a rate-based framework, the two inhibitory pathways twice try to explain the somatic firing rate away. Doesn't this lead to a too strong inhibition? Can some steady state with a positive firing rate caused by the recurrence, in the absence of an external drive, be proven? The argument must include the separation into Path 1 and Path 2. So far, this reasoning has not been entered.</p><p>In fact, it might be that, in a spiking implementation, some sparse spikes will survive. I wonder whether at least some of these spikes survive because of the other rescuing construction with the dynamic variable h (Equation 10, which is not transparent, and that is not taken up in the reasoning either, see my Point 4)</p><p>Perhaps it is helpful for the authors to add this text in the reply to them.</p></disp-quote><p>We have moved the former Supplemental Figure 8 to the main Figure 7. Please see our response above about the role of dual inhibitory connection types.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>The work shows how learned assembly structure and its influence on replay during spontaneous activity can reflect the statistics of stimulus input. In particular, stimuli that are more frequent during training elicit stronger wiring and more frequent activation during replay. Past works (Litwin-Kumar and Doiron, 2014; Zenke et al., 2015) have not addressed this specific question, as classic homeostatic mechanisms forced activity to be similar across all assemblies. Here, the authors use a dynamic gain and threshold mechanism to circumnavigate this issue and link this mechanism to cellular monitoring of membrane potential history.</p><p>Strengths:</p><p>(1) This is an interesting advance, and the authors link this to experimental work in sensory learning in environments with non-uniform stimulus probabilities.</p><p>(2) The authors consider their mechanism in a variety of models of increasing complexity (simple stimuli, complex stimuli; ignoring Dale's law, incorporating Dale's law).</p><p>(3) Links a cellular mechanism of internal gain control (their variable h) to assembly formation and the non-uniformity of spontaneous replay activity. Offers a promise of relating cellular and synaptic plasticity mechanisms under a common goal of assembly formation.</p><p>Weaknesses:</p><p>(1) However, while the manuscript does show that assembly wiring does follow stimulus likelihood, it is not clear how the assembly-specific statistics of h reflect these likelihoods. I find this to be a key issue.</p></disp-quote><p>We agree that our previous claim regarding the importance of the memory trace h for sampling may have been confusing. As shown in Supplementary Figure 7b, when we eliminated the dynamics of the memory trace, sampling performance did indeed decrease. However, we also observed that the assembly activity ratio continued to show a linear relationship with stimulus probabilities. Based on these findings, we revised our claim in the manuscript to clarify that the memory trace is primarily critical for learning to avoid trivial solutions, rather than directly influencing sampling within the learned network. We have explained this in ll. 446-448 in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>(2) The authors' model does take advantage of the sigmoidal transfer function, and after learning an assembly is either fully active or nearly fully silent (Figure 2a). This somewhat artificial saturation may be the reason that classic homeostasis is not required since runaway activity is not as damaging to network activity.</p></disp-quote><p>The reviewer's intuition is correct. The saturating nonlinearity is important for the network to form stable assembly structures. We have added an additional sentence in ll. 866-868 to mention this.</p><disp-quote content-type="editor-comment"><p>(3) Classic mechanisms of homeostatic regulation (synaptic scaling, inhibitory plasticity) try to ensure that firing rates match a target rate (on average). If the target rate is the same for all neurons then having elevated firing rates for one assembly compared to others during spontaneous activity would be difficult. If these homeostatic mechanisms were incorporated, how would they permit the elevated firing rates for assemblies that represent more likely stimuli?</p></disp-quote><p>LIF neurons may solve this problem by utilizing spike-timing statistics.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Minor issues:</p><p>Figure 1: It would be helpful to display the equation for output rate here as well.</p></disp-quote><p>We have included the equation in the revised Figure 1a.</p><disp-quote content-type="editor-comment"><p>Figure 3c: Typo &quot;indivisual neurons&quot;.</p></disp-quote><p>We have modified the typo. We thank the reviewer for their careful review.</p><disp-quote content-type="editor-comment"><p>Line 325: Do you mean Figure 3f,g?</p></disp-quote><p>We repeated the task with different numbers of stimuli in Supplementary Figure 1c,d.</p><disp-quote content-type="editor-comment"><p>Line 398: Winner-take-all can be misunderstood, as it typically stands for competition in inference, not in learning.</p></disp-quote><p>We have rephrased it as “unstable dynamics” in l. 400</p><disp-quote content-type="editor-comment"><p>Line 429: Are intra-assembly and within-assembly the same? If so please use consistent terminology.</p></disp-quote><p>We have made the terminology consistent.</p><disp-quote content-type="editor-comment"><p>Line 792 ff.: Please mention that (t) was left away.</p></disp-quote><p>We have included a sentence to mention it in ll. 847-848 in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Line 817: Should u_i be v_i?</p></disp-quote><p>We have modified the term.</p><disp-quote content-type="editor-comment"><p>Methods: What is the value of tau_h?</p></disp-quote><p>We have used 𝜏! = 10 s, which is mentioned in l. 853</p></body></sub-article></article>