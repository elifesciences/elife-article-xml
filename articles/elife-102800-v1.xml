<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">102800</article-id><article-id pub-id-type="doi">10.7554/eLife.102800</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102800.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Advantageous and disadvantageous inequality aversion can be taught through learning of others’ preferences</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Shen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5801-286X</contrib-id><email>shen.zhang@mail.bnu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>FeldmanHall</surname><given-names>Oriel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0726-3861</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hétu</surname><given-names>Sébastien</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5323-931X</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Otto</surname><given-names>A Ross</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9997-1901</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>State Key Laboratory of Cognitive Neuroscience and Learning &amp; IDG/McGovern Institute for Brain Research, Beijing Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f99v835</institution-id><institution>Department of Neurobiology, German Primate Center</institution></institution-wrap><addr-line><named-content content-type="city">Göttingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Cognitive, Linguistics and Psychological Sciences, Brown University</institution></institution-wrap><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Carney Institute for Brain Sciences, Brown University</institution></institution-wrap><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0161xgx34</institution-id><institution>Department of Psychology, Université de Montréal</institution></institution-wrap><addr-line><named-content content-type="city">Montréal</named-content></addr-line><country>Canada</country></aff><aff id="aff6"><label>6</label><institution>Centre Interdisciplinaire de Recherche sur le Cerveau et l’Apprentissage (CIRCA)</institution><addr-line><named-content content-type="city">Montréal</named-content></addr-line><country>Canada</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>Department of Psychology, McGill University</institution></institution-wrap><addr-line><named-content content-type="city">Montréal</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Press</surname><given-names>Clare</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>University Medical Center Hamburg-Eppendorf</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>11</day><month>12</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP102800</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-09-12"><day>12</day><month>09</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-08-26"><day>26</day><month>08</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2405.06500"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-06"><day>06</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102800.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-10-06"><day>06</day><month>10</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102800.2"/></event></pub-history><permissions><copyright-statement>© 2025, Zhang et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Zhang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-102800-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-102800-figures-v1.pdf"/><abstract><p>While enforcing egalitarian social norms is critical for human society, punishing social norm violators often incurs a cost to the self. This cost looms even larger when one can benefit from an unequal distribution of resources, a phenomenon known as advantageous inequity—for example, receiving a higher salary than a colleague with the identical role. In the Ultimatum Game, a classic testbed for fairness norm enforcement, individuals rarely reject (or punish) such unequal proposed divisions of resources because doing so entails a sacrifice of one’s own benefit. Recent work has demonstrated that observing and implementing another’s punitive responses to unfairness can efficiently alter the punitive preferences of an observer. It remains an open question, however, whether such contagion is powerful enough to impart advantageous inequity aversion to individuals—that is, can observing another’s preferences to punish inequity result in increased enforcement of equality norms, even in the difficult case of Advantageous inequity? Using a variant of the Ultimatum Game in which participants are tasked with responding to fairness violations on behalf of another ‘Teacher’—whose aversion to advantageous (versus disadvantageous) inequity was systematically manipulated—we probe whether individuals subsequently increase their punishment unfairly after experiencing fairness violations on their own behalf. In two experiments, we found individuals can acquire aversion to advantageous inequity through observing (and implementing) the Teacher’s preferences. Computationally, these learning effects were best characterized by a model which learns the latent structure of the Teacher’s preferences, rather than a simple Reinforcement Learning account. In summary, our study is the first to demonstrate that people can swiftly and readily acquire another’s preferences for advantageous inequity, suggesting in turn that behavioral contagion may be one promising mechanism through which social norm enforcement—which people rarely implement in the case of advantageous inequality—can be enhanced.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>fairness</kwd><kwd>learning</kwd><kwd>modeling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/019w4f821</institution-id><institution>European Union</institution></institution-wrap></funding-source><award-id>101041799</award-id></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01h531d29</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Discovery Grant</award-id><principal-award-recipient><name><surname>Otto</surname><given-names>A Ross</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b9f9778</institution-id><institution>Fonds de Recherche du Québec – Nature et Technologies</institution></institution-wrap></funding-source><award-id>New Researchers Startup Grant</award-id><principal-award-recipient><name><surname>Otto</surname><given-names>A Ross</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000az4664</institution-id><institution>Canada Foundation for Innovation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Otto</surname><given-names>A Ross</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Like disadvantageous inequity aversion, advantageous inequity aversion can be learned by observing another’s fairness preferences.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans can learn how to navigate through the world by observing the actions of others. For example, individuals can learn complex motor skills by observing and imitating how experts coordinate their movements (<xref ref-type="bibr" rid="bib27">Hayes et al., 2008</xref>). Observational learning can also transmit important information about social and moral norms, such as reciprocity (<xref ref-type="bibr" rid="bib18">Engelmann and Fischbacher, 2009</xref>), cooperating with others (<xref ref-type="bibr" rid="bib51">van Baar et al., 2019</xref>), or context in which punishment is considered appropriate (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>). In some cases, an individual learns from others in a straightforward and unambiguous social context, where the tensions endemic to many moral dilemmas—for example, benefit oneself versus the collective good—are not directly juxtaposed against one another. And yet, the social world is rarely straightforward, often ambiguous, and moral dilemmas that pit self-benefit over the collective good abound (<xref ref-type="bibr" rid="bib23">FeldmanHall and Shenhav, 2019</xref>; <xref ref-type="bibr" rid="bib53">Vives and FeldmanHall, 2018</xref>).</p><p>Consider the case of inequity, for which individuals exhibit a strong distaste: concerns for fairness are well documented in adults (<xref ref-type="bibr" rid="bib26">Güth et al., 1982</xref>; <xref ref-type="bibr" rid="bib45">Sanfey et al., 2003</xref>), children (<xref ref-type="bibr" rid="bib21">Fehr et al., 2008</xref>; <xref ref-type="bibr" rid="bib42">McAuliffe and Dunham, 2017</xref>), primates (<xref ref-type="bibr" rid="bib10">Brosnan and De Waal, 2003</xref>; <xref ref-type="bibr" rid="bib11">Brosnan and de Waal, 2014</xref>; <xref ref-type="bibr" rid="bib52">van Wolkenten et al., 2007</xref>), and even domesticated dogs (<xref ref-type="bibr" rid="bib19">Essler et al., 2017</xref>). This aversion to inequity manifests perhaps most famously in the Ultimatum Game, in which one player (the Proposer) decides how to split a sum of money with another player (the Receiver), a role typically assumed by the participant (<xref ref-type="bibr" rid="bib26">Güth et al., 1982</xref>; <xref ref-type="bibr" rid="bib45">Sanfey et al., 2003</xref>). A Receiver’s acceptance results in both parties receiving the offered money, whereas rejection results in neither party receiving any money—a form of costly punishment. A recurring observation supporting the notion of inequity aversion is that people tend to reject disadvantageous offers that unfairly benefit the other party (<xref ref-type="bibr" rid="bib11">Brosnan and de Waal, 2014</xref>; <xref ref-type="bibr" rid="bib20">Fehr and Schmidt, 1999</xref>).</p><p>At the same time, not all inequity is experienced in the same way. For example, when we stand to receive less than our ‘fair share’, such disadvantageous inequity (Dis-I for short) engenders feelings of envy, anger, and/or disappointment (<xref ref-type="bibr" rid="bib28">Heffner and FeldmanHall, 2022</xref>) which often manifests in punishment of unfair offers in the Ultimatum Game via rejection (<xref ref-type="bibr" rid="bib40">McAuliffe et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">McAuliffe et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Pedersen et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Pillutla and Murnighan, 1996</xref>). In contrast, when we stand to receive a favorable (albeit unfairly distributed) share of resources, these advantageous inequitable (Adv-I) offers often engender feelings of guilt or shame (<xref ref-type="bibr" rid="bib24">Gao et al., 2018</xref>). Despite these negative emotions, Receivers in Ultimatum Game settings are much less willing to engage in costly punishment of offers that are advantageously inequitable (<xref ref-type="bibr" rid="bib15">Civai et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Hennig-Schmidt et al., 2008</xref>; <xref ref-type="bibr" rid="bib38">Luo et al., 2018</xref>). In short, Adv-I versus Dis-I engender markedly different punishment preferences.</p><p>A growing body of developmental research demonstrates that this difference in punitive responses to Adv-I versus Dis-I manifests early in the developmental trajectory (<xref ref-type="bibr" rid="bib2">Amir et al., 2023</xref>; <xref ref-type="bibr" rid="bib9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="bib8">Blake and McAuliffe, 2011</xref>; <xref ref-type="bibr" rid="bib41">McAuliffe et al., 2017</xref>). Indeed, punishing Adv-I offers requires sacrificing a (larger) personal gain to achieve a fairer outcome, mirroring many moral dilemmas in which self- versus other-regarding interests are at odds. Because aversion to Adv-I is thought to arise from more abstract concerns about fairness (<xref ref-type="bibr" rid="bib49">Tomasello, 2019</xref>), Adv-I aversion is thought to impose considerable demands on more sophisticated cognitive processing (<xref ref-type="bibr" rid="bib24">Gao et al., 2018</xref>). This may explain, in part, why the developmental trajectory of Adv-I-averse preferences comes online much later relative to Dis-I (<xref ref-type="bibr" rid="bib41">McAuliffe et al., 2017</xref>).</p><p>Thus, one open question concerns how people acquire inequity-averse preferences. One influential framework posits that we often adapt our behaviors to people around us through a process of conformity (<xref ref-type="bibr" rid="bib14">Cialdini and Goldstein, 2004</xref>). Put simply, loyally following the behaviors of another—a social contagion effect—is a powerful motivator of social behavior. Such behavioral contagion effects are observed in diverse decision-making domains such as valuation (<xref ref-type="bibr" rid="bib13">Campbell-Meiklejohn et al., 2010</xref>), risk-taking (<xref ref-type="bibr" rid="bib47">Suzuki et al., 2016</xref>), delay of gratification (<xref ref-type="bibr" rid="bib25">Garvert et al., 2015</xref>), moral preferences (<xref ref-type="bibr" rid="bib6">Bandura and McDonald, 1963</xref>; <xref ref-type="bibr" rid="bib54">Vives et al., 2022</xref>), and social norms (<xref ref-type="bibr" rid="bib32">Hertz, 2021</xref>). Recently, we demonstrated that punitive responses to Dis-I can be ‘taught’ to participants in the context of the Ultimatum Game, whereby individuals’ preferences for rejecting disadvantageous unfair offers were strengthened as a result of observing another individual’s (a ‘Teacher’) desire to punish these offers (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>).</p><p>At the same time, social contagion effects may be far less robust if the behavior demands sacrifice of self-benefit. Since this type of dynamic places behavioral contagion and desire for self-gain in opposition, it remains unclear whether Adv-I aversion can also be acquired ‘vicariously’ through observing another’s preferences. Here, we investigate whether the act of observing the Adv-I-averse preferences of another punitive Receiver enhances an individual’s aversion to Adv-I, even if the rejection of such Adv-I offers requires sacrificing self-benefit. Answering this question not only enriches our understanding of the nature of inequity aversion, but also enables us to better understand the mechanisms underpinning learning of others’ moral preferences during social interactions.</p><p>Computationally, one can imagine different possible mechanisms driving learning of inequity aversion. One possibility is that observational learning of moral preference is based on simple action-outcome contingencies. On this view, a simple but elegant Reinforcement Learning (RL; <xref ref-type="bibr" rid="bib12">Burke et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Diaconescu et al., 2020</xref>; <xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Lindström et al., 2019</xref>) model formalizes how individuals adapt their behavior to recently observed outcomes. In its most basic form, RL makes choices on the basis of learned associations between actions and outcomes, and critically, actions are bound to the specific decision context—in the case of the Ultimatum Game, the unfairness of the amount offered by the Proposer. However, during social interactions, people might not consider the behaviors of others as resulting from simple action-outcome associations but alternatively, construct and use models of other agents, representing their stable intentions, beliefs, and preferences (<xref ref-type="bibr" rid="bib3">Anzellotti and Young, 2020</xref>). Accordingly, we also consider the possibility that moral preferences are immutable across contexts (<xref ref-type="bibr" rid="bib4">Bail et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Fehr and Schmidt, 1999</xref>; <xref ref-type="bibr" rid="bib48">Taber and Lodge, 2012</xref>), which would suggest that moral preferences are not learned merely as associations (i.e. specific responses tied to different unfairness levels), but rather, through a deeper inference process which models the underlying fairness preferences of the observed individual. Here, across two experiments, we build upon a well-characterized observational learning paradigm (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Son et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Vives et al., 2022</xref>) to examine the conditions under which individuals are able to learn Adv-I-averse preferences on the basis of exposure to another Receiver’s punitive preferences. In addition, to mechanistically probe how punitive preferences may be acquired in Adv-I and Dis-I contexts we also characterize trial-by-trial acquisition of punitive behavior with computational models of choice.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experiment 1</title><p>In Experiment 1, following the approach of previous experiments (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>), we test if the rejection of advantageous unfair offers can be learned on the basis of exposure to the preferences of another individual (the ‘Teacher’) exhibiting Adv-I-averse preferences. In a between-subject design with three phases, participants interacted with other individuals in a repeated Ultimatum Games (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We assess contagion effects by measuring participants’ Dis-I and Adv-I aversion both before and after observing (and implementing) the preferences of a Teacher who exhibits inequity aversion in both Dis-I and Adv-I contexts (‘Adv-Dis-I-Averse’ condition; N=100) and inequity aversion only in a Dis-I context (‘Dis-I-Averse’ condition; N=100).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experiment procedure and task design.</title><p>(<bold>a</bold>) Task Overview. Our main task consists of three phases. In the Baseline Phase, participants acted as a Receiver, responding to offers of different inequity levels and rated their perceived fairness of the offers on three out of every five trials. In the subsequent Learning Phase, participants acted as an Agent, deciding on behalf of the Receiver (Teacher) and Proposer. Again, they rated the fairness on three out of every five trials. Finally, participants made choices in a Transfer Phase which was identical to the Baseline Phase. (<bold>b</bold>) Preferences and Fairness Ratings governing the Teacher’s feedback in the Learning Phase (See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1A</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1B</xref>). (<bold>c</bold>) Baseline and Transfer phase, in which participants played the Ultimatum game as a Receiver, making choices on their own behalf. (<bold>d</bold>) In the Learning phase, participants acted as a third party (the agent), making decisions on behalf of the Proposer and the Receiver (Teacher), playing a Third-Party Ultimatum game. In a Third-Party Ultimatum game, the participant makes decisions on behalf of the Receiver: if they rejects the proposed split, both the Proposer and the Receiver receive nothing. If they accept, the Proposer and the Receiver are rewarded with the proposed split.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-fig1-v1.tif"/></fig><p>First, to assess participants’ baseline fairness preferences across inequity levels, in the Baseline Phase (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) participants acted as a Receiver in several one-shot UGs, responding to offers ranging from extreme Dis-I (e.g. the Proposer keeps 90 cents and offers 10 cents to the Receiver; a 90:10 split) to extreme Adv-I (e.g. the Proposer keeps 10 cents and offers 90 cents to the Receiver; a 10:90 split) in a total of five offer types (90:10, 70:30, 50:50, 30:70, 10:90). On each trial, participants interacted with a different Proposer, and unbeknownst to participants, the offers were pre-determined by the experimenters. Following the typical formulation of the UG (<xref ref-type="bibr" rid="bib26">Güth et al., 1982</xref>), participants made the choice between accepting versus rejecting each offer and also rated the fairness of the offer.</p><p>Next, in the Learning Phase (<xref ref-type="fig" rid="fig1">Figure 1d</xref>), participants played a repeated UG as a third party rather than a Receiver, in which they accepted or rejected offers on behalf of another Receiver (termed the Teacher in this phase) such that the participant’s decisions did not impact their own payoff but would impact the payoffs to the Proposer and the Teacher. Critically, after each decision, participants received feedback whether the Teachers <italic>would</italic> have preferred acceptance versus rejection (i.e. punishment) of the offer. Thus, through trial-by-trial feedback, the Teacher can signal to participants their preference to punish the Proposer for making unfair offers. Critically, in the ‘Dis-I-Averse’ condition, akin to the typical pattern of human preferences observed in the UG (<xref ref-type="bibr" rid="bib26">Güth et al., 1982</xref>; <xref ref-type="bibr" rid="bib45">Sanfey et al., 2003</xref>), the Teacher’s preferences exhibited a strong aversion to Dis-I, and thus routinely punishing unfair offers. Specifically, the Teacher was likely to reject Dis-I offers (i.e. 90:10 and 70:30), but not Adv-I offers (i.e. 30:70 and 10:90; see <xref ref-type="fig" rid="fig1">Figure 1b</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1A</xref>). However, in the ‘Adv-Dis-I-Averse’ condition, the Teacher was likely to reject any unfair offer, regardless of whether it was Dis-I or Adv-I, manifesting typical Dis-I-averse preferences as well as the less commonly observed aversion to Adv-I (i.e. punishing advantageous offers). Feedback from the Teacher was also accompanied by fairness ratings consistent with their preferences (see <xref ref-type="fig" rid="fig1">Figure 1b</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1B</xref>).</p><p>Finally, to examine contagion (or transmission) of the Teacher’s preferences to the participants, we assessed fairness preferences of the participants for a second time in a Transfer Phase (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). This third phase was identical to the Baseline Phase and thus allowed us to quantify changes in participants’ fairness preferences before and after the Learning Phase.</p><sec id="s2-1-1"><title>Preferences across baseline and transfer phases</title><p>Mirroring preferences typically observed in Western, Educated, Industrialized, Rich, and Democratic (WEIRD) participant populations (<xref ref-type="bibr" rid="bib31">Henrich et al., 2006</xref>), punishment choices in the Baseline Phase were Dis-I-averse, but not Adv-I-averse, as we observed similarly high rejection rates during the Baseline Phase for Dis-I offers across the Dis-I-Averse and the Adv-Dis-I-Averse conditions (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1C</xref>; all ps &lt; 0.001). As expected, the rejection rates for Adv-I offers were much lower than Dis-I offers (linear contrasts; e.g. 90:10 vs 10:90, all ps &lt; 0.001, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1C</xref> for logistic regression coefficients for rejection rates). Consistent with these Rejection rates, participants rated Dis-I offers as unfair (i.e. lower than the rating scale midpoint of 4, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1D</xref>; all ps &lt; 0.001) and Adv-I offers were rated as more fair than Dis-I offers (all ps &lt; 0.001)—despite the fact that the offers in the Adv-I and Dis-I contexts represent the same magnitude of inequity (e.g. 90:10 vs 10: 90 splits).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Behavioral contagion in Experiment 1.</title><p>(<bold>a</bold>) Rejection rates change significantly in Dis-I offers for both learning conditions, while changes in Adv-I offers were only evident in Adv-Dis-I-Averse condition. (<bold>b</bold>) Participants in the Adv-Dis-I-Averse condition exhibited significant changes in fairness ratings, and these fairness rating changes differered significantly between conditions. Dashed lines indicate punishment preferences (Panel a) and fairness ratings (Panel b) exhibited by the Teacher . Error bars indicate standard error (†indicates p&lt;0.1, *indicates p&lt;0.05, **indicates p&lt;0.01, ***indicates p&lt;0.001,×indicates interaction). Resutls from linear mixed models (LMM, n = 100 for both conditions).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-fig2-v1.tif"/></fig><p> To examine whether exposure to the Teacher’s punishment preferences in the Learning Phase resulted in changes to participants’preferences, we examined changes in Rejection rates and Fairness ratings between the Baseline and Transfer phases (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Overall, we found robust changes in Rejection rates and Fairness ratings between the Baseline and Transfer phases (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1E, F</xref>). Importantly, when comparing these changes between the two learning conditions, we observed significant differences in rejection rates for Adv-I offers: compared to exposure to a Teacher who rejected only Dis-I offers, participants exposed to a Teacher who rejected both Dis-I and Adv-I offers were more likely to reject Adv-I offers and rated these offers as more unfair. This difference between conditions was evident for both 30:70 offers (Rejection rates: <italic>β</italic>(<italic>SE</italic>)=0.10(0.04), p=0.013; Fairness ratings: <italic>β</italic>(<italic>SE</italic>)=−0.86(0.17), p&lt;0.001) and 10:90 offers (Rejection rates: <italic>β</italic>(<italic>SE</italic>)=0.15(0.04), p&lt;0.001, Fairness ratings: <italic>β</italic>(<italic>SE</italic>)=−1.04(0.17), p&lt;0.001). As a control, we also compared rejection rates and fairness rating changes between conditions in Dis-I offers (90:10 and 30:70) and Fair offers (i.e. 50:50) but observed no significant difference (all ps &gt;0.217).</p></sec><sec id="s2-1-2"><title>Learning another’s preferences</title><p>Having demonstrated that participants’ preferences to reject unfair Adv-I offers were altered on the basis of exposure to the Teacher’s preferences, we next examined trial-by-trial changes in rejection rates during the Learning phase (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). A mixed-effects logistic regression revealed a significantly larger (positive) effect of trial number upon rejection rates for Adv-I offers in the Adv-Dis-I-Averse condition compared to the Dis-I-Averse condition. This relative rejection rate increase was evident both in 30:70 offers (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1G</xref>; <italic>β</italic>(<italic>SE</italic>)=0.77(0.24), p&lt;0.001) and in 10:90 offers (<italic>β</italic>(<italic>SE</italic>)=1.10(0.33), p&lt;0.001). In contrast, comparing Dis-I and Fairness offers when the Teacher showed the same tendency to reject, we found no significant difference between the two conditions (90:10 splits:<italic>β</italic>(<italic>SE</italic>)=-0.48(0.21), p=0.593 ; 70:30 splits: <italic>β</italic>(<italic>SE</italic>)=-0.01(0.14), p=0.150; 50:50 splits: <italic>β</italic>(<italic>SE</italic>)=-0.00(0.21), p=0.086). In other words, participants by and large appeared to adjust their rejection choices in accordance with the Teacher’s feedback in an incremental fashion.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Learning phase behavior in Experiment 1.</title><p>(<bold>a</bold>) Rejection rate changes in the Learning Phase. For Dis-I Offers, rejection rates increased for both learning conditions, while rejection rates only increased for Adv-I offers in the Adv-Dis-I-Averse condition. Furthermore, in Adv-I offers, the increasing trend was larger in the Adv-Dis-I-Averse condition than in the Dis-I-averse condition, indicating a learning effect. Solid thin lines denote participants’ rejection choices, dashed lines denote the Teacher’s preferences, and solid thick lines represent predictions of the (best-fitting) Preference Inference Model. (<bold>b</bold>) Model comparison demonstrating that the Preference Inference model provided the best fit to participants’ Learning Phase behavior (AIC: Akaike Information Criterion) (<bold>c and d</bold>). Parameter learning for the Preference Inference model, which captured a significant rejection rate increase in Adv-I offers by updating the guilt parameter in a trial-by-trial manner.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Model recovery results for the Preference Inference model.</title><p>All parameters are recoverable, Although the correlation between the true value and the recovered value of the inverse temperature is relatively low.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-fig3-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-1-3"><title>Computational models of learning punishment preferences</title><p>Having established that rejection rates increased for both Dis-I offers (in both the Dis-I-Averse and Adv-Dis-I-Averse conditions) and Adv-I offers (only in the Adv-Dis-I-Averse condition), we then sought to better understand the learning mechanisms underpinning trial-by-trial learning (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). We used computational modeling to formalize two different sets of assumptions regarding how participants learn from Teachers’ feedback. Under one account, a simple RL model proposes that decision-makers learn punishment preferences by observing feedback resulting from actions made in response to specific offers. Previously, we have found that this ‘naive’ model provided a reasonable characterization of participants’ trial-by-trial learning of Dis-I-averse preferences (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>). However, this model may fail to capture a critical facet of learning: participants’ moral preferences may not be learned merely as associations—the type of response being tied to specific offers—but rather, through a deeper inference process which models the underlying fairness preferences of the Teacher. Accordingly, our alternative model assumes that participants use trial-by-trial feedback to infer the Teacher’s underlying preferences concerning inequality, which may shift depending on the context (Dis-I versus Adv-I). Following the simple Fehr-Schmidt inequality aversion formalism <xref ref-type="bibr" rid="bib20">Fehr and Schmidt, 1999</xref>, in our model—termed the Preference Inference model—the Teacher’s aversion to Dis-I is modeled by an ‘Envy’ parameter, while the ‘Guilt’ parameter captures the Teacher’s aversion to Adv-I (see Materials and methods). Critically, the RL model does not learn the Teacher’s preferences per se, but the value of each action (accept or reject), independently for each offer type. In contrast, the Preference Inference model explicitly represents the extent of the Teacher’s Adv-I- and Dis-I-aversion—that is their underlying preferences—by independently updating the envy and guilt parameters using trial-by-trial feedback from the Teacher.</p><p>We compared the goodness of fit of seven different models to participants’ choices in the Learning Phase: the Preference Inference model, which learns the ‘guilt’ and ‘envy’ parameters experientially, a Static Preference model that assumes the ‘guilt’ and ‘envy’ are fixed over the course of learning (baseline model 1, see Materials and methods), three variants of a simple RL model that make different assumptions about how action values are represented (models 3-5), and a baseline or ‘null’ model that assumes a fixed probability of each action (randomly choosing, baseline model 2). While simplistic, RL models were able to characterize observational learning of punishment successfully in our previous study (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>) and thus serve as a reasonable baseline for evaluating more sophisticated models. Finally, we explored the possibility that a simple RL model imbued with the ability to generalize learning between similar offer types could capture Learning Phase behavior (the Similarity RL model; see Materials and methods). We fit each of the seven models via maximum likelihood estimation, penalizing for model complexity, and found that the Preference Inference model provided the best characterization of learning (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1H</xref>), suggesting that participants were performing trial-by-trial inference of the Teacher’s underlying inequity preferences, rather than simply learning reinforced associations between experienced offer types and actions. Even the Static Preference model, which does not assume any learning mechanism (but rather assumes fixed preferences with respect to Dis-I and Adv-I) provided a better characterization of learning than any of the three naive RL models which do assume incremental learning over paired associations. Furthermore, the Preference Inference model also outperforms the Similarity RL, which assumes a generalization mechanism.</p><p>To examine the learning dynamics underpinning the best-fitting Preference Inference Model, we simulated Learning Phase choice behavior using participants’ estimated parameter values (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, see Materials and methods for details). The close correspondence between the simulated and observed learning curves indicates that the Preference Inference model captures the reinforcement-guided variations in punishment in Dis-I-context and, crucially, the marked differences in learning between Dis-I-Averse and Adv-Dis-I-Averse conditions in Adv-I-context (30:70 and 10:90 splits). To better understand how the Preference Inference model accounts for these patterns of change in rejection rates, we examined how model-inferred aversion to Dis-I (‘envy’) and Adv-I (‘guilt’)—the two components of the Fehr-Schmidt inequality aversion model (<xref ref-type="bibr" rid="bib20">Fehr and Schmidt, 1999</xref>) representing the latent structure of the Teachers’ preferences—emerge as a function of exposure to the Teacher’s preferences. In both the Dis-I-Averse and Adv-Dis-I-Averse conditions, the model inferred a similarly ‘envious’ Teacher (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), while the model only increased its estimate of the Teacher’s ‘guilt’ parameter (<xref ref-type="fig" rid="fig3">Figure 3d</xref>) in the Adv-Dis-I-Averse condition (wherein the Teacher’s feedback manifested Adv-I aversion) mirroring the model’s—and participants’—shift in rejection rates over the course of the Learning Phase and further suggesting that the Preference Inference model captured critical aspects of participants’ learning of the Teachers’ preferences.</p><p>Experiment 1 extends our previous work (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>) by revealing that Adv-I-averse preferences—which are believed to be less mutable (<xref ref-type="bibr" rid="bib38">Luo et al., 2018</xref>)—can be similarly shaped by exposure to another individual with manifesting a strong aversion to resource divisions that unfairly benefit them. These results suggest that individuals’ moral preferences can be learned even in cases where these preferences conflict with one’s own self-interest. Computationally, this learning process was best characterized by an account that prescribes that individuals build a representation of others’ moral preferences about Dis-I and Adv-I (akin to the <xref ref-type="bibr" rid="bib20">Fehr and Schmidt, 1999</xref> model of inequity aversion), rather than by a simple RL account. This preference inference account predicts that individuals, when exposed to only a fraction of inequity-related punishment preferences, should generalize these inferred preferences to other similar inequity contexts. In Experiment 2, we sought to test this generalization hypothesis more directly, buttressing the idea that the learning and transfer of Inequity-averse preferences observed in Experiment 1 came about as a result of participants modeling the Teacher’s inequality-averse preferences.</p></sec></sec><sec id="s2-2"><title>Experiment 2</title><p>Experiment 2 provides a more stringent test of whether participants model the Teacher’s underlying inequity preferences. Specifically, if individuals indeed learn the Teacher’s latent inequity-averse preferences in the Learning phase, we would expect that feedback-driven learning of Teacher’s punishment preferences on specific (moderate inequity) offers (30:70 splits) should generalize to offers in the same context (10:90 splits) without any direct experience of those offer types. Accordingly, to probe for this sort of generalization—a hallmark of the sort of latent structure learning we attribute to the behavior we observed in Experiment 1—we now eliminate feedback for extreme Dis-I (90:10) and Adv-I (10:90) offers from the Learning phase. If participants’ punishment preferences are informed by modeling inferred inequity-averse preferences of the Teacher, we should expect to see these preferences transfer to participants’ own fairness preferences in a similar, generalized manner in the Transfer Phase.</p><p>Mirroring Experiment 1, we employed two conditions governing the Teachers’ preferences in the Learning Phase: In the Dis-I-Averse condition, the Teacher only exhibited strong punishment preferences concerning moderate Dis-I offers (30:70 splits), while the Teacher in the Adv-Dis-I-Averse condition exhibited strong preferences in response for both moderate Dis-I and moderate Adv-I offers (30:70 and 70:30 splits, respectively).</p><sec id="s2-2-1"><title>Contagion effects for extreme unfair offers suggest generalization</title><p>In Experiment 2, we took the same analysis approach as in Experiment 1, examining changes in rejection rates between the Baseline Phase and the Transfer Phase (after participants experienced feedback with moderately unfair offers). Similar to what we observed in Experiment 1 (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), compared to the participants in the Dis-I-Averse Condition, participants in the Adv-I-Averse Condition increased their rates of rejection of extreme Adv-I (10:90) offers in the Transfer Phase, relative to the Baseline phase (<italic>β</italic>(<italic>SE</italic>)=0.12(0.04), p=0.004; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1I</xref>), suggesting that participants’ learned (and adopted) Adv-I-averse preferences generalized from one specific offer type (30:70) to an offer type for which they received no Teacher feedback (10:90). Examining extreme Dis-I offers where the Teacher exhibited identical preferences across the two learning conditions, we found no difference in the Changes of Rejection Rates from Baseline to Transfer phase between conditions (<italic>β</italic>(<italic>SE</italic>)=-0.05(0.04), p&lt;0.259). Mirroring the observed rejection rates (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), relative to the Dis-I-Averse Condition, participants’ fairness ratings for extreme Adv-I offers increased more from the Baseline to Transfer phase in the Adv-Dis-I-Averse Condition than in the Dis-I-Averse condition (<italic>β</italic>(<italic>SE</italic>)=-0.97(0.18), p&lt;0.001), but, importantly, changes in fairness ratings for extreme Dis-I offers did not differ significantly between learning conditions (<italic>β</italic>(<italic>SE</italic>)=0.06(0.18), p&lt;0.723).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Baseline and transfer phase behavior in Experiment 2.</title><p>(<bold>a</bold>) Contagion in extremely unfair offers. Although no feedback was provided in the Learning phase for 90:10 or 10:90 splits, we observed generalization of punishment preferences to these offers. Dashed lines represent the Teacher’s preferences. (<bold>b</bold>) Fairness rating changes. We found significant changes from Baseline to Transfer phase in fairness rating for 90:10 in both Adv-Dis-I-Averse and Dis-I-Averse Condition, but only in Adv-Dis-I-Averse Condition for 10:90 offers. Dashed lines represent the Teacher’s observed preferences (Panel a) and fairness ratings (Panel b). Error bars represent standard errors (†indicates p&lt;0.1, *indicates p&lt;0.05, **indicates p&lt;0.01, ***indicates p&lt;0.001). Resutls from linear mixed models (LMM, n = 97 for both conditions).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-fig4-v1.tif"/></fig><p> In short, we found evidence that participants generalized across learning contexts, which in turn shaped their own rejection responses to extreme offers. In other words, it appears that preferences acquired through contagion extend beyond mere associations between single offers and actions and instead rely on a mechanism that infers the latent structure of the Teacher’s fairness preferences.</p></sec><sec id="s2-2-2"><title>Preference changes in the learning phase suggest generalization</title><p>A primary goal of Experiment 2 was to demonstrate that learning the Teacher’s preferences with respect to moderately unfair offers generalized to extremely unfair offers, for which no feedback from the Teacher was provided. The time course of rejection rates in Adv-I contexts during the Learning phase (<xref ref-type="fig" rid="fig5">Figure 5</xref>) reveals that participants learned over time to punish mildly unfair (30:70) offers, and these punishment preferences generalized to more extreme (10:90) offers. Specifically, compared to participants in the Dis-I-Averse Condition, participants in the Adv-Dis-I-Averse condition exhibited a significantly larger increase in rejection rates for 10:90 (Adv-I) offers (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <italic>β</italic>(<italic>SE</italic>)=0.81(0.26), p=0.002; mixed-effects logistic regression, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1J</xref>). Again, when comparing the rejection rate increase in the extremely Dis-I offers (90:10), we didn’t find a significant difference between conditions (<italic>β</italic>(<italic>SE</italic>)=-0.25(0.19), p=0.707).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Learning phase choice behavior in Experiment 2.</title><p>Learning effects were documented in extremely unfair offers. Rejection choices were summarized across subjects. Dashed lines indicate the Rejection choice of the Teacher. The learning effect was evident for 90:10 offers in Dis-I-Averse condition and 10:90 offers in Adv-Dis-I Averse condition. Thin solid lines represent participants’ rejection choice, thick solid lines show the predictions of the Preference Inference Model, and the dashed lines indicate the Teacher’s preferences (not observed by participants in 90:10 and 10:90 splits).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Model comparison results in Experiment 2.</title><p>(<bold>a</bold>) AICs of the models considered in experiment 2. (<bold>b, c</bold>). Updating of the ‘guilt’ and ‘envy’ parameters indicates the sanity of the Preference Inference model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-fig5-figsupp1-v1.tif"/></fig></fig-group><p>Finally, following Experiment 1, we fit a series of computational models of Learning phase choice behavior, comparing the goodness of fit of the five best-fitting models from Experiment 1 (see Materials and methods). As before, we found that the Preference Inference model provided the best fit of participants’ Learning Phase behavior (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1L</xref>). Given that this model is able to infer the Teacher’s underlying inequity-averse preferences (rather than learns offer-specific rejection preferences), it is unsurprising that this model best describes the generalization behavior observed in Experiment 2. We also simulated this model’s Learning Phase behavior using participants’ estimated parameter values and found that the model, mirroring participants’ choices, exhibits clear incremental changes in rejection rates (<xref ref-type="fig" rid="fig5">Figure 5</xref>; thick lines), both for offers where the model received explicit feedback (70:30) and for offers where the model received no feedback (90:10). In other words, like participants, the model generalizes the learned inequity-averse preferences to extreme Adv-I offers (90:10), which stems from the model’s trial-by-trial updating of the parameters governing the Teacher’s preferences (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b, c</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>While people tend to reject proposed resource allocations where they stand to receive less than their peers (so-called disadvantageous inequity; Dis-I), they are markedly less averse to resource allocations where they stand to unfairly gain more than their peers (advantageous inequity or Adv-I; <xref ref-type="bibr" rid="bib9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Luo et al., 2018</xref>). Here we considered the possibility that these complex, other-regarding preferences for fairness can be imparted merely by observing (and enacting) the preferences of another person. We investigated, in an Ultimatum Game setting, whether Adv-I-averse preferences can be shaped by learning and implementing the preferences of another individual. We leveraged a well-characterized observational learning paradigm (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>), exposing participants to another individual (the Teacher) exhibiting a strong preference for punishment of advantageously unfair offers, and probed whether these punishment preferences in turn transferred to participants making choices on their own. We found that participants’ own Adv-I-averse preferences shifted towards the preferences of the Teacher they just observed.</p><p>Previous work has outlined a number of important differences with respect to how individuals respond to advantageous (Adv-I) and disadvantageous inequity (Dis-I). Aversion to Dis-I is thought to arise from negative emotions such as spite (<xref ref-type="bibr" rid="bib40">McAuliffe et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Pillutla and Murnighan, 1996</xref>) engendered by consideration of one’s standing relative to others, while Adv-I aversion, in contrast, is thought to stem from concerns about fairness or inequality (<xref ref-type="bibr" rid="bib39">McAuliffe et al., 2013</xref>). Hence, the expression of Adv-I aversion may signal, and even enforce, egalitarian social norms. Developmental evidence supports this distinction between Adv-I versus Dis-I aversion. While Dis-I aversion emerges at the age of 4, Adv-I aversion does not manifest until about 8 years (<xref ref-type="bibr" rid="bib9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="bib8">Blake and McAuliffe, 2011</xref>; <xref ref-type="bibr" rid="bib41">McAuliffe et al., 2017</xref>). In fact, Adv-I aversion—which entails trading off self-interest against a social norm enforcement—is not even commonly observed in adults (<xref ref-type="bibr" rid="bib9">Blake et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Hennig-Schmidt et al., 2008</xref>; <xref ref-type="bibr" rid="bib38">Luo et al., 2018</xref>), suggesting that Adv-I aversion is more difficult (and less likely) to be learned than Dis-I aversion. The observation that Adv-I averse preferences can be learned suggests that observational learning processes are a potent and promising means by which sophisticated fairness preferences can be imparted. However, it is important to note that our conclusions are based on the comparison between the Dis-I averse and Adv-Dis-I averse conditions, both of which expose participants to Teachers who exhibit some form of inequity-averse preferences. It is plausible that the Dis-I Averse condition cannot provide a ‘pure’ assessment of participants’ default tendency to punish advantageous inequity, but rather participants in that condition may be learning—in contrast to participants in the Adv-Dis-I averse condition—to avoid rejecting Adv-I offers. However, we note that our previous work examining transmission of Dis-I-averse preferences (in a structurally similar paradigm) employed a control condition defined by a teacher exhibiting indifferent (random) preferences and found little evidence for spontaneous changes in punishment rates, across unfairness levels <xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>. Still, work examining transmission of advantageous inequity aversion would benefit from stricter control conditions—either with no feedback or an indifferent teacher—to further clarify whether the rejection rates difference between these two conditions is driven by the Dis-I aversion or the Adv-Dis-I aversion condition.</p><p>Mechanistically, we found that participants’ feedback-based learning of punishment preferences was best characterized by a computational model that assumes individuals infer the Teacher’s latent and structured preferences for punishment—rather than a simple RL account assuming that individuals learn contextually bound punishment preferences. To support the interpretation that individuals indeed ‘model’ the fairness preferences of others, in a second experiment, we directly test whether participants can infer the Teachers’ inequity-averse preferences across contexts. We found that participants generalized the Teacher’s punitive preferences to other contexts that varied in their unfairness, and this occurred in both Adv-I and Dis-I contexts, suggesting that the discovery of latent structure is instrumental for generalization.</p><p>The representation of others’ beliefs in other interpersonal decision-making tasks has been previously formalized, computationally, by a Bayesian account of theory of mind (ToM) in which the hypothetical beliefs were described by a prior distribution, and participants update this distribution using Bayesian updating (<xref ref-type="bibr" rid="bib5">Baker et al., 2009</xref>; <xref ref-type="bibr" rid="bib34">Jara-Ettinger, 2019</xref>). This computational framework has been applied to describing behaviors in a group decision-making task (<xref ref-type="bibr" rid="bib35">Khalvati et al., 2019</xref>). In our Preference-inference model, on each trial, the learner makes a guess about the Teacher’s inequity aversion parameters, then the Teacher’s feedback is subsequently used to further constrain the range of parameter values which could conceivably produce the Teacher’s observed feedback. The learner then updates their initial guess range. Conceptually, this learning mechanism is consistent with the sort of Bayesian updating.</p><p>One open question concerning the observed contagion effects is how the identities—and number—of teachers experienced in the Learning phase bear on the strength of the learning and contagion effects observed. For simplicity, our Learning Phase employed only one distinct Teacher, with no meaningful identity or social attributes, which contrasts with many social interactions in daily life, which are almost always accompanied by identifying information or attributes concerning the other (<xref ref-type="bibr" rid="bib33">Hester and Gray, 2020</xref>). In contrast, our interactions with others are profoundly influenced by the identities of others—for example, whether they are conservative versus liberal (<xref ref-type="bibr" rid="bib36">Leong et al., 2020</xref>), whether they are in- versus out-group members (<xref ref-type="bibr" rid="bib29">Hein et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Vives et al., 2022</xref>). At the same time, the strength of social influence often increases with the number (or proportion) of individuals in a group expressing a particular preference (<xref ref-type="bibr" rid="bib14">Cialdini and Goldstein, 2004</xref>; <xref ref-type="bibr" rid="bib46">Son et al., 2019</xref>). However, it may also be the case that social contagion effects require repeated interactions with the same individual (<xref ref-type="bibr" rid="bib50">Tsvetkova and Macy, 2014</xref>), which the contagion observed in the present paradigm corroborates. Accordingly, future work should aim to examine the influence of the teacher’s identity—and its concordance with the learner’s identity—as well as seek to understand how the relative balance of repeated experience with identical teachers versus the number of distinct teachers modulates the strength of contagion effects in Adv-I/Dis-I punishment preferences.</p><p>In summary, our study provides an initial demonstration that despite the desire for self-gain, we observe that people can swiftly and readily acquire another’s preferences for advantageous inequity, even when it comes at a monetary cost to the self. Computationally, we find that this contagion of inequity-averse preferences occurs through representing the underlying structure of another’s preferences, rather than a RL-like process of learning simple context-action associations. Importantly, these inferred preferences are sufficient to induce individuals to change their preferences for punishing advantageous inequity, suggesting that social influence may be one promising route through which social norm enforcement—uncommonly observed in the case of Adv-I—can be promoted.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>We recruited US-based participants from Amazon Mechanical Turk (<xref ref-type="bibr" rid="bib16">Crump et al., 2013</xref>) for both Experiment 1 (N=200, <italic>M</italic> age = 37.53 (SD = 10.88), 75 females) and Experiment 2 (N=200, <italic>M</italic> age = 37.16 (SD = 11.79) years old, 80 females). These sample sizes are based on our previous work employing the same per-condition sample sizes (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>) in a punishment-learning task. Participants provided informed consent in accordance with the McGill University Research Ethics Board (#258–1118). Participants were randomly assigned to either the Adv-Dis-I-Averse (N=100) or the Dis-I-Averse (N=100) condition. As we replicated all key reported results when excluding participants who evidenced some disbelief that the Teacher (see below for procedural details), suggesting that these results are robust to potential disbelief about the task structure as described to them. In Experiment 2, we excluded the data of participants who failed to meet the requirements of each analysis due to missed trials (3 in Adv-Dis-I-Averse and 3 in Dis-I-Averse Conditions were removed in Contagion effects analysis).</p></sec><sec id="s4-2"><title>Rejection learning paradigm</title><p>We used a modified version of the Ultimatum Game (<xref ref-type="bibr" rid="bib26">Güth et al., 1982</xref>) to probe participants’ fairness preference. In this task, the proposer offers an allocation of a total amount (e.g. $1 out of $10). Then, another player, the Receiver, chooses to reject or accept the proposed allocation. If the Receiver accepts, both players receive the proposed amount; if they reject, both players receive nothing. Following our past work examining contagion effects (<xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref>), our task consisted of three phases: Baseline, Learning, and Transfer (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>).</p><p>In the Baseline and Transfer phases, participants responded to unfair offers as a receiver in multiple rounds of ultimatum game, and participants were informed that the Proposer on each trial was a different, randomly chosen individual with a unique (fictitious) participant ID (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), which permitted us to measure participants’ preferences and beliefs about fairness irrespective of any particular Proposer. We considered 5 unique offer levels (90:10, 70:30, 50:50, 30:70, 10:90) ranging from extreme Dis-I to extreme Adv-I, including ‘fair’ offers (50:50), which allowed us to measure participants’ baseline rejection tendency. We instructed participants that these Proposers were concurrently participating in this task, and these were ‘real time’ offers. Critically, participants were responding to offers made by fictitious players with predetermined offers to ensure that we could observe rejection preferences for each offer level. On 3/5 of Baseline and Transfer Phase trials, participants also rated the fairness of the offer on a 1–7 scale (1 being ‘Strongly unfair’; 7 being ‘Strongly fair’) after making an accept/reject choice. Participants experienced five trials of each offer type in both the Baseline and Transfer Phase.</p><p>In the Learning phase, participants played an Ultimatum Game, acting as a third party deciding to accept or reject Proposers’ offers for another receiver (the Teacher) receiving offers from Proposers (see <xref ref-type="fig" rid="fig1">Figure 1d</xref>). After the participant chose to accept or reject the offer, the Teacher’s preferred response was revealed. In the Adv-Dis-I-Averse condition, the Teacher indicated preference for rejection of fair (50:50) and Adv-I offers and uniformly rated these Adv-I offers as unfair (see <xref ref-type="fig" rid="fig1">Figure 1b</xref>). In the Dis-I-Averse condition, the Teacher accepted all fair and Adv-I offers and rated these offers as uniformly fair. In both the Dis-I-Averse and Adv-Dis-I-Averse, Dis-I offers were rejected and rated as unfair. The Teachers’ rejection rates and ratings in response to each offer type are provided in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1A and B</xref>.</p><p>Again, on 3/5 of the trials, participants rated the fairness of each offer, and on these trials, participants also saw the Teachers’ fairness rating of the offer. Like the Baseline and Transfer phases, participants saw a unique, randomly chosen Proposer on every trial but were informed that there was a single, unvarying Teacher over the entire Learning Phase. In Experiment 1, participants experienced 20 trials for each of the five offer types considered (90:10, 70:30, 50:50, 30:70, 10:90).</p><p>In all task phases, we added uniformly distributed noise to each trial’s offer (ranging from –9 to 9, inclusive, rounding to the nearest integer) such that the random amount added (or subtracted) from the Proposer’s share was subtracted (or added) to the Receiver’s share. We adopted this manipulation to make the proposers’ behavior appear more realistic. The orders of offers participants experienced were fully randomized within each experiment phase.</p><p>Experiment 2 followed the same procedure as Experiment 1 except for the following changes. First, and most importantly, in the Learning Phase, participants did not experience feedback on extreme Adv-I offers (10:90 splits). Second, we added 25 additional trials of each offer type (70:30 and 30:70 splits) to the Learning Phase to provide increased opportunity to observe the Teacher’s preferences. Third, participants were instructed that Proposers’ offers (which were predetermined) were generated by previous participants in previous similar experiments. Finally, in all task phases, participants were required to respond within a 3 s deadline when making choices, and a 4 s deadline when providing fairness ratings.</p></sec><sec id="s4-3"><title>Data analysis</title><p>We used mixed-effects regression, implemented in the ‘lmer’ package for R (<xref ref-type="bibr" rid="bib7">Bates et al., 2015</xref>) to estimate the effect of offer level and punishment condition upon contagion rates— the rejection rate change between the Baseline and Transfer phases. To do this, the rejection rate change was modeled by interactions between Offer types (factors of five levels: 90:10, 70:30, 50:50, 30:70, 10:90, coded using 5 dummy-coded columns) and Condition (Adv-Dis-I-Averse vs. Dis-I-Averse, coded using 2 dummy-coded columns), with random intercepts taken over participants (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1G</xref> for full coefficient estimates).</p><p>We estimated changes in rejection rates in the Learning Phase using mixed-effects logistic regressions in Learning. Specifically, rejection choices were predicted as a function of trial number, offer type, condition, and their resultant interactions, taking the two-way interactions between Trial number and offer type (random slope) as random effects over participants. To estimate changes in fairness ratings, we estimated linear mixed-effects models with the same terms.</p></sec><sec id="s4-4"><title>Computational models of learning phase behavior</title><p>We considered seven computational models of Learning Phase choice behavior, which we fit to individual participants’ observed sequences of choices, in Experiment 1, via Maximum Likelihood Estimation. Importantly, Models 1 (Random choosing) and 2 (Static preference) are baseline models which assume that rejection probabilities are fixed over time, while all other models allow for learning of rejection rates over time in accordance with Teacher feedback.</p><sec id="s4-4-1"><title>Model 1 (random choosing)</title><p>This model assumes that participants reject offers with a fixed probability governed by the parameter <inline-formula><alternatives><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>offertype</mml:mtext></mml:mrow></mml:msub></mml:math><tex-math id="inft1">\begin{document}$p_{\text{offertype}}$\end{document}</tex-math></alternatives></inline-formula> (one for each offer type; 5 free parameters in total).<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle P_{reject}\left ({offertype}\right)\sim Bernoulli\left (p_{{offertype}}\right)$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-4-2"><title>Model 2 (static preference)</title><p>This model assumes that when choosing for others, the utility of accepting an offer, relative to rejection of the offer, is governed by the Fehr-Schmidt (FS) inequity aversion (<xref ref-type="bibr" rid="bib20">Fehr and Schmidt, 1999</xref>; <xref ref-type="bibr" rid="bib38">Luo et al., 2018</xref>) function:<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mtext>offer</mml:mtext><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>α</mml:mi><mml:mo>∗</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>β</mml:mi><mml:mo>∗</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle U_{accept}\left ({offer}\right)\, =\, \text{offer}\, -\, \alpha *max\left (50\, -\, {offer},\, 0\right)\, -\, \beta *max\left ({offer}\, -\, 50,\, 0\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle U_{reject}\left ({offer}\right)\, =\, 0$$\end{document}</tex-math></alternatives></disp-formula></p><p>In this function, <italic>offer</italic> represents the share the Proposer gives to the Receiver, <italic>α</italic> parameterizes the Teacher’s disutility (or ‘envy’) in accepting disadvantageous unfair offers (Dis-I aversion), and <italic>β</italic> captures Teacher’s disutility (or ‘guilt’) for accepting advantageous unfair offers (Adv-I aversion), which can each range from 0 to 10.</p><p>These action utilities were then transformed to choice probabilities using the softmax choice rule:<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle P_{reject}\left ({offer}\right)=exp\left (\tau *U_{reject}\right)/\left (exp\left (\tau *U_{reject}\right)+exp\left (\tau *U_{accept}\right)\, \right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the inverse temperature (<italic>τ</italic>) parameter captures decision noise, such that a larger <italic>τ</italic> corresponds to a higher probability of choosing the action with larger utility, and as <inline-formula><alternatives><mml:math id="inf2"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft2">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> approaches 0, the two options are chosen with equal probability. In total, this model has three free parameters.</p></sec><sec id="s4-4-3"><title>Model 3 (basic RL)</title><p>Model 3 is a simple RL model following that used by <xref ref-type="bibr" rid="bib22">FeldmanHall et al., 2018</xref> only one learning rate, which represents and updates values of the two actions separately for each offer type, using a delta updating rule:<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle Q_{t+1}\left (action_{t},{offertype}_{t}\right)=Q_{t}\left (action_{t},\,{offertype}_{t}\right)+\eta *\left (R_{t}-Q_{t}\left (action_{t},\,{offertype}_{t}\right)\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>action<sub>t</sub></italic> is the action the participant chose (accept or reject) on the <italic>t-</italic>th trial, <italic>R<sub>t</sub></italic> is the reward on the <italic>t-</italic>th trial, which is defined as 1 (reward obtained) when the action taken was the same as the action the Teacher would have preferred, and 0 (no reward) otherwise. The softmax choice rule was used to translate these action values to predicted choice probabilities. We kept the initial values fixed in this model, that is <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$Q_{0} (reject,\, {offertype}){ = 0.5}$\end{document}</tex-math></alternatives></inline-formula>, (<inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>∈</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}${offertype}\in$\end{document}</tex-math></alternatives></inline-formula> 90:10, 70:30, 50:50, 30:70, 10:90). This model has two free parameters.</p></sec><sec id="s4-4-4"><title>Model 4 (offer-sensitive RL)</title><p>This RL model is a more complex variant of Model 3 and assumes a separate learning rate for each offer type. Similar to Model 3, we set the initial Q values to 0.5 for each offer type. In total, this model has six free parameters.</p></sec><sec id="s4-4-5"><title>Model 5 (offer-sensitive RL with separate initial values)</title><p>This RL model extends Model 3, assuming different initial action values for each offer type. Formally, this model treats <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$Q_{0} (reject,{offertype})$\end{document}</tex-math></alternatives></inline-formula>, (<inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>∈</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}${offertype}\in $\end{document}</tex-math></alternatives></inline-formula> 90:10, 70:30, 50:50, 30:70, 10:90) as free parameters with values between 0 and 1, resulting in seven free parameters.</p></sec><sec id="s4-4-6"><title>Model 6 (preference inference)</title><p>Model 6 posits that the participant infers the Fehr-Schmidt utility function (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) governing the Teacher’s preferences, and updates their modeled <italic>α</italic> and <italic>β</italic> parameters incrementally from feedback, under the assumption that the Teacher’s indicated choices are made in accordance with each offer’s Fehr-Schmidt utility (more formally, the Teacher rejects the offer when <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$U_{accept}\left ({offer}\right)\lt 0$\end{document}</tex-math></alternatives></inline-formula>). As <italic>α</italic> and <italic>β</italic> govern the disutility of unfair offers, the model infers the minimal value of <italic>α</italic> (or <italic>β</italic>) that would lead to rejection of Dis-I (or Adv-I) offers, and similarly, the maximum values of <italic>α</italic> (or <italic>β</italic>) that would lead to acceptance of Dis-I (or Adv-I) offers.</p><p>Accordingly, after observing that the Teacher prefers rejection in response to a Dis-I offer, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> can be transformed to the following inequality:<disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>α</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle U_{accept}\left ({ offer}\right)= offer\, -\, \alpha *\left (50\, -\, {offer}\right) \lt 0$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the model can infer a lower bound of <italic>α</italic>, that would lead to the offer’s rejection by solving (6):<disp-formula id="equ7"><label>(7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mfrac><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle \alpha \gt \frac{{offer} }{50\, -\, {offer}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>The right side of (7) can be denoted as <inline-formula><alternatives><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft8">\begin{document}$\alpha _{lb}$\end{document}</tex-math></alternatives></inline-formula> , and then that trial’s estimate of <italic>α</italic> (denoted <inline-formula><alternatives><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft9">\begin{document}$\alpha _{t}$\end{document}</tex-math></alternatives></inline-formula>) is updated as follows:<disp-formula id="equ8"><label>(8)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle \alpha _{t+1}=\, \begin{cases} \alpha _{t},\, if\, \alpha _{t}\gt \, \alpha _{lb} \\ \alpha _{t}+\eta *\left (\alpha _{lb}-\, \alpha _{t}\right), otherwise\end{cases} $$\end{document}</tex-math></alternatives></disp-formula></p><p>The parameter <italic>η</italic> governs the rate at which the learner’s estimate of the Teacher’s α value is updated and is constrained to the range [0, 5].</p><p>The updating procedure is similar when the Teacher indicates acceptance of a Dis-I offer, which implies that <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mtext>U</mml:mtext><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$\text{U}_{{accept}}\left ({offer}\right){\gt 0}$\end{document}</tex-math></alternatives></inline-formula>, and the following inequality:<disp-formula id="equ9"><label>(9)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>α</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle U_{accept}\left ({offer}\right)=\, {offer}\, -\, \alpha *\left (50\, -\, {offer}\right)\gt 0 $$\end{document}</tex-math></alternatives></disp-formula></p><p>which yields an upper bound for the envy parameter <italic>α</italic>:<disp-formula id="equ10"><label>(10)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle \mathrm{\alpha }\lt \frac{offer}{50-{offer}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>This upper bound, <inline-formula><alternatives><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft11">\begin{document}$\alpha _{ub}$\end{document}</tex-math></alternatives></inline-formula>, is in turn used to update <inline-formula><alternatives><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft12">\begin{document}$\alpha _{t}$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ11"><label>(11)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \alpha _{t+1}=\, \begin{cases} \alpha _{t},\, if\, \alpha _{t}\lt \, \alpha _{lb}\, \\\alpha _{t}+\eta *\left (\alpha _{ub}-\ \alpha _{t}\right),\ otherwise\end{cases} $$\end{document}</tex-math></alternatives></disp-formula></p><p>In the case of Adv-I offers, the model employs the identical procedure to update the ‘guilt’ parameter <italic>β</italic>, and <italic>α</italic> is updated only in Dis-I offers, while <italic>β</italic> is only updated in Adv-I offers. Following <xref ref-type="bibr" rid="bib38">Luo et al., 2018</xref>, <italic>α</italic> and <italic>β</italic> were restricted to the range of [0, 10]. The initial value of <italic>α</italic> and <italic>β</italic> is taken as free parameters in the range of [0, 10], resulting in a model with a total of 4 free parameters. This preference inference model predicts the generalization effects, as experience of the teacher’s rejection in moderately unfair offers already allows the updating of both Fehr-Schmidt parameters (<italic>α</italic> and <italic>β</italic>).</p></sec><sec id="s4-4-7"><title>Model 7 (similarity RL)</title><p>Model 7 assumes that while participants learn the value for each action by trial and error (as in Models 1–5), they can generalize learning across offers based on the similarity of the current (experienced) offer type to other (non-experienced) offer types. This generalization is governed by a normal distribution whereby generalization decreases with increasing similarity. Formally, this model assumes participants track <italic>Q</italic> values corresponding to every possible offer—that is, Q(reject, <italic>offer</italic>) and Q(accept, <italic>offer</italic>) for all integer offers from 1 to 100. After experiencing Teacher feedback for an experienced offer, the model then updates the action values for all offers:<disp-formula id="equ12"><label>(12)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle Q_{t+1}\left (action_{t},\, {offer}\right)=Q_{t}\left (action_{t},\, {offer}\right)+\eta _{{offer}}*\left (R_{t}-Q_{t}\left (action_{t},{offer}\right)\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mtext>offer</mml:mtext></mml:mrow></mml:msub></mml:math><tex-math id="inft13">\begin{document}$\eta _{\text{offer}}$\end{document}</tex-math></alternatives></inline-formula> is a learning rate computed by scaling the learning rate parameter <inline-formula><alternatives><mml:math id="inf14"><mml:mi>η</mml:mi></mml:math><tex-math id="inft14">\begin{document}$\eta $\end{document}</tex-math></alternatives></inline-formula> with respect to the difference between the just-experienced offer on trial t and the to-be-updated offer:<disp-formula id="equ13"><label>(13)</label><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>∗</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle \eta _{{offer}}=\eta *f\left ({offer}-{offer}_{t}\right)/f\left (0\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>such that<disp-formula id="equ14"><label>(14)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>σ</mml:mi><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle f\left (x\right)=\frac{1}{\sigma \sqrt{2\pi }}\mathrm{e}\mathrm{x}\mathrm{p} \left (\frac{-x^{2}}{2\sigma ^{2}}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf15"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft15">\begin{document}$\sigma $\end{document}</tex-math></alternatives></inline-formula> is a free parameter governing the width of the Gaussian, bound by the range [0,200]. To mimic the behavior of the Preference Inference model (Model 6), we modeled the starting value for rejection as a V-shape function. That is:<disp-formula id="equ15"><label>(15)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>α</mml:mi><mml:mn>50</mml:mn></mml:mfrac><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mfrac><mml:mi>β</mml:mi><mml:mn>50</mml:mn></mml:mfrac><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mn>50</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle Q_{0}\left (reject,{offer}\right)=\mathrm{m}\mathrm{a}\mathrm{x} \left (\frac{\alpha }{50}*\left (50-{offer}\right),\frac{\beta }{50}*\left ({offer}-50\right)\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>α</italic> and <italic>β</italic> are parameters (in the range [0,3]) governing the slope of the V-shape function.</p></sec></sec><sec id="s4-5"><title>Model fitting and validation</title><p>All models were fit via Maximum Likelihood Estimation, employing a nonlinear optimization procedure using 100 random start points in the parameter space in order to find the best-fitting parameter values for each participant. We then computed the Akaike Information Criterion (AIC; <xref ref-type="bibr" rid="bib1">Akaike, 1974</xref>) to select the best-fitting models of Learning phase choice behavior, penalizing each model’s goodness-of-fit score by its complexity (i.e. number of free parameters). See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1H, L</xref> for parameter estimates and goodness-of-fit metrics. Note that in Experiment 2, we did not evaluate Models 3 and 4 because on account of their poor performance in Experiment 1.</p><p>Finally, to verify if the free parameters of the winning model (Preference Inference) are recoverable, we simulated 200 artificial subjects, based on the Learning Phase of Experiment 1, with free parameters randomly chosen (uniformly) from their defined ranges. We then employed the same model-fitting procedure as described above to estimate these parameter values. We found that all parameters of this model could be recovered (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Participants provided informed consent in accordance with the McGill University Research Ethics Board. The ethical approval number is 258-1118.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Model reports.</title></caption><media xlink:href="elife-102800-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-102800-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All behavioral data and analysis code have been deposited at <ext-link ext-link-type="uri" xlink:href="https://osf.io/6xn5b">https://osf.io/6xn5b</ext-link> and are publicly available.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Fairness Contagion</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/6XN5B</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was funded by the European Union (ERC Starting Grant, NEUROGROUP, 101041799) and by an NSERC Discovery Grant, a New Researchers Startup Grant from the Fonds de Recherche du Québec - Nature et Technologies, and an infrastructure award from the Canadian Foundation for Innovation. Views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>A new look at the statistical model identification</article-title><source>IEEE Transactions on Automatic Control</source><volume>19</volume><fpage>716</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1109/TAC.1974.1100705</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amir</surname><given-names>D</given-names></name><name><surname>Melnikoff</surname><given-names>D</given-names></name><name><surname>Warneken</surname><given-names>F</given-names></name><name><surname>Blake</surname><given-names>PR</given-names></name><name><surname>Corbit</surname><given-names>J</given-names></name><name><surname>Callaghan</surname><given-names>TC</given-names></name><name><surname>Barry</surname><given-names>O</given-names></name><name><surname>Bowie</surname><given-names>A</given-names></name><name><surname>Kleutsch</surname><given-names>L</given-names></name><name><surname>Kramer</surname><given-names>KL</given-names></name><name><surname>Ross</surname><given-names>E</given-names></name><name><surname>Vongsachang</surname><given-names>H</given-names></name><name><surname>Wrangham</surname><given-names>R</given-names></name><name><surname>McAuliffe</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Computational signatures of inequity aversion in children across seven societies</article-title><source>Journal of Experimental Psychology. General</source><volume>152</volume><fpage>2882</fpage><lpage>2896</lpage><pub-id pub-id-type="doi">10.1037/xge0001385</pub-id><pub-id pub-id-type="pmid">37155284</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anzellotti</surname><given-names>S</given-names></name><name><surname>Young</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The acquisition of person knowledge</article-title><source>Annual Review of Psychology</source><volume>71</volume><fpage>613</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010419-050844</pub-id><pub-id pub-id-type="pmid">31553673</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bail</surname><given-names>CA</given-names></name><name><surname>Argyle</surname><given-names>LP</given-names></name><name><surname>Brown</surname><given-names>TW</given-names></name><name><surname>Bumpus</surname><given-names>JP</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Hunzaker</surname><given-names>MBF</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Mann</surname><given-names>M</given-names></name><name><surname>Merhout</surname><given-names>F</given-names></name><name><surname>Volfovsky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Exposure to opposing views on social media can increase political polarization</article-title><source>PNAS</source><volume>115</volume><fpage>9216</fpage><lpage>9221</lpage><pub-id pub-id-type="doi">10.1073/pnas.1804840115</pub-id><pub-id pub-id-type="pmid">30154168</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>CL</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Action understanding as inverse planning</article-title><source>Cognition</source><volume>113</volume><fpage>329</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2009.07.005</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bandura</surname><given-names>A</given-names></name><name><surname>McDonald</surname><given-names>FJ</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Influence of social reinforcement and the behavior of models in shaping children’s moral judgment</article-title><source>The Journal of Abnormal and Social Psychology</source><volume>67</volume><fpage>274</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1037/h0044714</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Mächler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>B</given-names></name><name><surname>Walker</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fitting linear mixed-effects models using lme4</article-title><source>Journal of Statistical Software</source><volume>67</volume><elocation-id>i01</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>PR</given-names></name><name><surname>McAuliffe</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>“I had so much it didn’t seem fair”: Eight-year-olds reject two forms of inequity</article-title><source>Cognition</source><volume>120</volume><fpage>215</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2011.04.006</pub-id><pub-id pub-id-type="pmid">21616483</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>PR</given-names></name><name><surname>McAuliffe</surname><given-names>K</given-names></name><name><surname>Corbit</surname><given-names>J</given-names></name><name><surname>Callaghan</surname><given-names>TC</given-names></name><name><surname>Barry</surname><given-names>O</given-names></name><name><surname>Bowie</surname><given-names>A</given-names></name><name><surname>Kleutsch</surname><given-names>L</given-names></name><name><surname>Kramer</surname><given-names>KL</given-names></name><name><surname>Ross</surname><given-names>E</given-names></name><name><surname>Vongsachang</surname><given-names>H</given-names></name><name><surname>Wrangham</surname><given-names>R</given-names></name><name><surname>Warneken</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The ontogeny of fairness in seven societies</article-title><source>Nature</source><volume>528</volume><fpage>258</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1038/nature15703</pub-id><pub-id pub-id-type="pmid">26580018</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosnan</surname><given-names>SF</given-names></name><name><surname>De Waal</surname><given-names>FBM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Monkeys reject unequal pay</article-title><source>Nature</source><volume>425</volume><fpage>297</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1038/nature01963</pub-id><pub-id pub-id-type="pmid">13679918</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosnan</surname><given-names>SF</given-names></name><name><surname>de Waal</surname><given-names>FBM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Evolution of responses to (un)fairness</article-title><source>Science</source><volume>346</volume><elocation-id>1251776</elocation-id><pub-id pub-id-type="doi">10.1126/science.1251776</pub-id><pub-id pub-id-type="pmid">25324394</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>CJ</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Baddeley</surname><given-names>M</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural mechanisms of observational learning</article-title><source>PNAS</source><volume>107</volume><fpage>14431</fpage><lpage>14436</lpage><pub-id pub-id-type="doi">10.1073/pnas.1003111107</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell-Meiklejohn</surname><given-names>DK</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>How the opinion of others affects our valuation of objects</article-title><source>Current Biology</source><volume>20</volume><fpage>1165</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.04.055</pub-id><pub-id pub-id-type="pmid">20619815</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cialdini</surname><given-names>RB</given-names></name><name><surname>Goldstein</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Social influence: compliance and conformity</article-title><source>Annual Review of Psychology</source><volume>55</volume><fpage>591</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142015</pub-id><pub-id pub-id-type="pmid">14744228</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Civai</surname><given-names>C</given-names></name><name><surname>Crescentini</surname><given-names>C</given-names></name><name><surname>Rustichini</surname><given-names>A</given-names></name><name><surname>Rumiati</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Equality versus self-interest in the brain: differential roles of anterior insula and medial prefrontal cortex</article-title><source>NeuroImage</source><volume>62</volume><fpage>102</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.037</pub-id><pub-id pub-id-type="pmid">22548807</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crump</surname><given-names>MJC</given-names></name><name><surname>McDonnell</surname><given-names>JV</given-names></name><name><surname>Gureckis</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Evaluating Amazon’s Mechanical Turk as a tool for experimental behavioral research</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e57410</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0057410</pub-id><pub-id pub-id-type="pmid">23516406</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diaconescu</surname><given-names>AO</given-names></name><name><surname>Stecy</surname><given-names>M</given-names></name><name><surname>Kasper</surname><given-names>L</given-names></name><name><surname>Burke</surname><given-names>CJ</given-names></name><name><surname>Nagy</surname><given-names>Z</given-names></name><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural arbitration between social and individual learning systems</article-title><source>eLife</source><volume>9</volume><elocation-id>e54051</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54051</pub-id><pub-id pub-id-type="pmid">32779568</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engelmann</surname><given-names>D</given-names></name><name><surname>Fischbacher</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Indirect reciprocity and strategic reputation building in an experimental helping game</article-title><source>Games and Economic Behavior</source><volume>67</volume><fpage>399</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1016/j.geb.2008.12.006</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Essler</surname><given-names>JL</given-names></name><name><surname>Marshall-Pescini</surname><given-names>S</given-names></name><name><surname>Range</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Domestication does not explain the presence of inequity aversion in dogs</article-title><source>Current Biology</source><volume>27</volume><fpage>1861</fpage><lpage>1865</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.05.061</pub-id><pub-id pub-id-type="pmid">28602652</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fehr</surname><given-names>E</given-names></name><name><surname>Schmidt</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A theory of fairness, competition, and cooperation</article-title><source>The Quarterly Journal of Economics</source><volume>114</volume><fpage>817</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1162/003355399556151</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fehr</surname><given-names>E</given-names></name><name><surname>Bernhard</surname><given-names>H</given-names></name><name><surname>Rockenbach</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Egalitarianism in young children</article-title><source>Nature</source><volume>454</volume><fpage>1079</fpage><lpage>1083</lpage><pub-id pub-id-type="doi">10.1038/nature07155</pub-id><pub-id pub-id-type="pmid">18756249</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FeldmanHall</surname><given-names>O</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning moral values: Another’s desire to punish enhances one’s own punitive behavior</article-title><source>Journal of Experimental Psychology. General</source><volume>147</volume><fpage>1211</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1037/xge0000405</pub-id><pub-id pub-id-type="pmid">29878807</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FeldmanHall</surname><given-names>O</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Resolving uncertainty in a social world</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>426</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0590-x</pub-id><pub-id pub-id-type="pmid">31011164</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Sáez</surname><given-names>I</given-names></name><name><surname>Blue</surname><given-names>PR</given-names></name><name><surname>Zhu</surname><given-names>L</given-names></name><name><surname>Hsu</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinguishing neural correlates of context-dependent advantageous- and disadvantageous-inequity aversion</article-title><source>PNAS</source><volume>115</volume><fpage>E7680</fpage><lpage>E7689</lpage><pub-id pub-id-type="doi">10.1073/pnas.1802523115</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garvert</surname><given-names>MM</given-names></name><name><surname>Moutoussis</surname><given-names>M</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning-induced plasticity in medial prefrontal cortex predicts preference malleability</article-title><source>Neuron</source><volume>85</volume><fpage>418</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.033</pub-id><pub-id pub-id-type="pmid">25611512</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güth</surname><given-names>W</given-names></name><name><surname>Schmittberger</surname><given-names>R</given-names></name><name><surname>Schwarze</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>An experimental analysis of ultimatum bargaining</article-title><source>Journal of Economic Behavior &amp; Organization</source><volume>3</volume><fpage>367</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/0167-2681(82)90011-7</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>SJ</given-names></name><name><surname>Ashford</surname><given-names>D</given-names></name><name><surname>Bennett</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Goal-directed imitation: the means to an end</article-title><source>Acta Psychologica</source><volume>127</volume><fpage>407</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2007.07.009</pub-id><pub-id pub-id-type="pmid">17880901</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>J</given-names></name><name><surname>FeldmanHall</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A probabilistic map of emotional experiences during competitive social interactions</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>1718</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-29372-8</pub-id><pub-id pub-id-type="pmid">35361768</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hein</surname><given-names>G</given-names></name><name><surname>Silani</surname><given-names>G</given-names></name><name><surname>Preuschoff</surname><given-names>K</given-names></name><name><surname>Batson</surname><given-names>CD</given-names></name><name><surname>Singer</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural responses to ingroup and outgroup members’ suffering predict individual differences in costly helping</article-title><source>Neuron</source><volume>68</volume><fpage>149</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.003</pub-id><pub-id pub-id-type="pmid">20920798</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig-Schmidt</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>ZY</given-names></name><name><surname>Yang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Why people reject advantageous offers-Non-monotonic strategies in ultimatum bargaining evaluating a video experiment run in PR China</article-title><source>Journal of Economic Behavior and Organization</source><volume>65</volume><fpage>373</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.jebo.2005.10.003</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henrich</surname><given-names>J</given-names></name><name><surname>McElreath</surname><given-names>R</given-names></name><name><surname>Barr</surname><given-names>A</given-names></name><name><surname>Ensminger</surname><given-names>J</given-names></name><name><surname>Barrett</surname><given-names>C</given-names></name><name><surname>Bolyanatz</surname><given-names>A</given-names></name><name><surname>Cardenas</surname><given-names>JC</given-names></name><name><surname>Gurven</surname><given-names>M</given-names></name><name><surname>Gwako</surname><given-names>E</given-names></name><name><surname>Henrich</surname><given-names>N</given-names></name><name><surname>Lesorogol</surname><given-names>C</given-names></name><name><surname>Marlowe</surname><given-names>F</given-names></name><name><surname>Tracer</surname><given-names>D</given-names></name><name><surname>Ziker</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Costly punishment across human societies</article-title><source>Science</source><volume>312</volume><fpage>1767</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1126/science.1127333</pub-id><pub-id pub-id-type="pmid">16794075</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning how to behave: cognitive learning processes account for asymmetries in adaptation to social norms</article-title><source>Proceedings of the Royal Society B</source><volume>288</volume><elocation-id>20210293</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2021.0293</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hester</surname><given-names>N</given-names></name><name><surname>Gray</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The moral psychology of raceless, genderless strangers</article-title><source>Perspectives on Psychological Science</source><volume>15</volume><fpage>216</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1177/1745691619885840</pub-id><pub-id pub-id-type="pmid">32013724</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jara-Ettinger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Theory of mind as inverse reinforcement learning</article-title><source>Current Opinion in Behavioral Sciences</source><volume>29</volume><fpage>105</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.04.010</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalvati</surname><given-names>K</given-names></name><name><surname>Park</surname><given-names>SA</given-names></name><name><surname>Mirbagheri</surname><given-names>S</given-names></name><name><surname>Philippe</surname><given-names>R</given-names></name><name><surname>Sestito</surname><given-names>M</given-names></name><name><surname>Dreher</surname><given-names>JC</given-names></name><name><surname>Rao</surname><given-names>RPN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Modeling other minds: bayesian inference explains human choices in group decision making</article-title><source>Science Advances</source><volume>1</volume><elocation-id>419515</elocation-id><pub-id pub-id-type="doi">10.1101/419515</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Willer</surname><given-names>R</given-names></name><name><surname>Zaki</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Conservative and liberal attitudes drive polarized neural responses to political content</article-title><source>PNAS</source><volume>117</volume><fpage>27731</fpage><lpage>27739</lpage><pub-id pub-id-type="doi">10.1073/pnas.2008530117</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindström</surname><given-names>B</given-names></name><name><surname>Golkar</surname><given-names>A</given-names></name><name><surname>Jangard</surname><given-names>S</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Olsson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Social threat learning transfers to decision making in humans</article-title><source>PNAS</source><volume>116</volume><fpage>4732</fpage><lpage>4737</lpage><pub-id pub-id-type="doi">10.1073/pnas.1810180116</pub-id><pub-id pub-id-type="pmid">30760585</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>Y</given-names></name><name><surname>Hétu</surname><given-names>S</given-names></name><name><surname>Lohrenz</surname><given-names>T</given-names></name><name><surname>Hula</surname><given-names>A</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Ramey</surname><given-names>SL</given-names></name><name><surname>Sonnier-Netto</surname><given-names>L</given-names></name><name><surname>Lisinski</surname><given-names>J</given-names></name><name><surname>LaConte</surname><given-names>S</given-names></name><name><surname>Nolte</surname><given-names>T</given-names></name><name><surname>Fonagy</surname><given-names>P</given-names></name><name><surname>Rahmani</surname><given-names>E</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name><name><surname>Ramey</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Early childhood investment impacts social decision-making four decades later</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4705</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-07138-5</pub-id><pub-id pub-id-type="pmid">30459305</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAuliffe</surname><given-names>K</given-names></name><name><surname>Blake</surname><given-names>PR</given-names></name><name><surname>Kim</surname><given-names>G</given-names></name><name><surname>Wrangham</surname><given-names>RW</given-names></name><name><surname>Warneken</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Social influences on inequity aversion in children</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e80966</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0080966</pub-id><pub-id pub-id-type="pmid">24312509</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAuliffe</surname><given-names>K</given-names></name><name><surname>Blake</surname><given-names>PR</given-names></name><name><surname>Warneken</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Children reject inequity out of spite</article-title><source>Biology Letters</source><volume>10</volume><elocation-id>20140743</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2014.0743</pub-id><pub-id pub-id-type="pmid">25540156</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAuliffe</surname><given-names>K</given-names></name><name><surname>Blake</surname><given-names>PR</given-names></name><name><surname>Steinbeis</surname><given-names>N</given-names></name><name><surname>Warneken</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The developmental foundations of human fairness</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0042</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-016-0042</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAuliffe</surname><given-names>K</given-names></name><name><surname>Dunham</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fairness overrides group bias in children’s second-party punishment</article-title><source>Journal of Experimental Psychology: General</source><volume>146</volume><fpage>485</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1037/xge0000244</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedersen</surname><given-names>EJ</given-names></name><name><surname>Kurzban</surname><given-names>R</given-names></name><name><surname>McCullough</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Do humans really punish altruistically? A closer look</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>280</volume><elocation-id>20122723</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2012.2723</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillutla</surname><given-names>MM</given-names></name><name><surname>Murnighan</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Unfairness, anger, and spite: emotional rejections of ultimatum offers</article-title><source>Organizational Behavior and Human Decision Processes</source><volume>68</volume><fpage>208</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1006/obhd.1996.0100</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanfey</surname><given-names>AG</given-names></name><name><surname>Rilling</surname><given-names>JK</given-names></name><name><surname>Aronson</surname><given-names>JA</given-names></name><name><surname>Nystrom</surname><given-names>LE</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The neural basis of economic decision-making in the Ultimatum Game</article-title><source>Science</source><volume>300</volume><fpage>1755</fpage><lpage>1758</lpage><pub-id pub-id-type="doi">10.1126/science.1082976</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Son</surname><given-names>JY</given-names></name><name><surname>Bhandari</surname><given-names>A</given-names></name><name><surname>FeldmanHall</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Crowdsourcing punishment: Individuals reference group preferences to inform their own punitive decisions</article-title><source>Scientific Reports</source><volume>9</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s41598-019-48050-2</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>S</given-names></name><name><surname>Jensen</surname><given-names>ELS</given-names></name><name><surname>Bossaerts</surname><given-names>P</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behavioral contagion during learning about another agent’s risk-preferences acts on the neural representation of decision-risk</article-title><source>PNAS</source><volume>113</volume><fpage>3755</fpage><lpage>3760</lpage><pub-id pub-id-type="doi">10.1073/pnas.1600092113</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taber</surname><given-names>CS</given-names></name><name><surname>Lodge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Motivated skepticism in the evaluation of political beliefs (2006)</article-title><source>Critical Review</source><volume>24</volume><fpage>157</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1080/08913811.2012.711019</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tomasello</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Becoming Human: A Theory of Ontogeny. In Belknap</source><publisher-name>Harvard University Press</publisher-name><pub-id pub-id-type="doi">10.4159/9780674988651</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsvetkova</surname><given-names>M</given-names></name><name><surname>Macy</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The social contagion of generosity</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e87275</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0087275</pub-id><pub-id pub-id-type="pmid">24551053</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Baar</surname><given-names>JM</given-names></name><name><surname>Chang</surname><given-names>LJ</given-names></name><name><surname>Sanfey</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The computational and neural substrates of moral strategies in social decision-making</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1483</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09161-6</pub-id><pub-id pub-id-type="pmid">30940815</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wolkenten</surname><given-names>M</given-names></name><name><surname>Brosnan</surname><given-names>SF</given-names></name><name><surname>de Waal</surname><given-names>FBM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Inequity responses of monkeys modified by effort</article-title><source>PNAS</source><volume>104</volume><fpage>18854</fpage><lpage>18859</lpage><pub-id pub-id-type="doi">10.1073/pnas.0707182104</pub-id><pub-id pub-id-type="pmid">18000045</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vives</surname><given-names>M-L</given-names></name><name><surname>FeldmanHall</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tolerance to ambiguous uncertainty predicts prosocial behavior</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2156</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04631-9</pub-id><pub-id pub-id-type="pmid">29895948</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vives</surname><given-names>ML</given-names></name><name><surname>Cikara</surname><given-names>M</given-names></name><name><surname>FeldmanHall</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Following your group or your morals? the in-group promotes immoral behavior while the out-group buffers against It</article-title><source>Social Psychological and Personality Science</source><volume>13</volume><fpage>139</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1177/19485506211001217</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102800.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Press</surname><given-names>Clare</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Incomplete</kwd></kwd-group></front-stub><body><p>This cleverly designed and potentially <bold>important</bold> work supports our understanding regarding how and whether social behaviours promoting egalitarianism can be learned, even when implementing these norms entails a cost for oneself. However, the evidence supporting the major claims is currently <bold>incomplete</bold>, with the major limitation being whether Ps truly learn egalitarianism from a teacher or instead exhibit reduced guilt across time that is reduced when observing others behaving more selfishly. With a strengthening of the supporting evidence, this work will be of interest to a wide range of fields, including cognitive psychology/neuroscience, neuroeconomics, and social psychology, as well as policy making.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102800.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Zhang et al. addressed the question of whether advantageous and disadvantageous inequality aversion can be vicariously learned and generalized. Using an adapted version of the ultimatum game (UG), in three phases, participants first gave their own preference (baseline phase), then interacted with a &quot;teacher&quot; to learn their preference (learning phase), and finally were tested again on their own (transfer phase). The key measure is whether participants exhibited similar choice preference (i.e., rejection rate and fairness rating) influenced by the learning phase, by contrasting their transfer phase and baseline phase. Through a series of statistical modeling and computational modeling, the authors reported that both advantageous and disadvantageous inequality aversion can indeed be learned (Study 1), and even be generalised (Study 2).</p><p>Strengths:</p><p>This study is very interesting, that directly adapted the lab's previous work on the observational learning effect on disadvantageous inequality aversion, to test both advantageous and disadvantageous inequality aversion in the current study. Social transmission of action, emotion, and attitude have started to be looked at recently, hence this research is timely. The use of computational modeling is mostly appropriate and motivated. Study 2 that examined the vicarious inequality aversion on conditions where feedback was never provided is interesting and important to strengthen the reported effects. Both studies have proper justifications to determine the sample size.</p><p>Weaknesses:</p><p>Despite the strengths, a few conceptual aspects and analytical decisions have to be explained, justified, or clarified.</p><p>INTRODUCTION/CONCEPTUALIZATION</p><p>(1) Two terms seem to be interchangeable, which should not, in this work: vicarious/observational learning vs preference learning. For vicarious learning, individuals observe others' actions (and optionally also the corresponding consequence resulted directly by their own actions), whereas, for preference learning, individuals predict, or act on behalf of, the others' actions, and then receive feedback if that prediction is correct or not. For the current work, it seems that the experiment is more about preference learning and prediction, and less so about vicarious learning. But the intro and set are heavily around vicarious learning, and late the use of vicarious learning and preference learning is rather mixed in the text. I think either tone down the focus on vicarious learning, or discuss how they are different. Some of the references here may be helpful: Charpentier et al., Neuron, 2020; Olsson et al., Nature Reviews Neuroscience, 2020; Zhang &amp; Glascher, Science Advances, 2020</p><p>EXPERIMENTAL DESIGN</p><p>(2) For each offer type, the experiment &quot;added a uniformly distributed noise in the range of (-10 ,10)&quot;. I wonder how this looks like? With only integers such as 25:75, or even with decimal points? More importantly, is it possible to have either 70:30 or 90:10 option, after adding the noise, to have generated an 80:20 split shown to the participants? If so, for the analyses later, when participants saw the 80:20 split, which condition did this trial belong to? 70:30 or 90:10? And is such noise added only to the learning phase, or also to the baseline/transfer phases? This requires some clarification.</p><p>(3) For the offer conditions (90:10, 70:30, 50:50, 30:70, 10:90) - are they randomized? If so, how is it done? Is it randomized within each participants, and/or also across participants (such that each participant experienced different trial sequences)? This is important, as the order especially for the leanring phase can largely impact on the preference learning of the participants.</p><p>STATISTICAL ANALYSIS &amp; COMPUTATIONAL MODELING</p><p>(4) In Study 1 DI offer types (90:10, 70:30), the rejection rate for DI-AI averse looks consistently higher than that for DI averse (ie, blue line is above the yellow line). Is this significant? If so, how come? Since this is a between-subject design, I would not anticipate such a result (especially for the baseline). Also, for the LME results (eg, Table S3), only interactions were reported but not the main results.</p><p>(5) I do not particularly find this analysis appealing: &quot;we examined whether participants' changes in rejection rates between Transfer and Baseline, could be explained by the degree to which they vicariously learned, defined as the change in punishment rates between the first and last 5 trials of the Learning phase.&quot; Naturally, participants' behavior in the first 5 trials in the learning phase will be similar to those in the baseline; and their behavior in the last 5 trials in the learning phase would echo those at the transfer phase. I think it would be stronger to link the preference learning results to the chance between baseline and transfer phase, eg, by looking at the difference between alpha (beta) at the end of the learning phase and the initial alpha (beta).</p><p>(6) I wonder if data from the baseline and transfer phases can also be modeled, using a simple Fehr-Schimdt model? This way, the change in alpha/beta can also be examined between the baseline and transfer phase.</p><p>(7) I quite liked Study 2 that tests the generalization effect, and I expected to see an adapted computational modeling to directly reflect this idea. Indeed, the authors wrote &quot;[...] given that this model [...] assumes the sort of generalization of preferences between offer types [...]&quot;. But where exactly did the preference learning model assumed the generalization? In the methods, the modeling seems to be only about Study 1; did the authors advise their model to accommodate Study 2? The authors also ran simulation for the learning phase in Study 2 (Figure 6), and how did the preference updated (if at all) for offers (90:10 and 10:90) where feedback was not given? Extending/Unpacking the computational modeling results for Study2 will be very helpful for the paper.</p><p>Comments on revisions:</p><p>I kept my original public review, so that future readers can see the progress and development of the manuscript.</p><p>The authors have largely addressed my original questions/concerns, and I have two outstanding comments.</p><p>(a) Related to my original comment #6, where I suggested to apply the F-S model also to the baseline and transfer phase. The authors were inclined not to do it, but in fact later in comment #7 and in the manuscript they opted to use a more complex F-S-based model to their learning phase. I agree that the rejection rate is indeed a clear indication, but for completeness, it'd be more consistent and compelling if the paper follows a model-free (model-agnostic) and model-based approach in all phases of the experiment.</p><p>(b) Related to my original comment #4, I appreciate that the authors have provided more details of their LMM models. But I don't think it is accurate regardless.First, all offer levels (50:50, 30:70, 10:90), should not be coded as pure categorical levels. In fact, they have an ordinal meaning, a single ordinal predictor with three levels should be used. This also avoids the excessive number of interactions the authors have pointed out.</p><p>Second, running a model with only interactions without main effects is flawed. All textbooks on stats emphasize that without the presence of the main effects, the interpretation of interaction only is biased.</p><p>So these LMMs needs to be revised before the manuscript eventually gets to a version of record.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102800.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study investigates whether individuals can learn to adopt egalitarian norms that incur a personal monetary cost, such as rejecting offers that benefit them more than the giver (advantageous inequitable offers). While these behaviors are uncommon, two experiments aim to demonstrate that individuals can learn to reject such offers by observing a &quot;teacher&quot; who follows these norms. The authors use computational modelling to argue that learners adopt these norms through a sophisticated process, inferring the latent structure of the teacher's preferences, akin to theory of mind.</p><p>Strengths:</p><p>This paper is well-written and tackles an important topic relevant to social norms, morality, and justice. The findings are promising (though further control conditions are necessary to support the conclusions). The study is well-situated in the literature, with a clever experimental design and a computational approach that may offer insights into latent cognitive processes. In the revision, the authors clarified some questions related to the initial submission.</p><p>Weaknesses:</p><p>Despite these strengths, I remain unconvinced that the current evidence supports the paper's central claims. Below, I outline several issues that, in my view, limit the strength of the conclusions.</p><p>(1) Experimental Design and Missing Control Condition:</p><p>The authors set out to test whether observing a &quot;teacher&quot; who is averse to advantageous inequity (Adv-I) will affect observers' own rejection of Adv-I offers. However, I think the design of the task lacks an important control condition needed to address this question. At present, participants are assigned to one of two teachers: DIS or DIS+ADV. Behavioral differences between these groups can only reveal relative differences in influence; they cannot establish whether (and how) either teacher independently affects participants' own behavior. For example, a significant difference between conditions can emerge even if participants are only affected by the DIS teacher and are not affected at all by the DIS+ADV teacher. What is crucially missing here is a no-teacher control condition, which can then be compared with each teacher condition separately. This control condition would also control for pure temporal effects unrelated to teacher influence (e.g., increasing Adv-I rejections due to guilt build-up).</p><p>While this criticism applies to both experiments, it is especially apparent in Experiment 2. As shown in Figure 4, the interaction for 10:90 offers reflects a decrease in rejection rates following the DIS teacher, with no significant change following the DIS+ADV teacher. Ignoring temporal effects, this pattern suggests that participants may be learning NOT to reject from the DIS teacher, rather than learning to reject from the DIS+ADV teacher. On this basis, I do not see convincing evidence that participants' own choices were shaped by observing Adv-I rejections.</p><p>In the Discussion, the authors write that &quot;We found that participants' own Adv-I-averse preferences shifted towards the preferences of the Teacher they just observed, and the strength of these contagion effects related to the degree of behavior change participants exhibited on behalf of the Teachers, suggesting that they internalized, at least somewhat, these inequity preferences.&quot; However, there is no evidence that directly links the degree of behaviour change (on the teacher's behalf) to contagion effects (own behavioural change). I think there was a relevant analysis in the original version, but it was removed from the current version.</p><p>(2) Modelling Efforts:The modelling approach is underdeveloped. The identification of the &quot;best model&quot; lacks transparency, as no model-recovery results are provided. Additionally, behavioural fits for the losing models are not shown, leaving readers in the dark about where these models fail. Readers would benefit from seeing qualitative/behavioural patterns that favour the winning model.Moreover, the reinforcement learning (RL) models used are overly simplistic, treating actions as independent when they are likely inversely related. For example, the feedback that the teacher would have rejected an offer provides evidence that rejection is &quot;correct&quot; but also that acceptance is &quot;an error,&quot; and the latter is not incorporated into the modelling. In other words, offers are modelled as two-armed bandits (where separate values are learned for reject and accept actions), but the situation is effectively a one-armed bandit (if one action is correct, the other is mistaken). It is unclear to what extent this limitation affects the current RL formulations. Can the authors justify/explain their reasoning for including these specific variants? The manuscript only states Q-values for reject actions, but what are the Q-values for accept actions? This is unclear.</p><p>In Experiment 2, only the preferred model is capable of generalization, so it is perhaps unsurprising that this model &quot;wins.&quot; However, this does not strongly support the proposed learning mechanism, lacking a comparison with simpler generalizing mechanisms (see following comments).</p><p>(3) Conceptual Leap in Modelling Interpretation:The distinction between simple RL models and preference-inference models seems to hinge on the ability to generalize learning from one offer to another. Whereas in the RL models, learning occurs independently for each offer (hence no cross-offer generalization), preference inference allows for generalization between different offers. However, the paper does not explore &quot;model-free&quot; RL models that allow generalization based on the similarity of features of the offers (e.g., payment for the receiver, payment for the offer-giver, who benefits more). Such models are more parsimonious and could explain the results without invoking a theory of mind or any modelling of the teacher. In such model versions, a learner acquires a functional form that allows prediction of the teacher's feedback based on offer features (e.g., linear or quadratic weighting). Because feedback for an offer modulates the parameters of this function (feature weights), generalization occurs without necessarily evoking any sophisticated model of the other person. This leaves open the possibility that RL models could perform just as well or even outperform the preference learning model, casting doubt on the authors' conclusions.</p><p>Of note: even the behaviourists knew that when Little Albert was taught to fear rats, this fear generalized to rabbits. This could occur simply because rabbits are somewhat similar to rats. But this doesn't mean Little Albert had a sophisticated model of animals that he used to infer how they behave.</p><p>In their rebuttal letter, the authors acknowledge these possibilities, but the manuscript still does not explore or address alternative mechanisms.</p><p>(4) Limitations of the Preference-Inference Model:The preference-inference model struggles to capture key aspects of the data, such as the increase in rejection rates for 70:30 DI offers during the learning phase (e.g., Fig. 3A, AI+DI blue group). This is puzzling.Thinking about this, I realized the model makes quite strong, unintuitive predictions which are not examined. For example, if a subject begins the learning phase rejecting the 70:30 offer more than 50% of the time (meaning the starting guilt parameter is higher than 1.5), then, over learning, the tendency to reject will decrease to below 50% (the guilt parameter will be pulled down below 1.5). This is despite the fact that the teacher rejects 75% of the offers. In other words, as learning continues, learners will diverge from the teacher. On the other hand, if a participant begins learning by tending to accept this offer (guilt &lt; 1.5), then during learning, they can increase their rejection rate but never above 50%. Thus, one can never fully converge on the teacher. I think this relates to the model's failure in accounting for the pattern mentioned above. I wonder if individuals actually abide by these strict predictions. In any case, these issues raise questions about the validity of the model as a representation of how individuals learn to align with a teacher's preferences (given that the model doesn't really allow for such an alignment).</p><p>In their rebuttal letter, the authors acknowledged these anomalies and stated that they were able to build a better model (where anomalies are mitigated, though not fully eliminated). But they still report the current model and do not develop/discuss alternatives. A more principled model may be a Bayesian model where participants learn a belief distribution (rather than point estimates) regarding the teacher's parameters.</p><p>(5) Statistical Analysis:The authors state in their rebuttal letter that they used the most flexible random effect structure in mixed-effects models. But this seems not to be the case in the model reported in Table SI3 (the very same model was used for other analyses too). Indeed, here it seems only intercepts are random effects. This left me confused about which models were used.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102800.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Shen</given-names></name><role specific-use="author">Author</role><aff><institution>Beijing Normal University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>FeldmanHall</surname><given-names>Oriel</given-names></name><role specific-use="author">Author</role><aff><institution>Brown University</institution><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Hétu</surname><given-names>Sébastien</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0161xgx34</institution-id><institution>Université de Montréal</institution></institution-wrap><addr-line><named-content content-type="city">Montreal</named-content></addr-line><country>Canada</country></aff></contrib><contrib contrib-type="author"><name><surname>Otto</surname><given-names>A Ross</given-names></name><role specific-use="author">Author</role><aff><institution>McGill University</institution><addr-line><named-content content-type="city">Quebec</named-content></addr-line><country>Canada</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>Zhang et al. addressed the question of whether advantageous and disadvantageous inequality aversion can be vicariously learned and generalized. Using an adapted version of the ultimatum game (UG), in three phases, participants first gave their own preference (baseline phase), then interacted with a &quot;teacher&quot; to learn their preference (learning phase), and finally were tested again on their own (transfer phase). The key measure is whether participants exhibited similar choice preferences (i.e., rejection rate and fairness rating) influenced by the learning phase, by contrasting their transfer phase and baseline phase. Through a series of statistical modeling and computational modeling, the authors reported that both advantageous and disadvantageous inequality aversion can indeed be learned (Study 1), and even be generalised (Study 2).</p><p>Strengths:</p><p>This study is very interesting, it directly adapted the lab's previous work on the observational learning effect on disadvantageous inequality aversion, to test both advantageous and disadvantageous inequality aversion in the current study. Social transmission of action, emotion, and attitude have started to be looked at recently, hence this research is timely. The use of computational modeling is mostly appropriate and motivated. Study 2, which examined the vicarious inequality aversion in conditions where feedback was never provided, is interesting and important to strengthen the reported effects. Both studies have proper justifications to determine the sample size.</p><p>Weaknesses:</p><p>Despite the strengths, a few conceptual aspects and analytical decisions have to be explained, justified, or clarified.</p><p>INTRODUCTION/CONCEPTUALIZATION</p><p>(1) Two terms seem to be interchangeable, which should not, in this work: vicarious/observational learning vs preference learning. For vicarious learning, individuals observe others' actions (and optionally also the corresponding consequence resulting directly from their own actions), whereas, for preference learning, individuals predict, or act on behalf of, the others' actions, and then receive feedback if that prediction is correct or not. For the current work, it seems that the experiment is more about preference learning and prediction, and less so about vicarious learning. The intro and set are heavily around vicarious learning, and later the use of vicarious learning and preference learning is rather mixed in the text. I think either tone down the focus on vicarious learning, or discuss how they are different. Some of the references here may be helpful: (Charpentier et al., Neuron, 2020; Olsson et al., Nature Reviews Neuroscience, 2020; Zhang &amp; Glascher, Science Advances, 2020)</p></disp-quote><p>We are appreciative of the Reviewer for raising this question and providing the reference. In response to this comment we have elected to avoid, in most cases, use of the term ‘vicarious’ and instead focus the paper on learning of others’ preferences (without specific commitment to various/observational learning per se). These changes are reflected throughout all sections of the revised manuscript, and in the revised title. We believe this simplified terminology has improved the clarity of our contribution.</p><disp-quote content-type="editor-comment"><p>EXPERIMENTAL DESIGN</p><p>(2) For each offer type, the experiment &quot;added a uniformly distributed noise in the range of (-10 ,10)&quot;. I wonder what this looks like? With only integers such as 25:75, or even with decimal points? More importantly, is it possible to have either 70:30 or 90:10 option, after adding the noise, to have generated an 80:20 split shown to the participants? If so, for the analyses later, when participants saw the 80:20 split, which condition did this trial belong to? 70:30 or 90:10? And is such noise added only to the learning phase, or also to the baseline/transfer phases? This requires some clarification.</p></disp-quote><p>We thank the Reviewer for pointing this out. The uniformly distributed noise was added to all three phases to make the proposers’ behavior more realistic. This added noise was rounded to integer numbers, constrained from -9 to 9, which means in both 70:30 and 90:10 offer types, an 80:20 split could not occur. We have made this feature of our design clear in the Method section Line 524 ~ 528:</p><p>“In all task phases, we added uniformly distributed noise to each trial’s offer (ranging from -9 to 9, inclusive, rounding to the nearest integer) such that the random amount added (or subtracted) from the Proposer’s share was subtracted (or added) to the Receiver’s share. We adopted this manipulation to make the proposers’ behavior appear more realistic. The orders of offers participants experienced were fully randomized within each experiment phase. ”</p><disp-quote content-type="editor-comment"><p>(3) For the offer conditions (90:10, 70:30, 50:50, 30:70, 10:90) - are they randomized? If so, how is it done? Is it randomized within each participant, and/or also across participants (such that each participant experienced different trial sequences)? This is important, as the order especially for the learning phase can largely impact the preference learning of the participants.</p></disp-quote><p>We agree with the Reviewer the order in which offers are experienced could be very important. The order of the conditions was randomized independently for each participant (i.e. each participant experienced different trial sequences). We made this point clear in the Methods part. Line 527 ~ 528:</p><p>“The orders of offers participants experienced were fully randomized within each experiment phase.”</p><disp-quote content-type="editor-comment"><p>STATISTICAL ANALYSIS &amp; COMPUTATIONAL MODELING</p><p>(4) In Study 1 DI offer types (90:10, 70:30), the rejection rate for DI-AI averse looks consistently higher than that for DI averse (ie, the blue line is above the yellow line). Is this significant? If so, how come? Since this is a between-subject design, I would not anticipate such a result (especially for the baseline). Also, for the LME results (eg, Table S3), only interactions were reported but not the main results.</p></disp-quote><p>We thank the Reviewer for pointing out this feature of the results. Prompted by this comment, we compared the baseline rejection rates between two conditions for these two offer types, finding in Experiment 1 that rejection rates in the DI-AI-averse condition were significantly higher than in the DI-averse condition (DI-AI-averse vs. DI-averse; Offer 90:10, β = 0.13, p &lt; 0.001, Offer 70:30, β = 0.09, p &lt; 0.034). We agree with the Reviewer that there should, in principle, be no difference between the experiences of participants in these two conditions is identical in the Baseline phase. However, we did not observe these difference in baseline preferences in Experiment 2 (DI-AI-averse vs. DI-averse; Offer 90:10, β = 0.07, p &lt; 0.100, Offer 70:30, β = 0.05, p &lt; 0.193). On the basis of the inconsistency of this effect across studies we believe this is a spurious difference in preferences stemming from chance.</p><p>Regarding the LME results, the reason why only interaction terms are reported is due to the specification of the model and the rationale for testing.</p><p>Taking the model reported in Table S3 as an example—a logistic model which examines Baseline phase rejection rates as a function of offer level and condition—the between-subject conditions (DI-averse and DI-AI-averse) are represented by dummy-coded variables. Similarly, offer types were also dummy-coded, such that each of the five columns (90:10, 70:30, 50:50, 30:70, and 10:90) correspond corresponded to a particular offer type. This model specification yields ten interaction terms (i.e., fixed effects) of interest—for example, the “DI-averse × Offer 90:10” indicates baseline rejection rates for 90:10 offers in DI-averse condition. Thus, to compare rejection rates across specific offer types, we estimate and report linear contrasts between these resultant terms. We have clarified the nature of these reported tests in our revised Results—for example, line189-190: “linear contrasts; e.g. 90:10 vs 10:90, all Ps&lt;0.001, see Table S3 for logistic regression coefficients for rejection rates.</p><p>Also in response to this comment that and a recommendation from Reviewer 2 (see below), we have revised our supplementary materials to make each model specification clearer as SI line 25:</p><p>“<italic>RejectionRate ~ 0 + (Disl + Advl):(Offer10 + Offer30 + Offer50 + Offer70 + Offer90)</italic> + (1|<italic>Subject</italic>)”</p><disp-quote content-type="editor-comment"><p>(5) I do not particularly find this analysis appealing: &quot;we examined whether participants' changes in rejection rates between Transfer and Baseline, could be explained by the degree to which they vicariously learned, defined as the change in punishment rates between the first and last 5 trials of the Learning phase.&quot; Naturally, the participants' behavior in the first 5 trials in the learning phase will be similar to those in the baseline; and their behavior in the last 5 trials in the learning phase would echo those at the transfer phase. I think it would be stronger to link the preference learning results to the change between the baseline and transfer phase, eg, by looking at the difference between alpha (beta) at the end of the learning phase and the initial alpha (beta).</p></disp-quote><p>Thanks for pointing this out. Also, considering the comments from Reviewer 2 concerning the interpretation of this analysis, we have elected to remove this result from our revision.</p><disp-quote content-type="editor-comment"><p>(6) I wonder if data from the baseline and transfer phases can also be modeled, using a simple Fehr-Schimdt model. This way, the change in alpha/beta can also be examined between the baseline and transfer phase.</p></disp-quote><p>We agree with the Reviewer that a simplified F-S model could be used, in principle, to characterize Baseline and Transfer phase behavior, but it is our view that the rejection rates provide readers with the clearest (and simplest) picture of how participants are responding to inequity. Put another way, we believe that the added complexity of using (and explaining) a new model to characterize simple, steady-state choice behavior (within these phases) would not be justified or add appreciable insights about participants’ behavior.</p><disp-quote content-type="editor-comment"><p>(7) I quite liked Study 2 which tests the generalization effect, and I expected to see an adapted computational modeling to directly reflect this idea. Indeed, the authors wrote, &quot;[...] given that this model [...] assumes the sort of generalization of preferences between offer types [...]&quot;. But where exactly did the preference learning model assume the generalization? In the methods, the modeling seems to be only about Study 1; did the authors advise their model to accommodate Study 2? The authors also ran simulation for the learning phase in Study 2 (Figure 6), and how did the preference update (if at all) for offers (90:10 and 10:90) where feedback was not given? Extending/Unpacking the computational modeling results for Study 2 will be very helpful for the paper.</p></disp-quote><p>We are appreciative of the Reviewer’s positive impression of Experiment 2. Upon reflection, we realize that our original submission was not clear about the modeling done in Experiment 2, and we should clarify here that we did also fit the Preference Inference model to this dataset. As in Experiment 1, this model assumes that the participants have a representation of the teacher’s preference as a Fehr-Schmidt form utility function and infer the Teacher’s Envy and Guilt parameters through learning. The model indicates that, on the basis of experience with the Teacher’s preferences on moderately unfair offers (i.e., offer 70:30 and offer 30:70), participants can successfully infer these guess of these two parameters, and in turn, compute Fehr-Schmidt utility to guide their decisions in the extreme unfair offers (i.e., offer 90:10 and offer 10:90).</p><p>In response to this comment, we have made this clearer in our Results (Line 377-382):</p><p>“Finally, following Experiment 1, we fit a series of computational models of Learning phase choice behavior, comparing the goodness-of-fit of the four best-fitting models from Experiment 1 (see Methods). As before, we found that the Preference Inference model provided the best fit of participants’ Learning Phase behavior (Figure S1a, Table S12). Given that this model is able to infer the Teacher’s underlying inequity-averse preferences (rather than learns offer-specific rejection preferences), it is unsurprising that this model best describes the generalization behavior observed in Experiment 2.”</p><p>and in our revised Methods (Line 551-553)</p><p>“We considered 6 computational models of Learning Phase choice behavior, which we fit to individual participants’ observed sequences of choices, in both Experiments 1 and 2, via Maximum Likelihood Estimation”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>This study investigates whether individuals can learn to adopt egalitarian norms that incur a personal monetary cost, such as rejecting offers that benefit them more than the giver (advantageous inequitable offers). While these behaviors are uncommon, two experiments demonstrate that individuals can learn to reject such offers through vicarious learning - by observing and acting in line with a &quot;teacher&quot; who follows these norms. The authors use computational modelling to argue that learners adopt these norms through a sophisticated process, inferring the latent structure of the teacher's preferences, akin to theory of mind.</p><p>Strengths:</p><p>This paper is well-written and tackles a critical topic relevant to social norms, morality, and justice. The findings, which show that individuals can adopt just and fair norms even at a personal cost, are promising. The study is well-situated in the literature, with clever experimental design and a computational approach that may offer insights into latent cognitive processes. Findings have potential implications for policymakers.</p><p>Weaknesses:</p><p>Note: in the text below, the &quot;teacher&quot; will refer to the agent from which a participant presumably receives feedback during the learning phase.</p><p>(1) Focus on Disadvantageous Inequity (DI): A significant portion of the paper focuses on responses to Disadvantageous Inequitable (DI) offers, which is confusing given the study's primary aim is to examine learning in response to Advantageous Inequitable (AI) offers. The inclusion of DI offers is not well-justified and distracts from the main focus. Furthermore, the experimental design seems, in principle, inadequate to test for the learning effects of DI offers. Because both teaching regimes considered were identical for DI offers the paradigm lacks a control condition to test for learning effects related to these offers. I can't see how an increase in rejection of DI offers (e.g., between baseline and generalization) can be interpreted as speaking to learning. There are various other potential reasons for an increase in rejection of DI offers even if individuals learn nothing from learning (e.g. if envy builds up during the experiment as one encounters more instances of disadvantageous fairness).</p></disp-quote><p>We are appreciative of the Reviewer’s insight here and for the opportunity to clarify our experimental logic. We included DI offers in order to (1) expose participants to the full spectrum of offer types, and avoid focusing participants exclusively upon AI offers, which might result in a demand characteristic and (2) to afford exploration of how learning dynamics might differ in DI context s—which was, to some extent, examined in our previous study (FeldmanHall, Otto, &amp; Phelps, 2018)—versus AI contexts. Furthermore, as this work builds critically on our previous study, we reasoned that replicating these original findings (in the DI context) would be important for demonstrating the generality of the learning effects in the DI context across experimental settings. We now remark on this point in our revised Introduction Line 129 ~132:</p><p>“In addition, to mechanistically probe how punitive preferences are acquired in Adv-I and Dis-I contexts—in turn, assessing the replicability of our earlier study investigating punitive preference acquisition in the Dis context—we also characterize trial-by-trial acquisition of punitive behavior with computational models of choice.”</p><disp-quote content-type="editor-comment"><p>(2) Statistical Analysis: The analysis of the learning effects of AI offers is not fully convincing. The authors analyse changes in rejection rates within each learning condition rather than directly comparing the two. Finding a significant effect in one condition but not the other does not demonstrate that the learning regime is driving the effect. A direct comparison between conditions is necessary for establishing that there is a causal role for the learning regime.</p></disp-quote><p>We agree with the Reviewer and upon reflection, believe that direct comparisons between conditions would be helpful to support the claim that the different learning conditions are responsible for the observed learning effects. In brief, these specific tests buttress the idea that exposure to AI-averse preferences result in increases in AI punishment rates in the Transfer phase (over and above the rates observed for participants who were only exposed to DI-averse preferences).</p><p>Accordingly, our revision now reports statistics concerning the differences between conditions for AI offers in Experiment 1 (Line 198~ 207):</p><p>“Importantly, when comparing these changes between the two learning conditions, we observed significant differences in rejection rates for Adv-I offers: compared to exposure to a Teacher who rejected only Dis-I offers, participants exposed to a Teacher who rejected both Dis-I and Adv-I offers were more likely to reject Adv-I offers and rated these offers more unfair. This difference between conditions was evident in both 30:70 offers (Rejection rates: β(SE) = 0.10(0.04), p = 0.013; Fairness ratings: β(SE) = -0.86(0.17), p &lt; 0.001) and 10:90 offers (Rejection rates: β(SE) = 0.15(0.04), p &lt; 0.001, Fairness ratings: β(SE) = -1.04(0.17), p &lt; 0.001). As a control, we also compared rejection rates and fairness rating changes between conditions in Dis-I offers (90:10 and 30:70) and Fair offers (i.e., 50:50) but observed no significant difference (all ps &gt; 0.217), suggesting that observing an Adv-I-averse Teacher’s preferences did not influence participants’ behavior in response to Dis-I offers.”</p><p>Line 222 ~ 230:</p><p>“A mixed-effects logistic regression revealed a significant larger (positive) effect of trial number on rejection rates of Adv-I offers for the Adv-Dis-I-Averse condition compared to the Dis-I-Averse condition. This relative rejection rate increase was evident both in 30:70 offers (Table S7; β(SE) = -0.77(0.24), p &lt; 0.001) and in 10:90 offers (β(SE) = -1.10(0.33), p &lt; 0.001). In contrast, comparing Dis-I and Fairness offers when the Teacher showed the same tendency to reject, we found no significant difference between the two conditions (90:10 splits: β(SE)=-0.48(0.21),p=0.593;70:30 splits: β(SE)=-0.01(0.14),p=0.150; 50:50 splits: β(SE)=-0.00(0.21),p=0.086). In other words, participants by and large appeared to adjust their rejection choices in accordance with the Teacher’s feedback in an incremental fashion.”</p><p>And in Experiment 2 Line 333 ~ 345:</p><p>“Similar to what we observed in Experiment 1 (Figure 4a), Compared to the participants in the Dis-I-Averse Condition, participants in the Adv-I-Averse Condition increased their rates of rejection of extreme Adv-I offerers (i.e., 10:90) in the Transfer Phase, relative to the Baseline phase (β(SE) = -0.12(0.04), p &lt; 0.004; Table S9), suggesting that participants’ learned (and adopted) Adv-I-averse preferences, generalized from one specific offer type (30:70) to an offer types for which they received no Teacher feedback (10:90). Examining extreme Dis-I offers where the Teacher exhibited identical preferences across the two learning conditions, we found no difference in the Changes of Rejection Rates from Baseline to Transfer phase between conditions (β(SE) = -0.05(0.04), p &lt; 0.259). Mirroring the observed rejection rates (Figure 4b), relative to the Dis-I-Averse Condition, participants’ fairness ratings for extreme Adv-I offers increased more from the Baseline to Transfer phase in the Adv-Dis-I-Averse Condition than in the Dis-I-Averse condition (β(SE) = -0.97(0.18), p &lt; 0.001), but, importantly, changes in fairness ratings for extreme Dis-I offers did not differ significantly between learning conditions (β(SE) = -0.06(0.18), p &lt; 0.723)”</p><p>Line 361 ~ 368:</p><p>“Examining the time course of rejection rates in Adv-I-contexts during the Learning phase (Figure 5) revealed that participants learned over time to punish mildly unfair 30:70 offers, and these punishment preferences generalized to more extreme offers (10:90). Specifically, compared to the Dis-I-Averse Condition, in the Adv-Dis-I-Averse condition we observed a significant larger trend of increase in rejections rates for 10:90 (Adv-I) offers (Figure 5, β(SE) = -0.81(0.26), p &lt; 0.002 mixed-effects logistic regression, see Table S10). Again, when comparing the rejection rate increase in the extremely Dis-I offers (90:10), we didn’t find significant difference between conditions (β(SE) = -0.25(0.19), p &lt; 0.707).”</p><disp-quote content-type="editor-comment"><p>(3) Correlation Between Learning and Contagion Effects:</p></disp-quote><p>The authors argue that correlations between learning effects (changes in rejection rates during the learning phase) and contagion effects (changes between the generalization and baseline phases) support the idea that individuals who are better aligning their preferences with the teacher also give more consideration to the teacher's preferences later during generalization phase. This interpretation is not convincing. Such correlations could emerge even in the absence of learning, driven by temporal trends like increasing guilt or envy (or even by slow temporal fluctuations in these processes) on behalf of self or others. The reason is that the baseline phase is temporally closer to the beginning of the learning phase whereas the generalization phase is temporally closer to the end of the learning phase. Additionally, the interpretation of these effects seems flawed, as changes in rejection rates do not necessarily indicate closer alignment with the teacher's preferences. For example, if the teacher rejects an offer 75% of the time then a positive 5% learning effect may imply better matching the teacher if it reflects an increase in rejection rate from 65% to 70%, but it implies divergence from the teacher if it reflects an increase from 85% to 90%. For similar reasons, it is not clear that the contagion effects reflect how much a teacher's preferences are taken into account during generalization.</p><p>This comment is very similar to a previous comment made by Reviewer 1, who also called into question the interpretability of these correlations. In response to both of these comments we have elected to remove these analyses from our revision.</p><disp-quote content-type="editor-comment"><p>(4) Modeling Efforts: The modelling approach is underdeveloped. The identification of the &quot;best model&quot; lacks transparency, as no model-recovery results are provided, and fits for the losing models are not shown, leaving readers in the dark about where these models fail. Moreover, the reinforcement learning (RL) models used are overly simplistic, treating actions as independent when they are likely inversely related (for example, the feedback that the teacher would have rejected an offer provides feedback that rejection is &quot;correct&quot; but also that acceptance is &quot;an error&quot;, and the later is not incorporated into the modelling). It is unclear if and to what extent this limits current RL formulations. There are also potentially important missing details about the models. Can the authors justify/explain the reasoning behind including these variants they consider? What are the initial Q-values? If these are not free parameters what are their values?</p></disp-quote><p>We are appreciative of the Reviewer for identifying these potentially unaddressed questions.</p><p>The RL models we consider in the present study are naïve models which, in our previous study (FeldmanHall, Otto, &amp; Phelps, 2018), we found to capture important aspects of learning. While simplistic, we believed these models serve as a reasonable baseline for evaluating more complex models, such as the Preference Inference model. We have made this point more explicit in our revised Introduction, Line 129 ~ 132:</p><p>“In addition, to mechanistically probe how punitive preferences may be acquired in Adv-I and Dis-I contexts—in turn, assessing the replicability of our earlier study investigating punitive preference acquisition in the Dis-I context—we also characterize trial-by-trial acquisition of punitive behavior with computational models of choice.”</p><p>Again, following from our previous modeling of observational learning (FeldmanHall et al., 2018), we believe that the feedback the Teacher provides here is ideally suited to the RL formalism. In particular, when the teacher indicates that the participant’s choice is what they would have preferred, the model receives a reward of ‘1’ (e.g., the participant rejects and the Teacher indicates they would preferred rejection, resulting in a positive prediction error) otherwise, the model receives a reward of ‘0’ (e.g., the participant accepts and the Teacher indicates they would preferred rejection, resulting in a negative prediction error), indicating that the participant did not choose in accordance with the Teacher’s preferences. Through an error driven learning process, these models provide a naïve way of learning to act in accordance with the Teacher’s preferences.</p><p>Regarding the requested model details: When treating the initial values as free parameters (model 5), we set Q(reject, offertype) as free values in [0,1] and Q(accept,offertype) as 0.5. This setting can capture participants' initial tendency to reject or accept offers from this offer type. When the initial values are fixed, for all offer types we set Q(reject, offertype) = Q(accept,offertype) = 0.5. In practice, when the initial values are fixed, setting them to 0.5 or 0 doesn’t make much difference. We have clarified these points in our revised Methods, Line 275 ~ 576:</p><p>“We kept the initial values fixed in this model, that is <italic>Q</italic><sub>0</sub>(<italic>reject,offertype</italic>) = 0.5, (<italic>offertype</italic> ∈ 90:10, 70:30, 50:50, 30:70, 10:90)”</p><p>And Line 582 ~ 584:</p><p>“Formally, this model treats <italic>Q</italic><sub>0</sub>(<italic>reject,offertype</italic>) = 0.5, (<italic>offertype</italic> ∈ 90:10, 70:30, 50:50, 30:70, 10:90) as free parameters with values between 0 and 1.”</p><disp-quote content-type="editor-comment"><p>(5) Conceptual Leap in Modeling Interpretation: The distinction between simple RL models and preference-inference models seems to hinge on the ability to generalize learning from one offer to another. Whereas in the RL models learning occurs independently for each offer (hence to cross-offer generalization), preference inference allows for generalization between different offers. However, the paper does not explore RL models that allow generalization based on the similarity of features of the offers (e.g., payment for the receiver, payment for the offer-giver, who benefits more). Such models are more parsimonious and could explain the results without invoking a theory of mind or any modelling of the teacher. In such model versions, a learner learns a functional form that allows to predict the teacher's feedback based on said offer features (e.g., linear or quadratic form). Because feedback for an offer modulates the parameters of this function (feature weights) generalization occurs without necessarily evoking any sophisticated model of the other person. This leaves open the possibility that RL models could perform just as well or even show superiority over the preference learning model, casting doubt on the authors' conclusions. Of note: even the behaviourists knew that as Little Albert was taught to fear rats, this fear generalized to rabbits. This could occur simply because rabbits are somewhat similar to rats. But this doesn't mean little Alfred had a sophisticated model of animals he used to infer how they behave.</p></disp-quote><p>We are appreciative of the Reviewer for their suggestion of an alternative explanation for the observed generalization effects. Our understanding of the suggestion, put simply, put simply, is that an RL model could capture the observed generalization effects if the model were to learn and update a functional form of the Teacher’s rejection preferences using an RL-like algorithm. This idea is similar, conceptually to our account of preference learning whereby the learner has a representation of the teacher’s preferences. In our experiment the offer is in the range of [0-100], the crux of this idea is why the participants should take the functional form (either v-shaped or quadratic) with the minimum at 50. This is important because, at the beginning of the learning phase, the rejection rates are already v-shaped with 50 as its minimum. The participants do not need to adjust the minimum of this functional form. Thus, if we assume that the participants represent the teacher’s rejection rate as a v-shape function with a minimum at [50,50], then this very likely implies that the participants have a representation that the teacher has a preference for fairness. Above all, we agree that with suitable setup of the functional form, one could implement an RL model to capture the generalization effects, without presupposing an internal “model” of the teacher’s preferences.</p><p>However, there is another way of modeling the generalization effect by truly “model-free” similarity-based Reinforcement learning. In this approach, we do not assume any particular functional form of the teacher’s preferences, but rather, assumes that experience acquired in one offer type can be generalized to offers that are close (i.e., similar) to the original offer. Accordingly, we implement this idea using a simple RL model in which the action values for each offer type is updated by a learning rate that is scaled by the distance between that offer and the experienced offer (i.e., the offer that generated the prediction error). This learning rate is governed by a Gaussian distribution, similar to the case in the Gaussian process regression (cf. Chulz, Speekenbrink, &amp; Krause, 2018). The initial value of the ‘Reject’ action, for each offer , is set to a free parameter between 0 and 1, and the initial value for the 'Accept’ action was set to 0.5. The results show that even though this model exhibits the trend of increasing rejection rates observed in the AI-DI punish condition, the initial preferences (i.e., starting point of learning) diverges markedly from the Learning phase behavior we observed in Experiment 1:</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-sa3-fig1-v1.tif"/></fig><p>This demonstrated that the participant at least maintains a representation of the teacher’s preference at the beginning. That is, they have prior knowledge about the shape of this preference. We incorporated this property into the model, that is, we considered a new model that assumes v-shaped starting values for rejection with two parameters, alpha and beta, governing the slope of this v-shaped function (this starting value actually mimics the shape of the preference functions of the Fehr-Schmidt model). We found that this new model (which we term the “Model RL Sim Vstart”) provided a satisfactory qualitative fit of the Transfer phase learning curves in Experiment 1 (see below).</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-sa3-fig2-v1.tif"/></fig><p>However, we didn’t adopt this model as the best model for the following reasons. First, this model yielded a larger AIC value (indicating worse quantitative fit) compared to our preference Inference model in both Experiments 1 and 2, likely owing to its increased complexity (5 free parameters versus 4 in the Preference Inference model). Accordingly, we believe that inclusion of this model in our revised submission would be more distracting than helpful on account of the added complexity of explaining and justifying these assumptions, and of course its comparatively poor goodness of fit (relative to the preference inference model).</p><disp-quote content-type="editor-comment"><p>(6) Limitations of the Preference-Inference Model: The preference-inference model struggles to capture key aspects of the data, such as the increase in rejection rates for 70:30 DI offers during the learning phase (e.g. Figure 3A, AI+DI blue group). This is puzzling.</p><p>Thinking about this I realized the model makes quite strong unintuitive predictions that are not examined. For example, if a subject begins the learning phase rejecting the 70:30 offer more than 50% of the time (meaning the starting guilt parameter is higher than 1.5), then overleaning the tendency to reject will decrease to below 50% (the guilt parameter will be pulled down below 1.5). This is despite the fact the teacher rejects 75% of the offers. In other words, as learning continues learners will diverge from the teacher. On the other hand, if a participant begins learning to tend to accept this offer (guilt &lt; 1.5) then during learning they can increase their rejection rate but never above 50%. Thus one can never fully converge on the teacher. I think this relates to the model's failure in accounting for the pattern mentioned above. I wonder if individuals actually abide by these strict predictions. In any case, these issues raise questions about the validity of the model as a representation of how individuals learn to align with a teacher's preferences (given that the model doesn't really allow for such an alignment).</p></disp-quote><p>In response to this comment we explain our efforts to build a new model that might be able conceptually resolves the issue identified by the Reviewer.</p><p>The key intuition guiding the Preference inference model is a Bayesian account of learning which we aimed to further simplify. In this setting, a Bayesian learner maintains a representation of the teacher’s inequity aversion parameters and updates it according to the teacher’s (observed) behavior. Intuitively, the posterior distribution shifts to the likelihood of the teacher’s action. On this view, when the teacher rejects, for instance, an AI offer, the learner should assign a higher probability to larger values of the Guilt parameter, and in turn the learner should change their posterior estimate to better capture the teacher’s preferences.</p><p>In the current study, we simplified this idea, implementing this sort of learning using incremental “delta rule” updating (e.g. Equation 8 of the main text). Then the key question is to define the “teaching signal”. Assuming that the teacher rejects an offer 70:30, based on Bayesian reasoning, the teacher’s envy parameter (α) is more likely to exceed 1.5 (computed as 30/(50-30), per equation 7) than to be smaller than 1.5. Thus, 1.5, which is then used in equation 8 to update α, can be thought of as a teaching signal. We simply assumed that if the initial estimate is already greater than 1.5, which means the prior is consistent with the likelihood, no updating would occur. This assumption raises the question of how to set the learning rate range. In principle, an envy parameter that is larger than 1.5 should be the target of learning (i.e., the teaching signal), and thus our model definition allows the learning rate to be greater than 1, incorporating this possibility.</p><p>Our simplified preference inference model has already successfully captured some key aspects of the participants’ learning behavior. However, it may fail in the following case: assume that the participant has an initial estimate of 1.51 for the envy parameter (β). Let’s say this corresponds to a rejection rate of 60%. Thus, no matter how many times the teacher rejects the offer 70:30, the participant’s estimate of the envy parameter remains the same, but observing only one offer acceptance would decrease this estimate, and in turn, would decrease the model’s predicted rejection rate. We believe this is the anomalous behavior—in 70:30 offers—identified by the Reviewer which the model does not appear able to recreate participants’ in these offers.</p><p>This issue actually touches the core of our model specification, that is, the choosing of the teaching signal. As we chose 1.5 as the teaching signal—i.e. lower bound on whenever the teacher rejects or accepts an offer of 70:30, a very small deviation of 1.5 would fail one part of updating. One way to mitigate this problem would be to choose a lower bound for α greater than 1.5, such that when the Teacher rejects a 70:30 offer, we assign a number greater than 1.5 (by ‘hard-coding’ this into the model via modification of equation 7). One sensible candidate value could be the middle point between 1.5 and 10 (the maximum value of α per our model definition). Intuitively, the model of this setting could still pull up the value of α to 1.51 when the teacher rejects 70:30, thus alleviating (but not completely eliminating) the anomaly.</p><p>We fitted this modified Preference Inference model to the data from Experiment 1 (see Author response image 3 below) and found that even though this model has a smaller AIC (and thus better quantitative fit than the original Preference Inference model), it still doesn’t fully capture the participants’ behavior for 70:30 offers.</p><fig id="sa3fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102800-sa3-fig3-v1.tif"/></fig><p>Accordingly, rather than revising our model to include an unprincipled ‘kludge’ to account for this minor anomaly in the model behavior, we have opted to report our original model in our revision as we still believe it parsimoniously captures our intuitions about preference learning and provides a better fit to the observed behavior than the other RL models considered in the present study.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>(1) I do not particularly prefer the acronyms AI and DI for disadvantageous inequity and advantageous inequity. Although they have been used in the literature, not every single paper uses them. More importantly, AI these days has such a strong meaning of artificial intelligence, so when I was reading this, I'd need to very actively inhibit this interpretation. I believe for the readability for a wider readership of eLife, I would advise not to use AI/DI here, but rather use the full terms.</p></disp-quote><p>We thank the Reviewer for this suggestion. As the full spelling of the two terms are somewhat lengthy, and appear frequently in the figures, we have elected to change the abbreviations for disadvantageous inequity and advantageous inequity to Dis-I and Adv-I, respectively in the main text and the supplementary information. We still use AI/DI in the response letter to make the terminology consistent.</p><disp-quote content-type="editor-comment"><p>(2) Do &quot;punishment rate&quot; and &quot;rejection rate&quot; mean the same? If so, it would be helpful to stick with one single term, eg, rejection rate.</p></disp-quote><p>We thank the Reviewer for this suggestion. As these terms have the same meaning, we have opted to use the term “rejection rate” throughout the main text.</p><disp-quote content-type="editor-comment"><p>(3) For the linear mixed effect models, were other random effect structures also considered (eg, random slops of experimental conditions)? It might be worth considering a few model specifications and selecting the best one to explain the data.</p></disp-quote><p>Thanks for this comment. Following established best practices (Barr, Levy, Scheepers, &amp; Tily, 2013) we have elected to use a maximal random effects structure, whereby all possible predictor variables in the fixed effects structure also appear in the random effects structure.</p><disp-quote content-type="editor-comment"><p>(4) For equation (4), the softmax temperature is denoted as tau, but later in the text, it is called gamma. Please make it consistent.</p></disp-quote><p>We are appreciative of the Reviewer’s attention to detail. We have corrected this error.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) Several Tables in SI are unclear. I wasn't clear if these report raw probabilities of coefficients of mixed models. For any mixed models, it would help to give the model specification (e.g., Walkins form) and explain how variables were coded.</p></disp-quote><p>We are appreciative of the Reviewer’s attention to detail. We have clarified, in the captions accompanying our supplemental regression tables, that these coefficients represent log-odds. Regretfully we are unaware of the “Walkins form” the Reviewer references (even after extensive searching of the scientific literature). However, in our new revision we do include lme4 model syntax in our supplemental information which we believe will be helpful for readers seeking replicate our model specification.</p><disp-quote content-type="editor-comment"><p>(2) In one of the models it was said that the guilt and envy parameters were bounded between 0-1 but this doesn't make sense and I think values outside this range were later reported.</p></disp-quote><p>We are again appreciative of the Reviewer’s attention to detail. This was an error we have corrected— the actual range is [0,10].</p><disp-quote content-type="editor-comment"><p>(3) It is unclear if the model parameters are recoverable.</p></disp-quote><p>In response to this comment our revision now reports a basic parameter recovery analysis for the winning Preference Inference model. This is reported in our revised Methods:</p><p>“Finally, to verify if the free parameters of the winning model (Preference Inference) are recoverable, we simulated 200 artificial subjects, based on the Learning Phase of Experiment 1, with free parameters randomly chosen (uniformly) from their defined ranges. We then employed the same model-fitting procedure as described above to estimate these parameter value, observing that parameters. We found that all parameters of the model can be recovered (see Figure S2).”</p><p>And scatter plots depicting these simulated (versus recovered) parameters are given in Figure S2 of our revised Supplementary Information:</p><disp-quote content-type="editor-comment"><p>(4) I was confused about what Figure S2 shows. The text says this is about correlating contagious effects for different offers but the captions speak about learning effects. This is an important aspect which is unclear.</p></disp-quote><p>We have removed this figure in response to both Reviewers’ comments about the limited insights that can be drawn on the basis of these correlations.</p></body></sub-article></article>