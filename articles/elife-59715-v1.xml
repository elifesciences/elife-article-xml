<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">59715</article-id><article-id pub-id-type="doi">10.7554/eLife.59715</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Learning excitatory-inhibitory neuronal assemblies in recurrent networks</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-197193"><name><surname>Mackwood</surname><given-names>Owen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8569-6614</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-191462"><name><surname>Naumann</surname><given-names>Laura B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7919-7349</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-43434"><name><surname>Sprekeler</surname><given-names>Henning</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0690-3553</contrib-id><email>h.sprekeler@tu-berlin.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Bernstein Center for Computational Neuroscience Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Department for Electrical Engineering and Computer Science, Technische Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution>University of Chicago</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>04</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e59715</elocation-id><history><date date-type="received" iso-8601-date="2020-06-05"><day>05</day><month>06</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-03-03"><day>03</day><month>03</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Mackwood et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Mackwood et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-59715-v1.pdf"/><abstract><p>Understanding the connectivity observed in the brain and how it emerges from local plasticity rules is a grand challenge in modern neuroscience. In the primary visual cortex (V1) of mice, synapses between excitatory pyramidal neurons and inhibitory parvalbumin-expressing (PV) interneurons tend to be stronger for neurons that respond to similar stimulus features, although these neurons are not topographically arranged according to their stimulus preference. The presence of such excitatory-inhibitory (E/I) neuronal assemblies indicates a stimulus-specific form of feedback inhibition. Here, we show that activity-dependent synaptic plasticity on input and output synapses of PV interneurons generates a circuit structure that is consistent with mouse V1. Computational modeling reveals that both forms of plasticity must act in synergy to form the observed E/I assemblies. Once established, these assemblies produce a stimulus-specific competition between pyramidal neurons. Our model suggests that activity-dependent plasticity can refine inhibitory circuits to actively shape cortical computations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>synaptic plasticity</kwd><kwd>neural assemblies</kwd><kwd>inhibition</kwd><kwd>interneurons</kwd><kwd>visual cortex</kwd><kwd>homeostasis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>FKZ 01GQ1201</award-id><principal-award-recipient><name><surname>Mackwood</surname><given-names>Owen</given-names></name><name><surname>Sprekeler</surname><given-names>Henning</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>FOR 2143</award-id><principal-award-recipient><name><surname>Mackwood</surname><given-names>Owen</given-names></name><name><surname>Sprekeler</surname><given-names>Henning</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The synaptic structure in mouse V1 is explained by a synergy of homeostatic plasticity in incoming and outgoing synapses of inhibitory interneurons, establishing a stimulus-specific balance of excitation and inhibition.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>With the advent of modern optogenetics, the functional role of inhibitory interneurons has developed into one of the central topics of systems neuroscience (<xref ref-type="bibr" rid="bib18">Fishell and Kepecs, 2020</xref>). Aside from the classical perspective that inhibition serves to stabilize recurrent excitatory feedback loops in neuronal circuits (<xref ref-type="bibr" rid="bib74">van Vreeswijk and Sompolinsky, 1996</xref>; <xref ref-type="bibr" rid="bib9">Brunel, 2000</xref>; <xref ref-type="bibr" rid="bib52">Murphy and Miller, 2009</xref>; <xref ref-type="bibr" rid="bib69">Sprekeler, 2017</xref>), it is increasingly recognised as an active player in cortical computation (<xref ref-type="bibr" rid="bib29">Isaacson and Scanziani, 2011</xref>; <xref ref-type="bibr" rid="bib61">Priebe and Ferster, 2008</xref>; <xref ref-type="bibr" rid="bib62">Rubin et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Pouille and Scanziani, 2001</xref>; <xref ref-type="bibr" rid="bib42">Letzkus et al., 2011</xref>; <xref ref-type="bibr" rid="bib1">Adesnik et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Phillips et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Barron et al., 2016</xref>; <xref ref-type="bibr" rid="bib4">Barron et al., 2017</xref>; <xref ref-type="bibr" rid="bib72">Tovote et al., 2015</xref>).</p><p>Within cortical neurons, excitatory and inhibitory currents are often highly correlated in their response to stimuli (<xref ref-type="bibr" rid="bib79">Wehr and Zador, 2003</xref>; <xref ref-type="bibr" rid="bib19">Froemke et al., 2007</xref>; <xref ref-type="bibr" rid="bib71">Tan et al., 2011</xref>; <xref ref-type="bibr" rid="bib6">Bhatia et al., 2019</xref>), in time (<xref ref-type="bibr" rid="bib56">Okun and Lampl, 2008</xref>; <xref ref-type="bibr" rid="bib15">Dipoppa et al., 2018</xref>) and across neurons (<xref ref-type="bibr" rid="bib82">Xue et al., 2014</xref>). This co-tuning of excitatory and inhibitory currents has been attributed to different origins. In topographically organised sensory areas such as cat primary visual cortex (V1), the co-tuning with respect to sensory stimuli could be a natural consequence of local feedback inhibition and does not impose strong constraints on inhibitory circuitry (<xref ref-type="bibr" rid="bib22">Harris and Mrsic-Flogel, 2013</xref>). In the case of feedforward inhibition, co-tuning of excitatory and inhibitory currents was suggested to arise from homeostatic synaptic plasticity in GABAergic synapses (<xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="bib12">Clopath et al., 2016</xref>; <xref ref-type="bibr" rid="bib78">Weber and Sprekeler, 2018</xref>; <xref ref-type="bibr" rid="bib24">Hennequin et al., 2017</xref>).</p><p>In sensory areas with poor feature topography, such as V1 of rodents (<xref ref-type="bibr" rid="bib55">Ohki et al., 2005</xref>), feedback inhibition has been hypothesised to be largely unspecific for stimulus features, a property inferred from the dense connectivity (<xref ref-type="bibr" rid="bib17">Fino and Yuste, 2011</xref>; <xref ref-type="bibr" rid="bib58">Packer and Yuste, 2011</xref>) and reliable presence of synapses connecting pyramidal (Pyr) neurons to inhibitory interneurons with dissimilar stimulus tuning (<xref ref-type="bibr" rid="bib22">Harris and Mrsic-Flogel, 2013</xref>; <xref ref-type="bibr" rid="bib8">Bock et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Hofer et al., 2011</xref>). However, recent results cast doubt on this idea of a ‘blanket of inhibition’ (<xref ref-type="bibr" rid="bib17">Fino and Yuste, 2011</xref>; <xref ref-type="bibr" rid="bib58">Packer and Yuste, 2011</xref>).</p><p>In mouse V1, <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref> report that although the presence of synaptic connections between Pyr cells and parvalbumin-expressing (PV) interneurons is independent of their respective stimulus responses, the efficacy of those synapses is correlated with their response similarity, both in PV → Pyr and in Pyr → PV connections. These mutual preferences in synaptic organisation suggest that feedback inhibition may be more stimulus-specific than previously thought and that Pyr and PV neurons form specialised—albeit potentially overlapping—excitatory-inhibitory (E/I) assemblies (<xref ref-type="bibr" rid="bib10">Chenkov et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Yoshimura et al., 2005</xref>; <xref ref-type="bibr" rid="bib47">Litwin-Kumar and Doiron, 2012</xref>; <xref ref-type="bibr" rid="bib48">Litwin-Kumar and Doiron, 2014</xref>). While the presence of such E/I assemblies (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>; <xref ref-type="bibr" rid="bib64">Rupprecht and Friedrich, 2018</xref>) suggests the need for an activity-dependent mechanism for their formation and/or refinement (<xref ref-type="bibr" rid="bib33">Khan et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Najafi et al., 2020</xref>), the requirements such a mechanism must fulfil remain unresolved.</p><p>Here, we use a computational model to identify requirements for the development of stimulus-specific feedback inhibition. We find that the formation of E/I assemblies requires a synergistic action of plasticity on two synapse types: the excitatory synapses from Pyr neurons onto PV interneurons and the inhibitory synapses from those interneurons onto the Pyr cells. Using ‘knock-out experiments’, in which we block plasticity in either synapse type, we show that both must be plastic to account for the observed functional microcircuits in mouse V1. In addition, after the formation of E/I assemblies, perturbations of individual Pyr neurons lead to a feature-specific suppression of other Pyr neurons as recently found in mouse V1 (<xref ref-type="bibr" rid="bib11">Chettih and Harvey, 2019</xref>). Thus, synergistic plasticity of the incoming and outgoing synapses of PV interneurons can drive the development of stimulus-specific feedback inhibition, resulting in a competition between Pyr neurons with similar stimulus preference.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To understand which activity-dependent mechanisms can generate specific feedback inhibition in circuits without feature topography—such as mouse V1 (<xref ref-type="fig" rid="fig1">Figure 1a</xref>), we studied a rate-based network model consisting of <inline-formula><mml:math id="inf1"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>E</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> excitatory Pyr neurons and <inline-formula><mml:math id="inf2"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>I</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math></inline-formula> inhibitory PV neurons. To endow the excitatory neurons with a stimulus tuning similar to Pyr cells in layer 2/3 of mouse V1 (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>), each excitatory neuron receives external excitatory input that is tuned to orientation, temporal frequency and spatial frequency (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The preferred stimuli of the Pyr neurons cover the stimulus space evenly. Because we are interested under which conditions feedback inhibition can acquire a stimulus selectivity, inhibitory neurons receive external inputs without stimulus tuning, but are recurrently connected to Pyr neurons. While the network has no stimulus topography, Pyr neurons are preferentially connected to other Pyr neurons with similar stimulus tuning (<xref ref-type="bibr" rid="bib27">Hofer et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Cossell et al., 2015</xref>), and connection strength is proportional to the signal correlation of their external inputs. Note that the Pyr → Pyr connections only play a decisive role for the results in Figure 4 but are present in all simulations for consistency. Connection probability across the network is <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula>, with the remaining network connectivity (Pyr → PV, PV → PV, PV → Pyr) initialised randomly according to a log-normal distribution (<xref ref-type="bibr" rid="bib68">Song et al., 2005</xref>; <xref ref-type="bibr" rid="bib50">Loewenstein et al., 2011</xref>), with a variability that is similar to that measured in the respective synapses (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Homeostatic plasticity in input and output synapses of interneurons drives the formation of excitatory-inhibitory (E/I) assemblies.</title><p>(<bold>a</bold>) Emergence of E/I assemblies comprised of pyramidal (Pyr) neurons (triangles) and parvalbumin-expressing (PV) interneurons (circles) in circuits without feature topography. (<bold>b</bold>) Network architecture and stimulus tuning of external inputs to Pyr cells. (<bold>c</bold>) Stimulus selectivity of Pyr neurons and PV interneurons (before and after learning). Arrows indicate the median. (<bold>d</bold>) Example responses of reciprocally connected Pyr cells and PV interneurons. Examples chosen for large, intermediate, and low response similarity (RS). Numbers correspond to points marked in (<bold>e</bold>). (<bold>e</bold>) Relationship of synaptic efficacies of output (left) and input connections (centre) of PV interneurons with RS. Relationship of input and output efficacies (right). Black lines are obtained via linear regression. Reported <italic>r</italic> and associated p-value are Pearson’s correlation. (<bold>f</bold>) Stimulus tuning of excitatory and inhibitory currents onto an example Pyr cell, before and after learning. For simplicity, currents are shown for spatial and temporal frequency only, averaged across all orientations. (<bold>g</bold>) Angle between the weight update and the gradient rule while following the local approximation for input (top) and output (bottom) connections of PV interneurons. Time course for first 4% of simulation (left) and final distribution (right) shown. Black lines are low-pass filtered time courses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Synaptic plasticity and convergence.</title><p>(<bold>a</bold>) Schematics of PV → Pyr plasticity (left) and Pyr → PV plasticity (right). PV → Pyr plasticity follows a simple logic: A given inhibitory synapse is potentiated if the postsynaptic Pyr neuron fires above target and is depressed if below. The Pyr → PV plasticity compares the excitatory input current received by the postsynaptic PV neuron to a target value, and adjusts the incoming synapses in proportion to this error and to presynaptic activity. (<bold>b</bold>) Time plots of the Pyr population firing rate (top), mean of all PV → Pyr synaptic weights (middle) and Pyr → PV weights (bottom). Columns correspond to simulations in which both PV → Pyr and Pyr → PV plasticity are present (left), only Pyr → PV is present (middle), and only PV → Pyr is present (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Gradient rules also require plasticity of both input and output synapses of parvalbumin-expressing (PV) interneurons.</title><p>(<bold>a</bold>) In a network learning with the derived gradient rules of <xref ref-type="disp-formula" rid="equ21">Equation 15</xref>, significant correlations are reliably detected between response similarity (RS) and excitatory weights (red bars), RS and inhibitory weights (blue bars), and excitatory and inhibitory weights for reciprocally connected pyramidal (Pyr)-PV cell pairs (black bars) only if both synapse types are plastic. (<bold>b</bold>) Interneurons fail to develop stimulus selectivity if their input weights do not change according to the gradient rule of <xref ref-type="disp-formula" rid="equ21">Equation 15</xref>a. (<bold>c</bold>) Synaptic currents onto Pyr neurons only develop reliable, strong excitatory-inhibitory (E/I) co-tuning if both input and output synapses are updated using the gradient rules. Currents are averaged across all Pyr neurons after centring according to the neuron’s preferred stimulus. The bottom row is a slice through the difference (third row) of the average excitatory (first row) and inhibitory currents (second row). (<bold>d</bold>) Violin plot of the distribution of E/I synaptic current similarity values for all Pyr neurons in the network (see Materials and methods).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Synaptic currents onto pyramidal (Pyr) neurons.</title><p>(<bold>a</bold>) Excitatory (red) and inhibitory (blue) synaptic currents onto a random selection of Pyr neurons, as a function of temporal and spatial stimulus frequency (averaged over all orientations), when both incoming and outgoing parvalbumin-expressing (PV) synapses are plastic. (<bold>b</bold>) The network-averaged excitatory (first row) and inhibitory (second row) synaptic currents onto Pyr neurons, both centred according to the peak excitatory current before averaging. After averaging, their difference is taken (third row) and a slice is plotted (bottom row). When both plasticities are present, currents are well balanced across stimuli with a modest excitatory bias for preferred stimuli. (<bold>c</bold>) Quality of excitatory-inhibitory (E-I) current co-tuning for every Pyr in the network quantified by the distribution of their cosine similarities. Only when both plasticities are present, do most Pyr neurons receive well co-tuned E-I synaptic currents.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Both input and output synapses must be plastic for feedback alignment to occur.</title><p>In a network with both local rules (left column), the update to Pyr → PV synapses rapidly aligns to the gradient (i.e. when the angle between the approximate update and the gradient is below <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>; bottom left). While updates to the Pyr → PV weights occasionally point away from the gradient, 79% of samples are below <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. For the knock-out (KO) experiments (right column), output plasticity closely follows the PV → Pyr gradient even if input plasticity is absent (upper right). In contrast, if output (PV → Pyr) plasticity is absent, the approximate Pyr → PV rule does not follow the gradient (lower right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Some networks contain experimentally undetectable weights.</title><p>(<bold>a</bold>) Plots of 2D histograms for PV → Pyr (top) and Pyr → PV (bottom) weight versus response similarity (RS), in different networks trained with the local plasticity rules (columns). White lines indicate the threshold of experimental detectability. Any weight &lt; 10<sup>-4</sup> is not included when computing Pearson’s correlation between RS and synaptic weight, or weight-weight correlations. (<bold>b</bold>) Same plots as (<bold>a</bold>), but for networks trained with the gradient rules.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig1-figsupp5-v1.tif"/></fig></fig-group><sec id="s2-1"><title>E/I assemblies are formed by homeostatic plasticity rules in input and output connections of PV interneurons</title><p>In feedforward networks, a stimulus-specific balance of excitation and inhibition can arise from homeostatic inhibitory synaptic plasticity that aims to minimise the deviation of a neuron’s firing rate from a target for all stimuli of a given set (<xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="bib12">Clopath et al., 2016</xref>; <xref ref-type="bibr" rid="bib78">Weber and Sprekeler, 2018</xref>). We wondered whether a stimulus-specific form of homeostasis can also generate stimulus-specific <italic>feedback</italic> inhibition by forming E/I assemblies. To that end, we derive synaptic plasticity rules for excitatory input and inhibitory output connections of PV interneurons that are homeostatic for the excitatory population (see 'Materials and methods'). A stimulus-specific homeostatic control can be seen as a ‘trivial’ supervised learning task, in which the objective is that all Pyr neurons should learn to fire at a given target rate <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> for all stimuli. Hence, a gradient-based optimisation would effectively require a backpropagation of error (<xref ref-type="bibr" rid="bib63">Rumelhart et al., 1985</xref>) through time (BPTT; <xref ref-type="bibr" rid="bib80">Werbos, 1990</xref>).</p><p>Because backpropagation rules rely on non-local information that might not be available to the respective synapses, their biological plausibility is currently debated (<xref ref-type="bibr" rid="bib44">Lillicrap et al., 2020</xref>; <xref ref-type="bibr" rid="bib65">Sacramento et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="bib81">Whittington and Bogacz, 2019</xref>; <xref ref-type="bibr" rid="bib5">Bellec et al., 2020</xref>). However, a local approximation of the full BPTT update can be obtained under the following assumptions: First, we assume that the sensory input to the network changes on a time scale that is slower than the intrinsic time scales in the network. This eliminates the necessity of backpropagating information through time, albeit still through the synapses in the network. This assumption results in what we call the ‘gradient-based’ rules (<xref ref-type="disp-formula" rid="equ21">Equation 15</xref> in Appendix 1), which are spatially non-local. Second, we assume that synaptic interactions in the network are sufficiently weak that higher-order synaptic interactions can be neglected. Third and finally, we assume that over the course of learning, the Pyr → PV connections and the PV → Pyr connections become positively correlated (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>), such that we can replace PV → Pyr synapses by the reciprocal Pyr → PV synapse in the Pyr → PV learning rule, without rotating the update too far from the true gradient (see Appendix 1).</p><p>The resulting learning rule for the output connections of the interneurons is similar to a previously suggested form of homeostatic inhibitory plasticity (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>, left) (<xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref>). Specifically, PV output synapses <inline-formula><mml:math id="inf7"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula> undergo Hebbian changes in proportion to presynaptic interneuron activity <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and the signed deviation of total postsynaptic Pyr cell input <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> from the homeostatic target:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∝</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In contrast, the PV input synapses <inline-formula><mml:math id="inf10"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula> are changed such that the total excitatory drive <inline-formula><mml:math id="inf11"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mtext>E,rec</mml:mtext></mml:msubsup></mml:math></inline-formula> from the Pyr population to each interneuron is close to some target value <italic>I</italic><sub>0</sub> (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>, right):<disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∝</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E,rec</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Both synapse types are subject to a weak weight decay, to avoid the redundancy that a multiplicative rescaling of input synapses can be compensated by a rescaling of the output synapses.</p><p>While our main results are obtained using the local approximations, we also simulated the gradient-based rules to verify that the approximation does not qualitatively change the results (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>When we endow the synapses of an initially randomly connected network of Pyr neurons and PV interneurons with plasticity in both the input and the output synapses of the interneurons, the network develops a synaptic weight structure and stimulus response that closely resemble that of mouse V1 (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>). Before learning, interneurons show poor stimulus selectivity (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), in line with the notion that in a random network, interneurons pool over many Pyr neurons with different stimulus tuning (<xref ref-type="bibr" rid="bib22">Harris and Mrsic-Flogel, 2013</xref>). The network is then exposed to randomly interleaved stimuli. By the end of learning, interneurons have developed a pronounced stimulus tuning, albeit weaker than that of Pyr neurons (<xref ref-type="fig" rid="fig1">Figure 1c,d</xref>). Interneurons form strong bidirectional connections preferentially with Pyr neurons with a similar stimulus tuning, whereas connections between Pyr-PV pairs with dissimilar stimulus tuning are weaker (<xref ref-type="fig" rid="fig1">Figure 1d,e</xref>). To make our results comparable to <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>, we randomly sample an experimentally feasible number of synaptic connections from the network (<inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>). Both the efficacy of PV input and output connections are highly correlated with the response similarity (RS) (see 'Materials and methods') of the associated Pyr neurons and interneurons (<xref ref-type="fig" rid="fig1">Figure 1e</xref>, left and centre). For bidirectionally connected cell pairs, the efficacies of the respective input and output connections are highly correlated (<xref ref-type="fig" rid="fig1">Figure 1e</xref>, right). The stimulus tuning of the inhibitory inputs onto the Pyr cells—initially flat—closely resembles that of the excitatory inputs after learning (<xref ref-type="fig" rid="fig1">Figure 1f</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>; <xref ref-type="bibr" rid="bib71">Tan et al., 2011</xref>), that is, the network develops a precise E/I balance (<xref ref-type="bibr" rid="bib24">Hennequin et al., 2017</xref>).</p><p>Finally, the optimal gradient rules produce very similar results to the local approximations (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Over the course of learning, the weight updates by the approximate rules align to the updates that would result from the gradient rules (<xref ref-type="fig" rid="fig1">Figure 1g</xref>, <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>), presumably by a mechanism akin to feedback alignment (<xref ref-type="bibr" rid="bib43">Lillicrap et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Akrout et al., 2019</xref>).</p><p>In summary, these results show that combined homeostatic plasticity in input and output synapses of interneurons can generate a similar synaptic structure as observed in mouse V1, including the formation of E/I assemblies.</p></sec><sec id="s2-2"><title>PV → Pyr plasticity is required for the formation of E/I assemblies</title><p>Having shown that homeostatic plasticity acting on both input and output synapses of interneurons are <italic>sufficient</italic> to learn E/I assemblies, we now turn to the question of whether both are <italic>necessary</italic>. To this end, we perform ‘knock-out’ experiments, in which we selectively block synaptic plasticity in either of the synapses. The motivation for these experiments is the observation that the incoming PV synapses follow a long-tailed distribution (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>). This could provide a sufficient stimulus selectivity in the PV population for PV → Pyr plasticity alone to achieve a satisfactory E/I balance. A similar reasoning holds for static, but long-tailed outgoing PV synapses. This intuition is supported by results from <xref ref-type="bibr" rid="bib46">Litwin-Kumar et al., 2017</xref>, where for a population of neurons analogous to our interneurons, the dimensionality of responses in that population can be high for static input synapses, when those are log-normally distributed.</p><p>When we knock out output plasticity but keep input plasticity intact, the network fails to develop E/I assemblies and a stimulus-specific E/I balance. While there is highly significant change in the distribution of PV interneuron stimulus selectivity (Mann-Whitney <italic>U</italic> test, <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mn>1207</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), the effect is much stronger when output plasticity is also present (<xref ref-type="fig" rid="fig2">Figure 2a,b</xref>). Importantly, excitatory and inhibitory currents in Pyr neurons are poorly co-tuned (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3b</xref>). In particular, feedback inhibition remains largely untuned because output connections are still random, so that Pyr neurons pool inhibition from many interneurons with different stimulus tuning.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Knock-out (KO) of plasticity in parvalbumin-expressing (PV) interneuron output connections prevents inhibitory co-tuning.</title><p>(<bold>a</bold>) Example responses of reciprocally connected pyramidal (Pyr) cells and PV interneurons. Numbers correspond to points marked in (<bold>d</bold>). (<bold>b</bold>) Stimulus selectivity of Pyr cells and PV interneurons (before and after learning; Mann-Whitney <italic>U</italic> test, <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). Arrows indicate median. (<bold>c</bold>) Stimulus tuning of excitatory and inhibitory input currents in a Pyr cell before and after learning. For simplicity, currents are shown for spatial and temporal frequency only, averaged across all orientations. (<bold>d</bold>) Relationship of output (left) and input (centre) synaptic efficacies of PV interneurons with response similarity. Relationship of input and output efficacies (right). Plotted lines are obtained via linear regression. Reported <italic>r</italic> and associated p-value are the Pearson’s correlation. (<bold>e</bold>) Distribution of Pearson’s correlation coefficients for multiple samples as shown in (<bold>d</bold>). Dashed line marks threshold of high significance (<inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>f</bold>) Fraction of samples with highly significant positive correlation before plasticity, after plasticity in both input and output connections, and for KO of plasticity in PV output connections (based on 10,000 random samples of 100 synaptic connections).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig2-v1.tif"/></fig><p>To investigate whether the model without output plasticity is consistent with the synaptic structure of mouse V1, we repeatedly sample an experimentally feasible number of synapses (<inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig2">Figure 2d</xref>) and plot the distribution of the three pairwise Pearson’s correlation coefficients between the two classes of synaptic weights and RS (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). When both forms of plasticity are present in the network, a highly significant positive correlation (<inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>) is detected in all samples for all three correlation types (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). When output plasticity is knocked out, we still find a highly significant positive correlation between input weights and RS in 99% of the samples (<xref ref-type="fig" rid="fig2">Figure 2d–f</xref>). In contrast, correlations between input and output synapses are weaker and cannot reliably be detected (2% of samples). Notably, we find a correlation between output weights and RS in &lt;0.01% of samples (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). Finally, for an experimentally realistic sample size of <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, the probability of a correlation coefficient equal or higher than that observed by <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref> is &lt;0.01% for the correlation between output weights and RS (<inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.55</mml:mn></mml:mrow></mml:math></inline-formula>), and &lt;0.01% for the correlation between input and output synapses (<inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.52</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>The non-local gradient rule for the PV input synapses alone also does not permit the formation of E/I assemblies (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). While the selectivity of interneurons increases more than for the local approximation (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b</xref>), feedback inhibition still remains untuned in the absence of output plasticity (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2c,d</xref>).</p><p>We therefore conclude that input plasticity alone is insufficient to generate the synaptic microstructure observed in mouse V1.</p></sec><sec id="s2-3"><title>Pyr → PV plasticity is required for assembly formation</title><p>When we knock out input plasticity but keep output plasticity intact, we again observe no formation of E/I assemblies. This remains true even when using the gradient-based rule (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The underlying reason is that input weights remain random. Interneurons collect excitation from many Pyr neurons with different preferences, and absent plasticity on their input synapses, they maintain their initial poor stimulus selectivity (<xref ref-type="fig" rid="fig3">Figure 3a–c</xref>). Because of the poor stimulus tuning of the interneurons, output plasticity cannot generate stimulus-specific inhibitory inputs to the Pyr neurons (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Instead, they essentially receive a tonic, unspecific background inhibition that is weakly modulated by the stimulus (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3b</xref>). While this weak modulation is correlated with the excitatory inputs, the overall similarity between excitatory and inhibitory input remains low (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3c</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Plasticity of parvalbumin-expressing (PV) interneuron input connections is required for inhibitory stimulus selectivity and current co-tuning.</title><p>(<bold>a</bold>) Example responses of reciprocally connected pyramidal (Pyr) cells and PV interneurons. (<bold>b</bold>) Stimulus selectivity of Pyr cells and PV interneurons (before and after learning). Arrows indicate median. (<bold>c</bold>) Violin plots of inhibitory stimulus selectivity before plasticity, after learning with plasticity in both input and output connections of PV interneurons and for knock-out (KO) of plasticity in PV input connections. (<bold>d</bold>) Stimulus tuning of excitatory and inhibitory currents in a Pyr cell before and after learning. Dimensions correspond to spatial and temporal frequency of the stimuli averaged across all orientations. (<bold>e</bold>) Fraction of samples with highly significant (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>) positive correlation before plasticity, after plasticity in both input and output connections, and for KO of plasticity in PV input connections (based on 10,000 random samples of 100 synaptic connections).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Correlation between weights and response similarity.</title><p>Scatter plots containing every synapse in networks without PV → Pyr plasticity (top), or without Pyr → PV plasticity (bottom). Pearson’s correlation is always highly significant, though sometimes weak.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Long-tailed Pyr → PV weight distribution does not reproduce experimentally observed correlations.</title><p>To ensure that the results of <xref ref-type="fig" rid="fig3">Figure 3</xref> were not simply a consequence of our chosen Pyr → PV weight distribution (<inline-formula><mml:math id="inf23"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.65</mml:mn></mml:mrow></mml:math></inline-formula>), we repeat those simulations with weights drawn from a log-normal distribution with a longer tail (<inline-formula><mml:math id="inf24"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1.6</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>a</bold>) The default and longer-tailed distributions of Pyr → PV weights. (<bold>b</bold>) PV neurons exhibit greater stimulus selectivity than in <xref ref-type="fig" rid="fig3">Figure 3</xref>, when Pyr→ PV plasticity is knocked out. (<bold>c</bold>) Excitatory currents are more precisely balanced by inhibition (compare with <xref ref-type="fig" rid="fig3">Figure 3</xref>). (<bold>d</bold>) When repeatedly sampling 100 synapses, the strength of inhibitory synapses that connect Pyr and PV cell pairs are always significantly correlated (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) with response similarity (RS). Excitatory synapses between cell pairs are only correlated with RS in about half of the samples. For reciprocally connected cell pairs, the strength of excitatory weights is rarely correlated with the inhibitory weights. (<bold>e</bold>) Similar to <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> (bottom), when all synapses for the Pyr → PV KO network are considered, Exc-RS and Exc-Inh correlations are weak but highly significant. Dashed lines indicate the threshold below which synapses are considered experimentally undetectable and discarded for the analysis in (d).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig3-figsupp2-v1.tif"/></fig></fig-group><p>This modulation is made possible by the fact that interneurons still possess a weak, but consistent stimulus tuning arising from random variations in their input weights. A particularly strong input connection will cause the postsynaptic interneuron to prefer similar stimuli to the presynaptic Pyr. Because of the resulting correlated activity, the Hebbian nature of the output plasticity potentiates inhibitory weights for such cell pairs that are reciprocally connected. The tendency of strong input synapses to generate a strong corresponding output synapse is reflected in a positive correlation between output synapses and RS (<xref ref-type="fig" rid="fig3">Figure 3e</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), despite the fact that input synapses remain random.</p><p>This effect further increases when input synapses are drawn from a distribution with an even heavier tail, beyond what is observed in mouse V1 (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2a</xref>). In this case, the stimulus tuning of the interneurons is dominated by a small number of very large synapses. The resulting higher selectivity of the interneurons (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2b</xref>) allows a better co-tuning of excitation and inhibition in Pyr neurons (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2c</xref>), in line with theoretical arguments for sparse connectivity (<xref ref-type="bibr" rid="bib46">Litwin-Kumar et al., 2017</xref>). However, the dominance of a small number of large synapses also makes it unlikely that those synapses are observed in an experiment in which a finite number of synapses are sampled. As a result, a heavier tail does not yield the correlation of reciprocal input and output synapses observed by <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref> (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2d,e</xref>), although it increases the probability of observing correlations between input synapses and RS when weak synapses are discarded. See Appendix 1 for a more extensive discussion.</p><p>Collectively, these results indicate that plasticity of both the inhibitory output and the excitatory input synapses of PV interneurons is required for the formation of E/I assemblies in cortical areas without feature topography, such as mouse V1.</p></sec><sec id="s2-4"><title>Single-neuron perturbations</title><p>Our findings demonstrate that in networks without feature topography, only a synergy of excitatory and inhibitory plasticity can account for the emergence of E/I assemblies. But how does stimulus-specific feedback inhibition affect interactions between excitatory neurons? In layer 2/3 of V1, similarly tuned excitatory neurons tend to have stronger and more frequent excitatory connections (<xref ref-type="bibr" rid="bib35">Ko et al., 2011</xref>). It has been hypothesised that this tuned excitatory connectivity supports reliable stimulus responses by amplifying the activity of similarly tuned neurons (<xref ref-type="bibr" rid="bib13">Cossell et al., 2015</xref>). However, the presence of co-tuned feedback inhibition could also induce the opposite effect, such that similarly tuned excitatory neurons are in competition with each other (<xref ref-type="bibr" rid="bib11">Chettih and Harvey, 2019</xref>; <xref ref-type="bibr" rid="bib51">Moreno-Bote and Drugowitsch, 2015</xref>).</p><p>To investigate the effect of stimulus-specific inhibition in our network, we simulate the perturbation experiment of <xref ref-type="bibr" rid="bib11">Chettih and Harvey, 2019</xref>: First, we again expose the network to the stimulus set, with PV input and output plasticity in place to learn E/I assemblies. Second, both before and after learning, we probe the network with randomly selected stimuli from the same stimulus set, while perturbing a single Pyr cell with additional excitatory input, and measure the resulting change in activity of other Pyr neurons in the network (<xref ref-type="fig" rid="fig4">Figure 4a</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Single-neuron perturbations suppress responses of similarly tuned neurons.</title><p>(<bold>a</bold>) Perturbation of a single pyramidal (Pyr) neuron. Responses of other Pyr neurons are recorded for different stimuli, both with and without perturbation. (<bold>b</bold>) Perturbation-induced change in activity (<inline-formula><mml:math id="inf26"><mml:mi mathvariant="normal">Δ</mml:mi></mml:math></inline-formula> Act.) of a subset of Pyr cells, for a random subset of stimuli (with neuron 1 being perturbed). (<bold>c</bold>) Influence of perturbing a Pyr neuron on the other Pyr neurons, averaged across all stimuli, for a subset of Pyr neurons. (<bold>d</bold>) Dependence of influence among Pyr neurons on their receptive field correlation (Pearson’s <italic>r</italic>), across all neurons in the network (see 'Materials and methods'). Dotted lines indicate plasticity knock-out (KO) experiments; see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1b</xref> for details. Error bars correspond to the standard error of the sample mean, but are not visible due to their small values. (<bold>e</bold>) Total strength of output synapses from a Pyr neuron predicts the average effect perturbing it has on other neurons. Dashed line is the result of a linear regression, while <italic>r</italic> and its associated p-value correspond to the Pearson’s correlation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Input and output plasticity together change correlations between pyramidal (Pyr) neurons, while plasticity knock-out (KO) eliminates feature competition.</title><p>(<bold>a</bold>) Receptive field correlations (Pearson’s) between Pyr neurons, before (top) and after (bottom) learning with both PV → Pyr and Pyr → PV synaptic plasticity. (<bold>b</bold>) The effect of perturbing a Pyr neuron on the response of other Pyr neurons (to random stimuli) as a function of their receptive field correlation (see 'Materials and methods'). On their own, both Pyr → PV and PV → Pyr plasticity have little effect on the feature amplification observed prior to learning. (<bold>c</bold>) Despite the absence of feature competition on average in the KO networks, the total strength of Pyr → PV synapses from a given Pyr neuron is still predictive of its influence on the rest of the network: The stronger its total weight, the more likely a Pyr is to suppressing the response of other Pyr neurons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59715-fig4-figsupp1-v1.tif"/></fig></fig-group><p>While the activity of the perturbed neuron increases, many of the other Pyr neurons are inhibited in response to the perturbation (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). Although comparing the pairwise influence of Pyr neurons on each other does not reveal any apparent trend (<xref ref-type="fig" rid="fig4">Figure 4c</xref>), recent experiments report that the influence a single-cell perturbation has on other neurons depends on the similarity of their stimulus feature tuning (<xref ref-type="bibr" rid="bib11">Chettih and Harvey, 2019</xref>). To test whether we observe the same feature-specific suppression, we compute the influence of perturbing a Pyr on the rest of the network as a function of the receptive field correlation of the perturbed cell and each measured cell. In line with recent perturbation studies (<xref ref-type="bibr" rid="bib11">Chettih and Harvey, 2019</xref>; <xref ref-type="bibr" rid="bib66">Sadeh and Clopath, 2020</xref>), we observe that—on average—neurons are more strongly inhibited if they have a similar tuning to the perturbed neuron (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). The opposite holds before learning: the effect of single-neuron perturbations on the network is increasingly excitatory as receptive field correlation increases. Notably, the networks in which input or output plasticity was knocked out during learning (and therefore did not develop E/I assemblies) show the same excitatory effect (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1b</xref>). This confirms that a ‘blanket of inhibition’ does not account for feature-specific suppression between excitatory neurons (<xref ref-type="bibr" rid="bib66">Sadeh and Clopath, 2020</xref>).</p><p>To better understand this behaviour, we use the Pyr-Pyr receptive field correlations to compute the coefficient of determination for all pairs (<inline-formula><mml:math id="inf27"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, which quantifies how well the receptive field of one Pyr neuron predicts that of another). Learning changes the correlative structure in the network (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1a</xref>) and thereby decreases the coefficient of determination on average, indicating a reduction in Pyr-Pyr correlations within the network (<inline-formula><mml:math id="inf28"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></inline-formula> before learning, 0.02 after). Thus, plasticity suppresses some of the strongest correlations, resulting in ‘feature competition’ which is believed to aid sensory processing (<xref ref-type="bibr" rid="bib49">Lochmann et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Moreno-Bote and Drugowitsch, 2015</xref>).</p><p>While on average the network exhibits feature competition, the influence of individual Pyr neurons on the rest of the network is highly variable. According to recent modelling work (<xref ref-type="bibr" rid="bib66">Sadeh and Clopath, 2020</xref>), the strength of Pyr → PV synapses strongly influences whether a network will exhibit feature competition. In our network, the total outgoing weight of a Pyr cell onto the PV neurons indeed predicts the average influence that neuron will have on the rest of the network when perturbed (<xref ref-type="fig" rid="fig4">Figure 4e</xref>; <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>In summary, the stimulus-specific feedback inhibition that emerges in the model also captures the paradoxical suppression of similarly tuned excitatory neurons observed in single-cell perturbation experiments.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The idea that feedback inhibition serves as a ‘blanket of inhibition’ (<xref ref-type="bibr" rid="bib58">Packer and Yuste, 2011</xref>; <xref ref-type="bibr" rid="bib17">Fino and Yuste, 2011</xref>) that can be selectively broken (<xref ref-type="bibr" rid="bib31">Karnani et al., 2016</xref>) has been gradually relaxed over recent years and replaced by the notion that feedback inhibition can be rather selective (<xref ref-type="bibr" rid="bib64">Rupprecht and Friedrich, 2018</xref>) and could thereby support specific neuronal computations (<xref ref-type="bibr" rid="bib77">Vogels and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib23">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib14">Denève and Machens, 2016</xref>; <xref ref-type="bibr" rid="bib53">Najafi et al., 2020</xref>), even in networks without topographic organisation (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>; <xref ref-type="bibr" rid="bib64">Rupprecht and Friedrich, 2018</xref>). Here, we used a computational model to show that the development of E/I assemblies similar to those observed in mouse V1 (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>) or zebrafish olfactory areas (<xref ref-type="bibr" rid="bib64">Rupprecht and Friedrich, 2018</xref>) can be driven by a homeostatic form of plasticity of the incoming and outgoing synapses of inhibitory interneurons. Based on the results of virtual knock-out experiments, we suggest that, on their own, input or output plasticity of interneurons are insufficient to explain the Pyr-PV microcircuitry in mouse V1 and that input and output plasticity in interneurons must act in synergy for stimulus-specific feedback inhibition to develop. To investigate how the presence of E/I assemblies affects interactions between excitatory neurons, we mimicked a perturbation experiment and found that—as in mouse visual cortex—stimulating single excitatory cells paradoxically suppresses similarly tuned neurons (<xref ref-type="bibr" rid="bib11">Chettih and Harvey, 2019</xref>). Our findings suggest that, by driving the development of tuned feedback inhibition, plasticity of interneurons can fundamentally shape cortical processing.</p><p>The learning rules for the input and output synapses of PV interneurons are based on a single homeostatic objective that aims to keep the net synaptic current onto Pyr neurons close to a given target for all stimuli. The two forms of plasticity fulfil different purposes, however. Plasticity of input synapses is required for interneurons to acquire a stimulus selectivity, whereas plasticity of output synapses can exploit interneuron selectivity to shape inhibitory currents onto excitatory cells. The output plasticity we derived for our recurrent network is very similar to a previously suggested form of inhibitory plasticity (<xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="bib69">Sprekeler, 2017</xref>). Homeostatic plasticity rules for inhibitory synapses are now used regularly in computational studies to stabilise model circuits (<xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="bib24">Hennequin et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Landau et al., 2016</xref>). In contrast, a theoretically grounded approach for the plasticity of excitatory input synapses onto inhibitory neurons is missing.</p><p>Homeostatic changes in excitatory synapses onto interneurons in response to lesions or sensory deprivation have been reported (<xref ref-type="bibr" rid="bib32">Keck et al., 2011</xref>; <xref ref-type="bibr" rid="bib70">Takesian et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Kuhlman et al., 2013</xref>), but the specific mechanisms and functions of this form of interneuron plasticity are not resolved. The plasticity rule we derived for the input synapses of interneurons effectively changes the selectivity of those neurons according to the demands of the Pyr cells, that is, such that the interneurons can best counteract deviations of Pyr activity from the target. By which mechanisms such a (nearly teleological) form of plasticity can be achieved is at its core a problem of credit assignment, whose biological implementation remains open (<xref ref-type="bibr" rid="bib43">Lillicrap et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Sacramento et al., 2018</xref>).</p><p>Here, we used a local approximation of the gradient, backpropagation rules, which produces qualitatively similar results, and which we interpret as a recurrent variant of feedback alignment, applied to the specific task of a stimulus-specific E/I balance (<xref ref-type="bibr" rid="bib43">Lillicrap et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Akrout et al., 2019</xref>). The excitatory input connections onto the interneurons serve as a proxy for the transpose of the output connections. The intuition why this replacement is reasonable is the following: The task of balancing excitation by feedback inhibition favours symmetric connections, because excitatory cells that strongly drive a particular PV interneuron should receive a strong feedback connection in return. Therefore, E/I balance favours a positive correlation between the incoming and outgoing synapses of PV neurons and thus the two weight matrices will be aligned in a final balanced state (<xref ref-type="bibr" rid="bib43">Lillicrap et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Akrout et al., 2019</xref>). This weight replacement effectively replaces the ‘true’ feedback errors by a deviation of the total excitatory input to the PV neurons from a target (<xref ref-type="bibr" rid="bib26">Hertäg and Sprekeler, 2020</xref>). The rule therefore has the structure of a homeostatic rule for the recurrent excitatory drive received by PV neurons.</p><p>A cellular implementation of such a plasticity rule would require the following ingredients: (i) a signal that reflects the cell-wide excitatory current and (ii) a mechanism that changes Pyr → PV synapses in response to variations in this signal. For the detection of excitatory inputs, postsynaptic sodium or calcium concentrations are natural candidates. Due to the lack of spines in PV dendrites, both are expected to diffuse more broadly in the dendritic arbor than in spiny neurons (<xref ref-type="bibr" rid="bib28">Hu et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Kullmann and Lamsa, 2007</xref>) and may thus provide a signal for overall dendritic excitatory currents. Depending on how excitatory inputs are distributed on PV interneuron dendrites (<xref ref-type="bibr" rid="bib40">Larkum and Nevian, 2008</xref>; <xref ref-type="bibr" rid="bib30">Jia et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Grienberger et al., 2015</xref>), the integration of the excitatory currents may not need to be cell-wide—which could limit the temporal resolution of the plasticity—but could be local, for example, to a dendrite, if local excitatory input is a sufficient proxy for the global input. Notably, in PV interneurons, NMDA receptors are enriched in excitatory feedback relative to feedforward connections (<xref ref-type="bibr" rid="bib41">Le Roux et al., 2013</xref>), suggesting those two sources of excitation are differentially treated on the postsynaptic side. As for many other excitatory synapses (<xref ref-type="bibr" rid="bib67">Sjöström et al., 2008</xref>), postsynaptic calcium is likely a key factor also for the plasticity of excitatory input synapses onto interneurons. Blocking NMDA receptors interferes with Hebbian long-term plasticity in some of these synapses (<xref ref-type="bibr" rid="bib38">Lamsa et al., 2007</xref>; <xref ref-type="bibr" rid="bib37">Kullmann and Lamsa, 2007</xref>), as does a block of excitatory input (<xref ref-type="bibr" rid="bib41">Le Roux et al., 2013</xref>). Furthermore, NMDAR-dependent plasticity in Pyr → PV synapses is expressed postsynaptically and seems to require presynaptic activation (<xref ref-type="bibr" rid="bib37">Kullmann and Lamsa, 2007</xref>). In summary, we believe that there are no conceptual issues that would rule out an implementation of the suggested plasticity rule for excitatory inputs onto PV interneurons.</p><p>We also expect that the rules we suggest here are only one set of many that can establish E/I assemblies. Given that the role of the input plasticity in the interneurons is the formation of a stimulus specificity, it is tempting to assume that this could equally well be achieved by classical forms of plasticity like the Bienenstock-Cooper-Munro (BCM) rule (<xref ref-type="bibr" rid="bib7">Bienenstock et al., 1982</xref>), which is commonly used in models of receptive field formation. However, in our hands, the combination of BCM plasticity in Pyr → PV synapses with homeostatic inhibitory plasticity in the PV → Pyr synapses showed complex dynamics, an analysis of which is beyond the scope of this article. In particular, this combination of rules often did not converge to a steady state, probably for the following reason. BCM rules tend to make the postsynaptic neuron as stimulus-selective as possible. Given the limited number of interneurons in our circuit, this can lead to a situation in which parts of stimulus space are not represented by any interneurons. As a result, Pyr neurons that respond to those stimuli cannot recruit inhibition and maintain a high firing rate far above the target. Other Pyr cells, which have access to interneurons with a similar stimulus tuning, can recruit inhibition to gradually reduce their firing rates towards the target rate. Because the BCM rule is Hebbian, it tends to strengthen input synapses from Pyr neurons with high activity. This shifts the stimulus tuning of the interneurons to those stimuli that were previously underrepresented. However, this in turn renders a different set of stimuli uncovered by inhibition and withdraws feedback inhibition from the corresponding set of Pyr cells, which can now fire at high rates.</p><p>We suspect that this instability can also arise for other Hebbian forms of plasticity in interneuron input synapses when they are combined with homeostatic inhibitory plasticity (<xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref>) in their output synapses. The underlying reason is that for convergence, the two forms of plasticity need to work synergistically towards the same goal, that is, the same steady state. For two arbitrary synaptic plasticity rules acting in different sets of synapses, it is likely that they aim for two different overall network configurations. Such competition can easily result in latching dynamics with a continuing turn-over of transiently stable states, in which the form of plasticity that acts more quickly gets to reach its goal transiently, only to be undermined by the other one later.</p><p>Both Pyr → PV and PV→ Pyr plasticity have been studied in slice (for reviews, see, e.g., <xref ref-type="bibr" rid="bib37">Kullmann and Lamsa, 2007</xref>; <xref ref-type="bibr" rid="bib76">Vogels et al., 2013</xref>), but mostly in isolation. The idea that the two forms of plasticity should act in synergy suggests that it may be interesting to study both forms in the same system, for example, in reciprocally connected Pyr-PV pairs.</p><p>Like all computational models, the present one contains simplifying design choices. First, we did not include stimulus-specific <italic>feedforward</italic> inhibition, because the focus lay on the formation of stimulus-specific <italic>feedback</italic> inhibition. The model could be enriched by feedforward inhibition in different ways. In particular, we expect that the two forms of plasticity will establish E/I assemblies even in the presence of stimulus-selective external inputs to the interneurons, because stimulus-specific external excitation should always be more supportive of the homeostatic objective than unspecific inputs. It may be worth exploring whether adding feedforward inhibition leaves more room for replacing the PV input plasticity that we used by classical Hebbian rules, because the activity of the external inputs remains unaltered by the plasticity in the network (such that the complex instability described above may be mitigated). Given that the focus of this work was on feedback inhibition, an extensive evaluation of the different variants of feedforward inhibition is beyond the scope of the present article.</p><p>Second, we neglected much of the complexity of cortical interneuron circuits by including only one class of interneurons. We interpret these interneurons as PV interneurons, given that PV interneurons provide local feedback inhibition (<xref ref-type="bibr" rid="bib28">Hu et al., 2014</xref>) and show a stimulus-selective circuitry akin to E/I assemblies (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>). With their peri-somatic targets on Pyr cells, PV-expressing (basket) cells are also a prime candidate for the classical feedback model of E/I balance (<xref ref-type="bibr" rid="bib74">van Vreeswijk and Sompolinsky, 1996</xref>). Note that our results do not hinge on any assumptions that are specific to PV neurons and may thus also hold for other interneuron classes that provide feedback inhibition (<xref ref-type="bibr" rid="bib73">Tremblay et al., 2016</xref>). Given that the division of labour of the various cortical interneuron classes is far from understood, an extension to complex interneuron circuits (<xref ref-type="bibr" rid="bib45">Litwin-Kumar et al., 2016</xref>; <xref ref-type="bibr" rid="bib25">Hertäg and Sprekeler, 2019</xref>) is clearly beyond the present study.</p><p>Similarly tuned Pyr cells tend to be recurrently connected (<xref ref-type="bibr" rid="bib13">Cossell et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Harris and Mrsic-Flogel, 2013</xref>), in line with the notion that excitatory cells with similar tuning mutually excite each other. This notion is questioned by a recent perturbation experiment demonstrating feature-specific suppression between Pyr cells with similar tuning (<xref ref-type="bibr" rid="bib11">Chettih and Harvey, 2019</xref>). It has been suggested that this apparently paradoxical effect requires strong and tuned connections between excitatory and inhibitory neurons (<xref ref-type="bibr" rid="bib66">Sadeh and Clopath, 2020</xref>). The E/I assemblies that develop in our model provide sufficiently strong and specific inhibitory feedback to cause a suppression between similarly tuned Pyr neurons in response to perturbations. Hence, despite the presence of stimulus-specific excitatory recurrence, Pyr neurons with similar stimulus preference effectively compete. Computational arguments suggest that this feature competition may be beneficial for stimulus processing, for example, by generating a sparser and more efficient representation of the stimuli (<xref ref-type="bibr" rid="bib57">Olshausen and Field, 2004</xref>; <xref ref-type="bibr" rid="bib14">Denève and Machens, 2016</xref>).</p><p>In addition to predicting that knocking out plasticity of inhibitory input or output synapses should prevent the development of E/I assemblies, our model also predicts different outcomes for single-neuron perturbation experiments in juvenile and adult mice. Given that in rodents, stimulus tuning of inhibitory currents occurs later in development than that of excitation (<xref ref-type="bibr" rid="bib16">Dorrn et al., 2010</xref>), we expect that in juvenile mice single-cell perturbations would not cause feature-specific suppression but amplification due to excitatory recurrence and unspecific feedback inhibition.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Network and stimuli</title><p>We use custom software to simulate a rate-based recurrent network model containing <inline-formula><mml:math id="inf30"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>E</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> excitatory and <inline-formula><mml:math id="inf31"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>I</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math></inline-formula> inhibitory neurons. The activation of the neurons follows Wilson-Cowan dynamics:<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(1a)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>bg</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(1b)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>bg</mml:mtext></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the firing rates of the excitatory and inhibitory neurons, which are given by their rectified activation. <inline-formula><mml:math id="inf34"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>←</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> denotes the matrix of synaptic efficacies from population <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to population <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mtext>E</mml:mtext><mml:mo>,</mml:mo><mml:mtext>I</mml:mtext><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). The external inputs <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the excitatory neurons have a bell-shaped tuning in the three-dimensional stimulus space consisting of spatial frequency, temporal frequency, and orientation (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>). To avoid edge effects, the stimulus space is periodic in all three dimensions, with stimuli ranging from -<inline-formula><mml:math id="inf39"><mml:mi>π</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf40"><mml:mi>π</mml:mi></mml:math></inline-formula>. The stimulus tuning of the external inputs is modelled by a von Mises function with a maximum of 50 Hz and a tuning width <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The preferred stimuli of the <inline-formula><mml:math id="inf42"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>E</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> excitatory cells cover the stimulus space evenly on a <inline-formula><mml:math id="inf43"><mml:mrow><mml:mn>12</mml:mn><mml:mo>×</mml:mo><mml:mn>12</mml:mn><mml:mo>×</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula> grid. All neurons receive a constant background input of <inline-formula><mml:math id="inf44"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mtext>bg</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> Hz.</p><p>Recurrent connections <inline-formula><mml:math id="inf45"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula> among excitatory neurons have synaptic weight between neurons <italic>i</italic> and <italic>j</italic> that grows linearly with the signal correlation of their external inputs:<disp-formula id="equ4"><label>(2)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mpadded width="+1.7pt"><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mpadded></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The cropping threshold <italic>C</italic> is chosen such that the overall connection among the excitatory neurons probability is 0.6. The remaining synaptic connections (E→I, I→E, I→I) are initially random, with a connection probability <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula> and log-normal weights. For parameters, please refer to <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Model parameters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th><inline-formula><mml:math id="inf47"><mml:msup><mml:mi>N</mml:mi><mml:mtext>E</mml:mtext></mml:msup></mml:math></inline-formula></th><th>512</th><th><inline-formula><mml:math id="inf48"><mml:msup><mml:mi>N</mml:mi><mml:mtext>I</mml:mtext></mml:msup></mml:math></inline-formula></th><th>64</th><th>Number of exc. and inh. neurons.</th></tr></thead><tbody><tr><td><inline-formula><mml:math id="inf49"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>E</mml:mtext></mml:msub></mml:math></inline-formula></td><td>50 ms</td><td><inline-formula><mml:math id="inf50"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>I</mml:mtext></mml:msub></mml:math></inline-formula></td><td>25 ms</td><td>Rate dynamics time constants</td></tr><tr><td><inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula></td><td>1 ms</td><td/><td/><td>Numerical integration time step</td></tr><tr><td><inline-formula><mml:math id="inf52"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td><td>0.6</td><td><inline-formula><mml:math id="inf53"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td><td>0.6</td><td>Connection probability to exc. and inh. neurons</td></tr><tr><td><inline-formula><mml:math id="inf54"><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td>2</td><td><inline-formula><mml:math id="inf55"><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td>5</td><td>Total of exc. weights onto neuron <italic>i</italic>: <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></td></tr><tr><td><inline-formula><mml:math id="inf57"><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td>1</td><td><inline-formula><mml:math id="inf58"><mml:msubsup><mml:mi>J</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td>1</td><td>Total of inh. weights onto neuron <italic>i</italic>: <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></td></tr><tr><td><inline-formula><mml:math id="inf60"><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td><td>0.65</td><td><inline-formula><mml:math id="inf61"><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td><td>0.65</td><td>Std. deviation of the logarithm of the weights</td></tr><tr><td><inline-formula><mml:math id="inf62"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf63"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf64"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf65"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td>Experimental detection threshold for synapses</td></tr><tr><td><inline-formula><mml:math id="inf66"><mml:msup><mml:mi>I</mml:mi><mml:mtext>bg</mml:mtext></mml:msup></mml:math></inline-formula></td><td>5 Hz</td><td><inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>50 Hz</td><td>Background and maximum stimulus-specific input</td></tr><tr><td><inline-formula><mml:math id="inf68"><mml:msup><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf69"><mml:mrow><mml:mn>12</mml:mn><mml:mo>×</mml:mo><mml:mn>12</mml:mn><mml:mo>×</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf70"><mml:msup><mml:mi>N</mml:mi><mml:mtext>trials</mml:mtext></mml:msup></mml:math></inline-formula></td><td>500</td><td>Number of stimuli and trials</td></tr><tr><td><inline-formula><mml:math id="inf71"><mml:msup><mml:mi>R</mml:mi><mml:mi>S</mml:mi></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf72"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf73"><mml:mi>κ</mml:mi></mml:math></inline-formula></td><td>1</td><td>Range of stimuli and Pyr RF von Mises width</td></tr><tr><td><inline-formula><mml:math id="inf74"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula></td><td>10 Hz</td><td/><td/><td>Change of input for perturbation experiments</td></tr><tr><td><inline-formula><mml:math id="inf75"><mml:msup><mml:mi>η</mml:mi><mml:mtext>Approx.</mml:mtext></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf76"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf77"><mml:msup><mml:mi>η</mml:mi><mml:mtext>Grad.</mml:mtext></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf78"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td>Learning rates (approximate and gradient rules)</td></tr><tr><td><inline-formula><mml:math id="inf79"><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula></td><td>0.1</td><td><inline-formula><mml:math id="inf80"><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula></td><td>0.1</td><td>Weight decay rates</td></tr><tr><td><inline-formula><mml:math id="inf81"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></td><td>1 Hz</td><td/><td/><td>Homeostatic plasticity target</td></tr><tr><td><inline-formula><mml:math id="inf82"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></td><td>0.9</td><td><inline-formula><mml:math id="inf83"><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></td><td>0.999</td><td>Adam parameters for gradient rules</td></tr><tr><td><inline-formula><mml:math id="inf84"><mml:mi>ϵ</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf85"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td/><td/><td/></tr></tbody></table></table-wrap><p>During learning, we repeatedly draw all 12 × 12 × 12 preferred stimuli of the Pyr neurons, in random order. This procedure is repeated 500 times to ensure convergence of synaptic weights. To reduce simulation time, we present each stimulus long enough for all firing rates to reach steady state and only then update the synaptic weights.</p></sec><sec id="s4-2"><title>Synaptic plasticity</title><p>The PV → Pyr and Pyr → PV synapses follow plasticity rules that aim to minimise the deviation of the excitatory activations from a target rate <inline-formula><mml:math id="inf86"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> Hz):<disp-formula id="equ5"><label>(3)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf88"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold">𝐬</mml:mi></mml:msub></mml:math></inline-formula> denotes the average over all stimuli. When plastic, synaptic weights change according to<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(4a)</mml:mtext></mml:mtd><mml:mtd><mml:mrow/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>∝</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(4b)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>∝</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mlabeledtr><mml:mtd><mml:mtext>(4c)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E,rec</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>After every update of the Pyr → PV matrix, the incoming weights for each PV interneuron are multiplicatively scaled such that their sum is <inline-formula><mml:math id="inf89"><mml:msup><mml:mi>J</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib2">Akrout et al., 2019</xref>). In that case, the rule in <xref ref-type="disp-formula" rid="equ6">Equation 4</xref>b is approximately local in that it compares the excitatory input current <inline-formula><mml:math id="inf90"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>rec</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> received by the postsynaptic PV neuron to a target value <inline-formula><mml:math id="inf91"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>J</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and adjusts the incoming synapses in proportion to this error and to presynaptic activity (see <xref ref-type="disp-formula" rid="equ6">Equation 4</xref>c).</p><p>Both plasticity rules are approximations of the gradient of the objective function <xref ref-type="disp-formula" rid="equ5">Equation 3</xref>. Interested readers are referred to Appendix 1 for their mathematical derivation. For the results in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, we use the Adaptive Moment Estimation (Adam) algorithm (<xref ref-type="bibr" rid="bib34">Kingma and Ba, 2014</xref>) to improve optimisation performance.</p><p>We used a standard reparameterisation method to ensure the sign constraints of an E/I network. Moreover, all weights are subject to a small weight-dependent decay term, which aids to keep the firing rates of the interneurons in a reasonable range. For details, please refer to Appendix 1 . The learning rule <xref ref-type="disp-formula" rid="equ6">Equation 4</xref>a for the output synapses of the inhibitory neurons is similar to the rule proposed by <xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref>, wherein each inhibitory synapse increases in strength if the deviation of the postsynaptic excitatory cell from the homeostatic target <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is positive (and decreases it when negative). In contrast, the learning rule <xref ref-type="disp-formula" rid="equ6">Equation 4</xref>b increases activated input synapses for an interneuron if the weighted sum of deviations in its presynaptic excitatory population is positive (and decreases them if it is negative). Though it is local, when operating in conjunction with the plasticity of <xref ref-type="disp-formula" rid="equ6">Equation 4</xref>a, this leads to feedback alignment in our simulations and effectively performs backpropagation without the need for weight transport (<xref ref-type="bibr" rid="bib2">Akrout et al., 2019</xref>).</p><p>Note that the objective function <xref ref-type="disp-formula" rid="equ5">Equation 3</xref> can also be interpreted differently. The activation <inline-formula><mml:math id="inf93"><mml:msup><mml:mi>h</mml:mi><mml:mtext>E</mml:mtext></mml:msup></mml:math></inline-formula> of a neuron is essentially the difference between its excitatory and inhibitory inputs. Therefore, the objective function <xref ref-type="disp-formula" rid="equ5">Equation 3</xref> is effectively the mean squared error between excitation and inhibition, aside from a small constant offset <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. The derived learning rules can therefore be seen as supervised learning of the inhibitory inputs, with excitation as the label. They hence aim to establish the best co-tuning of excitation and inhibition that is possible given the circuitry.</p></sec><sec id="s4-3"><title>Perturbation experiments</title><p>The perturbation experiments in <xref ref-type="fig" rid="fig4">Figure 4</xref> are performed in a network in which both forms of plasticity have converged. The network is then exposed to different stimuli, while the afferent drive to a single excitatory cell <italic>i</italic> is transiently increased by <inline-formula><mml:math id="inf95"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> Hz. For each stimulus, we compute the steady-state firing rates <italic>r</italic><sub><italic>j</italic></sub> of all excitatory cells both with and without the perturbation. The influence of the perturbation of neuron <italic>i</italic> on neuron <italic>j</italic> is defined as the difference between these two firing rates, normalised by the pertubation magnitude (<xref ref-type="bibr" rid="bib66">Sadeh and Clopath, 2020</xref>). This stimulation protocol is repeated for 90 randomly selected excitatory neurons. The dependence of the influence on the tuning similarity (<xref ref-type="fig" rid="fig4">Figure 4d</xref>) is obtained by binning the influence of the perturbed neuron <italic>i</italic> and the influenced neuron <italic>j</italic> according to their stimulus response correlation, and then averaging across all influences in the bin. During the perturbation experiments, synaptic plasticity was disabled.</p></sec><sec id="s4-4"><title>Quantitative measures</title><p>The response similarity (RS) of the stimulus tuning of two neurons <italic>i</italic> and <italic>j</italic> is measured by the dot product of their steady-state firing rates in response to all stimuli, normalised by the product of their norms (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>):<disp-formula id="equ7"><label>(5)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>RS</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>𝐬</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>𝐬</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>𝐬</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The same measure is used for the similarity of synaptic currents onto excitatory neurons in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3c</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2d</xref>.</p><p>There is no structural plasticity, that is, synapses are never added or pruned. However, when calculating Pearson’s correlation between synaptic weights and RS, we exclude synapses that are too weak to be detected using the experimental protocol employed by <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>. The threshold values <inline-formula><mml:math id="inf96"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:math></inline-formula> were chosen to be approximately four orders of magnitude weaker than the strongest synapses in the network. The rules that we investigate here tend to produce bimodal distributions of weights, with the lower mode well below this threshold (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>).</p><p>The stimulus selectivity of the neurons is measured by the skewness of their response distribution across all stimuli:<disp-formula id="equ8"><label>(6)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mi>𝐬</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mi>𝐬</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf98"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mi mathvariant="bold">𝐬</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Both the RS <xref ref-type="disp-formula" rid="equ7">Equation 5</xref> and the stimulus selectivity <xref ref-type="disp-formula" rid="equ8">Equation 6</xref> are adapted from <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>.</p><p>Finally, the angle <inline-formula><mml:math id="inf99"><mml:mi>θ</mml:mi></mml:math></inline-formula> between the gradient <italic>G</italic> from <xref ref-type="disp-formula" rid="equ21">Equation 15</xref> and its approximation <italic>A</italic> from <xref ref-type="disp-formula" rid="equ6">Equation 4</xref> is given by<disp-formula id="equ9"><label>(7)</label><mml:math id="m9"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>arccos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Joram Keijser for helpful discussions that inspired parts of this work. He, along with Denis Alevi, Loreen Hertäg and Robert T Lange also provided careful proof-reading of the manuscript. This project was funded by the German Federal Ministry for Science and Education through a Bernstein Award (BMBF, FKZ 01GQ1201) and by the German Research Foundation (DFG, collaborative research centre FOR 2143).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-59715-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Source code for the simulator that generated all data is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/owenmackwood/ei-assemblies">https://github.com/owenmackwood/ei-assemblies</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:e2f029a7e7285230cbbdbc7e817e25c8c5535fc1">https://archive.softwareheritage.org/swh:1:rev:e2f029a7e7285230cbbdbc7e817e25c8c5535fc1</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adesnik</surname> <given-names>H</given-names></name><name><surname>Bruns</surname> <given-names>W</given-names></name><name><surname>Taniguchi</surname> <given-names>H</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Scanziani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A neural circuit for spatial summation in visual cortex</article-title><source>Nature</source><volume>490</volume><fpage>226</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nature11526</pub-id><pub-id pub-id-type="pmid">23060193</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Akrout</surname> <given-names>M</given-names></name><name><surname>Wilson</surname> <given-names>C</given-names></name><name><surname>Humphreys</surname> <given-names>P</given-names></name><name><surname>Lillicrap</surname> <given-names>T</given-names></name><name><surname>Tweed</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning without weight transport</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>976</fpage><lpage>984</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname> <given-names>HC</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Emir</surname> <given-names>UE</given-names></name><name><surname>Makin</surname> <given-names>TR</given-names></name><name><surname>O'Shea</surname> <given-names>J</given-names></name><name><surname>Clare</surname> <given-names>S</given-names></name><name><surname>Jbabdi</surname> <given-names>S</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Unmasking latent inhibitory connections in human cortex to reveal dormant cortical memories</article-title><source>Neuron</source><volume>90</volume><fpage>191</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.031</pub-id><pub-id pub-id-type="pmid">26996082</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname> <given-names>HC</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Ramaswami</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inhibitory engrams in perception and memory</article-title><source>PNAS</source><volume>114</volume><fpage>6666</fpage><lpage>6674</lpage><pub-id pub-id-type="doi">10.1073/pnas.1701812114</pub-id><pub-id pub-id-type="pmid">28611219</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellec</surname> <given-names>G</given-names></name><name><surname>Scherr</surname> <given-names>F</given-names></name><name><surname>Subramoney</surname> <given-names>A</given-names></name><name><surname>Hajek</surname> <given-names>E</given-names></name><name><surname>Salaj</surname> <given-names>D</given-names></name><name><surname>Legenstein</surname> <given-names>R</given-names></name><name><surname>Maass</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A solution to the learning dilemma for recurrent networks of spiking neurons</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3625</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17236-y</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhatia</surname> <given-names>A</given-names></name><name><surname>Moza</surname> <given-names>S</given-names></name><name><surname>Bhalla</surname> <given-names>US</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Precise excitation-inhibition balance controls gain and timing in the Hippocampus</article-title><source>eLife</source><volume>8</volume><elocation-id>e43415</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43415</pub-id><pub-id pub-id-type="pmid">31021319</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bienenstock</surname> <given-names>EL</given-names></name><name><surname>Cooper</surname> <given-names>LN</given-names></name><name><surname>Munro</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>2</volume><fpage>32</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.02-01-00032.1982</pub-id><pub-id pub-id-type="pmid">7054394</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname> <given-names>DD</given-names></name><name><surname>Lee</surname> <given-names>WC</given-names></name><name><surname>Kerlin</surname> <given-names>AM</given-names></name><name><surname>Andermann</surname> <given-names>ML</given-names></name><name><surname>Hood</surname> <given-names>G</given-names></name><name><surname>Wetzel</surname> <given-names>AW</given-names></name><name><surname>Yurgenson</surname> <given-names>S</given-names></name><name><surname>Soucy</surname> <given-names>ER</given-names></name><name><surname>Kim</surname> <given-names>HS</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Network anatomy and in vivo physiology of visual cortical neurons</article-title><source>Nature</source><volume>471</volume><fpage>177</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1038/nature09802</pub-id><pub-id pub-id-type="pmid">21390124</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title><source>Journal of Computational Neuroscience</source><volume>8</volume><fpage>183</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1023/a:1008925309027</pub-id><pub-id pub-id-type="pmid">10809012</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chenkov</surname> <given-names>N</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name><name><surname>Kempter</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Memory replay in balanced recurrent networks</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005359</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005359</pub-id><pub-id pub-id-type="pmid">28135266</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chettih</surname> <given-names>SN</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-neuron perturbations reveal feature-specific competition in V1</article-title><source>Nature</source><volume>567</volume><fpage>334</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0997-6</pub-id><pub-id pub-id-type="pmid">30842660</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Froemke</surname> <given-names>RC</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Receptive field formation by interacting excitatory and inhibitory synaptic plasticity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/066589</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cossell</surname> <given-names>L</given-names></name><name><surname>Iacaruso</surname> <given-names>MF</given-names></name><name><surname>Muir</surname> <given-names>DR</given-names></name><name><surname>Houlton</surname> <given-names>R</given-names></name><name><surname>Sader</surname> <given-names>EN</given-names></name><name><surname>Ko</surname> <given-names>H</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional organization of excitatory synaptic strength in primary visual cortex</article-title><source>Nature</source><volume>518</volume><fpage>399</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1038/nature14182</pub-id><pub-id pub-id-type="pmid">25652823</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denève</surname> <given-names>S</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Efficient codes and balanced networks</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>375</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1038/nn.4243</pub-id><pub-id pub-id-type="pmid">26906504</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dipoppa</surname> <given-names>M</given-names></name><name><surname>Ranson</surname> <given-names>A</given-names></name><name><surname>Krumin</surname> <given-names>M</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vision and locomotion shape the interactions between neuron types in mouse visual cortex</article-title><source>Neuron</source><volume>98</volume><fpage>602</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.037</pub-id><pub-id pub-id-type="pmid">29656873</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorrn</surname> <given-names>AL</given-names></name><name><surname>Yuan</surname> <given-names>K</given-names></name><name><surname>Barker</surname> <given-names>AJ</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name><name><surname>Froemke</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Developmental sensory experience balances cortical excitation and inhibition</article-title><source>Nature</source><volume>465</volume><fpage>932</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1038/nature09119</pub-id><pub-id pub-id-type="pmid">20559387</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fino</surname> <given-names>E</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dense inhibitory connectivity in neocortex</article-title><source>Neuron</source><volume>69</volume><fpage>1188</fpage><lpage>1203</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.025</pub-id><pub-id pub-id-type="pmid">21435562</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishell</surname> <given-names>G</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Interneuron types as attractors and controllers</article-title><source>Annual Review of Neuroscience</source><volume>43</volume><fpage>1</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070918-050421</pub-id><pub-id pub-id-type="pmid">31299170</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froemke</surname> <given-names>RC</given-names></name><name><surname>Merzenich</surname> <given-names>MM</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A synaptic memory trace for cortical receptive field plasticity</article-title><source>Nature</source><volume>450</volume><fpage>425</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1038/nature06289</pub-id><pub-id pub-id-type="pmid">18004384</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grienberger</surname> <given-names>C</given-names></name><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Konnerth</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dendritic function in vivo</article-title><source>Trends in Neurosciences</source><volume>38</volume><fpage>45</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2014.11.002</pub-id><pub-id pub-id-type="pmid">25432423</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerguiev</surname> <given-names>J</given-names></name><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Richards</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards deep learning with segregated dendrites</article-title><source>eLife</source><volume>6</volume><elocation-id>e22901</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22901</pub-id><pub-id pub-id-type="pmid">29205151</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical connectivity and sensory coding</article-title><source>Nature</source><volume>503</volume><fpage>51</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1038/nature12654</pub-id><pub-id pub-id-type="pmid">24201278</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname> <given-names>G</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title><source>Neuron</source><volume>82</volume><fpage>1394</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id><pub-id pub-id-type="pmid">24945778</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname> <given-names>G</given-names></name><name><surname>Agnes</surname> <given-names>EJ</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inhibitory plasticity: balance, control, and codependence</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>557</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031005</pub-id><pub-id pub-id-type="pmid">28598717</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertäg</surname> <given-names>L</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Amplifying the redistribution of somato-dendritic inhibition by the interplay of three interneuron types</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006999</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006999</pub-id><pub-id pub-id-type="pmid">31095556</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertäg</surname> <given-names>L</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning prediction error neurons in a canonical interneuron circuit</article-title><source>eLife</source><volume>9</volume><elocation-id>e57541</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57541</pub-id><pub-id pub-id-type="pmid">32820723</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hofer</surname> <given-names>SB</given-names></name><name><surname>Ko</surname> <given-names>H</given-names></name><name><surname>Pichler</surname> <given-names>B</given-names></name><name><surname>Vogelstein</surname> <given-names>J</given-names></name><name><surname>Ros</surname> <given-names>H</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name><name><surname>Lein</surname> <given-names>E</given-names></name><name><surname>Lesica</surname> <given-names>NA</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differential connectivity and response dynamics of excitatory and inhibitory neurons in visual cortex</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1045</fpage><lpage>1052</lpage><pub-id pub-id-type="doi">10.1038/nn.2876</pub-id><pub-id pub-id-type="pmid">21765421</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname> <given-names>H</given-names></name><name><surname>Gan</surname> <given-names>J</given-names></name><name><surname>Jonas</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interneurons. Fast-spiking, parvalbumin⁺ GABAergic interneurons: from cellular design to microcircuit function</article-title><source>Science</source><volume>345</volume><elocation-id>1255263</elocation-id><pub-id pub-id-type="doi">10.1126/science.1255263</pub-id><pub-id pub-id-type="pmid">25082707</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isaacson</surname> <given-names>JS</given-names></name><name><surname>Scanziani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How inhibition shapes cortical activity</article-title><source>Neuron</source><volume>72</volume><fpage>231</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.027</pub-id><pub-id pub-id-type="pmid">22017986</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname> <given-names>H</given-names></name><name><surname>Rochefort</surname> <given-names>NL</given-names></name><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Konnerth</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dendritic organization of sensory input to cortical neurons in vivo</article-title><source>Nature</source><volume>464</volume><fpage>1307</fpage><lpage>1312</lpage><pub-id pub-id-type="doi">10.1038/nature08947</pub-id><pub-id pub-id-type="pmid">20428163</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karnani</surname> <given-names>MM</given-names></name><name><surname>Jackson</surname> <given-names>J</given-names></name><name><surname>Ayzenshtat</surname> <given-names>I</given-names></name><name><surname>Hamzehei Sichani</surname> <given-names>A</given-names></name><name><surname>Manoocheri</surname> <given-names>K</given-names></name><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Opening holes in the blanket of inhibition: localized lateral disinhibition by VIP interneurons</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>3471</fpage><lpage>3480</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3646-15.2016</pub-id><pub-id pub-id-type="pmid">27013676</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keck</surname> <given-names>T</given-names></name><name><surname>Scheuss</surname> <given-names>V</given-names></name><name><surname>Jacobsen</surname> <given-names>RI</given-names></name><name><surname>Wierenga</surname> <given-names>CJ</given-names></name><name><surname>Eysel</surname> <given-names>UT</given-names></name><name><surname>Bonhoeffer</surname> <given-names>T</given-names></name><name><surname>Hübener</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Loss of sensory input causes rapid structural changes of inhibitory neurons in adult mouse visual cortex</article-title><source>Neuron</source><volume>71</volume><fpage>869</fpage><lpage>882</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.034</pub-id><pub-id pub-id-type="pmid">21903080</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname> <given-names>AG</given-names></name><name><surname>Poort</surname> <given-names>J</given-names></name><name><surname>Chadwick</surname> <given-names>A</given-names></name><name><surname>Blot</surname> <given-names>A</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct learning-induced changes in stimulus selectivity and interactions of GABAergic interneuron classes in visual cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>851</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0143-z</pub-id><pub-id pub-id-type="pmid">29786081</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname> <given-names>H</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name><name><surname>Pichler</surname> <given-names>B</given-names></name><name><surname>Buchanan</surname> <given-names>KA</given-names></name><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specificity of local synaptic connections in neocortical networks</article-title><source>Nature</source><volume>473</volume><fpage>87</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1038/nature09880</pub-id><pub-id pub-id-type="pmid">21478872</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhlman</surname> <given-names>SJ</given-names></name><name><surname>Olivas</surname> <given-names>ND</given-names></name><name><surname>Tring</surname> <given-names>E</given-names></name><name><surname>Ikrar</surname> <given-names>T</given-names></name><name><surname>Xu</surname> <given-names>X</given-names></name><name><surname>Trachtenberg</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A disinhibitory microcircuit initiates critical-period plasticity in the visual cortex</article-title><source>Nature</source><volume>501</volume><fpage>543</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1038/nature12485</pub-id><pub-id pub-id-type="pmid">23975100</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kullmann</surname> <given-names>DM</given-names></name><name><surname>Lamsa</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Long-term synaptic plasticity in hippocampal interneurons</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>687</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1038/nrn2207</pub-id><pub-id pub-id-type="pmid">17704811</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamsa</surname> <given-names>K</given-names></name><name><surname>Irvine</surname> <given-names>EE</given-names></name><name><surname>Giese</surname> <given-names>KP</given-names></name><name><surname>Kullmann</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>NMDA receptor-dependent long-term potentiation in mouse hippocampal interneurons shows a unique dependence on Ca(2+)/calmodulin-dependent kinases</article-title><source>The Journal of Physiology</source><volume>584</volume><fpage>885</fpage><lpage>894</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2007.137380</pub-id><pub-id pub-id-type="pmid">17884930</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname> <given-names>ID</given-names></name><name><surname>Egger</surname> <given-names>R</given-names></name><name><surname>Dercksen</surname> <given-names>VJ</given-names></name><name><surname>Oberlaender</surname> <given-names>M</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The impact of structural heterogeneity on excitation-inhibition balance in cortical networks</article-title><source>Neuron</source><volume>92</volume><fpage>1106</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.027</pub-id><pub-id pub-id-type="pmid">27866797</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname> <given-names>ME</given-names></name><name><surname>Nevian</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic clustering by dendritic signalling mechanisms</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>321</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.08.013</pub-id><pub-id pub-id-type="pmid">18804167</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Roux</surname> <given-names>N</given-names></name><name><surname>Cabezas</surname> <given-names>C</given-names></name><name><surname>Böhm</surname> <given-names>UL</given-names></name><name><surname>Poncer</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Input-specific learning rules at excitatory synapses onto hippocampal parvalbumin-expressing interneurons</article-title><source>The Journal of Physiology</source><volume>591</volume><fpage>1809</fpage><lpage>1822</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2012.245852</pub-id><pub-id pub-id-type="pmid">23339172</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letzkus</surname> <given-names>JJ</given-names></name><name><surname>Wolff</surname> <given-names>SB</given-names></name><name><surname>Meyer</surname> <given-names>EM</given-names></name><name><surname>Tovote</surname> <given-names>P</given-names></name><name><surname>Courtin</surname> <given-names>J</given-names></name><name><surname>Herry</surname> <given-names>C</given-names></name><name><surname>Lüthi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A disinhibitory microcircuit for associative fear learning in the auditory cortex</article-title><source>Nature</source><volume>480</volume><fpage>331</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1038/nature10674</pub-id><pub-id pub-id-type="pmid">22158104</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Cownden</surname> <given-names>D</given-names></name><name><surname>Tweed</surname> <given-names>DB</given-names></name><name><surname>Akerman</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Santoro</surname> <given-names>A</given-names></name><name><surname>Marris</surname> <given-names>L</given-names></name><name><surname>Akerman</surname> <given-names>CJ</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Backpropagation and the brain</article-title><source>Nature Reviews Neuroscience</source><volume>21</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0277-3</pub-id><pub-id pub-id-type="pmid">32303713</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Rosenbaum</surname> <given-names>R</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inhibitory stabilization and visual coding in cortical circuits with multiple interneuron subtypes</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>1399</fpage><lpage>1409</lpage><pub-id pub-id-type="doi">10.1152/jn.00732.2015</pub-id><pub-id pub-id-type="pmid">26740531</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Axel</surname> <given-names>R</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal degrees of synaptic connectivity</article-title><source>Neuron</source><volume>93</volume><fpage>1153</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.030</pub-id><pub-id pub-id-type="pmid">28215558</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Slow dynamics and high variability in balanced cortical networks with clustered connections</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1498</fpage><lpage>1505</lpage><pub-id pub-id-type="doi">10.1038/nn.3220</pub-id><pub-id pub-id-type="pmid">23001062</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Formation and maintenance of neuronal assemblies through synaptic plasticity</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>5319</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms6319</pub-id><pub-id pub-id-type="pmid">25395015</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lochmann</surname> <given-names>T</given-names></name><name><surname>Ernst</surname> <given-names>UA</given-names></name><name><surname>Denève</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Perceptual inference predicts contextual modulations of sensory responses</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>4179</fpage><lpage>4195</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0817-11.2012</pub-id><pub-id pub-id-type="pmid">22442081</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname> <given-names>Y</given-names></name><name><surname>Kuras</surname> <given-names>A</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>9481</fpage><lpage>9488</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6130-10.2011</pub-id><pub-id pub-id-type="pmid">21715613</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Drugowitsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Causal inference and explaining away in a spiking network</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>17531</elocation-id><pub-id pub-id-type="doi">10.1038/srep17531</pub-id><pub-id pub-id-type="pmid">26621426</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>BK</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Balanced amplification: a new mechanism of selective amplification of neural activity patterns</article-title><source>Neuron</source><volume>61</volume><fpage>635</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.005</pub-id><pub-id pub-id-type="pmid">19249282</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najafi</surname> <given-names>F</given-names></name><name><surname>Elsayed</surname> <given-names>GF</given-names></name><name><surname>Cao</surname> <given-names>R</given-names></name><name><surname>Pnevmatikakis</surname> <given-names>E</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Excitatory and inhibitory subnetworks are equally selective during Decision-Making and emerge simultaneously during learning</article-title><source>Neuron</source><volume>105</volume><fpage>165</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.045</pub-id><pub-id pub-id-type="pmid">31753580</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neftci</surname> <given-names>EO</given-names></name><name><surname>Mostafa</surname> <given-names>H</given-names></name><name><surname>Zenke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Surrogate gradient learning in spiking neural networks: bringing the power of Gradient-Based optimization to spiking neural networks</article-title><source>IEEE Signal Processing Magazine</source><volume>36</volume><fpage>51</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1109/MSP.2019.2931595</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohki</surname> <given-names>K</given-names></name><name><surname>Chung</surname> <given-names>S</given-names></name><name><surname>Ch'ng</surname> <given-names>YH</given-names></name><name><surname>Kara</surname> <given-names>P</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional imaging with cellular resolution reveals precise micro-architecture in visual cortex</article-title><source>Nature</source><volume>433</volume><fpage>597</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1038/nature03274</pub-id><pub-id pub-id-type="pmid">15660108</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Lampl</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>535</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1038/nn.2105</pub-id><pub-id pub-id-type="pmid">18376400</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sparse coding of sensory inputs</article-title><source>Current Opinion in Neurobiology</source><volume>14</volume><fpage>481</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2004.07.007</pub-id><pub-id pub-id-type="pmid">15321069</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packer</surname> <given-names>AM</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dense, unspecific connectivity of neocortical parvalbumin-positive interneurons: a canonical microcircuit for inhibition?</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13260</fpage><lpage>13271</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3131-11.2011</pub-id><pub-id pub-id-type="pmid">21917809</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname> <given-names>EAK</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name><name><surname>Hasenstaub</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cortical interneurons differentially regulate the effects of acoustic context</article-title><source>Cell Reports</source><volume>20</volume><fpage>771</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.07.001</pub-id><pub-id pub-id-type="pmid">28746863</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouille</surname> <given-names>F</given-names></name><name><surname>Scanziani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Enforcement of temporal fidelity in pyramidal cells by somatic feed-forward inhibition</article-title><source>Science</source><volume>293</volume><fpage>1159</fpage><lpage>1163</lpage><pub-id pub-id-type="doi">10.1126/science.1060342</pub-id><pub-id pub-id-type="pmid">11498596</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priebe</surname> <given-names>NJ</given-names></name><name><surname>Ferster</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inhibition, spike threshold, and stimulus selectivity in primary visual cortex</article-title><source>Neuron</source><volume>57</volume><fpage>482</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.02.005</pub-id><pub-id pub-id-type="pmid">18304479</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname> <given-names>DB</given-names></name><name><surname>Van Hooser</surname> <given-names>SD</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex</article-title><source>Neuron</source><volume>85</volume><fpage>402</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.026</pub-id><pub-id pub-id-type="pmid">25611511</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Rumelhart</surname> <given-names>DE</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name><name><surname>Williams</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1985">1985</year><source>Learning Internal Representations by Error Propagation</source><publisher-name>Technical report, California Univ San Diego La Jolla Inst for Cognitive Science</publisher-name></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rupprecht</surname> <given-names>P</given-names></name><name><surname>Friedrich</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Precise synaptic balance in the zebrafish homolog of olfactory cortex</article-title><source>Neuron</source><volume>100</volume><fpage>669</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.013</pub-id><pub-id pub-id-type="pmid">30318416</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sacramento</surname> <given-names>J</given-names></name><name><surname>Costa</surname> <given-names>RP</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Senn</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>8721</fpage><lpage>8732</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeh</surname> <given-names>S</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Theory of neuronal perturbome: linking connectivity to coding via perturbations</article-title><source>PNAS</source><volume>117</volume><fpage>26966</fpage><lpage>26976</lpage><pub-id pub-id-type="doi">10.1073/pnas.2004568117</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>Rancz</surname> <given-names>EA</given-names></name><name><surname>Roth</surname> <given-names>A</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dendritic excitability and synaptic plasticity</article-title><source>Physiological Reviews</source><volume>88</volume><fpage>769</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1152/physrev.00016.2007</pub-id><pub-id pub-id-type="pmid">18391179</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname> <given-names>S</given-names></name><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>Reigl</surname> <given-names>M</given-names></name><name><surname>Nelson</surname> <given-names>S</given-names></name><name><surname>Chklovskii</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e68</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030068</pub-id><pub-id pub-id-type="pmid">15737062</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprekeler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Functional consequences of inhibitory plasticity: homeostasis, the excitation-inhibition balance and beyond</article-title><source>Current Opinion in Neurobiology</source><volume>43</volume><fpage>198</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.03.014</pub-id><pub-id pub-id-type="pmid">28500933</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takesian</surname> <given-names>AE</given-names></name><name><surname>Kotak</surname> <given-names>VC</given-names></name><name><surname>Sharma</surname> <given-names>N</given-names></name><name><surname>Sanes</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hearing loss differentially affects thalamic drive to two cortical interneuron subtypes</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>999</fpage><lpage>1008</lpage><pub-id pub-id-type="doi">10.1152/jn.00182.2013</pub-id><pub-id pub-id-type="pmid">23719211</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname> <given-names>AY</given-names></name><name><surname>Brown</surname> <given-names>BD</given-names></name><name><surname>Scholl</surname> <given-names>B</given-names></name><name><surname>Mohanty</surname> <given-names>D</given-names></name><name><surname>Priebe</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Orientation selectivity of synaptic input to neurons in mouse and cat primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12339</fpage><lpage>12350</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2039-11.2011</pub-id><pub-id pub-id-type="pmid">21865476</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tovote</surname> <given-names>P</given-names></name><name><surname>Fadok</surname> <given-names>JP</given-names></name><name><surname>Lüthi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal circuits for fear and anxiety</article-title><source>Nature Reviews Neuroscience</source><volume>16</volume><fpage>317</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1038/nrn3945</pub-id><pub-id pub-id-type="pmid">25991441</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname> <given-names>R</given-names></name><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Rudy</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>GABAergic interneurons in the neocortex: from cellular properties to circuits</article-title><source>Neuron</source><volume>91</volume><fpage>260</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.033</pub-id><pub-id pub-id-type="pmid">27477017</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname> <given-names>C</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name><name><surname>Zenke</surname> <given-names>F</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title><source>Science</source><volume>334</volume><fpage>1569</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1126/science.1211095</pub-id><pub-id pub-id-type="pmid">22075724</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Froemke</surname> <given-names>RC</given-names></name><name><surname>Doyon</surname> <given-names>N</given-names></name><name><surname>Gilson</surname> <given-names>M</given-names></name><name><surname>Haas</surname> <given-names>JS</given-names></name><name><surname>Liu</surname> <given-names>R</given-names></name><name><surname>Maffei</surname> <given-names>A</given-names></name><name><surname>Miller</surname> <given-names>P</given-names></name><name><surname>Wierenga</surname> <given-names>CJ</given-names></name><name><surname>Woodin</surname> <given-names>MA</given-names></name><name><surname>Zenke</surname> <given-names>F</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibitory synaptic plasticity: spike timing-dependence and putative network function</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>119</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00119</pub-id><pub-id pub-id-type="pmid">23882186</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gating multiple signals through detailed balance of excitation and inhibition in spiking networks</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>483</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1038/nn.2276</pub-id><pub-id pub-id-type="pmid">19305402</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname> <given-names>SN</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity</article-title><source>eLife</source><volume>7</volume><elocation-id>e34560</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34560</pub-id><pub-id pub-id-type="pmid">29465399</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehr</surname> <given-names>M</given-names></name><name><surname>Zador</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex</article-title><source>Nature</source><volume>426</volume><fpage>442</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1038/nature02116</pub-id><pub-id pub-id-type="pmid">14647382</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Werbos</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Backpropagation through time: what it does and how to do it</article-title><conf-name>Proceedings of the IEEE</conf-name><fpage>1550</fpage><lpage>1560</lpage><pub-id pub-id-type="doi">10.1109/5.58337</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname> <given-names>JCR</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Theories of error Back-Propagation in the brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>235</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.12.005</pub-id><pub-id pub-id-type="pmid">30704969</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname> <given-names>M</given-names></name><name><surname>Atallah</surname> <given-names>BV</given-names></name><name><surname>Scanziani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Equalizing excitation-inhibition ratios across visual cortical neurons</article-title><source>Nature</source><volume>511</volume><fpage>596</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1038/nature13321</pub-id><pub-id pub-id-type="pmid">25043046</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshimura</surname> <given-names>Y</given-names></name><name><surname>Dantzker</surname> <given-names>JL</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Excitatory cortical neurons form fine-scale functional networks</article-title><source>Nature</source><volume>433</volume><fpage>868</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1038/nature03252</pub-id><pub-id pub-id-type="pmid">15729343</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Znamenskiy</surname> <given-names>P</given-names></name><name><surname>Kim</surname> <given-names>M-H</given-names></name><name><surname>Muir</surname> <given-names>DR</given-names></name><name><surname>Iacaruso</surname> <given-names>MF</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Functional selectivity and specific connectivity of inhibitory neurons in primary visual cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/294835</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Plasticity rules</title><boxed-text><p>The general framework we follow to derive homeostatic rules is to minimise the mean squared deviation of individual excitatory (Pyr) neuron activations from a target for all stimuli. More specifically, we perform gradient descent on the following objective function:<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that the activations <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are given by the difference between the excitatory and the inhibitory inputs to the excitatory neurons. Our approach can hence be interpreted as supervised learning of the inhibitory circuitry, with the goal of minimising the mean squared loss between the inhibitory and the excitatory inputs (plus the constant target <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>). In this sense, the derived gradient rules aim to generate the best possible E/I balance across stimuli that is possible with the circuitry at hand.</p><p>For reasons of readability, we will first simply state the derived rules. The details of their derivation can be found in the following section.</p><p>The sign constraints in excitatory-inhibitory networks require all synaptic weights to remain positive. To ensure this, we reparameterised all plastic weights of the network by a strictly positive soft-plus function <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and optimised the weight parameter <italic>V</italic> by gradient descent.</p><p>In summary, the derived learning rules for the synaptic weight parameters between excitatory neuron <italic>j</italic> and inhibitory interneuron <italic>i</italic> are given by<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(8a)</mml:mtext></mml:mtd><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(8b)</mml:mtext></mml:mtd><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Please note that we added a small weight decay to both learning rules. The purpose of this decay term is to avoid an ambiguity in the solution. When the firing rates of the interneurons are increased, but their output weights are decreased accordingly, the firing rates of the excitatory population remain unchanged. Pure gradient-based rules can therefore generate extreme values for the synaptic weights, in which the interneurons have biologically unrealistic firing rates. The additional decay terms in the learning rules solve this issue.</p><p>Finally, we replaced the derivative <inline-formula><mml:math id="inf103"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> (which should be a Heaviside function, because rates are the rectified activations) by the derivative of a soft-plus function with finite sharpness (<inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). This allows interneurons to recover from a silent state, in which all gradients vanish. Note that this replacement is done only in the learning rules. The firing rates are still the rectified activations. This method is similar to recent surrogate gradient approaches in spiking networks (<xref ref-type="bibr" rid="bib54">Neftci et al., 2019</xref>).</p><sec id="s9"><title>Derivation of the homeostatic plasticity rules in recurrent networks</title><p>The challenging aspect of the derivation of the learning rules lies in the recurrence of the network. The effects of changes in individual synapses can percolate through the network and thereby change the firing rates of all neurons. Moreover, the temporal dynamics of the network would in principle require a backpropagation of the gradient through time. We circumvent this complication by assuming that the external stimuli to the network change slowly compared to the dynamical time scales of the network, and that the network adiabatically follows the fixed point in its dynamics as the stimulus changes. This assumption significantly simplifies the derivation of the gradient.</p><p>The goal is to minimise the total deviation of the excitatory activations <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> from the homeostatic target value <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. To this end, we calculate the gradient of the objective function in <xref ref-type="disp-formula" rid="equ5">Equation 3</xref> with respect to a given synaptic weight parameter <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf108"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ12"><label>(9)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℰ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊺</mml:mo></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We therefore need the gradient of the activations <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> of excitatory cells with respect to a parameter <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the steady state, the activations are given by<disp-formula id="equ13"><label>(10)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>bg</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The gradient of the activations <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is therefore given by the following implicit condition:<disp-formula id="equ14"><label>(11)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where we introduced the diagonal matrices <inline-formula><mml:math id="inf112"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>/</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup><mml:mo>:=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>/</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>/</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for notational convenience, <inline-formula><mml:math id="inf113"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> being the Kronecker symbol. Derivatives of expressions that do not depend on any of the synaptic weights in question are excluded.</p><p><xref ref-type="disp-formula" rid="equ14">Equation 11</xref> requires the gradient <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> of the inhibitory activations with respect to the parameter <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which can be calculated by a similar approach:<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>bg</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Introducing the effective interaction matrix <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mi>𝕀</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mtext>I</mml:mtext></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> among the interneurons (<inline-formula><mml:math id="inf117"><mml:mi>𝕀</mml:mi></mml:math></inline-formula> being the identity matrix) allows to solve for the gradient of <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ16"><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Inserting this expression into <xref ref-type="disp-formula" rid="equ14">Equation 11</xref> yields<disp-formula id="equ17"><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Introducing the effective interaction matrix <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒲</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>𝕀</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mtext>E</mml:mtext></mml:msup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mtext>I</mml:mtext></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mtext>E</mml:mtext></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> among the excitatory neurons yields an explicit expression for the gradient of <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ18"><label>(12)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To obtain gradients with respect to a particular network parameter, we simply substitute the chosen parameter into <xref ref-type="disp-formula" rid="equ18">Equation 12</xref>. For the parameters <inline-formula><mml:math id="inf121"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> of the input synapses to the interneurons, the gradient reduces to<disp-formula id="equ19"><label>(13)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and for the parameters <inline-formula><mml:math id="inf122"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> of the output synapses from the interneurons we get<disp-formula id="equ20"><label>(14)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>By inserting these expressions into <xref ref-type="disp-formula" rid="equ12">Equation 9</xref> and dropping the average, we obtain online learning rules for the input and output synapses of the interneurons:<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(15a)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>⊺</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>I</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(15b)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>⊺</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that the same approach also yields learning rules for the threshold and the gain of the transfer function of the inhibitory interneurons, if those are parameters of the system. Although we did not use such intrinsic plasticity rules, we include them here for the interested reader. We assumed a threshold linear transfer function of the interneurons: <inline-formula><mml:math id="inf123"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mtext>I</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mtext>I</mml:mtext></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the gain of the neuronal transfer function and <inline-formula><mml:math id="inf125"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> a firing threshold. While the firing threshold can become negative, gain is reparameterised via the strictly positive soft-plus <inline-formula><mml:math id="inf126"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The gradient-based learning rule for the firing thresholds <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the interneurons is given by<disp-formula id="equ22"><label>(16)</label><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊺</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>and the corresponding learning rule for the interneuron gain <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is<disp-formula id="equ23"><label>(17)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊺</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s10"><title>Approximating the gradient rules</title><p>In the gradient-based rules derived in the previous section, the <inline-formula><mml:math id="inf129"><mml:msup><mml:mi class="ltx_font_mathcaligraphic">𝒲</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> terms account for the fact that a change in a given synaptic connections percolates through the network. As a result, the learning rules are highly non-local and hard to implement in a biologically plausible way. To resolve this challenge, we begin by noting that<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mrow><mml:msup><mml:mi class="ltx_font_mathcaligraphic">𝒲</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>𝕀</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">𝒲</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:msup><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">𝒲</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which holds if <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a matrix that depends on the synaptic weights in the network. A similar relation holds for <inline-formula><mml:math id="inf133"><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. Since those matrices are contained in <xref ref-type="disp-formula" rid="equ21">Equation 15</xref>a, we substitute the equivalent sums into the relevant sub-expression and truncate the geometric series after the zeroth order, as in<disp-formula id="equ25"><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">𝒲</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">ℳ</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo stretchy="false">←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The truncation to zeroth order in the last line should yield an acceptable approximation if synapses are sufficiently weak. The effect of higher-order interactions in the network can then be ignored. This approximation can be substituted into <xref ref-type="disp-formula" rid="equ21">Equation 15</xref>a and yields an equation that resembles a backpropagation rule in a feedforward network (E → I → E) with one hidden layer—the interneurons. The final, local approximation used for the simulations in the main text is then reached by replacing the output synapses of the interneurons by the transpose of their input synapses. While there is no mathematical argument why this replacement is valid, it turns out to be in the simulations, presumably because of a mechanism akin to feedback alignment (<xref ref-type="bibr" rid="bib43">Lillicrap et al., 2016</xref>; see discussion in the main text). In feedback alignment, the matrix that backpropagates the errors is replaced by a random matrix <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, we instead use the feedforward weights in the layer below. Similar to the extension to feedback alignment of <xref ref-type="bibr" rid="bib2">Akrout et al., 2019</xref>, those weights are themselves plastic. However, we believe that the underlying mechanism of feedback alignment still holds. The representation in the hidden layer (the interneurons) changes as if the weights to the output layer (the Pyr neurons) were equal to the weight matrix they are replaced with (here, the input weights to the PV neurons). To exploit this representation, the weights to the output layer then align to the replacement weights, justifying the replacement post hoc (<xref ref-type="fig" rid="fig1">Figure 1G</xref>).</p><p>Note that the condition for feedback alignment to provide an update in the appropriate direction (<inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>W</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the error, <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> the weights in the second layer, and <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> the random feedback matrix) reduces to the condition that <inline-formula><mml:math id="inf139"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>←</mml:mo><mml:mtext>I</mml:mtext></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>←</mml:mo><mml:mtext>E</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is positive definite (assuming the errors are full rank). One way of assuring this is a sufficiently positive diagonal of this matrix product, that is, a sufficiently high correlation between the incoming and outgoing synapses of the interneurons. A positive correlation of these weights is one of the observations of <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref> and also a result of learning in our model.</p><p>While such a positive correlation is not necessarily present for all learning tasks or network models, we speculate that it will be for the task of learning an E/I balance in networks that obey Dale’s law.</p><p>The same logic of using a zeroth order approximation of <inline-formula><mml:math id="inf140"><mml:msup><mml:mi class="ltx_font_mathcaligraphic">𝒲</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> that neglects higher-order interactions is employed to recover the inhibitory synaptic plasticity rule of <xref ref-type="bibr" rid="bib75">Vogels et al., 2011</xref> from <xref ref-type="disp-formula" rid="equ21">Equation 15b</xref>.</p><p>Overall, the local approximation of the learning rule relies on three assumptions: slowly varying inputs, weak synaptic weights, and alignment of input and output synapses of the interneurons. These assumptions clearly limit the applicability of the learning rules for other learning tasks. In particular, the learning rules will not allow the network to learn temporal sequences.</p></sec><sec id="s11"><title>The effect of static but heavy-tailed PV input weights</title><p>We investigated whether plasticity is required on both input and output synapses. The main argument why plasticity may not be required is that a static but heavy-tailed weight distribution for Pyr → PV synapses might provide sufficient stimulus selectivity in PV neurons, such that plasticity of output synapses alone can account for the experimental data of <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>. To test this, we sampled weights from a log-normal distribution (<xref ref-type="bibr" rid="bib50">Loewenstein et al., 2011</xref>), with parameters that are in line with the data reported by <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>. Weights spanned approximately two orders of magnitude. For this setting, PV neurons do not have sufficient stimulus selectivity for output plasticity alone to co-tune inhibitory currents with excitatory currents, when Pyr → PV plasticity is knocked out. Moreover, if we sample PV input and output synapses, correlations between excitatory input synapse strength and RS are not reliably found, nor are correlations between input and output synapse strength (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>Nevertheless, one could expect that a sufficiently heavy-tailed distribution of PV excitatory input weights could account for the experimentally observed correlations, even in the absence of Pyr → PV plasticity. We therefore repeated the simulations of <xref ref-type="fig" rid="fig3">Figure 3</xref> with a weight distribution that spans almost five orders of magnitude (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). We also tested other distributions, but the results remain qualitatively unchanged.</p><p>With this heavy-tailed distribution of excitatory input weights, PV neurons indeed exhibit increased stimulus selectivity (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2b</xref>, <italic>cf</italic>. <xref ref-type="fig" rid="fig3">Figure 3c</xref>). This increased selectivity enables greater stimulus-specific co-tuning of inhibitory and excitatory currents (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2c</xref>, <italic>cf</italic>. <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2d</xref>). Despite this, repeated random sampling of excitatory input synaptic weights (<inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> drawn 10<sup>4</sup> times) produces positive correlations with RS only about half the time, and for reciprocally connected Pyr-PV cell pairs, input synaptic weights are unlikely to be correlated with output weights (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2d</xref>). Observing the correlations of <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref> is therefore unlikely in the absence of some kind of PV input plasticity.</p><p>Note that the setting in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> is conservative in that we chose parameters that made the observation of positive correlations likely. In particular, it is important to note that—similar to the results of <xref ref-type="fig" rid="fig3">Figure 3</xref>—we consider synapses below a chosen threshold as too weak to be detected during the sampling procedure. We set the threshold of detectability to be approximately two orders of magnitude below the strongest weights (dashed lines in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2e</xref>). For lower thresholds, the fraction of significant Exc-RS and Exc-Inh correlations rapidly decreases, to essentially zero if we include all weights. This is due to the fact that the inhibitory stimulus tuning is determined by a small number of large weights, and if samples are drawn from all synapses, the probability of sampling one of these large weights is small.</p><p>In summary, randomly drawn input weights to PV neurons from the distribution observed by <xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref> are not sufficiently sparse to ensure enough PV selectivity for the observed E/I assemblies, and sampling from even sparser distributions makes the observation of the few influential weights too unlikely. This suggests that the input weights to PV neurons are not random, but that these synaptic weights and the stimulus tuning of their inputs are correlated, consistent with the presence of synaptic plasticity.</p><p>Of course, the question whether random connectivity is sufficient for selectivity not only depends on the weight distribution, but also on the total number of synapses received by the PV neurons and the stimulus selectivity of the Pyr neurons they receive input from. Even when the weights are not sparsely distributed, a small number of inputs combined with sufficiently sparse inputs could generate substantial selectivity. We did not explore these additional parameters (in-degree of PV neurons, Pyr selectivity) in detail here, because we believe that our choices are already rather conservative. Due to the small size of the network, PV neurons receive only about 300 excitatory inputs, which is unlikely to be an overestimate, and the stimulus tuning of the Pyr neurons in the model is comparable to V1 (<xref ref-type="bibr" rid="bib84">Znamenskiy et al., 2018</xref>, <xref ref-type="fig" rid="fig1">Figure 1</xref>), although the diversity of the Pyr neuron selectivity in the data is of course higher.</p></sec></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59715.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution>University of Chicago</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Understanding the connectivity patterns observed in the brain and how these connectivity patterns emerge from local, cell-to-cell plasticity is a grand challenge in modern neuroscience. This manuscript describes conditions under which synaptic plasticity can organize excitatory/inhibitory networks in the brain into assemblies in which both kinds of neurons have structured connectivity, as observed in real networks. The work makes predictions about what kinds of plasticity can give rise to this and accounts for recent experimental findings about manipulating inhibitory neurons and, thereby, the stimulus-dependence of their connectivity. It will be of interest to both the experimental and theoretical neuroscience community, and asks and answers a deep question about how the brain self-organizes its detailed connectivity patterns.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;Learning excitatory-inhibitory neuronal assemblies in recurrent networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by a Senior Editor. The reviewers have opted to remain anonymous.</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>The work detailed here explores model of recurrent cortical networks and shows that synaptic plasticity must be present in both excitatory to inhibitory neurons and vice versa to produce the known E/I assemblies found in cortex. There are some interesting findings about the consequences of assemblies formed in this way. A major claim in the manuscript (that argues for the broad impact of the work) is that this shows for the first time how a local approximation rule can instantiate feedback in a biologically plausible way.</p><p>While the reviewers found the work to be solid and interesting, they failed to find that the work was appropriate for <italic>eLife</italic>, specifically because of other recent papers that show that a biologically plausible alternative to backpropagation can be instantiated in recurrent neural nets, e.g. a paper published here last year by J. Murray, Local online learning in recurrent networks with random feedback. It's understood that the authors were focusing here on the E/I interactions, but in that case it seems that the novelty of the result needs to be somewhat reframed.</p><p>The reviewers were also concerned about the exposition in the introduction and some results that could have been added to a few figures, and had questions about why exactly a BCM rule did not work in this model. Those technical concerns along with doubts about the strong novelty claim led to this decision.</p><p><italic>Reviewer #1:</italic></p><p>The manuscript investigates the situations in which stimulus-specific assemblies can emerge in a recurrent network of excitatory (E) and inhibitory (I, presumed parvalbumin-positive) neurons. The authors combine (1) Hebbian plasticity of I-&gt;E synapses that is proportional to the difference between the E neuron's firing rate and a homeostatic target and (2) plasticity of E-&gt;I synapses that is proportional to the difference between the total excitatory input to the I neuron and a homeostatic target. These are sufficient to produce E/I assemblies in a network in which only the excitatory recurrence exhibits tuning at the initial condition. While the full implementation of the plasticity rules, derived from gradient descent on an objective function, would rely on nonlocal weight information, local approximations of the rules still lead to the desired results.</p><p>Overall the results make sense and represent a new unsupervised method for generating cell assemblies consisting of both excitatory and inhibitory neurons. My main concerns are that the proposed rule ends up predicting a rather nonstandard form of plasticity for certain synapses, and that the results could be fleshed out more.</p><p>1) The main text would benefit from greater exposition of the plasticity rule and the distinction between the full expression and the approximation. While the general idea of backpropagation may be familiar to a good number of readers, here it is being used in a nonstandard way (to implement homeostasis), and this should be described more fully, with a few key equations.</p><p>Additionally, the point that, for a recurrent network, the proposed rules are only related to gradient descent under the assumption that the network adiabatically follows the stimulus, seems important enough to state in the main text.</p><p>2) The paper has a clear and simple message, but not much exploration of that message or elaboration on the results. Figure 2 and Figure 3 do not convey much information, other than the fact that blocking either form of plasticity fails to produce the desired effects. This seems somewhat obvious -- almost by definition one can't have E/I assemblies if E-&gt;I or I-&gt;E connections are forced to remain random. I would think this point deserves at most one figure, or maybe even just a few panels.</p><p>3) The derived plasticity rule for E-&gt;I synapses, which requires modulation of I synapses based on a difference from a target value for the excitatory subcomponent of the input current, does not take a typical form for biologically plausible learning rules (which usually operate on firing rates or voltages, for example). The authors should explore and discuss in more depth this assumption. Is there experimental evidence for it? It seems like it might be a difficult quantity to signal to the synapse in order to guide plasticity. The authors note in the discussion that BCM-type rules fail here -- are there other approaches that would work? What about a more local form of plasticity that involves only the excitatory current local to a dendrite, for example?</p><p>4) Does the initial structure in excitatory recurrence play a role, or is it just there to match the data?</p><p><italic>Reviewer #2:</italic></p><p>In this work, the authors simulated a rate-based recurrent network with 512 excitatory and 64 inhibitory neurons. The authors use this model to investigate which forms of synaptic plasticity are needed to reproduce the stimulus-specific interactions observed between pyramidal neurons and parvalbumin-expressing (PV) interneurons in mouse V1. When there is homeostatic synaptic plasticity from both excitatory to inhibitory and reciprocally from inhibitory to excitatory neurons in the simulated networks, they showed that the emergent E/I assemblies are qualitatively similar to those observed in mouse V1, i.e., stronger synapses for neurons responding to similar stimuli. They also identified that synaptic plasticity must be present in both directions (from pyramidal neurons to PV neurons and vice versa) to produce such E/I assemblies. Furthermore, they identified that these E/I assemblies enable the excitatory population in their simulations to show feature-specific suppression. Therefore, the author claimed that they found evidence that these inhibitory circuits do not provide a &quot;blanket of inhibition&quot;, but rather a specific, activity-dependent sculpting of the excitatory response. They also claim that the learning rule they developed in this model shows for the first time how a local approximation rule can instantiate feedback alignment in their network, which is a method for achieving an approximation to a backpropagation-like learning rule in realistic neural networks.</p><p>1) The authors claim that their synaptic plastic rule implements a recurrent variant of feedback alignment. Namely, &quot;When we compare the weight updates the approximate rules perform to the updates that would occur using the gradient rule, the weight updates of the local approximations align to those of the gradient rules over learning&quot;. They also claim that this is the first time feedback alignment is demonstrated in a recurrent network. It seems that the weight replacement in this synaptic plastic rule is uniquely motivated by E/I balance, but the feedback alignment in [Lillicrap et al., 2016] is much more general. Thus, the precise connections between feedback alignment and this work remains a bit unclear.</p><p>It would be good if the following things about this major claim of the manuscript could be expanded and/or clarified:</p><p>i) In Figure 1—figure supplement 3 (upper, right vs. left), it is surprising that the Pyr-&gt;PV knock-out seems to produce a better alignment in PV-&gt;Pyr. Comparing the upper right of Figure 1—figure supplement 3 and the bottom figure of Figure 1G, it seems that the Pyr-&gt;PV knock-out performs equally well with a local approximation for the output connections of PV interneurons. Is this a special condition in this model that results in the emergence of the overall feedback alignment?</p><p>ii) In the feedback alignment paper [Lillicrap et al., 2016], they introduced a &quot;Random Feedback Weights Support&quot;: this uses a random matrix B to replace the transpose of the backpropagation weight matrix. Here, the alignment seems to be based on the intuition that &quot;The excitatory input connections onto the interneurons serve as a proxy for the transpose of the output connections,&quot; and &quot;the task of balancing excitation by feedback inhibition favours symmetric connection.&quot; It seems synaptic plasticity here is mechanistically different; it is only similar to the feedback alignment [Lillicrap et al., 2016] because both reach a final balanced state. Please clarify how the results here are interpreted as an instantiation of feedback alignment – if it is simply that the end state is similar or if the mechanism is thought to be more deeply connected.</p><p>iii) The feedback alignment [Lillicrap et al., 2016] works when the weight matrix has its entries near zero (e^TWBe&gt;0). Are there any analogous conditions for the synaptic plastic rule to succeed?</p><p>iv) In the Appendix, the local approximation rule is developed using a 0th-order truncation of Equations 15a and 15b. Is it noted that &quot;If synapses are sufficiently weak.…, this approximation can be substituted into Equation 15a and yields an equation that resembles a backpropagation rule in a feedforward network (E -&gt; I -&gt; E) with one hidden layer -- the interneurons.&quot; It would be helpful if the authors can discuss how this learning rule works in a general recurrent network, or if it will work for any network with sufficiently weak synapses.</p><p>v) This synaptic plasticity rule seems to be closely related to another local approximation of backpropagation in recurrent neural network: e-prop in (Bellec et al., 2020, https://www.nature.com/articles/s41467-020-17236-y) and broadcast alignment (Nøkland, 2016, Samadi et al., 2017). These previous works do not consider E/I balance in their approximations, but is E/I balance necessary for successful local approximation to these rules?</p><p>2) In the Discussion, it reads as if the BCM rule cannot apply to this recurrent network because of the limited number of interneurons in the simulation (&quot;parts of stimulus space are not represented by any interneurons&quot;). Is this a limitation of the size of the model? Would scaling up the simulation change how applicable the BCM learning rule is? It would be helpful if the authors offer a more detailed discussion on why Hebbian forms of plasticity in interneurons fail to produce stimulus specificity.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Learning excitatory-inhibitory neuronal assemblies in recurrent networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>The manuscript describes conditions under which synaptic plasticity can organize excitatory/inhibitory networks into assemblies in which both neurons have structured connectivity. The work makes predictions about what kinds of plasticity can give rise to this and accounts for recent experimental findings about manipulating inhibitory neurons and the stimulus-dependence of their connectivity.</p><p>While recurrent networks in which connections between excitatory (E) neurons are structured into neural &quot;assemblies&quot; are a classic model for the cortex (e.g. the Hopfield model), inhibitory (I) neurons are often assumed to be unstructured in computational models. This simplification is at odds with the observation of E/I connectivity that is organized with respect to stimulus preference and the results of experiments in which activity of I neurons are perturbed. In brain regions spatially organized according to stimulus preference this may be a consequence of local connectivity, but in regions like mouse V1 that lack an obvious topography, it may arise from experience-dependent synaptic plasticity.</p><p>The manuscript investigates the situations in which stimulus-specific assemblies can emerge through synaptic plasticity in a recurrent network of E and I (presumed parvalbumin-positive) neurons. The authors combine (1) Hebbian plasticity of I-&gt;E synapses that is proportional to the difference between the E neuron's firing rate and a homeostatic target and (2) plasticity of E-&gt;I synapses that is proportional to the difference between the total excitatory input to the I neuron and a homeostatic target. These are sufficient to produce E/I assemblies in a network in which only the excitatory recurrence exhibits tuning at the initial condition. While the full implementation of the plasticity rules, derived from gradient descent on an objective function, would rely on nonlocal weight information, local approximations of the rules still lead to the desired results.</p><p>The manuscript makes predictions about the results of blocking plasticity of E-&gt;I or I-&gt;E synapses as well as accounting for the results of experiments in which activating a pyramidal neuron suppresses similarly tuned neurons through recurrent inhibition.</p><p>An interesting prediction of the analysis, that is derived through the approximation to the gradient-based weight update, is that synaptic plasticity rules depend on the deviation of recurrent excitatory input current to a neuron from a target value. This quantity is different from those often used in computational models of synaptic plasticity, such as firing rates or voltages. Experiments targeting this quantity and its influence on the results of synaptic plasticity protocols would be an interesting direction for future study.</p><p>Essential revisions:</p><p>The modifications the authors have made have improved the manuscript. Addressing the following points will further enhance the clarity of the presentation, and will make the work suitable for publication in <italic>eLife</italic>.</p><p>1) Could the authors please expand on the negative result about developing selectivity in PV neurons through quenched random connections? Would it be possible to adjust the parameters of the model, such as the weight distribution or number of connections onto PV neurons, so that they would exhibit enough selectivity?</p><p>2) Relatedly, the text around Figure 3D says that, &quot;Because of the poor stimulus tuning of the interneurons, output plasticity cannot generate stimulus-specific inhibitory inputs to the Pyr neurons (Figure 3D).&quot; But it looks like there is a definite stimulus-specific inhibitory input that emerges. The authors should clarify what they mean.</p><p>3) It would be good if the Abstract could be revised to be accessible to a broader audience of readers.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59715.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: The authors appealed the original decision. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The manuscript investigates the situations in which stimulus-specific assemblies can emerge in a recurrent network of excitatory (E) and inhibitory (I, presumed parvalbumin-positive) neurons. The authors combine (1) Hebbian plasticity of I-&gt;E synapses that is proportional to the difference between the E neuron's firing rate and a homeostatic target and (2) plasticity of E-&gt;I synapses that is proportional to the difference between the total excitatory input to the I neuron and a homeostatic target. These are sufficient to produce E/I assemblies in a network in which only the excitatory recurrence exhibits tuning at the initial condition. While the full implementation of the plasticity rules, derived from gradient descent on an objective function, would rely on nonlocal weight information, local approximations of the rules still lead to the desired results.</p><p>Overall the results make sense and represent a new unsupervised method for generating cell assemblies consisting of both excitatory and inhibitory neurons. My main concerns are that the proposed rule ends up predicting a rather nonstandard form of plasticity for certain synapses, and that the results could be fleshed out more.</p><p>1) The main text would benefit from greater exposition of the plasticity rule and the distinction between the full expression and the approximation. While the general idea of backpropagation may be familiar to a good number of readers, here it is being used in a nonstandard way (to implement homeostasis), and this should be described more fully, with a few key equations.</p><p>Additionally, the point that, for a recurrent network, the proposed rules are only related to gradient descent under the assumption that the network adiabatically follows the stimulus, seems important enough to state in the main text.</p></disp-quote><p>Thanks, that's a good point. We modified the relevant portion of the main text as follows:</p><p>“[…] To that end, we derive synaptic plasticity rules for excitatory input and inhibitory output connections of PV interneurons that are homeostatic for the excitatory population (see Materials and methods). A stimulus-specific homeostatic control can be seen as a &quot;trivial&quot; supervised learning task, in which the objective is that all pyramidal neurons should learn to fire at a given target rate ρ​0 for all stimuli. Hence, a gradient-based optimisation would effectively require a backpropagation of error [Rumelhart et al., 1985] through time [BPTT; Werbos, 1990].</p><p>Because backpropagation rules rely on non-local information that might not be available to the respective synapses, their biological plausibility is currently debated [Lillicrap et al., 2020, Sacramento et al., 2018, Guerguiev et al., 2017, Whittington and Bogacz, 2019, Bellec et al., 2020]. However, a local approximation of the full BPTT update can be obtained under the following assumptions: First, we assume that the sensory input to the network changes on a time scale that is slower than the intrinsic time scales in the network. This eliminates the necessity of backpropagating information through time, albeit still through the synapses in the network. This assumption results in what we call the “gradient-based” rules (Equation 15 in the Appendix), which are spatially non-local. Second, we assume that synaptic interactions in the network are sufficiently weak that higher-order synaptic interactions can be neglected. Third and finally, we assume that over the course of learning, the Pyr→PV connections and the PV→Pyr connections become positively correlated [Znamenskiy et al., 2018], such that we can replace PV-&gt;Pyr synapses by the reciprocal Pyr-&gt;PV synapse in the Pyr-&gt;PV learning rule, without rotating the update too far from the true gradient (see Appendixs).&quot;</p><p>We also added the learning rules to the main text.</p><disp-quote content-type="editor-comment"><p>2) The paper has a clear and simple message, but not much exploration of that message or elaboration on the results. Figure 2 and Figure 3 do not convey much information, other than the fact that blocking either form of plasticity fails to produce the desired effects. This seems somewhat obvious -- almost by definition one can't have E/I assemblies if E-&gt;I or I-&gt;E connections are forced to remain random. I would think this point deserves at most one figure, or maybe even just a few panels.</p></disp-quote><p>We appreciate that the result that both forms of plasticity are necessary may feel somewhat obvious. However, it may not be as obvious as it appears, because the incoming synapses onto INs follow a long-tailed distribution, like many other synapse types. Randomly sampling from such a distribution could in principle generate sufficient stimulus selectivity to render learning in the E-&gt;I connections superfluous (see Litwin-Kumar et al., 2017). That’s why we made sure to initialize the E-&gt;I weights such that they show a similar variability as in the data. We now comment on this aspect in the Results section:</p><p>&quot;Having shown that homeostatic plasticity acting on both input and output synapses of interneurons are <italic>sufficient</italic>​​ to learn E/I assemblies, we now turn to the question of whether both are <italic>necessary</italic>​​. To this end, we perform &quot;knock-out&quot; experiments, in which we selectively block synaptic plasticity in either of the synapses. The motivation for these experiments is the observation that the incoming PV synapses follow a long-tailed distribution (Znamenskiy et al., 2018). This could provide a sufficient stimulus selectivity in the PV population for PV-&gt;Pyr plasticity alone to achieve a satisfactory E/I balance. A similar reasoning holds for static, but long-tailed outgoing PV synapses. This intuition is supported by result of Litwin-Kumar et al., (2017) that in a population of neurons analogous to our interneurons, the dimensionality of responses in that population can be high for static input synapses, when those are log-normally distributed.&quot;</p><p>Secondly, we tried to write a manuscript for both fellow modelers (how to self-organize an E/I assembly?) and to our experimental colleagues (what conclusions can we draw from the Znamenskiy data?). In electrophysiological studies, the plasticity of incoming and outgoing synapses of INs both have been studied independently. The insight that those two forms of plasticity should act in synergy is something that we wanted to emphasize, because it could be studied in parallel in paired recordings. Hence the two figures. Looks like we got only modelers as reviewers. Along these lines, we added a short paragraph to the Discussion:</p><p>“Both Pyr-&gt;PV and PV-&gt;Pyr plasticity have been studied in slice (for reviews, see, Kullmann et al., 2007, Vogels et al., 2013), but mostly in isolation. The idea that the two forms of plasticity should act in synergy suggests that it may be interesting to study both forms in the same system, e.g., in reciprocally connected Pyr-PV pairs.”</p><disp-quote content-type="editor-comment"><p>3) The derived plasticity rule for E-&gt;I synapses, which requires modulation of I synapses based on a difference from a target value for the excitatory subcomponent of the input current, does not take a typical form for biologically plausible learning rules (which usually operate on firing rates or voltages, for example). The authors should explore and discuss in more depth this assumption. Is there experimental evidence for it? It seems like it might be a difficult quantity to signal to the synapse in order to guide plasticity. The authors note in the discussion that BCM-type rules fail here -- are there other approaches that would work? What about a more local form of plasticity that involves only the excitatory current local to a dendrite, for example?</p></disp-quote><p>We agree that the rule we propose for E-&gt;I synapses warrants a more extensive discussion regarding its potential biological implementation. We have added the following paragraph to the manuscript:</p><p>&quot;A cellular implementation of such a plasticity rule would require the following ingredients: (i) a signal that reflects the cell-wide excitatory current (ii) a mechanism that changes Pyr-&gt;PV synapses in response to variations in this signal. For the detection of excitatory inputs, postsynaptic sodium or calcium concentrations are natural candidates. Due to the lack of spines in PV dendrites, both are expected to diffuse more broadly in the dendritic arbor than in spiny neurons [Hu et al., 2014, Kullmann and Lamsa, 2007], and may thus provide a signal for overall dendritic excitatory currents. Depending on how excitatory inputs are distributed on PV interneuron dendrites [Larkum and Nevian, 2008, Jia et al., 2010, Grienberger et al., 2015], the integration of the excitatory currents may not need to be cell-wide -- which could limit the temporal resolution of the plasticity --, but could be local, e.g. to a dendrite, if local excitatory input is a sufficient proxy for the global input. Notably, in PV interneurons, NMDA receptors are enriched in excitatory feedback relative to feedforward connections [LeRoux et al., 2013], suggesting those two sources of excitation are differentially treated on the postsynaptic side.</p><p>As for many other excitatory synapses [Sjöström et al., 2008], postsynaptic calcium is likely a key factor also for the plasticity of excitatory input synapses onto interneurons. Blocking NMDA receptors interferes with Hebbian long-term plasticity in some of these synapses [Kullmann and Lamsa, 2007], as does a block of excitatory input [LeRoux et al., 2013]. Furthermore, NMDAR-dependent plasticity in Pyr-&gt;PV synapses is expressed postsynaptically and seems to require presynaptic activation [Kullmann and Lamsa, 2007]. In summary, we believe that there are no conceptual issues that would rule out an implementation of the suggested plasticity rule for excitatory inputs onto PV interneurons.”</p><p>Concerning other potential types of plasticity, we certainly do not expect that the suggested pair of rules is the only one that will work. The above paragraph is now followed by:</p><p>“We also expect that the rules we suggest here are only one set of many that can establish E/I assemblies. Given that the role of the input plasticity in the interneurons is the formation of a stimulus specificity, it is tempting to assume that this could equally well be achieved by classical forms of plasticity like the Bienenstock-Cooper-Munro (BCM) rule [Bienenstock et al., 1982], which is commonly used in models of receptive field formation. However, in our hands, the combination of BCM plasticity in Pyr-&gt;PV synapses with homeostatic inhibitory plasticity in the \ItoE synapses showed complex dynamics, an analysis of which is beyond the scope of this article. In particular, this combination of rules often did not converge to a steady state, probably for the following reason. BCM rules tend to […].</p><p>We suspect that this instability can also arise for other Hebbian forms of plasticity in interneuron input synapses when they are combined with homeostatic inhibitory plasticity [Vogels et al., 2011] in their output synapses. The underlying reason is that for convergence, the two forms of plasticity need to work synergistically towards the same goal, i.e., the same steady state. For two arbitrary synaptic plasticity rules acting in different sets of synapses, it is likely that they aim for two different overall network configurations. Such competition can easily result in latching dynamics with a continuing turn-over of transiently stable states, in which the form of plasticity that acts more quickly gets to reach its goal transiently, only to be undermined by the other one later [Clopath et al., 2016].”</p><disp-quote content-type="editor-comment"><p>4) Does the initial structure in excitatory recurrence play a role, or is it just there to match the data?</p></disp-quote><p>For the results of Figure 4, the structure of excitatory recurrence is essential, because similarly tuned Pyr neurons should excite each other (absent the E-I assemblies). Without that structure in the Pyr-&gt;Pyr connections, the “paradoxical” inhibitory effect we report would not be paradoxical at all. For the results of Figure 1, Figure 2, Figure 3, we primarily included them for consistency, because it's known that this structure is present in V1, and because it's good to see that their presence doesn't change the results. It plays a role only insofar as it permits and reinforces stimulus selectivity in pyramidal neurons. If those synapses were unstructured (and strong), it could disrupt the Pyr selectivity, and there would be nothing to guide the formation of E/I assemblies. We have added the following sentence to the beginning of the Results section:</p><p>“[…] Note that the Pyr-&gt;Pyr connections only play a decisive role for the results in Figure 4, but are present in all simulations for consistency. […]”</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>In this work, the authors simulated a rate-based recurrent network with 512 excitatory and 64 inhibitory neurons. The authors use this model to investigate which forms of synaptic plasticity are needed to reproduce the stimulus-specific interactions observed between pyramidal neurons and parvalbumin-expressing (PV) interneurons in mouse V1. When there is homeostatic synaptic plasticity from both excitatory to inhibitory and reciprocally from inhibitory to excitatory neurons in the simulated networks, they showed that the emergent E/I assemblies are qualitatively similar to those observed in mouse V1, i.e., stronger synapses for neurons responding to similar stimuli. They also identified that synaptic plasticity must be present in both directions (from pyramidal neurons to PV neurons and vice versa) to produce such E/I assemblies. Furthermore, they identified that these E/I assemblies enable the excitatory population in their simulations to show feature-specific suppression. Therefore, the author claimed that they found evidence that these inhibitory circuits do not provide a &quot;blanket of inhibition&quot;, but rather a specific, activity-dependent sculpting of the excitatory response. They also claim that the learning rule they developed in this model shows for the first time how a local approximation rule can instantiate feedback alignment in their network, which is a method for achieving an approximation to a backpropagation-like learning rule in realistic neural networks.</p></disp-quote><p>We thank you for your thorough evaluation of the role of feedback alignment (FA) in our model. While we will attempt to address them point-by-point below, we feel that we may have misled this reviewer regarding the focus of the article.</p><p>For us, the core novelty of this work lies in elucidating potential mechanisms of experimentally observed E/I neuronal assemblies in mouse V1, and furthermore in proposing plasticity rules that can achieve such E/I assemblies. That they do so via a</p><p>mechanism akin to feedback alignment is mentioned relatively briefly in the manuscript, and is merely offered as a mechanistic explanation for how inhibitory currents are ultimately balanced with excitation. We are fully aware of the fact that the suggested rules are by no means a local approximation of the full BPTT problem in RNNs. We rephrased the paper and hope that the emphasis of the paper is now clearer.</p><disp-quote content-type="editor-comment"><p>1) The authors claim that their synaptic plastic rule implements a recurrent variant of feedback alignment. Namely, &quot;When we compare the weight updates the approximate rules perform to the updates that would occur using the gradient rule, the weight updates of the local approximations align to those of the gradient rules over learning&quot;. They also claim that this is the first time feedback alignment is demonstrated in a recurrent network. It seems that the weight replacement in this synaptic plastic rule is uniquely motivated by E/I balance, but the feedback alignment in [Lillicrap et al., 2016] is much more general. Thus, the precise connections between feedback alignment and this work remains a bit unclear.</p></disp-quote><p>We had hoped that our claims in the manuscript were phrased sufficiently carefully, and regret that the reviewer was led to believe that our goal was to provide a general solution to biological backprop in recurrent networks. Of course, the problem we are tackling is not the full backprop problem, and we do not expect that the approximation holds for general tasks. It clearly won't, given that it effectively relies on a truncation after two time steps and makes a stationarity assumption. Still, we felt that it would have been a lost opportunity not to discuss the relation to feedback alignment, because any approximation warrants a justification, and for the replacement of I-&gt;E weights by E-&gt;I weights, feedback alignment readily provides one. We now discuss the assumptions underlying the local approximation more extensively in the main paper (see reply to reviewer 1, comment 1).</p><p>We also added a discussion to the section in the Appendix, where the local approximations are derived:</p><p>“Overall, the local approximation of the learning rule relies on three assumptions: Slowly varying inputs, weak synaptic weights and alignment of input and output synapses of the interneurons. These assumptions clearly limit the applicability of the learning rules for other learning tasks. In particular, the learning rules will not allow the network to learn temporal sequences.”</p><disp-quote content-type="editor-comment"><p>It would be good if the following things about this major claim of the manuscript could be expanded and/or clarified:</p><p>i) In Figure 1—figure supplement 3 (upper, right vs. left), it is surprising that the Pyr-&gt;PV knock-out seems to produce a better alignment in PV-&gt;Pyr. Comparing the upper right of Figure 1—figure supplement 3 and the bottom figure of Figure 1G, it seems that the Pyr-&gt;PV knock-out performs equally well with a local approximation for the output connections of PV interneurons. Is this a special condition in this model that results in the emergence of the overall feedback alignment?</p></disp-quote><p>The 0-th order approximation of PV-&gt;Pyr plasticity is, by itself, relatively good at following the full gradient for those synapses, because PV-&gt;Pyr synapses have virtually unmediated control over Pyr neuron activity. When Py-&gt;PV plasticity is also present, we believe that the higher variance in angle to the gradient (for PV-&gt;Pyr updates) may be due to perturbations introduced by the Pyr-&gt;PV updates. Each update to one weight matrix changes the gradient for the other, but this is ultimately what brings them into alignment with one another. ​ ​Because this is a very technical point, we prefer not to discuss this at length in the manuscript. The more important point is conveyed in the two bottom figures, which demonstrate that the gradients on the Pyr-&gt;PV synapses only align within 90 degrees when both synapse types are plastic.</p><disp-quote content-type="editor-comment"><p>ii) In the feedback alignment paper [Lillicrap et al., 2016], they introduced a &quot;Random Feedback Weights Support&quot;: this uses a random matrix B to replace the transpose of the backpropagation weight matrix. Here, the alignment seems to be based on the intuition that &quot;The excitatory input connections onto the interneurons serve as a proxy for the transpose of the output connections,&quot; and &quot;the task of balancing excitation by feedback inhibition favours symmetric connection.&quot; It seems synaptic plasticity here is mechanistically different; it is only similar to the feedback alignment [Lillicrap et al., 2016] because both reach a final balanced state. Please clarify how the results here are to interpreted as an instantiation of feedback alignment – if it is simply that the end state is similar or if the mechanism is thought to be more deeply connected.</p></disp-quote><p>We believe that the mechanisms are indeed more deeply connected, as supported by the fact that the gradients align early on during learning. We added an extended discussion to the Appendix:</p><p>“In feedback alignment, the matrix that backpropagates the errors is replaced by a random matrix B. Here, we instead use the feedforward weights in the layer below. Similar to the extension to feedback alignment of Akrout et al., [2019], those weights are themselves plastic. However, we believe that the underlying mechanism of feedback alignment still holds. The representation in the hidden layer (the interneurons) changes as if the weights to the output layer (the Pyr neurons) were equal to the weights they are replaced with (here, the input weights to the PV neurons). To exploit this representation, the weights to the output layer then align to the replacement weights, justifying the replacement post-hoc (Figure 1G).”</p><disp-quote content-type="editor-comment"><p>iii) The feedback alignment [Lillicrap et al., 2016] works when the weight matrix has its entries near zero (e^TWBe&gt;0). Are there any analogous conditions for the synaptic plastic rule to succeed?</p></disp-quote><p>Yes, the condition is very similar. We have added a corresponding discussion to the Appendix:</p><p>“Note that the condition for feedback alignment to provide an update in the appropriate direction (e​<sup>T​</sup> B​<sup>T​</sup> W e&gt;0, where e denotes the error, W the weights in the second layer, and B the random feedback matrix) reduces to the condition that W​ei W<sub>ie</sub>​ is positive definite (assuming the errors are full rank).</p><p>​ ​</p><p>One way of assuring this is a sufficiently positive diagonal of this matrix product, i.e., a sufficiently high correlation between the incoming and outgoing synapses of the interneurons. A positive correlation of these weights is one of the observations of Znamenskiy et al., 2018 and also a result of learning in our model.</p><p>While such a positive correlation is not necessarily present for all learning tasks or network models, we speculate that it will be for the task of learning an E/I balance in a Dalean network.”</p><disp-quote content-type="editor-comment"><p>iv) In the Supplementary material, the local approximation rule is developed using a 0th-order truncation of Equations 15a and 15b. Is it noted that &quot;If synapses are sufficiently weak.…, this approximation can be substituted into Equation 15a and yields an equation that resembles a backpropagation rule in a feedforward network (E -&gt; I -&gt; E) with one hidden layer -- the interneurons.&quot; It would be helpful if the authors can discuss how this learning rule works in a general recurrent network, or if it will work for any network with sufficiently weak synapses.</p></disp-quote><p>We now discuss the assumptions and their consequences more extensively, see reply to reviewer 1, comment 1.</p><disp-quote content-type="editor-comment"><p>v) This synaptic plasticity rule seems to be closely related to another local approximation of backpropagation in recurrent neural network: e-prop in (Bellec et al., 2020, https://www.nature.com/articles/s41467-020-17236-y) and broadcast alignment (Nøkland, 2016, Samadi et al., 2017). These previous works do not consider E/I balance in their approximations, but is E/I balance necessary for successful local approximation to these rules?</p></disp-quote><p>We are not sure if we fully understand the comment. We do not expect that E/I balance is necessary for other biologically plausible approximations of BPTT. We merely suggest that for the task of learning E/I balance, the presented local approximation is valid.</p><disp-quote content-type="editor-comment"><p>2) In the Discussion, it reads as if the BCM rule cannot apply to this recurrent network because of the limited number of interneurons in the simulation (&quot;parts of stimulus space are not represented by any interneurons&quot;). Is this a limitation of the size of the model? Would scaling up the simulation change how applicable the BCM learning rule is? It would be helpful if the authors offer a more detailed discussion on why Hebbian forms of plasticity in interneurons fail to produce stimulus specificity.</p></disp-quote><p>Increasing the size of the model would help only if it would increase the redundancy in the Pyr population response. Otherwise, the problem can only be solved by changing the E to I ratio.</p><p>We feel that an exhaustive discussion of the dynamics of BCM in our network is beyond the scope of the paper, particularly because BCM comes in a broad variety (weight normalisation, weight limits, exact form of the sliding threshold?) and the exact behavior depends on various parameter choices. Similarly, we preferred to limit the discussion of other Hebbian rules, because it would be somewhat arbitrary which rules to discuss. Instead we added the following more abstract arguments to the Discussion section:</p><p>“We expect that the rules we suggest here are only one set of many that can establish E/I assemblies. Given that the role of the input plasticity in the interneurons is the formation of a stimulus specificity, it is tempting to assume that this could equally well be achieved by classical forms of plasticity like the Bienenstock-Cooper-Munro (BCM) rule {Bienenstock82}, which is commonly used in models of receptive field formation. However, in our hands, the combination of BCM plasticity in Pyr-&gt;PV synapses with homeostatic inhibitory plasticity in the PV-&gt;Pyr synapses showed complex dynamics, an analysis of which is beyond the scope of this article. In particular, this combination of rules often did not converge to a steady state, probably for the following reason. […]</p><p>We suspect that this instability can also arise for other Hebbian forms of plasticity in interneuron input synapses when they are combined with homeostatic inhibitory plasticity (Vogels et al., 2011) in their output synapses. The underlying reason is that for convergence, the two forms of plasticity need to work synergistically towards the same goal, i.e., the same steady state. For two arbitrary synaptic plasticity rules acting in different sets of synapses, it is likely that they aim for two different overall network configurations. Such competition can easily result in dynamics with a continuing turn-over of transiently stable states, in which the form of plasticity that acts more quickly gets to reach its goal transiently, only to be undermined by the other one later.”</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The modifications the authors have made have improved the manuscript. Addressing the following points will further enhance the clarity of the presentation, and will make the work suitable for publication in eLife.</p><p>1) Could the authors please expand on the negative result about developing selectivity in PV neurons through quenched random connections? Would it be possible to adjust the parameters of the model, such as the weight distribution or number of connections onto PV neurons, so that they would exhibit enough selectivity?</p></disp-quote><p>The reviewers raised an interesting question, which motivated us to investigate if sampling PV input weights from a distribution with an even longer tail may affect these results. In brief, the broader distribution can indeed increase the selectivity in the PV interneurons, but it is not sufficient to account for the correlation structure in the synaptic weights observed by Znamenskiy et al. The underlying reason is that the selectivity of the interneurons is then controlled by a very small number of large excitatory input weights, which are unlikely to be sampled in an experiment. We have added a new figure supplement and corresponding text to the manuscript (see below).</p><p>We would like to emphasize that our original choice of the weight distribution is motivated by observations in the mouse cortex (Song et al., 2005, Loewenstein et al., 2011), covering about two orders of magnitude. Similarly, the connection probability of 60% we used is in line mouse V1 connectivity (Znamenskiy et al., 2018). We do not exclude the possibility that the parameters in our model could in principle be fine-tuned to produce a measurable correlation between PV input and output synapses. However the underlying network model would probably not be consistent with observations from mouse V1.</p><p>“[…] A particularly strong input connection will cause the postsynaptic interneuron to prefer similar stimuli to the presynaptic Pyr. Because of the resulting correlated activity, the Hebbian nature of the output plasticity potentiates inhibitory weights for such cell pairs that are reciprocally connected. This tendency of strong input synapses to generate a strong corresponding output synapse is reflected in a positive correlation between output synapses and response similarity (Figure 3E, Figure 3—figure supplement 1A), despite the fact that input synapses remain random.</p><p>This effect further increases when input synapses are drawn from a distribution with an even heavier tail, beyond what is observed in mouse V1 [Znamenskiy et al., 2018] (Figure 3—figure supplement 2A). In this case, the stimulus tuning of the interneurons is dominated by a small number of very large synapses. The resulting higher selectivity of the interneurons (Figure 3—figure supplement 2B) allows a better co-tuning of excitation and inhibition in Pyr neurons (Figure 3—figure supplement 2C), in line with theoretical arguments for sparse connectivity [Litwin-Kumar et al., 2017]. However, the dominance of a small number of large synapses also makes it unlikely that those synapses are observed in an experiment in which a finite number of synapses is sampled. As a result, a heavier tail does not yield the correlation of reciprocal in- and output synapses observed by Znamenskiy et al., [2018] (Figure 3—figure supplement 2D,E), although it increases the probability of observing correlations between input synapses and response similarity when weak synapses are discarded. See Appendix for a more extensive discussion.”</p><disp-quote content-type="editor-comment"><p>2) Relatedly, the text around Figure 3D says that, &quot;Because of the poor stimulus tuning of the interneurons, output plasticity cannot generate stimulus-specific inhibitory inputs to the Pyr neurons (Figure 3D).&quot; But it looks like there is a definite stimulus-specific inhibitory input that emerges. The authors should clarify what they mean.</p></disp-quote><p>Thank you for pointing this out. We agree that this was not clear in the text and have extended the paragraph:</p><p>“Because of the poor stimulus tuning of the interneurons, output plasticity cannot generate stimulus-specific inhibitory inputs to the Pyr neurons (Figure 3D). Instead, they essentially receive a tonic, unspecific background inhibition that is weakly modulated by the stimulus (Figure 1—figure supplement 2B). This weak modulation is indeed correlated with the excitatory inputs, but the overall similarity between excitation and inhibition remains low (Figure 1—figure supplement 2C).”</p><disp-quote content-type="editor-comment"><p>3) It would be good if the Abstract could be revised to be accessible to a broader audience of readers.</p></disp-quote><p>We appreciate the feedback on the current Abstract and agree that particularly the first few sentences needed to be refined. We have revised the Abstract to convey the problem setting and our main findings to a broader audience. You can find the revised Abstract below:</p><p>“In contrast to sensory circuits with feature topography, neurons in the mouse primary visual cortex (V1) are not arranged according to the stimulus features they respond to. Yet, synapses between excitatory pyramidal neurons and inhibitory parvalbumin-expressing (PV) interneurons tend to be stronger for neurons that respond to similar stimulus features. The presence of such excitatory-inhibitory (E/I) neuronal assemblies indicates a stimulus-specific form of feedback inhibition despite the absence of feature topography. Here, we show that activity-dependent synaptic plasticity on input and output synapses of PV interneurons generates a circuit structure that is consistent with mouse V1. Using a computational model, we show that both forms of plasticity must act in synergy to form the observed E/I assemblies. Once established, these assemblies produce a stimulus-specific competition between pyramidal neurons. Our model suggests that activity-dependent plasticity can refine inhibitory circuits to actively shape cortical computations.”</p></body></sub-article></article>