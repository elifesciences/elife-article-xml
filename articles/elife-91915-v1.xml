<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91915</article-id><article-id pub-id-type="doi">10.7554/eLife.91915</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91915.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Heron, a Knowledge Graph editor for intuitive implementation of Python-based experimental pipelines</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Dimitriadis</surname><given-names>George</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1419-1173</contrib-id><email>g.dimitriadis@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Svahn</surname><given-names>Ella</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1976-1531</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>MacAskill</surname><given-names>Andrew F</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0196-3779</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Akrami</surname><given-names>Athena</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5711-0903</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Sainsbury Wellcome Centre, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Department of Neuroscience, Physiology and Pharmacology, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>07</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP91915</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-08-24"><day>24</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-05-01"><day>01</day><month>05</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.01.538947"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-05"><day>05</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91915.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-02-24"><day>24</day><month>02</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91915.2"/></event></pub-history><permissions><copyright-statement>© 2024, Dimitriadis et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Dimitriadis et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91915-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-91915-figures-v1.pdf"/><abstract><p>To realise a research project, experimenters face conflicting design and implementation choices across hardware and software. These include balancing ease of implementation – time, expertise, and resources – against future flexibility, the number of opaque (black box) components and reproducibility. To address this, we present Heron, a Python-based platform for constructing and running experimental and data analysis pipelines. Heron allows researchers to design experiments according to their own mental schemata, represented as a Knowledge Graph – a structure that mirrors the logical flow of an experiment. This approach speeds up implementation (and subsequent updates), while minimising black box components, increasing transparency and reproducibility. Heron supports the integration of software and hardware combinations that are otherwise too complex or costly, making it especially useful in experimental sciences with a large number of interconnected components such as robotics, neuroscience, behavioural sciences, physics, chemistry, and environmental sciences. Unlike visual-only tools, Heron combines full control (of instrument and software combinations) and flexibility with the ease of high-level programming and Graphical User Interfaces. It assumes intermediate Python proficiency and offers a clean, modular code base that encourages documentation and reuse. By removing inaccessible technical barriers, Heron enables researchers without formal engineering backgrounds to construct sophisticated, reliable and reproducible experimental setups – bridging the gap between scientific creativity and technical implementation.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Complex scientific experiments often require setting up several pieces of hardware and software that must work together seamlessly. It is crucial not only to connect these components correctly, but also to design the setup in a way that others can easily replicate and adapt for similar experiments.</p><p>Striking a balance between a system that works correctly and one that can be quickly adjusted and understood by other users can be difficult. This is especially relevant when systems incorporating complicated software and hardware are to be designed and used by scientists who may not have expertise in those areas.</p><p>Dimitriadis et al. set out to create a software tool that would help researchers of all disciplines build and run complex experiments more easily. The resulting platform, known as Heron, lets users create setups by combining visual building blocks representing parts of an experiment. These blocks are arranged in what is called a Knowledge Graph, which shows how different steps in the experiment connect in a way that closely mirrors the thought process of the researcher.</p><p>This approach makes experiments quicker to set up, easier to update, and more transparent for others to replicate or understand, especially in fields like robotics or neuroscience where complex setups are common. It also results in code that is easier to understand, maintain and share with others. These factors will help Heron to enhance how reproducible experimental setups are and allow researchers to use combinations of hardware and software that would be difficult to achieve otherwise.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>analysis</kwd><kwd>experimental control</kwd><kwd>experimental setup</kwd><kwd>software</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><award-id>GAT3755</award-id><principal-award-recipient><name><surname>Dimitriadis</surname><given-names>George</given-names></name><name><surname>Akrami</surname><given-names>Athena</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>219627/Z/19/Z</award-id><principal-award-recipient><name><surname>Dimitriadis</surname><given-names>George</given-names></name><name><surname>Akrami</surname><given-names>Athena</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A Python-based, open source, GUI application for straightforward development and execution of scientific experiments over multiple machines.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>One can divide an experiment’s life circle, from concept to a running system, into a number of transformations. At first, the scientific question is mapped into an abstract schema of experimental steps (i.e. what needs to happen in order to answer the question). Subsequently, these conceptual steps are transformed into a schema of hardware connectivity and software logic. At this stage, the experimenter thinks in terms of high-level objects like cameras and other types of sensors, time lines, triggers and events, agents and rewards, inputs and outputs. The final challenging step, which is rarely addressed (or even cognitively acknowledged), is to map the schema of the hardware and software logic to the actual hardware connectivity and operational code bases. At this level, the experimenter has to work with much lower-level objects like voltage differences, light intensities, TTL pulses, GPU shaders, and information flow loops. This last and most time-consuming step can limit the number of iterations for ideas to be piloted and tested. Once the mental schema has been translated into code, it is this code that is usually addressed by other experimentalists in reproducibility efforts. These efforts usually face a high barrier in understanding the original experimental schema starting from its code implementation. This is because efficiently translating many lines of code (even if well documented) back to what the code actually accomplishes is in most cases a difficult task that requires years of coding experience, as well as familiarity with experimental designs. This barrier hinders efforts of reproducibility and quality control and is one that cannot be addressed solely by open sourcing one’s work. Finally, the complexity of the translation from mental schema to running code base makes design iteration efforts practically impossible. The prohibitively large iteration time, on the one hand, and the inter-dependency of engineering decisions throughout the implementation cycle, on the other hand, make updates of the experimental flow extremely cumbersome. This often results in practically one of the most serious experiment design and implementation hurdles. The need to create radically new implementations for only small changes in the underlying mental schema. These arguments can be seen as the main driver for the development and the rapid acceptance of high-level languages in software engineering and of micro-controller kits (e.g. Arduino) in electrical engineering and robotics. Concepts like object-oriented programming (<xref ref-type="bibr" rid="bib12">Kindler and Krivy, 2011</xref>) or actor-based programming (<xref ref-type="bibr" rid="bib2">Agha, 1986</xref>) for example, have been nothing else but an effort to take away the low-level concepts that one needs to ultimately manipulate and replace them with bundles of higher-level ones, easier to cognise with.</p><p>In order to address this discrepancy between the mental schemata and implementation outcomes in experimental construction, we developed Heron, an open source software (MIT license) platform for the construction of data flow pipelines (e.g. experiments, data analysis, robotics, etc.). Heron comes with a series of distinguishing features. The primary one is that it creates experimental pipelines that visually and structurally bear a very significant resemblance to the original mental schema of the experimental pipeline. So what one gets as the final experiment implementation is both semantically and syntactically very close to how one originally envisions the experiment should work. Because of that, Heron creates final implementations that are easy to understand, construct, communicate, and change. In this way, it often makes it fairly easy to put together helpful diagrams and documentation by following Heron’s visual representation of the experiment and its underlying code. It also allows for accurate inference of the real-time complexity of any proposed change even before any new code is written.</p><p>A second distinguishing feature of Heron is that it targets the experimenter without expertise in arcane subjects like networking, hardware connectivity, or low-level software–hardware interactions. By abstracting away these low-level features in its Graphical User Interface (GUI), it allows the construction of experiments with multiple, diverse hardware components, even using networks of computers running different operating systems, without bothering the experimenter with having to deal with all the low-level connectivity issues that arise. It achieves this without limiting its users to preconceived ideas of how any single specific piece of hardware or code should be used. This is possible as Heron offers users an Application Programming Interface – API – letting them write the code that implements their own ideas at the optimal level of abstraction given the situation. Offering Python as the main (but not only) language for this user-centric code implementation, Heron makes it even easier for code novices to achieve highly complex experimental setups that are easy to both construct and reconfigure. In the following section, we will focus on Heron as a general-purpose tool for constructing pipelines used to conduct different types of experiments. Firstly, we will describe the specific meaning of an experimental pipeline implemented as a Knowledge Graph (KG) (<xref ref-type="bibr" rid="bib9">Fensel et al., 2020</xref>). We then catalogue the design benefits and distinguishing features offered by Heron, in comparison to other efforts targeting the construction of experiments. Subsequently, we will describe the internal architecture of Heron with enough details to allow any developer to quickly get up to speed with Heron’s code and contribute to its open source. Finally, we will illustrate a number of Heron experimental implementations, currently in use in the lab, each showcasing one of Heron’s special features.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Mental schemata, KGs and Heron processes. The philosophy behind Heron’s design</title><p>A mental schema is a psychological concept (<xref ref-type="bibr" rid="bib5">Bartlett and Burt, 1933</xref>, <xref ref-type="bibr" rid="bib22">Williams, 2019</xref>) that is meant to define the way humans cognise. According to it, when people think, they categorise sensory experiences and abstract notions into groups. They then utilise the relations between these groups to draw conclusions about some hypotheses. Upon inputs from the environment and prior cognitive outcomes, the categories and their relations can update fluidly (<xref ref-type="bibr" rid="bib22">Williams, 2019</xref>). An example, for the case of interest here, is the mental schema of an experiment. In order for an experiment to be developed, the experimenter brings together a set of categories, both based on their sensory experiences (e.g. a laser, a data acquisition board, or a camera) and on abstract notions (e.g. time concurrency or subject’s choices). Then a set of relationships is generated between them. The sets of these concepts with their interactions define the mental schema of the specific experiment. For example, a camera frame must be captured immediately after a specific event has taken place. Yet, today’s implementations of experimental pipelines are written (with visual or text-based code) such that they obfuscate the mental schemata they derive from. A receiver of such a code base, irrespective of their understanding of the underlying language, always needs a significant amount of time and mental effort to map back the initial mental schema. A KG (<xref ref-type="bibr" rid="bib9">Fensel et al., 2020</xref>) is a mathematical structure designed to capture the unstructured human knowledge on a subject (i.e. the mental schemata of different individuals relating to the same knowledge corpus) in such a way that a machine could use it to test propositions against the knowledge and also create novel propositions that a human with the knowledge would find to be true. It is practically an effort to implement the fuzzy notion of a mental schema into a concrete structure of objects and relations that is machine implementable. The fundamental structures of a KG are its nodes and their attributes. Nodes are meant to represent a group of objects at a desired level of abstraction, which does not have to be uniform among the different nodes of the same KG. Nodes’ attributes define the state of each node and the edges that connect different nodes representing their relationships. Nodes and their attributes usually have semantic labels.</p><p>Heron implements an experiment in the form of a KG and does so at two separate levels. One is the graphical level, where a series of Nodes and their in-between links (Edges) are defined. The second is the code that defines each Node’s functionality. The graphical level is used to construct a KG representing the experiment. The text-based code level is used to implement the dynamics of each Node in the Graph and define its level of abstraction. The KG’s Nodes are labelled and have human-readable attributes partially defining their state. They are also connected with directed Edges (links) through their named input and output points. The Edges represent message passing between Nodes (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). In this way, a graphical representation of an experimenter’s mental schema is created with each Node at the appropriate level of abstraction for the specific experiment. This individualised level of abstraction is achieved through Heron’s second level of implementation, that is a text-based code that defines each Node’s functionality. This is an important distinction between Heron and other node-based software like LabView and Bonsai (see Background section). Heron expects that each Node’s behaviour and connectivity is defined by the experimenter, in normal, text-based code, and comes with an appropriate API to facilitate this. In this way, Heron does not offer a long list of predefined Nodes which would make very strong assumptions about the structure of the experiment’s mental schema. Instead, it offers the tools for designing and implementing one’s own Nodes in a case-by-case approach at a level of abstraction required at any time. For instance, if in one case a Node representing a camera is required, while in another a Node should represent a large group of synchronised cameras acting as one, Heron provides the tooling to create either of these Nodes with minimum effort.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Heron’s combination of graphical and text-based code development.</title><p>(<bold>A</bold>) Schematic of Heron’s two levels of operation. Left shows a Knowledge Graph made of Nodes with Names, named Parameters and named Input/Output points. Nodes are connected with links (Edges) that define the flow of information. Right shows a piece of textual code that fully defines both the Node’s Graphical User Interface (GUI) appearance and the processes run by the machine. (<bold>B</bold>) Heron’s implementation of A, where the Knowledge Graph is generated in Heron’s Node Editor window while the code that each Node needs to be defined and the code that each Node is running is written in a separate Integrated Development Environment (IDE, in this case PyCharm).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig1-v1.tif"/></fig><p>Heron’s code base is implemented in Python. This choice was informed by the need to keep the code simple to implement while at the same time being able to interact with a very diverse list of hardware drivers and software analysis libraries (see Design benefits section). It also means that Python is the go-to language for the user’s creation of new Node behaviours. Yet the language that Heron itself is implemented in (Python) does not enforce the language new Nodes can be implemented in by Heron users. Heron offers users a comfortable starting point to implement their own Nodes (in Python), while allowing for expert users to utilise lower-level languages that offer other advantages like faster run times and lower-level control of a machine’s components (e.g. CPUs, GPUs, RAM, etc.). Heron poses very few (if any) limitations in what types of behaviour can be implemented in any Node. Breaking up the implementation into two levels of KG and running codes, as mentioned above, confers a much-required ‘break complex problems into simpler ones’ functionality at the heart of Heron’s operation.</p></sec><sec id="s2-2"><title>Background</title><sec id="s2-2-1"><title>Computational graphs</title><p>Heron’s KG approach has its root in a number of software frameworks where experimental and data analysis pipelines are constructed as a computational graph. MATLAB Simulink (<xref ref-type="bibr" rid="bib1">Acklam, 2003</xref>) and LabVIEW (<xref ref-type="bibr" rid="bib6">Bitter and Nawrocki, 2006</xref>) are two of the more well-known frameworks where experiments can be designed as computational graphs using predefined elements (i.e. nodes provided by the developers of these systems). Bonsai (<xref ref-type="bibr" rid="bib13">Lopes et al., 2015</xref>) is a new entry to this field originally designed to ease the implementation of neuroscience and behavioural science experiments. The use of directed acyclic graphs (<xref ref-type="bibr" rid="bib21">Thulasiraman and Swamy, 1992</xref>) has increased dramatically in the Big Data analytics tools where frameworks like Apache Airflow (<xref ref-type="bibr" rid="bib4">Apache, 2015</xref>) and Dask (<xref ref-type="bibr" rid="bib16">Rocklin, 2015</xref>) allow parallelisation of algorithms and data queries over large clusters of machines. Two efforts that are very similar to Heron in the way they structure computational graphs to define experimental pipelines are EPypes (<xref ref-type="bibr" rid="bib18">Semeniuta and Falkman, 2019</xref>) and the Robot Operating System (ROS) (<xref ref-type="bibr" rid="bib20">Stanford Artificial Intelligence Laboratory, 2018</xref>). EPypes is (according to the developers’ own description) a ‘Python-based software framework for developing vision algorithms in a form of computational graphs and their integration with distributed systems based on publish-subscribe communication’. The basic idea of message passing between individual processes, each responsible for its own algorithm, running on different machines is identical to Heron (even at the level of using the publish–subscribe communication protocol), although EPypes’s focus is on computer vision algorithms. The exact same idea is utilised by ROS where (in their own words again) ‘The ROS runtime ‘graph’ is a peer-to-peer network of processes (potentially distributed across machines) that are loosely coupled using the ROS communication infrastructure. ROS implements several different styles of communication, including synchronous RPC-style communication over services, asynchronous streaming of data over topics, and storage of data on a Parameter Server’.</p></sec><sec id="s2-2-2"><title>Hybrid programming</title><p>Heron’s approach is based on an existing programming idea, that is combining both visual and textual programming in a hybrid manner. One of the most widely used examples of such realisations is VVVV (<xref ref-type="bibr" rid="bib11">Holzer, 1998</xref>), a framework utilising visual programming with the C# or HLSL programming languages for textual programming. This hybrid approach was found to allow for a better retention of computer software university students, when comparing only textual or only visual styles in the learning of programming (<xref ref-type="bibr" rid="bib15">Noone and Mooney, 2018</xref>), showing that it better suits beginner level programmers (a category into which a large percentage of experimentalists fall in).</p></sec><sec id="s2-2-3"><title>Behavioural sciences toolboxes</title><p>Heron was originally conceptualised to be a framework for creating experiments in the fields of behavioural sciences (e.g. neuroscience, experimental psychology, etc.) and although its philosophy and use cases span a much wider spectrum, its current usage derives from experiments in this field. Other frameworks that specifically target the same fields are pyControl (<xref ref-type="bibr" rid="bib3">Akam et al., 2022</xref>), Bpod (Sanworks LLC, USA) (building on the central design concept of B-control; <xref ref-type="bibr" rid="bib7">Brody, 2007</xref>), and Autopilot (<xref ref-type="bibr" rid="bib17">Saunders and Wehr, 2019</xref>). Bpod and pyControl are software-dedicated hardware efforts, while Autopilot is a software framework that, in the same spirit as EPypes, ROS and Heron, allows a distributed experimental pipeline, albeit restricting the machines to Raspberry Pi computers. All three efforts pay special attention to offering their users tools for creating state machines to define their experiments (each utilising its own way of doing so). Heron currently allows the users to decide if their pipeline would benefit from a state machine design or not, and being Python-based allows for the use of a plethora of state machine tools in the Python ecosystem (<xref ref-type="bibr" rid="bib14">Macedo, 2025</xref>; <xref ref-type="bibr" rid="bib19">Sivji, 2020</xref>). This includes the capability to script Nodes that can wrap the Python APIs of pyControl and Bpod (through the Champalimaud Foundation’s pybpod API). This can be of interest to those experimentalists who have invested in the respective hardware modules but would like to expand the capabilities of their pipelines beyond the reach of pyControl or Bpod.</p></sec></sec><sec id="s2-3"><title>Design benefits</title><sec id="s2-3-1"><title>Self-documentation</title><p>The KG of Heron immediately offers a succinct overview of the experimental workflow and the dynamics it implements, thus acting also as the primary documentation of the experiment. Armed with a coherent picture of the experiment’s information flow over time, one can access the code of individual Nodes, for a deeper understanding of its details. Grasping the meaning of a few hundred lines of Python code that most Nodes require to be implemented, one Node at a time, is a much more appealing proposition than opening up a whole code base of a non-Heron experiment and being faced with thousands of lines of obscurely interconnected code arranged in a file system that only makes sense to the developer (and only for a short while after the code’s implementation). Moreover, Python code, in comparison to other lower-level languages, helps with readability in a self-documenting fashion (notwithstanding the plethora of in-code documentation tools in the Python ecosystem). This self-documentation capability of Heron’s experimental implementations confers obvious benefits to the exchange and reproducibility of experiments and minimises the possibility of misunderstandings when researchers from different groups try to interact with the experiment.</p></sec><sec id="s2-3-2"><title>Multiple machines, operating systems, and environments</title><p>In Heron, each Node runs its own process (practically its own little program, separate from all the programs of the other Nodes). This multi-process approach offers an important competence; running different Nodes on different machines (albeit by taking a hit on system resources vs a multi-threaded approach). This is important since experiments should not be constrained by the Operating System (OS) or the chip architecture that a small part of the experiment might require to run. For example, a fast, high-resolution camera might have drivers only for Windows, while raspberry pi cameras can be advantageous since they are easy to multiplex (due to the pi’s GPIO and low cost of a raspberry pi with a camera) while online, million-parameter, deep learning algorithms will definitely not run on anything other than high spec Linux machines. Heron removes the need to choose between these capabilities since its Nodes (i.e. processes) can run on any machine connected to the main one that runs Heron, through a Secure Shell Protocol (SSH) accessible network connection. When a KG is initiated (i.e. a task is launched), Heron will connect to all the defined machines in the network and will initialise whatever processes it has been directed to start at each of its predefined machines. While the experiment is running, it will take care of message passing between machines, and when the Graph is terminated, it will make sure all processes are also gracefully terminated. Since Heron uses standard Python to implement most of its Nodes (something that users, as we mentioned above, do not have to adhere to since functionality to work with other code bases exists – see the Rats playing computer games: State machines and non-Python code Nodes example), a Heron experiment can be easily defined on machines with different chip architectures, different OSes and different levels of virtualisation. The general rule is that if a machine with a certain configuration can run the scripts of Python (or other non-Python code) that define the Nodes that need to run on that machine, then that script can be part of a Heron experiment and have its Node’s inputs and outputs interact with Nodes running on other machines, all set up through Heron’s GUI and with minimal user effort. Heron itself (the GUI and underlying communication functionality) has been tested to be operational under Windows (10 and 11, both ×64), Linux PC (Ubuntu 20.04.6, ×64) MacOS, and Raspberry Pi 4 (Debian GNU/Linux 12 -bookworm-, aarch64).</p></sec><sec id="s2-3-3"><title>Python and the ease of implementing code</title><p>Finally, Python as an implementation language offers Heron another set of desirable (and some not so) consequences. These include the standard pros and cons of Python versus other computer languages. Apart from this, batteries included, Python’s approach to problems, the main advantage for experiment implementation is Python’s extensive community of developers that have contributed to the open source ecosystem of Python libraries for practically any computation imaginable. This extends to drivers and control APIs for most hardware that an experimenter might require to use. From standard data crunching algorithms to state-of-the-art machine learning ones and from serial communication to drivers controlling high-spec equipment, there is very little that has not been covered by a Python library. This wealth of ready-to-go solutions makes the two-tier approach of Heron (design the KG in a GUI and implement the Nodes’ behaviour in Python) not just a viable but the preferred approach for any experimental designer. Especially for those experimenters who may not be versed in the latest nuances of low-level computer code, but still would like to be fully in control of the behaviour of their experiment. For the cases where a user with deeper knowledge of software engineering has a specific need to use other languages, Heron offers one last benefit arising from the use of its message passing library, 0MQ (<xref ref-type="bibr" rid="bib10">Hintjens, 2013</xref>). 0MQ is a versatile and easy to use library for passing messages across different processes running on different machines. Most importantly, it includes bindings for almost all commonly used languages. Utilising this library (with minimal effort), a user can create an application in any language that communicates with a Heron Node (practically implementing a small part of Heron’s protocol in another application). Then a Python wrapper Heron Node can be made responsible for the executable’s lifetime. In this way, one has just created, with very little effort, a Node that runs an executable written in some other language but acts just like any other Heron Node passing data to and from any other Nodes.</p></sec></sec><sec id="s2-4"><title>Heron’s architecture</title><sec id="s2-4-1"><title>Node types</title><p>Heron defines three different types of Nodes, each implementing a different basic functionality of message passing. Those are the Sources, the Transforms and the Sinks (see the orange, green, and purple Nodes, respectively, in the example KG of <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The Sources are Nodes that generate data (either computationally or by reading them from a device) and thus can only transmit data through the Nodes’ outputs. The Transform Nodes can both receive and transmit data through both input and output points and are meant to allow data manipulation. Finally, the Sink Nodes can only receive data and only feature input points. The Sink Nodes are designed to either save data or talk to devices that require only computer input and not input from the external world (e.g. a motor). The Nodes’ types only exist to generate a cleaner code base by separating the three types of message manipulation (output only, input then output, and input only). There is nothing, though (except Python’s rule No 7: Readability counts), stopping a user to create side effects of functions implemented by these Nodes other than message passing, thus interacting with machines in ways different to how the Node’s type would suggest.</p></sec><sec id="s2-4-2"><title>Heron’s actor-based model</title><p>Most of Heron’s advantages over similar software tools stem from the way it structures the communication between the different Nodes (and thus processes underlying those). Heron’s processes do not allow each other to take control of each other and change each other’s state. Each process has full control of its state and will only allow another process to influence it through the passing of messages. This is known as the actor-based model. In contrast, the most commonly used Object-Oriented Programming (OOP) model will allow an object to directly change the state of another. For example, let us consider the situation where the result of an online analysis on incoming data should be used to change a camera’s gain. In an OOP world, the object responsible for the analysis would also have to carry a pointer to the memory that represents the object responsible for controlling the camera and directly change the gain (by changing the value of the instance’s gain variable) when required. But what happens when another object is introduced later on that also needs to control the gain of the same camera? What if the change of gain is also dependent on the gain’s history? Who is responsible for correctly changing this parameter when there is more than one object vying for control, and will the introduction of the new object require changing the code on both the camera and the analysis object code bases? The actor-based model solves these problems by allowing the camera’s gain to be changed only by the object that controls the camera itself. All other objects can only request such a change by sending request messages to the camera object (in Heron’s case the camera Node). In this way, when a user composes a Node, they have to think only about what that Node does and how it communicates with other Nodes, and never about the way code outside it might change its behaviour (which Heron with its actor-based model will never allow).</p></sec><sec id="s2-4-3"><title>Heron GUI’s multiple uses</title><p>To understand Heron’s code structure, one must initially appreciate its dual role in designing and running an experiment. When a Graph (short for KG) is not running, Heron acts as a Graph designer, offering a GUI where a user can create and delete Nodes, connect them with Links and assign values to their parameters. During the design period, only one process is active, the one running the Heron GUI (Editor, see <xref ref-type="fig" rid="fig2">Figure 2</xref>). When, on the other hand, a KG is running, the Heron process stops being a Graph design application and assumes the role of a director in an actor-based model (<xref ref-type="bibr" rid="bib2">Agha, 1986</xref>). It can then concurrently compute and run a GUI for the experiment where the user can update the parameters of the different Nodes on the fly (as an experimental Control Panel). In this actor-based model, each Node is represented by two processes (Worker and Communication, see <xref ref-type="fig" rid="fig2">Figure 2</xref>), while there are three more processes acting as message forwarders between all other processes (Proof of life, Parameters and Data). That means a running experiment is constituted by (Number of Nodes) × 2 + 4 processes (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Each process is an actor that can receive and transmit messages, make local decisions (i.e. decisions that can affect only itself) and determine how to respond to incoming messages. In the (most common) cases where the Nodes running are all implemented by Python code, then the Heron process is responsible for initiating the three forwarders and the com processes for all the Nodes. Each com process will then initiate the corresponding worker process. In the special case (see Rats playing computer games: State machines and non-Python code Nodes example) where a Node will call an executable instead of a Python script, then the worker process can also be responsible for initialising (and terminating) the executable’s process. As mentioned above, each Node is represented by two processes. In the code, those are called the com and worker process. The worker process is the one that runs the Node’s script defined by the user. The com process is responsible for (1) grabbing messages that come out of other Nodes and are meant to reach the Node (as defined by a Link between two Nodes in the Node Editor), (2) passing those messages to the worker process, (3) receiving any messages the worker process has to pass to other Nodes, and (4) passing those messages to the com processes of all the Nodes that should receive them. The passing of messages between com processes of different Nodes is facilitated by the Data Forwarder process. The worker processes also communicate directly with the Heron process through two separate forwarders. The Parameters forwarder is responsible for passing to the worker processes the parameter values assigned by the user to the processes’ respective Nodes on their GUI. This allows the user to also manipulate the state of each Node while an experiment is running. Through this functionality, the Heron GUI becomes (while a Graph is running) also a control centre through which an experimenter can interact with the experiment by changing live the Nodes’ parameters. The Proof-of-life forwarder is responsible for receiving an initial message from the worker process which is then passed to the GUI process, letting it know the worker is up and running.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Heron’s design principles.</title><p>(<bold>A</bold>) On the left, Heron’s Node structure showing the two processes that define a running Node and how messages are passed between them. The design of the special message defined as ‘data’ is shown on the right. (<bold>B</bold>) Heron’s processes and message passing diagram. Each rectangle represents a process. The top orange rectangle is the Heron’s Editor process, while the other three green rectangles are the three Forwarders. The Proof-of-life forwarder deals with messages relating to whether a process has initialised correctly. The parameters forwarder deals with messages that pass the different parameter values from the editor process (the Graphical User Interface [GUI]) to the individual Nodes. Finally, the data forwarder deals with the passing of the data messages between the com processes of the Nodes. The squares represent the worker (top) and com (bottom) process of three Nodes (a Source, a Transform, and a Sink from left to right). Solid arrows represent message passing between processes using either the PUB SUB or the PUSH PULL type of sockets as defined in the 0MQ protocol. Dashed line arrows represent data passing within a single process.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig2-v1.tif"/></fig><p>As mentioned above, Heron allows any of the Nodes in a Graph to be initiated and executed in machines different to the one running the main Heron process. At the level of the processes, that means that Heron, if instructed to run a Node on a different machine, will run only the worker process of that Node on the different machine while its com process will run on the same machine as Heron. That has as a drawback that a user cannot put multiple Nodes on a separate machine and expect them to interact (through messages) within that machine since all message passing happens through the com processes which will always run on the Heron running machine. Future versions of Heron will address this limitation by allowing Heron to run Graphs headless (without the GUI process being active), which will allow sub-Graphs to fully run on one machine and communicate their result to Nodes in the machine running Heron.</p></sec><sec id="s2-4-4"><title>Code architecture</title><p>Heron’s code is separated into three main folders, each pertaining to one of the three aspects of its basic functionality (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> for all of Heron’s folder structures). The Communications folder includes scripts that deal with the low-level communication between all Nodes that make up a running Graph (experiment). The GUI folder holds scripts that deal with Heron’s GUI. The Operations folder keeps all the code that defines all the Nodes that Heron recognises and can use to create a Graph with. The Operations folder is further subdivided into the Source, Transform, and Sink folders, each holding codes according to the type of the Node it implements. The Operations folder also holds symbolic links to code repositories separate from Heron. Those, assuming a specific folder structure, are recognised by Heron as valid Node describing codes. The scripts in the Communications folder are class definitions for 8 objects: 6 for the worker and com objects for each Node type which implement Heron’s communication protocol, one for the object that deals with the network connectivity (SSHCom) and one re-implementing pyzmq’s (Python’s 0MQ bindings) Socket object, adding to it the ability to pass numpy arrays and dictionaries as messages. This is required because in Heron all messages are either numpy arrays or Python dictionaries. That means that the worker functions of the worker scripts of the Source and Transform Nodes will always return either a list of numpy arrays or a list of dictionaries. Each element of the list corresponds to one output of the Node. The Operations folder has three levels of subdivision. Immediately below it are the Source, Transform and Sink folders, and inside those are folders representing groups of Nodes in each Node type (e.g. Vision for Nodes that have to do with computer vision). Inside those subcategory folders are the folders that hold the scripts for each Node (e.g. Camera which holds the scripts that read a web camera into Heron). Inside each Node’s folder, there are a minimum of two scripts with name suffixes _com and _worker. The _com script allows the user to define a Node’s characteristics (parameters, inputs and outputs) with a few lines of code and without requiring any GUI relevant code (Heron takes care of that). The _worker script is responsible for the functionality of the Node (being the script that is run by the Node’s worker process through the node_type_worker object) and implements a minimum of three functions. These are the initialisation, the worker and the end-of-life functions (the names are arbitrary and the user can define them as they please). The initialisation function is run when a Node is first started by Heron (i.e. its com process is up and running and its worker process has just started but is being tested before it starts receiving and transmitting data). The worker function is the main function that implements what the Node is supposed to do. The worker functions of the three types of Nodes are implemented differently. In the case of a Source Node, the worker function needs to be an infinite loop that somehow generates data and passes them on to the Node’s com process at the end of every loop. The Transform and Sink Nodes need a worker function implemented as a callback since their worker processes will call the worker function every time there is any data arriving at the input of the Nodes (i.e. any time their com process has received a message from another com process and has passed this to its worker process). Both the Transform and the Sink Nodes will stop accepting messages until their worker functions have returned, and Heron is designed to have no message buffering, thus automatically dropping any messages that come into a Node’s inputs while the Node’s worker function is still running. Finally, the end-of-life function will be called when a worker process hasn’t received a heartbeat signal from the source process for a pre-determined amount of time, and its role is to gracefully terminate the process.</p></sec></sec><sec id="s2-5"><title>Usage</title><p>There are two skills that a user should possess in order to aptly use Heron. Firstly, one requires a familiarity with Heron’s GUI which allows (1) downloading and installing new Nodes from existing repositories, (2) defining a local network of computers on which the different Nodes can run, (3) setting up a pipeline using the existing Nodes, and (4) running the pipeline all the while being comfortable in debugging it as problems arise. The second skill is the implementation of new Nodes based on the user’s individual needs. In this section, we will provide a basic description of both the GUI usage and the development of new Nodes.</p></sec><sec id="s2-6"><title>Using Heron’s GUI</title><sec id="s2-6-1"><title>Adding new Nodes from pre-existing code</title><p>Heron comes pre-packaged with a small set of Nodes that have a generic enough usage that most users would find useful. An important point though about Heron is that every user will be developing their own Nodes, which in most cases will take the form of code shared in some online repository. Heron is designed to easily access repos that have been developed following a specific file structure to represent a set of Heron Nodes and integrate them into its GUI and workflow without the user needing to do anything else other than create/download the repository and point Heron to it. This also simplifies the further development of Nodes by the community of users since a new Node repository does not have to interact with the main repository of Heron and thus avoids all the pitfalls of pushing, pulling, and merging code repositories at different levels of maturity.</p></sec></sec><sec id="s2-7"><title>Local network</title><p>Heron’s GUI allows an easy definition of the local access network (LAN) of machines that will run Nodes forming a single pipeline. A user has only to provide the IP, port, user name, and password of a machine in the LAN, and Heron will communicate between machines using an SSH protocol, taking care of issues like process lifecycle on different machines, opening and closing ports and proper passing of messaging between processes over the network.</p></sec><sec id="s2-8"><title>Setting up a pipeline</title><p>Once all the Nodes’ repositories have been made known to Heron and the LAN of all machines has been set up, a user needs to implement the experiment’s pipeline. This is achieved again graphically by introducing the required Nodes in the Node Editor (main window of Heron), setting up their parameter values and finally connecting the Nodes together by creating links between outputs and inputs. Heron allows many-to-many connectivity, meaning a Node’s output can connect to any number of inputs and an input can receive any number of outputs.</p></sec><sec id="s2-9"><title>Running a pipeline</title><p>Once a pipeline has been defined (generating the KG of the experiment), then running it is achieved by pressing the Start Graph button of Heron. Heron will go through each Node (in order of addition to the Node Editor) and will start the processes that the Node represents (see Heron’s Architecture for more details). It will then connect all the processes with 0MQ sockets as defined by the links between the Nodes and pass the Nodes’ parameters to the worker process of each Node. The pipeline of data being generated by the Source Nodes, being transformed by the Transform Nodes and finally saved or turned into control of machines by the Sink Nodes will keep on running until the user presses the End Graph button. At this point, Heron will gracefully terminate all processes (including the ones running on separate machines) and close down all communication sockets.</p></sec><sec id="s2-10"><title>Creating a new Node</title><p>Heron users will develop their own Nodes for their specific experiments. To facilitate this, Heron provides a series of tools. Firstly, there is a set of templates that offer a scaffold on top of which a user can build their own code. The templates have the required code elements that all valid Heron Nodes must possess and are fully documented to help a user quickly build functioning Nodes. An abbreviated and annotated Transform template for the com and worker scripts can be seen in Appendix 2. On top of Node templates, Heron provides a GUI-based system for generating new Nodes. This will create the required files and all the common between Nodes, skeleton code, appropriate to the type of Node under development and using names (for functions and variables) chosen by the user during the GUI-based creation process. Finally, in order to expedite the correct placement of new Nodes in pipelines, Heron offers a Node (User Defined Function 1I 1O) designed to allow Python functions to be run as Nodes in Heron (without the user having to follow Heron’s API). This Node can be used to simulate inputs and outputs to an under-development Node in order to test and debug its functionality before incorporating it into a full pipeline.</p></sec><sec id="s2-11"><title>Node repositories</title><p>As described above, Heron offers the tools to integrate any new code (designed with the correct file structure) into its collection of Nodes and make it available in its GUI. Although not necessary, good practice would be to develop any new Node (or closely related group of Nodes) as part of a separate repository so that the Node can be easily shared with the rest of the community. Currently, a public GitHub organisation called (rather unimaginatively) <ext-link ext-link-type="uri" xlink:href="https://github.com/Heron-Repositories">Heron-Repositories</ext-link> is hosting both the main Heron Git repository and all other Git repositories of Nodes developed to cover the developers’ experimental needs. Any of the individual Heron Nodes repositories can serve as an easy-to-follow example on the file structure expected by Heron for successful integration of new code. All of the Nodes presented in the Results paragraphs examples can be found in this repository.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have presented Heron, a new tool for coding up experimental pipelines. We have put forward the proposition that using Heron instead of the many other frameworks that one can utilise to create software to run experiments has a series of advantages. It can practically self-document, creating KG that are as close as possible to one’s mental schema of the experiment. These KGs and code bases are easy to follow by researchers other than the developers of the experiment, irrespective of the complexity of the experiment. They can trivially connect processes that run on different operating systems and machines in a single, unified pipeline. For example, a series of raspberry pi computers, each reading some cameras or other sensors, can connect and pass the data to a Linux-based, many GPU machine that does online machine learning analysis, while these results can pass to a PC machine running a computer game controlled by those results. It is using a language (Python) for the development of experiments that is one of the easiest and most versatile computer languages with a large community of developers and rich libraries for most functionalities. Finally, it is versatile enough to allow easy integration of code bases written with languages other than the one it has been developed in. We are arguing here that Heron’s learning curve, starting from a basic capability in Python, is measured in the few hours of trying to create a couple of new Nodes and joining those together in order to create some toy experiment. Once that is understood, then the limit to what can be achieved is defined by the level of Python knowledge of the developer. Heron has been conceptualised to grow into a community project. Both itself and the repositories holding extra Heron Nodes are open source under an MIT licence. The separation of repositories that hold the main Heron code and the individual Nodes’ code bases allows for the growth of a Node ecosystem where users will be able to share their development using standard repository-based tools. Finally, the developers welcome efforts for collaboration with the aim for Heron to eventually become a multi-developer, collaborative project expanding with capabilities covering the needs of experimental scientists in all experimental fields and beyond.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>Here, we showcase a number of experiments implemented in Heron. Each example has been chosen to highlight one of Heron’s competences as described in the Design benefits. As mentioned above, all the Nodes used to construct the examples presented here can be found in the Heron-Repositories GitHub organisation. We are not making public the specific experiment files since these are hardware specific and would need large changes to be made compatible to any other hardware. But Heron’s graphical nature makes it easy to go from an image capture of an experiment’s Heron GUI (see <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref>) to a working experiment by simply combining the required Nodes.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The Knowledge Graph of the Probabilistic Reversal Learning experiment.</title><p>(<bold>A</bold>) The Knowledge Graph of the Probabilistic Reversal Learning experiment showing four Nodes comprising the task, two of which (Trial Generator and Trial Controller) are connected in a loop (the output of one is fed into the input of the other). (<bold>B</bold>) The schematic or mental schema of the experiment. Notice the direct correspondence (<bold>i, ii</bold>) between the Heron Nodes and the two main components of the experiment’s mental schema, as well as the Node’s parameters and the schema components’ basic variables.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>The Probabilistic Reversal Learning experiment.</title><p>(<bold>A</bold>) A schematic of the task. (<bold>i</bold>) The block structure for the four odours (stimuli) showing the transitions over trials between blocks 1 and 2 for each odour. These can also be seen (and set for each session) in the ‘Reward Contingency Block 1’ and ‘Reward Contingency Block 2’ parameters of the ‘Trials Generator’ Node (see <bold>B</bold>). (<bold>ii</bold>) A diagrammatic description of what a block transition for one odour (in this case, odour A) means. (<bold>iii</bold>) The dynamics of a single trial. All the timings can be seen (and set) in the parameters of the ‘Trial Controller’ Node. (<bold>B</bold>) A snapshot of the experiment running. Top right image is Heron showing the full Knowledge Graph. Images to the left of the Heron Graphical User Interface (GUI) are the outputs of the different Node visualisations showing live while Heron is running. The images below the GUI are snapshots of the saved results for the two Sinks (‘Save Pandas DF’ and ‘Save FFMPEG Video’). Which output is generated by which Node is marked with green stars, blue circles, and red squares and diamonds at the bottom of the Node and the corresponding image.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig3-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The Knowledge Graph of the Fetching the cotton bud experiment.</title><p>The coloured circles at the bottom middle of each Node and the coloured star and square (where shown) are not part of the Heron Graphical User Interface (GUI) but are used in the figure to indicate in which Machine/OS/Python configuration each Node is running its worker script (circles) and which visualisation image at the bottom corresponds to which Node. For the circle, the colour code is: White = PC/Windows 11/Python 3.9, Red = PC/Windows 11/Python 3.8, Blue = Nvidia Jetson Nano/Ubuntu Linux/Python 3.9, Green = PC/Ubuntu Linux in WSL/Python 3.9. The two smaller windows below Heron’s GUI are visualisations created by the two Visualiser Nodes. The right visualisation (coming from Node Visualiser##1) is the output of the Detectron2 algorithm showing how well it is detecting the whole cotton bud (detection box 0) and the cotton bud’s tips (detection boxes 3). The left visualisation box (output of the Visualiser##0 Node) is showing the angle of the cotton bud (in an arbitrary frame of reference where the 0 degrees would be almost horizontal on the screen and the 90 almost vertical). This angle is calculated in the TL Cottonbud Angle Node, which is responsible for running the deep learning algorithm and using its inference results to calculate the angle of the cotton bud. As shown, the TL Cottonbud Angle Node is running on a Linux virtual machine (since Detectron 2 cannot run on Windows machines).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Fetching the cotton bud experiment as controlled by Heron.</title><p>The bottom image is the Heron Knowledge Graph. The top three images are the visualisations of Nodes ‘Spinnaker Camera’, ‘Visualiser##0’, and ‘Visualiser##1’. Which visualisation belongs to which Node is marked by a green star, a blue circle, and a green square at the bottom of the Nodes and its corresponding visualisation image. Visualiser##0 is receiving its values from the Angle Out output of the ‘TL Cottonbud Angle’ Node, while ‘Visualiser##1’ is receiving its frames from the Image Out output of the ‘TL Cottonbud Angle’ Node. The ‘TL Cottonbud Angle’ Node is receiving frames of the bottom of the cotton bud receptacle from the Frame Out output of the ‘Arducam Quadrascope Camera’ Node and is using one of the algorithms in the Detectron2 package (which one is defined in the ‘Model zoo yaml’ parameter of the ‘TL Cottonbud Angle’ Node) to both calculate the angle of the cotton bud and to superimpose on the receiving frame the detected boxes of the cotton bud’s tips and of the cotton bud itself. The frame of the cotton bud is captured on a Jetson Nano computer, is passed onto the Windows machine where the Heron Graphical User Interface (GUI) is running, to be then passed to the WSL virtual machine inside the Windows machine for the deep learning algorithm to use it as input. The output of the algorithm (the angle and the boxes superimposed image) is then passed back to the Windows machine to be used by the logic of the experiment running in the ‘TL Experiment Phases 1 and 3’ Node.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig4-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Computer game for rats.</title><p>(<bold>A</bold>) The experimental design (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for a detailed explanation). (<bold>B</bold>) The Heron Knowledge Graph. (<bold>C</bold>) An overview of some of the hardware connectivity of the experimental setup and the way Heron deals with the time alignment of the data packets from different Nodes. The green rectangle to the right shows how one can use Heron’s data recording facilities to correlate the frames of the Spinnaker Camera saved video (the example here highlights the last frame #216244) with the video frames of the Arducam cameras (#54071 in this example) which are generated and saved by a separate Node running on a different machine. (<bold>D</bold>) The Unity project used to create the visuals and their reactions to the commands sent by the TL Experiment Phase 2v2##0, logic Node. (<bold>E</bold>) A zoom of the parameters of the two logic Nodes in the experiment (TL Experiment Phase2v2##0 and TL Poke Controller##0). Updating those parameters during an experiment, an experimenter can control the way the arena interacts with the subject and allows Heron’s Graphical User Interface (GUI) to act as a control panel for the experiment while a Graph is running.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Heron running the ‘computer game for rats’ experiment.</title><p>(<bold>A</bold>) The experimental design. The animal can access a nose poke with two levers box (left bottom, Light Green box) and can see two screens (represented here as one rectangle, left top, Red box). The experiment detects the animal’s nose poke and presents three lines (white, yellow, and green) on a red background. If the animal presses any of the two levers, the white line rotates either clockwise or counter-clockwise. If the white line is rotated to reach the green line, then a reward availability cue (black wheel with spokes) is presented and animated, indicating to the animal that reward is available in a separate reward port. The detection of the nose poke and the levers is controlled by the ‘TL Levers##0’ Node (the joystick of the game). The timing of the availability period, the detection of the animal at the reward port, and the dispensing of reward is controlled by the ‘TL Poke Controller##0’ Node. The logic of the game is controlled by the ‘TL Experiment Phase 2v2##0’ Node which receives the input from the ‘joystick’ and decides what to show the animal and whether to reward it. Finally, the visuals of the game are controlled by the ‘TL Task2 Screens Unity##0’ Node which receives commands from the logic Node and passes them to a Unity executable which deals with the proper presentation of the sprites and their animations. (<bold>B</bold>) Heron with its live visualisations of the FLIR camera, the TTL pulses used for timing of the different events (as captured by a NIDAQ board (National Instruments)) and the different ‘sub cameras’ parts of the Arducam 4 camera system. The four coloured boxed surrounding the Nodes ‘TL Levers##0’ (Light Green), ‘TL Experiment Phase 2v2##0’ (Light Blue), ‘TL Poke Controller##0’ (Yellow), and ‘TL Task2 Screens Unity##0’ (Red) are not shown on the Heron Graphical User Interface (GUI) but are used here to indicate which Node controls which part of the experimental design shown in <bold>A</bold>. The arrows show which Node is responsible for which aspect of the experiment as it appears on the captured video streams. The correspondence between the Nodes and their visualisation is marked with green stars, a blue circle and a blue square at the bottom of the Nodes and their corresponding visualisation windows. The green window shows two time points as seen by two of the three sub cameras used in the system. In the first time point (left column of bottom green rectangle, showing sub cameras 4 and 2), the animals are pressing the left lever, making the white jagged line on the screens go towards the darker line (green on the coloured monitors). A couple of seconds later, the animals had rotated the white line to touch the green one, and the reward availability cue (dark spoked disk) was animating on the screens. The animal then rushes to receive its reward from the reward tray as shown on the second column of the sub camera captures.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-fig5-figsupp1-v1.tif"/></fig></fig-group><sec id="s4-1"><title>Probabilistic reversal learning. Implementation as self-documentation</title><p>The first experimental example is provided here to showcase how Heron’s implementation of an experiment becomes the easiest way for non-developers to acquaint themselves with the experiment and its logic. Thus, here we describe the implementation from the point of view of someone who sees it for the first time and is trying to understand what the experiment is (without accessing any other publication or written explanation). As seen in <xref ref-type="fig" rid="fig3">Figure 3</xref>, this experimental pipeline is made up of four Nodes. One is called a ‘KeyPress’, which since it connects to an input named ‘Start/Previous Trial Result’ seems to play the role of the start button of the experiment. The last Node is a ‘Save Pandas DF’ which suggests saving the output of the third Node (the ‘Trial Controller’) in a row of a pandas DataFrame. The main experiment seems to be defined by two Nodes, one named ‘Trial Generator’ and one named ‘Trial Controller’. We can immediately conceptualise the pipeline as a two-part one, where the first part generates some kind of trial state which it then outputs as its ‘Trial Definition’ output to the second Node which inputs that ‘Trial Definition’ and runs (Controls) that trial given its state. We notice that these two Nodes are reciprocally connected, meaning that the ‘Trial Generator’ requires the output of the ‘Trial Controller’ (named ‘Trial History’) to generate the definition of the next trial. Looking a bit more closely at the names of the parameters of the Nodes, we can deduce a number of things about the experiment’s structure and function. From the Trial Generator parameters, we see that trials seem to fall into two blocks. We can also see that the experiment has trials with four types of Stim (maybe short for Stimulation, or Stimuli). We can assume (but would need to verify from the code) that the Reward Block Contingencies mentioned for the two Blocks are the probabilities of a reward given the type of Stimulation. So, we have surmised that the trials come into blocks of specific length (probably a random variable drawn from some distribution from the user-provided Blocks Length Range parameter) and of specific trial type (one of four), and each trial type in each block has a user-defined reward probability. So far so intelligible. By looking at the Trial Controller Node’s parameters, we see first of all that the Node requires a COM Port and Baud Rate to be defined, showing that it is controlling some device through a serial port. The ‘Reward Only After Lick’ parameter tells us that this is an experiment where the subject needs to lick (and in some cases this is the only way to get a reward). The names of the rest of the parameters indicate that the experiment is an olfactory one (see ‘Odour Window’) where the subject gets to experience an odour (maybe one of the four stimulations described in the Trial Generator), then gets a pre-response delay, then a response window time and finally a reward window. In order to get a better picture of what the experiment is actually doing, we next look at the code. But most importantly, we know exactly where in the code base we should be looking without needing to search through tens of files to follow an obscure logic to figure out where the bits of the code that implement the actual logic are. There are two Nodes of interest, so we should, as a first step, look at the worker scripts of those two Nodes. The Trial Generator is an undaunting 127 lines of code, while the Trial Controller is 251 (empty lines included). Yet we can zoom even more in where we should initially look by simply ignoring the initialisation and end_of_life functions and just concentrate on the worker_function of each script. Those two functions are shown in the Worker_function Code in Appendix 3. We need only two pieces of prior knowledge to understand the code. One is that the worker functions of the Transform (and Sink) Nodes are callbacks that are called every time the Node receives a message to any of its inputs. Both the Trial Generator and the Trial Controller are Transforms (they have both inputs and outputs and are marked as green on the GUI). The second is that each Node outputs through the return statement of its worker function a list of numpy arrays with each element of the list being the array that is outputted by a specific output of the Node (in order from left to right in the list corresponding from top to bottom in the Node). So, a Node with two outputs will have a worker function that returns a list with two numpy arrays/dictionaries. In this case, for example, the Trial Controller’s worker function should return a list with two arrays, i.e. the Trial Result output and the Trial History one. With this in mind and looking at the Trial Generator, we can see that it takes in the previous trial’s stimulation number and, if the correct port was licked, uses this information to keep a running track of the correct licks for each port, decide which block we currently are in, decide which stimulation to generate next and offer or not a reward. By following the Trial Controller worker_function’s code, we can easily validate our prior speculation about the temporal structure of a single trial following a path of delays interspersed with stimulation and potential reward. An overview of the actual experiment running on Heron, also showcasing how Heron communicates with the Arduino board that does the hardware control, can also be seen in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, together with a schematic of the experimental timeline.</p></sec><sec id="s4-2"><title>Fetching. Four environments, three operating systems, two machines, one pipeline</title><p>This next experiment showcases Heron’s ability to run Nodes whose worker process runs on machines separate from the one running the Heron GUI. The experiment itself requires the monitoring of a rodent as it tries to fetch a cotton bud from a random point in a large arena and drop it into a shallow pot under a hole at one of its corners. The hardware setup has a Grasshopper3 USB 3.0 colour camera (GS3-U3-23S6C-C, FLIR) which records the whole of the arena from the top at 120 frames per second with HD resolution, while 4 smaller, black and white, 30 frames per second cameras (OV9281, ArduCam) are used to capture the animal from three different angles and to also monitor the target pot from underneath (since its base is transparent plastic) using the fourth camera. The experiment needs to also know when the animal has deposited the cotton bud and also record at which angle this has happened. When the animal has deposited the cotton bud, it is rewarded with a treat from a nearby reward port. The pot rotates, removing the cotton bud from the arena (while another, empty pot takes its place), and the cotton bud is thrown back into the arena for the animal to fetch again. The Grasshopper camera at the time of the experiment’s development could run only on Python 3.8. The four smaller cameras are part of a system that synchronises them, providing a stitched up single frame of 5120 × 800 pixels resolution, (ArduCam 1MP*4 Quadrascopic Monochrome Camera Kit) which can run only on either a Raspberry Pi (≥3) (Raspberry Pi Foundation), a Jetson Nano (Nvidia) or a Xavier (Nvidia) single board computers. The angle detection of the cotton bud when it is in the pot is done using Meta’s Detectron 2 (<xref ref-type="bibr" rid="bib23">Wu et al., 2019</xref>), a deep learning algorithm which we trained with a few hundred samples to detect the cotton bud’s edges when in the pot using as an input the part of the OV9281 camera frames that come from the camera underneath the pot. Detectron 2 requires an Intel or AMD CPU-based computer running Linux. Heron itself and all the other Nodes for this experiment run on a Windows PC with Python 3.9. The choice to run the Heron GUI on the Windows machine was made to keep the latency of the 120 fps camera to a minimum. So, the experimental pipeline needs the following Machine/OS/Python configurations: Intel/Windows/Python 3.9+, Intel/Windows/Python 3.8, Intel/Linux/Python 3.9+, and ARM/Linux/Python 3.9+. To create all the above configurations, we first made in the main Windows 11 machine (that Heron runs on in a Python 3.9 environment) a separate (conda) environment with Python 3.8 and the required Spinnaker (for the Flir camera) python package. Then we set up on the same machine a Windows Linux Subsystem (WLS 2) virtual machine, running Ubuntu and Python 3.9 with all the required packages to run Detectron 2. Finally, we connected the Windows machine through an LAN to an NVIDIA Jetson Nano with the Arducam Quadrascopic system running Ubuntu and Python 3.9 with all the packages for the Arducam system to operate. To summarise, the pipeline (which can be seen in <xref ref-type="fig" rid="fig4">Figure 4</xref>) when run, is utilising two physically separate machines, three operating systems (one Windows 11 and two Linux, one on a virtual machine) and four different Python environments. Once each machine/OS/Python environment is up and running and each one can run the Node(s) that it is supposed to run by itself (something that can be tested and debugged at the level of an individual Node without requiring the whole pipeline to be up and running), then assembling the pipeline is as simple as connecting the Nodes appropriately on Heron’s Node Editor and telling each one (through the Node’s secondary parameters window). Which computer it should run on and which Python executable it should call to run the worker script. Heron hides from the user all of the work required for the different processes in all the machines to start at the right time, connect correctly to each other, exchange data while the pipeline is running and finally gracefully stop when the pipeline stops without leaving hanging processes and inaccessible bits of memory all over the place. An overview of which machine runs what Node can be seen in <xref ref-type="fig" rid="fig4">Figure 4</xref> while <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows a snapshot of an animal having fetched a cotton bud while its angle is being live detected by one of the Detectron 2 algorithms.</p></sec><sec id="s4-3"><title>Rats playing computer games</title><p>The rats playing computer games experiment teaches a rat to rotate a line on a screen using a left and a right lever press to hit a target line and avoid a trap line. It presents a rat with a nose poke hole, two levers (left and right to the hole) and two screens to the animal’s front and right, when it is poking. At the final stage of training, the animal should be able to first nose poke, look at the screen at a set of jagged lines and press one of the two levers to make one of the lines rotate towards the correct line (target) and away from the wrong one (trap). Once this has been achieved, the jagged lines disappear and a separate visual cue appears (usually animated) letting the animal know that there is reward in the reward port (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). This is a very challenging behaviour for a rat, and the experiment needs several stages of shaping to teach it to the animals. Each stage of shaping towards the final behaviour requires a set of different visual stimuli and a different set of states in a rather large state machine. Also, at the conception of the experiment, there was no prior experience on the ideal path to the final behaviour, so a very malleable stimulus generation technique and state machine development was required in order for a large number of ideas to be tested in a small amount of time. Because of the above, this experiment is ideal in showcasing a series of Heron’s capabilities. These are Heron’s ability to work with non-Python code, Heron’s workload management, the tools Heron provides to keep track of complicated experiments and record all required information and finally Heron GUI’s capability to act not just as a Graph designer but also as a control panel while a Graph is running.</p></sec><sec id="s4-4"><title>State machines and non-Python code Nodes</title><p>As mentioned above, although Heron’s code base for Node development is in Python, it is relatively easy to create Nodes with code bases on different languages. One way to do this, (which is the way most scientific computing Python libraries are using), is to use Python interop technologies which allow functions written in other languages (most commonly languages of the C family) to be called by Python. Although this is very powerful, it is a painstaking task, can be very time consuming, and usually requires significant experience with both Python and whatever other language one is working with. Another, not as a low-level approach, offering less control but faster implementation times, is the use of the 0MQ library to pass messages between a Heron Node and an executable written in another language (what Heron is doing to connect all of the Nodes of a pipeline but with a significantly toned-down communication protocol). Here we took the second route. In the Unity game engine (Unity Technologies, US) we made a simple 2D game using C# code, that covered all the possible visual stimuli we needed to show to the animals at any stage (see <xref ref-type="fig" rid="fig5">Figure 5D</xref> for a snapshot of the Unity development environment). Instead of using the standard game inputs (e.g. keyboard or game controller) to control what and when was played on the screen, we used a string of commands that was received by the game executable through a 0MQ SUB socket (using NetMQ, one of many C# 0MQ bindings). The Unity-generated executable was also designed to do a handshake through a PUSH PULL socket with whatever process initiated it. This made sure the initialising process knew if anything was wrong and also was able to send some initialisation messages. Once the game executable was ready, we created a standard Heron Sink Node whose initialisation function would start the executable, handshake with it and send it the Node’s parameters (e.g. if the game was meant to show its stimuli on one or two screens). Then its worker function would just pass to the correct socket any string that would be received at its input from the other Heron Nodes. Properly formatted strings would be understood by the game and update its state accordingly. The Node’s end_of_life function would finally close down the executable when Heron’s KG was terminated. This use of process control done not directly from the main Heron process but from a Node’s worker process is compatible with the concept of Heron as an actor-based framework where an actor (a Node’s worker process) can also initialise and end other actors (in this case the Unity executable). Regarding the state machines used in this experiment, we found that no matter how large and complicated a state machine gets, the difficulty lies in its initial design and not in its implementation. Here, we used statemachine (<xref ref-type="bibr" rid="bib14">Macedo, 2025</xref>), a Python library that allows the definition of a state machine and its individual states with easily attachable callbacks at state transitions.</p></sec><sec id="s4-5"><title>CPU load</title><p>Using multiple processes rather than multiple threads does incur a hit in the total CPU load Heron will require in order to execute a Graph. The ‘Rats playing computer games’ experiment, with a 15 node Graph (the largest one we had ever had to execute over multiple experiments implemented in Heron) offers a good test bed to showcase Heron’s CPU requirements. The experiment was conducted on a Windows 10 × 64 machine with an 8 core i7-7700K CPU clocking at 4.20 GHz. The Graph runs an acquisition and save of a 120-fps 576 × 676 pixels video, an acquisition and save of a 20-kHz, 5-channel DAQ trace updated at 10 Hz, a Unity game running concurrently on two HD screens updating at 120 Hz and an acquisition and save of a 30-fps 5460 × 800 pixels video running on the Jetson Nano machine. When the computer was idle, it showed a base CPU load of just around 10–15%. When Heron was up but not running a KG, it consumed about 2–5% of the CPU for a total load of 15–20%. With the above experiment running, the CPU load hit 100% with the Unity game requiring around 24% and the remaining 45–55% used by the Graph’s 32 processes (one process was running on an NVIDIA Jetson Nano machine). The above numbers, though, are true if the operating system was allowed to assign CPU cores to the different Heron operations as it saw fit. Heron has a feature which allows the user to force the operating system to keep a worker process running on a specific CPU core. Rerunning the above Graph while setting 10 of the 15 worker processes to specific CPUs and allowing the system to optimise the load for only 5 of them (the Spinnaker Camera, Save FFMPEG Video, Arducam Quadrascopic Camera, Save Array To Binary, and TL Taks2 Screens Unity) dropped the total CPU load to 80%, again with the Unity game consuming around 24% while all Graph python processes consuming around 25%. The above specifications show that Heron is capable of running Graphs with heavy acquisition and save loads. Also, Heron’s ability to utilise multiple machines in a single Graph means that processes requiring exceptional CPU, memory and/or GPU loads can be offloaded to machines other than the one running the Heron GUI. In the above example, this is the case with the acquisition and save of the Arducam video. Also, before implementing the CPU setting feature, we used to run the Unity game on a 3rd machine in order to keep the 24% CPU load requirement out of the main Heron machine. We stopped using this 3rd machine once the CPU setting feature was in place, and the transition from three to two machines required only a single line change in the Node’s secondary window to indicate where the worker script was to be found.</p><sec id="s4-5-1"><title>Synchronisation (time alignment)</title><p>Multiple processes, running over different machines, can become extremely difficult to synchronise, especially to the tight requirements of a scientific experiment. Heron offers a set of tools to allow users to time-align every single packet of data generated and saved by any Node to any other packet irrespective of whether the packets were generated by Nodes on the same or separate machines. Here we use the ‘Rats playing computer games’ experiment to showcase how such time alignment would be achieved between a few of the experiment’s Nodes.</p><p>More specifically, the experiment has two separate video captures which, since they happen on different machines, can only be synchronised through an external clock. This is similar to an electrophysiology data acquisition system whose data points can be synchronised to other data sets (e.g videos, or button presses), only by an external clock (a TTL pulse generator usually) that is simultaneously recorded by all devices. The whole setup is shown diagrammatically in <xref ref-type="fig" rid="fig5">Figure 5C</xref>.</p><p>We use a 120-fps projector (whose output is controlled by the Node TL Projector Output##0) to act as our base clock. By pressing a key after the Graph has started running, a part of the projector’s output (depicted as a screen in <xref ref-type="fig" rid="fig5">Figure 5C</xref>) turns blue and thus stimulates a blue sensitive photodiode. This happens at every frame, thus the photodiode generates a 120-Hz TTL pulse train timed to the onset of the blue pass of the projector. That train starts when the key is pressed and stops when it is pressed a second time (thus demarcating the start and stop of the recording part of the session). The photodiode’s TTL pulse acts as an external trigger to the Spinnaker (FLIR) camera whose output is being captured by Heron’s Spinnaker Camera##0 Node. The same pulse is also fed to a National Instruments USB Data Acquisition System (NIDAQ) and to an Arduino. The NIDAQ is being captured (at 10 packets of 2000 points each per second) by Heron’s NIDAQ Analog Volt In##0 Node. The Arduino runs a small loop that generates a second TTL pulse for every four TTL pulses coming in from the Spinnaker camera (thus creating a 30-Hz TTL train). This TTL train is fed as an external trigger to the four cameras of the Arducam system as well as to the GPIO of the Jetson Nano machine that is running the Heron Node which captures the Arducam’s video frames output (Arducam Quadrascopic Camera ##0). The Arducam system generates a TTL pulse every time the global shutters of the four cameras fire, and that is fed into the same NIDAQ as the Spinnaker TTL pulse out (see left column of <xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p><p>The Spinnaker camera at every trigger generates not only a frame but also the frame’s id. This is captured by Heron’s Spinnaker Node’s Worker process and saved to its Substate dataframe together with the equivalent id of the Node’s data packet and the timestamp of this (see <xref ref-type="fig" rid="fig5">Figure 5C</xref> ‘Spinnaker Camera Worker Process Data’ table). A Substate dataframe is Heron’s way to allow the designer of a Node to save information (to a pandas dataframe) every time the Worker process runs a pass through its loop. This is done through Heron’s API, requiring only a single line of code with the information to be saved to the dataframe.</p><p>Any failures of the Node to capture a frame get registered in this correlation between camera ids and packet ids. The Spinnaker Node sends the captured frame to the SaveFFMPEGvideo##0 where an ffmpeg pipeline saves each frame to a video stream. As mentioned before, Heron does not offer any capability to buffer packets, so if the FFMPEG Node is too late to save any frame, then the next one gets lost. Heron records this in the FFMPEG Node’s Com process log file. A Com processes log file (optionally set for any Node by filling in the log file’s path to the Node’s secondary window appropriate entry) records the id of any packet that arrives from an upstream Node as given by that Node and the id of that packet as given by the current Node. In this way, any dropped packets are registered, and a one-to-one equivalence between packets sent and packets received can always be established. In this case, the FFMPEG Node’s Com process log file indicates the id of each packet that has left the Spinnaker Node and arrived in the FFMPEG Node and the id assigned to it by the FFMPEG Node. The id of the FFMPEG Node corresponds to the frame id of the video generated by the ffmpeg pipeline in the Node. In this way, we can now correlate any frame of the produced video to the id of that frame generated by the Spinnaker camera, despite some dropped frames in the process. In the shown example, the camera generated a total of 216300 frames, and the final video had a total of 216,244 frames, a drop of 56 frames dropped over the two Nodes, that is a 0.02%.</p><p>At the same time, the Arducam system (triggered by the sub-sampled TTL pulse from the Spinnaker camera) generates frames at a rate of 30 fps. These are captured and saved by the Arducam Node running on the Jetson Nano. The Node is also registering (for every incoming frame) the equivalent id of the TTL pulse that generated that frame by also reading the Jetson Nano’s GPIO port. In this way, the Arducam Node’s Worker process can generate a Substate dataframe that correlates the packet’s id as given by the process (and thus the id of the video’s frame) with the TTL pulse id that produced that frame.</p><p>In order to now correlate the frames of the Arducam video with those of the Spinnaker video, we use the saved TTL pulses from the NIDAQ instrument as saved by the Heron NIDAQ Node. The id of the frame captured by the Spinnaker camera corresponds to the number of the pulse in the Spinnaker pulse train, and the id of the pulse saved in the Jetson Nano’s GPIO port corresponds to the number of the pulse in the Arducam pulse train. By knowing which Spinnaker pulse has generated which Arducam pulse, one can work backwards through the above described correlations all the way to knowing which frame in the Spinnaker video corresponds to which frame in the Arducam one, despite those being captured on different devices and some frames getting lost by Heron’s ‘dropping of packets’ architecture. In <xref ref-type="fig" rid="fig5">Figure 5C</xref>, we show more specifically how the last pulses of the TTL trains in the NIDAQ can be used to trace the correspondence between the last frames in the videos of the Spinnaker and Arducam systems using Heron’s Substate dataframes and log files.</p></sec><sec id="s4-5-2"><title>On-the-fly experimental control</title><p>As mentioned above, the ‘Rats playing computer games’ experiment is a rather demanding task that requires slow and careful shaping on the subjects’ behaviour. Certain parts of the training could not be automated (especially when we were trying ideas that we did not know if they were relevant to the final shaping protocol). During those sessions, the experimenter had to change a number of the experiment’s parameters on the fly as the subjects produced behaviour, given both how well they were doing and how the experimenter perceived certain aspects of their behaviour that were too hard to quantify (and thus use in an automated system). Examples of these experimental parameters were the number of reward pellets offered to the subjects, the maximum angle between the moving line and the target line and the speed of the moving line (and thus the amount of time required for a lever to be kept pressed), the amount of time the reward was available after a successful trial and whether the levers would vibrate or not when pressed correctly, etc. (see <xref ref-type="fig" rid="fig5">Figure 5E</xref>). Heron allowed us to operate in this ‘experimenter in the loop’ fashion because it allows the Worker process of any Node to update its parameters as they are changed by the experimenter through the GUI while the Graph is running. Which parameters are capable of online updating is up to the author of the Node, and Heron allows both updatable and non-updatable parameters (which are easily defined in the Worker process code through Heron’s API).</p></sec></sec><sec id="s4-6"><title>Documentation and repositories</title><p>The code for all repositories used in this report can be found at the <ext-link ext-link-type="uri" xlink:href="https://github.com/Heron-Repositories">Heron GitHub</ext-link> (<xref ref-type="bibr" rid="bib8">Dimitriadis, 2025</xref>), while Heron’s documentation is hosted at <ext-link ext-link-type="uri" xlink:href="https://heron-42ad.readthedocs.io/en/latest/">Herton docs</ext-link>. Heron’s name is a tribute to one of the first known creators of automata, <ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Hero_of_Alexandria">Hero of Alexandria</ext-link>.</p></sec><sec id="s4-7"><title>Ethical approval and animal details</title><sec id="s4-7-1"><title>Ethical approval</title><p>All experiments were performed in accordance with the UK Home Office regulations Animal (Scientific Procedures) Act 1986 and the Animal Welfare and Ethical Review Body (AWERB).</p></sec></sec><sec id="s4-8"><title>Animal details</title><p>For the Fetching and Computer Games experiments, subjects were adult male Lister Hooded rats (Charles River), which were kept on a reversed 12-hr light–dark cycle. Rats were food-restricted and received chocolate pellet reward while performing the behavioural task. A total of 10 were used in the behavioural experiments. Mouse experiments for the Probabilistic Reversal Learning experiment were conducted using male WT C57/BL6 (Charles River), aged at 6–8 weeks at the start of experiments. Mice were housed in cages of two to four animals under a 12-hr light/dark cycle. Mice had ad libitum access to food, but were water restricted to 85% of base weight during behavioural training and received water rewards in the experiment. A total of four mice were used.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Methodology, Writing – original draft, Writing – review and editing, Project administration, Validation</p></fn><fn fn-type="con" id="con2"><p>Validation</p></fn><fn fn-type="con" id="con3"><p>Resources</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were performed in accordance with the UK Home Office regulations Animal (Scientific Procedures) Act 1986 and the Animal Welfare and Ethical Review Body (AWERB). For the Fetching and Computer Games experiments subjects were adult male Lister Hooded rats, which were kept on a reversed 12-hr light–dark cycle. Rats were food-restricted and received chocolate pellet reward while performing the behavioural task. A total of 10 were used in the behavioural experiments. Mouse experiments for Probabilistic Reversal Learning experiment were conducted using male WT C57/BL6 (Charles River), aged at 6–8 weeks at the start of experiments. Mice were housed in cages of two to four animals under a 12-hr light/dark cycle. Mice had ad libitum access to food, but were water restricted to 85% of base weight during behavioural training and received water rewards in the experiment. A total of four mice were used.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91915-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>No data were generated. All code is open source and available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/Heron-Repositories">https://github.com/Heron-Repositories</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Acklam</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>MATLAB Array Manipulation Tips and Tricks</source><publisher-name>No Publisher</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Agha</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1986">1986</year><source>Actors: A Model of Concurrent Computation in Distributed Systems</source><publisher-name>IEEE xplore</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/1086.001.0001</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Lustig</surname><given-names>A</given-names></name><name><surname>Rowland</surname><given-names>JM</given-names></name><name><surname>Kapanaiah</surname><given-names>SK</given-names></name><name><surname>Esteve-Agraz</surname><given-names>J</given-names></name><name><surname>Panniello</surname><given-names>M</given-names></name><name><surname>Márquez</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>MM</given-names></name><name><surname>Kätzel</surname><given-names>D</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Open-source, Python-based, hardware and software for controlling behavioural neuroscience experiments</article-title><source>eLife</source><volume>11</volume><elocation-id>e67846</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67846</pub-id><pub-id pub-id-type="pmid">35043782</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><collab>Apache</collab></person-group><year iso-8601-date="2015">2015</year><source>Apache Airflow - A Platform to Programmatically Author, Schedule, and Monitor Workflows</source><publisher-name>Apache Airflow</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlett</surname><given-names>FC</given-names></name><name><surname>Burt</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1933">1933</year><article-title>Remembering: a study in experimental and social psychology</article-title><source>British Journal of Educational Psychology</source><volume>3</volume><fpage>187</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8279.1933.tb02913.x</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bitter</surname><given-names>R</given-names></name><name><surname>Nawrocki</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>LabVIEW: Advanced Programming Techniques</source><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brody</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Bcontrol</source><publisher-name>Brody Lab</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dimitriadis</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Heron-repositories</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Heron-Repositories">https://github.com/Heron-Repositories</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fensel</surname><given-names>D</given-names></name><name><surname>Şimşek</surname><given-names>U</given-names></name><name><surname>Angele</surname><given-names>K</given-names></name><name><surname>Huaman</surname><given-names>E</given-names></name><name><surname>Kärle</surname><given-names>E</given-names></name><name><surname>Panasiuk</surname><given-names>O</given-names></name><name><surname>Toma</surname><given-names>I</given-names></name><name><surname>Umbrich</surname><given-names>J</given-names></name><name><surname>Wahler</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Introduction: What Is a Knowledge Graph Knowledge Graphs: Methodology, Tools and Selected Use Cases</source><publisher-name>Springer International Publishing</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-030-37439-6_1</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hintjens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Pieter Hintjens</source><publisher-name>O’Reilly Media</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holzer</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Vvvv - a Multipurpose Toolkit</source><publisher-name>vvvv group</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kindler</surname><given-names>E</given-names></name><name><surname>Krivy</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Object-oriented simulation of systems with sophisticated control</article-title><source>International Journal of General Systems</source><volume>40</volume><fpage>313</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1080/03081079.2010.539975</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Frazão</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><name><surname>Medina</surname><given-names>RE</given-names></name><name><surname>Calcaterra</surname><given-names>L</given-names></name><name><surname>Dreosti</surname><given-names>E</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Macedo</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Fgmacedo/python-statemachine</data-title><version designator="b0367f0">b0367f0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="http://github.com/fgmacedo/python-statemachine">http://github.com/fgmacedo/python-statemachine</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noone</surname><given-names>M</given-names></name><name><surname>Mooney</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual and textual programming languages: a systematic review of the literature</article-title><source>Journal of Computers in Education</source><volume>5</volume><fpage>149</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1007/s40692-018-0101-5</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rocklin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dask: Parallel Computation with Blocked algorithms and Task Scheduling</article-title><conf-name>Python in Science Conference</conf-name><conf-loc>Austin, Texas</conf-loc><pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-013</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Saunders</surname><given-names>JL</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Autopilot: Automating Behavioral Experiments with Lots of Raspberry Pis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/807693</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semeniuta</surname><given-names>O</given-names></name><name><surname>Falkman</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>EPypes: a framework for building event-driven data processing pipelines</article-title><source>PeerJ. Computer Science</source><volume>5</volume><elocation-id>e176</elocation-id><pub-id pub-id-type="doi">10.7717/peerj-cs.176</pub-id><pub-id pub-id-type="pmid">33816829</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sivji</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Finite-state-machine. lightweight, decorator-based python implementation of a finite state machine</data-title><version designator="16d93fe">16d93fe</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/alysivji/finite-state-machine">https://github.com/alysivji/finite-state-machine</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><collab>Stanford Artificial Intelligence Laboratory</collab></person-group><year iso-8601-date="2018">2018</year><source>Robotic Operating System</source><publisher-name>ROS</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Thulasiraman</surname><given-names>K</given-names></name><name><surname>Swamy</surname><given-names>MNS</given-names></name></person-group><year iso-8601-date="1992">1992</year><chapter-title>Directed graphs</chapter-title><person-group person-group-type="editor"><name><surname>Thulasiraman</surname><given-names>K</given-names></name></person-group><source>Graphs: Theory and Algorithms</source><publisher-name>John Wiley &amp; Sons, Ltd</publisher-name><fpage>97</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1002/9781118033104</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Thinking through death and employment: The automatic yet temporary use of schemata in everyday reasoning</article-title><source>European Journal of Cultural Studies</source><volume>22</volume><fpage>110</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1177/1367549417719061</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Kirillov</surname><given-names>A</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lo</surname><given-names>WY</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Detectron</data-title><version designator="65184fc">65184fc</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Heron folder structures</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Heron’s folder structures.</title><p>(<bold>A</bold>) The Heron folder structure together with the basic scripts that comprise Heron’s core functionality. (<bold>B</bold>) The Heron Node repository folder structure (to be found in the Operations folder for each group of Nodes in the same repository). (<bold>C</bold>) An example of a repository with four Nodes: the Weird_Camera Source in subcategory Vision, the Super_Motion_Controller Transform in subcategory Motion and the Sinks Saving_CSVs in Saving and Some_Other_Sink_Node in General. The __top__ folder with the file ignore.ignore git file is required to allow Heron to create the list of Nodes presented to the users live as its folder structure is updated with new Node folders.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-app1-fig1-v1.tif"/></fig></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Com template</title><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>An abbreviated and annotated template for the com script of a Transport Operation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-app2-fig1-v1.tif"/></fig><fig id="app2fig2" position="float"><label>Appendix 2—figure 2.</label><caption><title>An abbreviated and annotated template for the worker script of a Transport Operation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91915-app2-fig2-v1.tif"/></fig></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Com code example</title><p><code>def work_function(data, parameters):
  global vis
  global lengths_blockx
  global current_block
  global reward_contingencies
  global correct_licks


  vis = parameters [0]


  message = data[1:] # data[0] is the topic
  message =
  Socket.reconstruct_array_from_bytes_message_cv2correction(message)


  # If the message comes from the Trial controller
   if len (message)==2:
     # The Trial controller sends [previous_stim, correct_port_licks]
      previous_stim = message[0]
      correct_port_licks = message [1]


     # Add the number of correct licks to the current running sum
      correct_licks[previous_stim]=
      correct_licks[previous_stim] + correct_port_licks


     # If the number of correct licks reaches the block length of
     # that stim then swap block and zero the running sum of
     # licks for that stim
     if correct_licks[previous_stim]==
       lengths_block[current_block[previous_stim]][previous_stim]:
        create_new_block_sizes(previous_stim)
       temp = copy.copy(current_block[previous_stim])
        current_block[previous_stim]=
       int (not current_block[previous_stim])
        correct_licks[previous_stim]=0
   if vis:
         print(f’Changing block of stim {previous_stim},
         from block {temp} to
         block {current_block[previous_stim]}’)
         print(f’Current block lengths = {lengths_block}’)


  current_stim = np.random.randint(0, 4)
  current_correct_reward_port_probability =
      reward_contingencies[current_block[current_stim]][current_stim]
  current_correct_reward_port =
  np.random.binomial(1,current_correct_reward_port_probability)


  result = [np.array([current_stim,
        current_correct_reward_port,
        current_block[current_stim]])]
  return result</code></p></sec><sec sec-type="appendix" id="s11"><title>Worker code example</title><p><code>def work_function(data, parameters):
global vis
global respond_after_lick
global start_delay
global odour_window
global pre_response_delay
global response_window
global reward_window
global inter_trial_window
global trial_number
trial_number +=1

try:
    vis = parameters[0]
except:
    pass

# Get message in from previous Node
message = data[1:] # data[0] is the topic
message =
Socket.reconstruct_array_from_bytes_message_cv2correction(message)
stim = message[0]
reward_port = message [1]
block_of_stim = message [2]

if vis:
    print (f’===========Starting Trial {trial_number} ==========’)
    print(f’Current Stim = {stim},
    current correct reward port = {reward_port}’)

start_trial_time = now()
if vis:
    print (f’ ooo Waiting Start Delay {start_delay}’)
 gu.accurate_delay(start_delay * 1000)

open_stim_time = now()

if vis:
    print(’====Arduino Open Stim {}’.format (stim))
 arduino_serial.write(stim_on_commands[stim])

if vis:
    print (f’ooo Starting Odour Delay {odour_window}’)
gu.accurate_delay(odour_window * 1000)

close_stim_time = now()
if vis:
 print (f’===Arduino Close Stim {stim}’)
arduino_serial.write(stim_off_commands[stim])

if vis:
 print (f’ooo Starting Pre-Response Delay{pre_response_delay}’)
gu.accurate_delay(pre_response_delay * 1000)

correct_port_lick, start_response_time = read_arduino(reward_port)
if respond_after_lick and correct_port_lick:
   start_reward_time, end_reward_time =
   reward(correct_port_lick, reward_port)
elif respond_after_lick and not correct_port_lick:
    if vis:
       print(&quot; ooo Correct port hasn’t been licked = No reward&quot;)
    start_reward_time = datetime.datetime.now()
    end_reward_time = start_reward_time
elif not respond_after_lick:
    start_reward_time, end_reward_time =
   reward(correct_port_lick, reward_port)

if vis:
    print (f’ooo Starting Response Delay {reward_window}’)
gu.accurate_delay(reward_window * 1000)
iti = np.random.uniform(inter_trial_window[0], inter_trial_window[1])

if vis:
    print (f’ooo Starting ITI Delay {iti}’)
gu.accurate_delay(iti * 1000)
end_iti_time = now()
result = [np.array([stim, correct_port_lick]),
       np.array([start_trial_time, open_stim_time,
       close_stim_time,
       start_response_time, start_reward_time,
       end_reward_time, end_iti_time, str (stim),
       str (reward_port), str (block_of_stim),
       str (correct_port_lick)])]

if vis:
    print (f’ ooo Stim and Correct Port Lick = {result[0]}’)
    print (’============Ended Trial{trial_number}==============’)
    print (’———————————————-———————————————-————————————’)
return result</code></p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91915.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> paper introduces Heron, lightweight scientific software that is designed to streamline the implementation of complex experimental pipelines. The software is tailored for workflows that require coordinating many logical steps across interconnected hardware components with heterogeneous computing environments. The authors <bold>convincingly</bold> demonstrate Heron's utility and effectiveness in the context of behavioral experiments, addressing a growing need among experimentalists for flexible and scalable solutions that accommodate diverse and evolving hardware requirements.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91915.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors provide an open-source graphic user interface (GUI) called Heron, implemented in Python, that is designed to help experimentalists to:</p><p>(1) Design experimental pipelines and implement them in a way that is closely aligned with their mental schemata of the experiments</p><p>(2) Execute and control the experimental pipelines with numerous interconnected hardware and software on a network.</p><p>The former is achieved by representing an experimental pipeline using a Knowledge Graph and visually representing this graph in the GUI. The latter is accomplished by using an actor model to govern the interaction among interconnected nodes through messaging, implemented using ZeroMQ. The nodes themselves execute user-supplied code in, but not limited to, Python.</p><p>Using three showcases of behavioral experiments on rats, the authors highlighted four benefits of their software design:</p><p>(1) The knowledge graph serves as a self-documentation of the logic of the experiment, enhancing the readability and reproducibility of the experiment,</p><p>(2) The experiment can be executed in a distributed fashion across multiple machines that each has different operating system or computing environment, such that the experiment can take advantage of hardware that sometimes can only work on a specific computer/OS, a commonly seen issue nowadays,</p><p>(3) The users supply their own Python code for node execution that is supposed to be more friendly to those who do not have a strong programming background,</p><p>(4) The GUI can also be used as an experiment control panel for users to control/update parameters on the fly.</p><p>Strengths:</p><p>(1) The software is light-weight and open-source, provides a clean and easy-to-use GUI,</p><p>(2) The software answers the need of experimentalists, particularly in the field of behavioral science, to deal with the diversity of hardware that becomes restricted to run on dedicated systems. It can also be widely adopted in many other experimental settings.</p><p>(3) The software has a solid design that seems to be functionally reliable and useful under many conditions, demonstrated by a number of sophisticated experimental setups.</p><p>(4) The software is well documented. The authors pay special attention to documenting the usage of the software and setting up experiments using this software.</p><p>Comments on revisions: The authors have addressed my concerns from the initial review.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91915.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dimitriadis</surname><given-names>George</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Svahn</surname><given-names>Ella</given-names></name><role specific-use="author">Author</role><aff><institution>University of Edinburgh</institution><addr-line><named-content content-type="city">Edinburgh</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>MacAskill</surname><given-names>Andrew F</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Akrami</surname><given-names>Athena</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The authors have created a system for designing and running experimental pipelines to control and coordinate different programs and devices during an experiment, called Heron. Heron is based around a graphical tool for creating a Knowledge Graph made up of nodes connected by edges, with each node representing a separate Python script, and each edge being a communication pathway connecting a specific output from one node to an iput on another. Each node also has parameters that can be set by the user during setup and runtime, and all of this behavior is concisely specified in the code that defines each node. This tool tries to marry the ease of use, clarity, and selfdocumentation of a purely graphical system like Bonsai with the flexibility and power of a purely code-based system like Robot Operating System (ROS).</p><p>Strengths:</p><p>The underlying idea behind Heron, of combining a graphical design and execution tool with nodes that are made as straightforward Python scripts seems like a great way to get the relative strengths of each approach. The graphical design side is clear, selfexplanatory, and self-documenting, as described in the paper. The underlying code for each node tends to also be relatively simple and straightforward, with a lot of the complex communication architecture successfully abstracted away from the user. This makes it easy to develop new nodes, without needing to understand the underlying communications between them. The authors also provide useful and well-documented templates for each type of node to further facilitate this process. Overall this seems like it could be a great tool for designing and running a wide variety of experiments, without requiring too much advanced technical knowledge from the users.</p><p>The system was relatively easy to download and get running, following the directions and already has a significant amount of documentation available to explain how to use it and expand its capabilities. Heron has also been built from the ground up to easily incorporate nodes stored in separate Git repositories and to thus become a large community-driven platform, with different nodes written and shared by different groups. This gives Heron a wide scope for future utility and usefulness, as more groups use it, write new nodes, and share them with the community. With any system of this sort, the overall strength of the system is thus somewhat dependent on how widely it is used and contributed to, but the authors did a good job of making this easy and accessible for people who are interested. I could certainly see Heron growing into a versatile and popular system for designing and running many types of experiments.</p><p>Weaknesses:</p><p>(1) The number one thing that was missing from the paper was any kind of quantification of the performance of Heron in different circumstances. Several useful and illustrative examples were discussed in depth to show the strengths and flexibility of Heron, but there was no discussion or quantification of performance, timing, or latency for any of these examples. These seem like very important metrics to measure and discuss when creating a new experimental system.</p></disp-quote><p>Heron is practically a thin layer of obfuscation of signal passing across processes. Given its design approach it is up to the code of each Node to deal with issues of timing, synching and latency and thus up to each user to make sure the Nodes they author fulfil their experimental requirements. Having said that, Heron provides a large number of tools to allow users to optimise the generated Knowledge Graphs for their use cases. To showcase these tools, we have expanded on the third experimental example in the paper with three extra sections, two of which relate to Heron’s performance and synching capabilities. One is focusing on Heron’s CPU load requirements (and existing Heron tools to keep those at acceptable limits) and another focusing on post experiment synchronisation of all the different data sets a multi Node experiment generates.</p><disp-quote content-type="editor-comment"><p>(2) After downloading and running Heron with some basic test Nodes, I noticed that many of the nodes were each using a full CPU core on their own. Given that this basic test experiment was just waiting for a keypress, triggering a random number generator, and displaying the result, I was quite surprised to see over 50% of my 8-core CPU fully utilized. I don’t think that Heron needs to be perfectly efficient to accomplish its intended purpose, but I do think that some level of efficiency is required. Some optimization of the codebase should be done so that basic tests like this can run with minimal CPU utilization. This would then inspire confidence that Heron could deal with a real experiment that was significantly more complex without running out of CPU power and thus slowing down.</p></disp-quote><p>The original Heron allowed the OS to choose how to manage resources over the required process. We were aware that this could lead to significant use of CPU time, as well as occasionally significant drop of packets (which was dependent on the OS and its configuration). This drop happened mainly when the Node was running a secondary process (like in the Unity game process in the 3rd example). To mitigate these problems, we have now implemented a feature allowing the user to choose the CPU that each Node’s worker function runs on as well as any extra processes the worker process initialises. This is accessible from the Saving secondary window of the node. This stops the OS from swapping processes between CPUs and eliminates the dropping of packages due to the OS behaviour. It also significantly reduces the utilised CPU time. To showcase this, we initially run the simple example mentioned by the reviewer. The computer running only background services was using 8% of CPU (8 cores). With Heron GUI running but with no active Graph, the CPU usage went to 15%. With the Graph running and Heron’s processes running on OS attributed CPU cores, the total CPU was at 65% (so very close to the reviewer’s 50%). By choosing a different CPU core for each of the three worker processes the CPU went down to 47% and finally when all processes were forced to run on the same CPU core the CPU load dropped to 30%. So, Heron in its current implementation running its GUI and 3 Nodes takes 22% of CPU load. This is still not ideal but is a consequence of the overhead of running multiple processes vs multiple threads. We believe that, given Heron’s latest optimisation, offering more control of system management to the user, the benefits of multi process applications outweigh this hit in system resources.</p><p>We have also increased the scope of the third example we provide in the paper and there we describe in detail how a full-scale experiment with 15 Nodes (which is the upper limit of number of Nodes usually required in most experiments) impacts CPU load.</p><p>Finally, we have added on Heron’s roadmap projects extra tasks focusing only on optimisation (profiling and using Numba for the time critical parts of the Heron code).</p><disp-quote content-type="editor-comment"><p>(3) I was also surprised to see that, despite being meant specifically to run on and connect diverse types of computer operating systems and being written purely in Python, the Heron Editor and GUI must be run on Windows. This seems like an unfortunate and unnecessary restriction, and it would be great to see the codebase adjusted to make it fully crossplatform-compatible.</p></disp-quote><p>This point was also mentioned by reviewer 2. This was a mistake on our part and has now been corrected in the paper. Heron (GUI and underlying communication functionality) can run on any machine that the underlying python libraries run, which is Windows, Linux (both for x86 and Arm architectures) and MacOS. We have tested it on Windows (10 and 11, both x64), Linux PC (Ubuntu 20.04.6, x64) and Raspberry Pi 4 (Debian GNU/Linux 12 (bookworm), aarch64). The Windows and Linux versions of Heron have undergone extensive debugging and all of the available Nodes (that are not OS specific) run on those two systems. We are in the process of debugging the Nodes’ functionality for RasPi. The MacOS version, although functional requires further work to make sure all of the basic Nodes are functional (which is not the case at the moment). We have also updated our manuscript (Multiple machines, operating systems and environments) to include the above information.</p><disp-quote content-type="editor-comment"><p>(4) Lastly, when I was running test experiments, sometimes one of the nodes, or part of the Heron editor itself would throw an exception or otherwise crash. Sometimes this left the Heron editor in a zombie state where some aspects of the GUI were responsive and others were not. It would be good to see a more graceful full shutdown of the program when part of it crashes or throws an exception, especially as this is likely to be common as people learn to use it. More problematically, in some of these cases, after closing or force quitting Heron, the TCP ports were not properly relinquished, and thus restarting Heron would run into an &quot;address in use&quot; error. Finding and killing the processes that were still using the ports is not something that is obvious, especially to a beginner, and it would be great to see Heron deal with this better. Ideally, code would be introduced to carefully avoid leaving ports occupied during a hard shutdown, and furthermore, when the address in use error comes up, it would be great to give the user some idea of what to do about it.</p></disp-quote><p>A lot of effort has been put into Heron to achieve graceful shut down of processes, especially when these run on different machines that do not know when the GUI process has closed. The code that is being suggested to avoid leaving ports open has been implemented and this works properly when processes do not crash (Heron is terminated by the user) and almost always when there is a bug in a process that forces it to crash. In the version of Heron available during the reviewing process there were bugs that caused the above behaviour (Node code hanging and leaving zombie processes) on MacOS systems. These have now been fixed. There are very seldom instances though, especially during Node development, that crashing processes will hang and need to be terminated manually. We have taken on board the reviewer’s comments that users should be made more aware of these issues and have also described this situation in the Debugging part of Heron’s documentation. There we explain the logging and other tools Heron provides to help users debug their own Nodes and how to deal with hanging processes.</p><p>Heron is still in alpha (usable but with bugs) and the best way to debug it and iron out all the bugs in all use cases is through usage from multiple users and error reporting (we would be grateful if the errors the reviewer mentions could be reported in Heron’s github Issues page). We are always addressing and closing any reported errors, since this is the only way for Heron to transition from alpha to beta and eventually to production code quality.</p><disp-quote content-type="editor-comment"><p>Overall I think that, with these improvements, this could be the beginning of a powerful and versatile new system that would enable flexible experiment design with a relatively low technical barrier to entry. I could see this system being useful to many different labs and fields.</p></disp-quote><p>We thank the reviewer for positive and supportive words and for the constructive feedbacks. We believe we have now addressed all the raised concerns.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors provide an open-source graphic user interface (GUI) called Heron, implemented in Python, that is designed to help experimentalists to</p><p>(1) design experimental pipelines and implement them in a way that is closely aligned with their mental schemata of the experiments,</p><p>(2) execute and control the experimental pipelines with numerous interconnected hardware and software on a network.</p><p>The former is achieved by representing an experimental pipeline using a Knowledge Graph and visually representing this graph in the GUI. The latter is accomplished by using an actor model to govern the interaction among interconnected nodes through messaging, implemented using ZeroMQ. The nodes themselves execute user-supplied code in, but not limited to, Python.</p><p>Using three showcases of behavioral experiments on rats, the authors highlighted three benefits of their software design:</p><p>(1) the knowledge graph serves as a self-documentation of the logic of the experiment, enhancing the readability and reproducibility of the experiment,</p><p>(2) the experiment can be executed in a distributed fashion across multiple machines that each has a different operating system or computing environment, such that the experiment can take advantage of hardware that sometimes can only work on a specific computer/OS, a commonly seen issue nowadays,</p><p>(3) he users supply their own Python code for node execution that is supposed to be more friendly to those who do not have a strong programming background.</p><p>Strengths:</p><p>(1) The software is light-weight and open-source, provides a clean and easy-to-use GUI,</p><p>(2) The software answers the need of experimentalists, particularly in the field of behavioral science, to deal with the diversity of hardware that becomes restricted to run on dedicated systems.</p><p>(3) The software has a solid design that seems to be functionally reliable and useful under many conditions, demonstrated by a number of sophisticated experimental setups.</p><p>(4) The software is well documented. The authors pay special attention to documenting the usage of the software and setting up experiments using this software.</p><p>Weaknesses:</p><p>(1) While the software implementation is solid and has proven effective in designing the experiment showcased in the paper, the novelty of the design is not made clear in the manuscript. Conceptually, both the use of graphs and visual experimental flow design have been key features in many widely used softwares as suggested in the background section of the manuscript. In particular, contrary to the authors’ claim that only pre-defined elements can be used in Simulink or LabView, Simulink introduced MATLAB Function Block back in 2011, and Python code can be used in LabView since 2018. Such customization of nodes is akin to what the authors presented.</p></disp-quote><p>In the Heron manuscript we have provided an extensive literature review of existing systems from which Heron has borrowed ideas. We never wished to say that graphs and visual code is what sets Heron apart since these are technologies predating Heron by many years and implemented by a large number of software. We do not believe also that we have mentioned that LabView or Simulink can utilise only predefined nodes. What we have said is that in such systems (like LabView, Simulink and Bonsai) the focus of the architecture is on prespecified low level elements while the ability for users to author their own is there but only as an afterthought. The difference with Heron is that in the latter the focus is on the users developing their own elements. One could think of LabView style software as node-based languages (with low level visual elements like loops and variables) that also allow extra scripting while Heron is a graphical wrapper around python where nodes are graphical representations of whole processes. To our knowledge there is no other software that allows the very fast generation of graphical elements representing whole processes whose communication can also be defined graphically. Apart from this distinction, Heron also allows a graphical approach to writing code for processes that span different machines which again to our knowledge is a novelty of our approach and one of its strongest points towards ease of experimental pipeline creation (without sacrificing expressivity).</p><disp-quote content-type="editor-comment"><p>(2) The authors claim that the knowledge graph can be considered as a self-documentation of an experiment. I found it to be true to some extent. Conceptually it’s a welcoming feature and the fact that the same visualization of the knowledge graph can be used to run and control experiments is highly desirable (but see point 1 about novelty). However, I found it largely inadequate for a person to understand an experiment from the knowledge graph as visualized in the GUI alone. While the information flow is clear, and it seems easier to navigate a codebase for an experiment using this method, the design of the GUI does not make it a one-stop place to understand the experiment. Take the Knowledge Graph in Supplementary Figure 2B as an example, it is associated with the first showcase in the result section highlighting this self-documentation capability. I can see what the basic flow is through the disjoint graph where (1) one needs to press a key to start a trial, and (2) camera frames are saved into an avi file presumably using FFMPEG. Unfortunately, it is not clear what the parameters are and what each block is trying to accomplish without the explanation from the authors in the main text. Neither is it clear about what the experiment protocol is without the help of Supplementary Figure 2A.</p><p>In my opinion, text/figures are still key to documenting an experiment, including its goals and protocols, but the authors could take advantage of the fact that they are designing a GUI where this information, with properly designed API, could be easily displayed, perhaps through user interaction. For example, in Local Network -&gt; Edit IPs/ports in the GUI configuration, there is a good tooltip displaying additional information for the &quot;password&quot; entry. The GUI for the knowledge graph nodes can very well utilize these tooltips to show additional information about the meaning of the parameters, what a node does, etc, if the API also enforces users to provide this information in the form of, e.g., Python docstrings in their node template. Similarly, this can be applied to edges to make it clear what messages/data are communicated between the nodes. This could greatly enhance the representation of the experiment from the Knowledge graph.</p></disp-quote><p>In the first showcase example in the paper “Probabilistic reversal learning.</p><p>Implementation as self-documentation” we go through the steps that one would follow in order to understand the functionality of an experiment through Heron’s Knowledge Graph. The Graph is not just the visual representation of the Nodes in the GUI but also their corresponding code bases. We mention that the way Heron’s API limits the way a Node’s code is constructed (through an Actor based paradigm) allows for experimenters to easily go to the code base of a specific Node and understand its 2 functions (initialisation and worker) without getting bogged down in the code base of the whole Graph (since these two functions never call code from any other Nodes). Newer versions of Heron facilitate this easy access to the appropriate code by also allowing users to attach to Heron their favourite IDE and open in it any Node’s two scripts (worker and com) when they double click on the Node in Heron’s GUI. On top of this, Heron now (in the versions developed as answers to the reviewers’ comments) allows Node creators to add extensive comments on a Node but also separate comments on the Node’s parameters and input and output ports. Those can be seen as tooltips when one hovers over the Node (a feature that can be turned off or on by the Info button on every Node).</p><p>As Heron stands at the moment we have not made the claim that the Heron GUI is the full picture in the self-documentation of a Graph. We take note though the reviewer’s desire to have the GUI be the only tool a user would need to use to understand an experimental implementation. The solution to this is the same as the one described by the reviewer of using the GUI to show the user the parts of the code relevant to a specific Node without the user having to go to a separate IDE or code editor. The reason this has not been implemented yet is the lack of a text editor widget in the underlying gui library (DearPyGUI). This is in their roadmap for their next large release and when this exists we will use it to implement exactly the idea the reviewer is suggesting, but also with the capability to not only read comments and code but also directly edit a Node’s code (see Heron’s roadmap). Heron’s API at the moment is ideal for providing such a text editor straight from the GUI.</p><disp-quote content-type="editor-comment"><p>(3) The design of Heron was primarily with behavioral experiments in mind, in which highly accurate timing is not a strong requirement. Experiments in some other areas that this software is also hoping to expand to, for example, electrophysiology, may need very strong synchronization between apparatus, for example, the record timing and stimulus delivery should be synced. The communication mechanism implemented in Heron is asynchronous, as I understand it, and the code for each node is executed once upon receiving an event at one or more of its inputs. The paper, however, does not include a discussion, or example, about how Heron could be used to address issues that could arise in this type of communication. There is also a lack of information about, for example, how nodes handle inputs when their ability to execute their work function cannot keep up with the frequency of input events. Does the publication/subscription handle the queue intrinsically? Will it create problems in real-time experiments that make multiple nodes run out of sync? The reader could benefit from a discussion about this if they already exist, and if not, the software could benefit from implementing additional mechanisms such that it can meet the requirements from more types of experiments.</p></disp-quote><p>In order to address the above lack of explanation (that also the first reviewer pointed out) we expanded the third experimental example in the paper with three more sections. One focuses solely on explaining how in this example (which acquires and saves large amounts of data from separate Nodes running on different machines) one would be able to time align the different data packets generated in different Nodes to each other. The techniques described there are directly implementable on experiments where the requirements of synching are more stringent than the behavioural experiment we showcase (like in ephys experiments).</p><p>Regarding what happens to packages when the worker function of a Node is too slow to handle its traffic, this is mentioned in the paper (Code architecture paragraph): “Heron is designed to have no message buffering, thus automatically dropping any messages that come into a Node’s inputs while the Node’s worker function is still running.” This is also explained in more detail in Heron’s documentation. The reasoning for a no buffer system (as described in the documentation) is that for the use cases Heron is designed to handle we believe there is no situation where a Node would receive large amounts of data in bursts while very little data during the rest of the time (in which case a buffer would make sense). Nodes in most experiments will either be data intensive but with a constant or near constant data receiving speed (e.g. input from a camera or ephys system) or will have variable data load reception but always with small data loads (e.g. buttons). The second case is not an issue and the first case cannot be dealt with a buffer but with the appropriate code design, since buffering data coming in a Node too slow for its input will just postpone the inevitable crash. Heron’s architecture principle in this case is to allow these ‘mistakes’ (i.e. package dropping) to happen so that the pipeline continues to run and transfer the responsibility of making Nodes fast enough to the author of each Node. At the same time Heron provides tools (see the Debugging section of the documentation and the time alignment paragraph of the “Rats playing computer games” example in the manuscript) that make it easy to detect package drops and either correct them or allow them but also allow time alignment between incoming and outgoing packets. In the very rare case where a buffer is required Heron’s do-it-yourself logic makes it easy for a Node developer to implement their own Node specific buffer.</p><disp-quote content-type="editor-comment"><p>(4) The authors mentioned in &quot;Heron GUI’s multiple uses&quot; that the GUI can be used as an experimental control panel where the user can update the parameters of the different Nodes on the fly. This is a very useful feature, but it was not demonstrated in the three showcases. A demonstration could greatly help to support this claim.</p></disp-quote><p>As the reviewer mentions, we have found Heron’s GUI double role also as an experimental on-line controller a very useful capability during our experiments. We have expanded the last experimental example to also showcase this by showing how on the “Rats playing computer games” experiment we used the parameters of two Nodes to change the arena’s behaviour while the experiment was running, depending on how the subject was behaving at the time (thus exploring a much larger set of parameter combinations, faster during exploratory periods of our shaping protocols construction).</p><disp-quote content-type="editor-comment"><p>(5) The API for node scripts can benefit from having a better structure as well as having additional utilities to help users navigate the requirements, and provide more guidance to users in creating new nodes. A more standard practice in the field is to create three abstract Python classes, Source, Sink, and Transform that dictate the requirements for initialisation, work_function, and on_end_of_life, and provide additional utility methods to help users connect between their code and the communication mechanism. They can be properly docstringed, along with templates. In this way, the com and worker scripts can be merged into a single unified API. A simple example that can cause confusion in the worker script is the &quot;worker_object&quot;, which is passed into the initialise function. It is unclear what this object this variable should be, and what attributes are available without looking into the source code. As the software is also targeting those who are less experienced in programming, setting up more guidance in the API can be really helpful. In addition, the self-documentation aspect of the GUI can also benefit from a better structured API as discussed in point 2 above.</p></disp-quote><p>The reviewer is right that using abstract classes to expose to users the required API would be a more standard practice. The reason we did not choose to do this was to keep Heron easily accessible to entry level Python programmers who do not have familiarity yet with object oriented programming ideas. So instead of providing abstract classes we expose only the implementation of three functions which are part of the worker classes but the classes themselves are not seen by the users of the API. The point about the users’ accessibility to more information regarding a few objects used in the API (the worker object for example) has been taken on board and we have now addressed this by type hinting all these objects both in the templates and more importantly in the automatically generated code that Heron now creates when a user chooses to create a Node graphically (a feature of Heron not present in the version available in the initial submission of this manuscript).</p><disp-quote content-type="editor-comment"><p>(6) The authors should provide more pre-defined elements. Even though the ability for users to run arbitrary code is the main feature, the initial adoption of a codebase by a community, in which many members are not so experienced with programming, is the ability for them to use off-the-shelf components as much as possible. I believe the software could benefit from a suite of commonly used Nodes.</p></disp-quote><p>There are currently 12 Node repositories in the Heron-repositories project on Github with more than 30 Nodes, 20 of which are general use (not implementing a specific experiment’ logic). This list will continue to grow but we fully appreciate the truth of the reviewer’s comment that adoption will depend on the existence of a large number of commonly used Nodes (for example Numpy, and OpenCV Nodes) and are working towards this goal.</p><disp-quote content-type="editor-comment"><p>(7) It is not clear to me if there is any capability or utilities for testing individual nodes without invoking a full system execution. This would be critical when designing new experiments and testing out each component.</p></disp-quote><p>There is no capability to run the code of an individual Node outside Heron’s GUI. A user could potentially design and test parts of the Node before they get added into a Node but we have found this to be a highly inefficient way of developing new Nodes. In our hands the best approach for Node development was to quickly generate test inputs and/or outputs using the “User Defined Function 1I 1O” Node where one can quickly write a function and make it accessible from a Node. Those test outputs can then be pushed in the Node under development or its outputs can be pushed in the test function, to allow for incremental development without having to connect it to the Nodes it would be connected in an actual pipeline. For example, one can easily create a small function that if a user presses a key will generate the same output (if run from a “User Defined Function 1I 1O” Node) as an Arduino Node reading some buttons. This output can then be passed into an experiment logic Node under development that needs to do something with this input. In this way during a Node development Heron allows the generation of simulated hardware inputs and outputs without actually running the actual hardware. We have added this way of developing Nodes also in our manuscript (Creating a new Node).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>The authors present a Python tool, Heron, that provides a framework for defining and running experiments in a lab setting (e.g. in behavioural neuroscience). It consists of a graphical editor for defining the pipeline (interconnected nodes with parameters that can pass data between them), an API for defining the nodes of these pipelines, and a framework based on ZeroMQ, responsible for the overall control and data exchange between nodes. Since nodes run independently and only communicate via network messages, an experiment can make use of nodes running on several machines and in separate environments, including on different operating systems.</p><p>Strengths:</p><p>As the authors correctly identify, lab experiments often require a hodgepodge of separate hardware and software tools working together. A single, unified interface for defining these connections and running/supervising the experiment, together with flexibility in defining the individual subtasks (nodes) is therefore a very welcome approach. The GUI editor seems fairly intuitive, and Python as an accessible programming environment is a very sensible choice. By basing the communication on the widely used ZeroMQ framework, they have a solid base for the required non-trivial coordination and communication. Potential users reading the paper will have a good idea of how to use the software and whether it would be helpful for their own work. The presented experiments convincingly demonstrate the usefulness of the tool for realistic scientific applications.</p><p>Weaknesses:</p><p>(1) In my opinion, the authors somewhat oversell the reproducibility and &quot;selfdocumentation&quot; aspect of their solution. While it is certainly true that the graph representation gives a useful high-level overview of an experiment, it can also suffer from the same shortcomings as a &quot;pure code&quot; description of a model - if a user gives their nodes and parameters generic/unhelpful names, reading the graph will not help much.</p></disp-quote><p>This is a problem that to our understanding no software solution can possibly address. Yet having a visual representation of how different inputs and outputs connect to each other we argue would be a substantial benefit in contrast to the case of “pure code” especially when the developer of the experiment has used badly formatted variable names.</p><disp-quote content-type="editor-comment"><p>(2) Making the link between the nodes and the actual code is also not straightforward, since the code for the nodes is spread out over several directories (or potentially even machines), and not directly accessible from within the GUI.</p></disp-quote><p>This is not accurate. The obligatory code of a Node always exists within a single folder and Heron’s API makes it rather cumbersome to spread scripts relating to a Node across separate folders. The Node folder structure can potentially be copied over different machines but this is why Heron is tightly integrated with git practices (and even politely asks the user with popup windows to create git repositories of any Nodes they create whilst using Heron’s automatic Node generator system). Heron’s documentation is also very clear on the folder structure of a Node which keeps the required code always in the same place across machines and more importantly across experiments and labs. Regarding the direct accessibility of the code from the GUI, we took on board the reviewers’ comments and have taken the first step towards correcting this. Now one can attach to Heron their favourite IDE and then they can double click on any Node to open its two main scripts (com and worker) in that IDE embedded in whatever code project they choose (also set in Heron’s settings windows). On top of this, Heron now allows the addition of notes both for a Node and for all its parameters, inputs and outputs which can be viewed by hovering the mouse over them on the Nodes’ GUIs. The final step towards GUI-code integration will be to have a Heron GUI code editor but this is something that has to wait for further development from Heron’s underlying GUI library DearPyGUI.</p><disp-quote content-type="editor-comment"><p>(3) The authors state that &quot;[Heron’s approach] confers obvious benefits to the exchange and reproducibility of experiments&quot;, but the paper does not discuss how one would actually exchange an experiment and its parameters, given that the graph (and its json representation) contains user-specific absolute filenames, machine IP addresses, etc, and the parameter values that were used are stored in general data frames, potentially separate from the results. Neither does it address how a user could keep track of which versions of files were used (including Heron itself).</p></disp-quote><p>Heron’s Graphs, like any experimental implementation, must contain machine specific strings. These are accessible either from Heron’s GUI when a Graph json file is opened or from the json file itself. Heron in this regard does not do anything different to any other software, other than saving the graphs into human readable json files that users can easily manipulate directly.</p><p>Heron provides a method for users to save every change of the Node parameters that might happen during an experiment so that it can be fully reproduced. The dataframes generated are done so in the folders specified by the user in each of the Nodes (and all those paths are saved in the json file of the Graph). We understand that Heron offers a certain degree of freedom to the user (Heron’s main reason to exist is exactly this versatility) to generate data files wherever they want but makes sure every file path gets recorded for subsequent reproduction. So, Heron behaves pretty much exactly like any other open source software. What we wanted to focus on as the benefits of Heron on exchange and reproducibility was the ability of experimenters to take a Graph from another lab (with its machine specific file paths and IP addresses) and by examining the graphical interface of it to be able to quickly tweak it to make it run on their own systems. That is achievable through the fact that a Heron experiment will be constructed by a small amount of Nodes (5 to 15 usually) whose file paths can be trivially changed in the GUI or directly in the json file while the LAN setup of the machines used can be easily reconstructed from the information saved in the secondary GUIs.</p><p>Where Heron needs to improve (and this is a major point in Heron’s roadmap) is the need to better integrate the different saved experiments with the git versions of Heron and the Nodes that were used for that specific save. This, we appreciate is very important for full reproducibility of the experiment and it is a feature we will soon implement. More specifically users will save together with a graph the versions of all the used repositories and during load the code base utilised will come from the recorded versions and not from the current head of the different repositories. This is a feature that we are currently working on now and as our roadmap suggests will be implemented by the release of Heron 1.0.</p><disp-quote content-type="editor-comment"><p>(4) Another limitation that in my opinion is not sufficiently addressed is the communication between the nodes, and the effect of passing all communications via the host machine and SSH. What does this mean for the resulting throughput and latency - in particular in comparison to software such as Bonsai or Autopilot? The paper also states that &quot;Heron is designed to have no message buffering, thus automatically dropping any messages that come into a Node’s inputs while the Node’s worker function is still running.&quot;- it seems to be up to the user to debug and handle this manually?</p></disp-quote><p>There are a few points raised here that require addressing. The first is Heron’s requirement to pass all communication through the main (GUI) machine. We understand (and also state in the manuscript) that this is a limitation that needs to be addressed. We plan to do this is by adding to Heron the feature of running headless (see our roadmap). This will allow us to run whole Heron pipelines in a second machine which will communicate with the main pipeline (run on the GUI machine) with special Nodes. That will allow experimenters to define whole pipelines on secondary machines where the data between their Nodes stay on the machine running the pipeline. This is an important feature for Heron and it will be one of the first features to be implemented next (after the integration of the saving system with git).</p><p>The second point is regarding Heron’s throughput latency. In our original manuscript we did not have any description of Heron’s capabilities in this respect and both other reviewers mentioned this as a limitation. As mentioned above, we have now addressed this by adding a section to our third experimental example that fully describes how much CPU is required to run a full experimental pipeline running on two machines and utilising also non python code executables (a Unity game). This gives an overview of how heavy pipelines can run on normal computers given adequate optimisation and utilising Heron’s feature of forcing some Nodes to run their Worker processes on a specific core. At the same time, Heron’s use of 0MQ protocol makes sure there are no other delays or speed limitations to message passing. So, message passing within the same machine is just an exchange of memory pointers while messages passing between different machines face the standard speed limitations of the Local Access Network’s ethernet card speeds.</p><p>Finally, regarding the message dropping feature of Heron, as mentioned above this is an architectural decision given the use cases of message passing we expect Heron to come in contact with. For a full explanation of the logic here please see our answer to the 3rd comment by Reviewer 2.</p><disp-quote content-type="editor-comment"><p>(5) As a final comment, I have to admit that I was a bit confused by the use of the term &quot;Knowledge Graph&quot; in the title and elsewhere. In my opinion, the Heron software describes &quot;pipelines&quot; or &quot;data workflows&quot;, not knowledge graphs - I’d understand a knowledge graph to be about entities and their relationships. As the authors state, it is usually meant to make it possible to &quot;test propositions against the knowledge and also create novel propositions&quot; - how would this apply here?</p></disp-quote><p>We have described Heron as a Knowledge Graph instead of a pipeline, data workflow or computation graph in order to emphasise Heron’s distinct operation in contrast to what one would consider a standard pipeline and data workflow generated by other visual based software (like LabView and Bonsai). This difference exists on what a user should think of as the base element of a graph, i.e. the Node. In all other visual programming paradigms, the Node is defined as a low-level computation, usually a language keyword, language flow control or some simple function. The logic in this case is generated by composing together the visual elements (Nodes). In Heron the Node is to be thought of as a process which can be of arbitrary complexity and the logic of the graph is composed by the user both within each Node and by the way the Nodes are combined together. This is an important distinction in Heron’s basic operation logic and it is we argue the main way Heron allows flexibility in what can be achieved while retaining ease of graph composition (by users defining their own level of complexity and functionality encompassed within each Node). We have found that calling this approach a computation graph (which it is) or a pipeline or data workflow would not accentuate this difference. The term Knowledge Graph was the most appropriate as it captures the essence of variable information complexity (even in terms of length of shortest string required) defined by a Node.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>- No buffering implies dropped messages when a node is busy. It seems like this could be very problematic for some use cases...</p></disp-quote><p>This is a design principle of Heron. We have now provided a detailed explanation of the reasoning behind it in our answer to Reviewer 2 (Paragraph 3) as well as in the manuscript.</p><disp-quote content-type="editor-comment"><p>- How are ssh passwords stored, and is it secure in some way or just in plain text?</p></disp-quote><p>For now they are plain text in an unencrypted file that is not part of the repo (if one gets Heron from the repo). Eventually, we would like to go to private/public key pairs but this is not a priority due to the local nature of Heron’s use cases (all machines in an experiment are expected to connect in a LAN).</p><disp-quote content-type="editor-comment"><p>Minor notes / copyedits:</p><p>- Figure 2A: right and left seem to be reversed in the caption.</p></disp-quote><p>They were. This is now fixed.</p><disp-quote content-type="editor-comment"><p>- Figure 2B: the text says that proof of life messages are sent to each worker process but in the figure, it looks like they are published by the workers? Also true in the online documentation.</p></disp-quote><p>The Figure caption was wrong. This is now fixed.</p><disp-quote content-type="editor-comment"><p>- psutil package is not included in the requirements for GitHub</p></disp-quote><p>We have now included psutil in the requirements.</p><disp-quote content-type="editor-comment"><p>- GitHub readme says Python &gt;= 3.7 but Heron will not run as written without python &gt;= 3.9 (which is alluded to in the paper)</p></disp-quote><p>The new Heron updates require Python 3.11. We have now updated GitHub and the documentation to reflect this.</p><disp-quote content-type="editor-comment"><p>- The paper mentions that the Heron editor must be run on Windows, but this is not mentioned in the Github readme.</p></disp-quote><p>This was an error in the manuscript that we have now corrected.</p><disp-quote content-type="editor-comment"><p>- It’s unclear from the readme/manual how to remove a node from the editor once it’s been added.</p></disp-quote><p>We have now added an X button on each Node to complement the Del button on the keyboard (for MacOS users that do not have this button most of the times).</p><disp-quote content-type="editor-comment"><p>- The first example experiment is called the Probabilistic Reversal Learning experiment in text, but the uncertainty experiment in the supplemental and on GitHub.</p></disp-quote><p>We have now used the correct name (Probabilistic Reversal Learning) in both the supplemental material and on GitHub</p><disp-quote content-type="editor-comment"><p>- Since Python &gt;= 3.9 is required, consider using fstrings instead of str.format for clarity in the codebase</p></disp-quote><p>Thank you for the suggestion. Latest Heron development has been using f strings and we will do a refactoring in the near future.</p><disp-quote content-type="editor-comment"><p>- Grasshopper cameras can run on linux as well through the spinnaker SDK, not just Windows.</p></disp-quote><p>Fixed in the manuscript.</p><disp-quote content-type="editor-comment"><p>- Figure 4: Square and star indicators are unclear.</p></disp-quote><p>Increased the size of the indicators to make them clear.</p><disp-quote content-type="editor-comment"><p>- End of page 9: &quot;an of the self&quot; presumably a typo for &quot;off the shelf&quot;?</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>- Page 10 first paragraph. &quot;second root&quot; should be &quot;second route&quot;</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>- When running Heron, the terminal constantly spams Blowfish encryption deprecation warnings, making it difficult to see the useful messages.</p></disp-quote><p>The solution to this problem is to either update paramiko or install Heron through pip. This possible issue is mentioned in the documentation.</p><disp-quote content-type="editor-comment"><p>- Node input /output hitboxes in the GUI are pretty small. If they could be bigger it would make it easier to connect nodes reliably without mis-clicks.</p></disp-quote><p>We have redone the Node GUI, also increasing the size of the In/Out points.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>(1) There are quite a few typos in the manuscript, for example: &quot;one can accessess the code&quot;, &quot;an of the self&quot;, etc.</p></disp-quote><p>Thanks for the comment. We have now screened the manuscript for possible typos.</p><disp-quote content-type="editor-comment"><p>(2) Heron’s GUI can only run on Windows! This seems to be the opposite of the key argument about the portability of the experimental setup.</p></disp-quote><p>As explained in the answers to Reviewer 1, Heron can run on most machines that the underlying python libraries run, i.e. Windows and Linux (both for x86 and Arm architectures). We have tested it on Windows (10 and 11, both x64), Linux PC (Ubuntu 20.04.6, x64) and Raspberry Pi 4 (Debian GNU/Linux 12 (bookworm), aarch64). We have now revised the manuscript and the GitHub repo to reflect this.</p><disp-quote content-type="editor-comment"><p>(3) Currently, the output is displayed along the left edge of the node, but the yellow dot connector is on the right. It would make more sense to have the text displayed next to the connectors.</p></disp-quote><p>We have redesigned the Node GUI and have now placed the Out connectors on the right side of the Node.</p><disp-quote content-type="editor-comment"><p>(4) The edges are often occluded by the nodes in the GUI. Sometimes it leads to some confusion, particularly when the number of nodes is large, e.g., Fig 4.</p></disp-quote><p>This is something that is dependent on the capabilities of the DearPyGUI module. At the moment there is no way to control the way the edges are drawn.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>A few comments on the software and the documentation itself:</p><p>- From a software engineering point of view, the implementation seems to be rather immature. While I get the general appeal of &quot;no installation necessary&quot;, I do not think that installing dependencies by hand and cloning a GitHub repository is easier than installing a standard package.</p></disp-quote><p>We have now added a pip install capability which also creates a Heron command line command to start Heron with.</p><disp-quote content-type="editor-comment"><p>-The generous use of global variables to store state (minor point, given that all nodes run in different processes), boilerplate code that each node needs to repeat, and the absence of any kind of automatic testing do not give the impression of a very mature software (case in point: I had to delete a line from editor.py to be able to start it on a non-Windows system).</p></disp-quote><p>As mentioned, the use of global variables in the worker scripts is fine partly due to the multi process nature of the development and we have found it is a friendly approach to Matlab users who are just starting with Python (a serious consideration for Heron). Also, the parts of the code that would require a singleton (the Editor for example) are treated as scripts with global variables while the parts that require the construction of objects are fully embedded in classes (the Node for example). A future refactoring might make also all the parts of the code not seen by the user fully object oriented but this is a decision with pros and cons needing to be weighted first.</p><p>Absence of testing is an important issue we recognise but Heron is a GUI app and nontrivial unit tests would require some keystroke/mouse movement emulator (like QTest of pytest-qt for QT based GUIs). This will be dealt with in the near future (using more general solutions like PyAutoGUI) but it is something that needs a serious amount of effort (quite a bit more that writing unit tests for non GUI based software) and more importantly it is nowhere as robust as standard unit tests (due to the variable nature of the GUI through development) making automatic test authoring an almost as laborious a process as the one it is supposed to automate.</p><disp-quote content-type="editor-comment"><p>- From looking at the examples, I did not quite see why it is necessary to write the ..._com.py scripts as Python files, since they only seem to consist of boilerplate code and variable definitions. Wouldn’t it be more convenient to represent this information in configuration files (e.g. yaml or toml)?</p></disp-quote><p>The com is not a configuration file, it is a script that launches the communication process of the Node. We could remove the variable definitions to a separate toml file (which then the com script would have to read). The pros and cons of such a set up should be considered in a future refactoring.</p><disp-quote content-type="editor-comment"><p>Minor comments for the paper:</p><p>- p.7 (top left): &quot;through its return statement&quot; - the worker loop is an infinite loop that forwards data with a return statement?</p></disp-quote><p>This is now corrected. The worker loop is an infinite loop and does not return anything but at each iteration pushes data to the Nodes output.</p><disp-quote content-type="editor-comment"><p>- p.9 (bottom right): &quot;of the self&quot; → &quot;off-the-shelf&quot;</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>- p.10 (bottom left): &quot;second root&quot; → &quot;second route&quot;</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>- Supplementary Figure 3: Green start and square seem to be swapped (the green star on top is a camera image and the green star on the bottom is value visualization - inversely for the green square).</p></disp-quote><p>The star and square have been swapped around.</p><disp-quote content-type="editor-comment"><p>- Caption Supplementary Figure 4 (end): &quot;rashes to receive&quot; → &quot;rushes to receive&quot;</p></disp-quote><p>Corrected.</p></body></sub-article></article>