<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">73870</article-id><article-id pub-id-type="doi">10.7554/eLife.73870</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Recurrent neural networks enable design of multifunctional synthetic human gut microbiome dynamics</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-254419"><name><surname>Baranwal</surname><given-names>Mayank</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9354-2826</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-254418"><name><surname>Clark</surname><given-names>Ryan L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7865-2496</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-254432"><name><surname>Thompson</surname><given-names>Jaron</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5967-0234</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-254433"><name><surname>Sun</surname><given-names>Zeyu</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-254434"><name><surname>Hero</surname><given-names>Alfred O</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2531-9670</contrib-id><email>hero@eecs.umich.edu</email><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-157872"><name><surname>Venturelli</surname><given-names>Ophelia S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2200-1963</contrib-id><email>venturelli@wisc.edu</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qyf5152</institution-id><institution>Department of Systems and Control Engineering, Indian Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Bombay</named-content></addr-line><country>India</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01b9n8m42</institution-id><institution>Division of Data &amp; Decision Sciences, Tata Consultancy Services Research</institution></institution-wrap><addr-line><named-content content-type="city">Mumbai</named-content></addr-line><country>India</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y2jtd41</institution-id><institution>Department of Biochemistry, University of Wisconsin-Madison</institution></institution-wrap><addr-line><named-content content-type="city">Madison</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y2jtd41</institution-id><institution>Department of Chemical &amp; Biological Engineering, University of Wisconsin-Madison</institution></institution-wrap><addr-line><named-content content-type="city">Madison</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jmfr291</institution-id><institution>Department of Electrical Engineering &amp; Computer Science, University of Michigan</institution></institution-wrap><addr-line><named-content content-type="city">Ann Arbor</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jmfr291</institution-id><institution>Department of Biomedical Engineering, University of Michigan</institution></institution-wrap><addr-line><named-content content-type="city">Ann Arbor</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jmfr291</institution-id><institution>Department of Statistics, University of Michigan</institution></institution-wrap><addr-line><named-content content-type="city">Ann Arbor</named-content></addr-line><country>United States</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y2jtd41</institution-id><institution>Department of Bacteriology, University of Wisconsin-Madison</institution></institution-wrap><addr-line><named-content content-type="city">Madison</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mitri</surname><given-names>Sara</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/019whta54</institution-id><institution>University of Lausanne</institution></institution-wrap><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS LPENS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e73870</elocation-id><history><date date-type="received" iso-8601-date="2021-09-14"><day>14</day><month>09</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-22"><day>22</day><month>05</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-09-28"><day>28</day><month>09</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.09.27.461983"/></event></pub-history><permissions><copyright-statement>© 2022, Baranwal, Clark et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Baranwal, Clark et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-73870-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-73870-figures-v1.pdf"/><abstract><p>Predicting the dynamics and functions of microbiomes constructed from the bottom-up is a key challenge in exploiting them to our benefit. Current models based on ecological theory fail to capture complex community behaviors due to higher order interactions, do not scale well with increasing complexity and in considering multiple functions. We develop and apply a long short-term memory (LSTM) framework to advance our understanding of community assembly and health-relevant metabolite production using a synthetic human gut community. A mainstay of recurrent neural networks, the LSTM learns a high dimensional data-driven non-linear dynamical system model. We show that the LSTM model can outperform the widely used generalized Lotka-Volterra model based on ecological theory. We build methods to decipher microbe-microbe and microbe-metabolite interactions from an otherwise black-box model. These methods highlight that Actinobacteria, Firmicutes and Proteobacteria are significant drivers of metabolite production whereas <italic>Bacteroides</italic> shape community dynamics. We use the LSTM model to navigate a large multidimensional functional landscape to design communities with unique health-relevant metabolite profiles and temporal behaviors. In sum, the accuracy of the LSTM model can be exploited for experimental planning and to guide the design of synthetic microbiomes with target dynamic functions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>human gut microbiome</kwd><kwd>ecological network</kwd><kwd>dynamical systems</kwd><kwd>microbiome engineering</kwd><kwd>machine learning</kwd><kwd>microbial metabolism</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R35GM124774</award-id><principal-award-recipient><name><surname>Venturelli</surname><given-names>Ophelia S</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000183</institution-id><institution>Army Research Office</institution></institution-wrap></funding-source><award-id>W911NF1910269</award-id><principal-award-recipient><name><surname>Hero</surname><given-names>Alfred O</given-names></name><name><surname>Venturelli</surname><given-names>Ophelia S</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007015</institution-id><institution>University of Wisconsin-Madison</institution></institution-wrap></funding-source><award-id>R01 EB030340</award-id><principal-award-recipient><name><surname>S Venturelli</surname><given-names>Ophelia</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Recurrent neural network models enable prediction and design of health-relevant metabolite dynamics in synthetic human gut communities.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Microbial communities perform chemical and physical transformations to shape the properties of nearly every environment on Earth from driving biogeochemical cycles to mediating human health and disease. These functions performed by microbial communities are shaped by a multitude of abiotic and biotic interactions and vary as a function of space and time. The complex dynamics of microbial communities are influenced by pairwise and higher order interactions, wherein interactions between pairs of species can be modified by other community members (<xref ref-type="bibr" rid="bib53">Sanchez-Gorostiaga et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Mickalide and Kuehn, 2019</xref>; <xref ref-type="bibr" rid="bib20">Hsu et al., 2019</xref>). In addition, the interactions between community members can change as a function of time as the community continuously reacts to and modifies its environment (<xref ref-type="bibr" rid="bib16">Hart et al., 2019</xref>). Therefore, flexible modeling frameworks that can capture the complex and temporally changing interactions that determine the dynamic behaviors of microbiomes are needed. These predictive modeling frameworks could be used to guide the design of interventions to precisely manipulate community-level functions to our benefit.</p><p>The generalized Lotka-Volterra (gLV) model has been widely used to predict community dynamics and deduce pairwise microbial interactions shaping community assembly (<xref ref-type="bibr" rid="bib37">MacArthur, 1970</xref>). For example, the gLV model has been used to predict the assembly of tens of species based on absolute abundance measurements of lower species richness (i.e. number of species) communities (<xref ref-type="bibr" rid="bib66">Venturelli et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Mounier et al., 2008</xref>; <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). The parameters of the gLV model can be efficiently inferred based on properly collected absolute abundance measurements and can provide insight into significant microbial interactions shaping community assembly (<xref ref-type="bibr" rid="bib2">Bucci et al., 2016</xref>). However, this model does not represent higher order interactions or microbial community functions beyond species growth. To capture such microbial community functions, composite gLV models have been developed to predict a community-level functional activity based on species abundance at an endpoint (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Stein et al., 2018</xref>). However, these approaches have been limited to the prediction of a single community-level function at a single time point. Therefore, new modeling frameworks are needed to capture temporal changes in multiple community-level functions, such as tailoring the metabolite profile of the human gut microbiome (<xref ref-type="bibr" rid="bib8">Fischbach and Sonnenburg, 2011</xref>).</p><p>Neural network architectures, such as recurrent neural networks (RNNs), are universal function approximators (<xref ref-type="bibr" rid="bib6">Dambre et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">Schäfer and Zimmermann, 2006</xref>) that enable greater flexibility compared to gLV models for modeling dynamical systems. However, neural network based models often require significantly more model parameters, which poses additional challenges to model fitting and generalizability. A particular RNN model architecture called long short-term memory (LSTM) addresses challenges associated with training on sequential data by incorporating gating mechanisms that learn to regulate the influence of information from previous instances in the sequence (<xref ref-type="bibr" rid="bib34">Lipton et al., 2015</xref>). From their initial successes in speech recognition (<xref ref-type="bibr" rid="bib13">Graves et al., 2005</xref>) and computer vision (<xref ref-type="bibr" rid="bib3">Byeon et al., 2015</xref>), LSTMs have recently been applied to modeling biological data such as subcellular localization of proteins (<xref ref-type="bibr" rid="bib61">Sonderby et al., 2015</xref>) and prediction of biological age from activity collected from wearable devices (<xref ref-type="bibr" rid="bib48">Rahman and Adjeroh, 2019</xref>). Related to microbiomes, deep learning frameworks have been applied to predict gut microbiome metabolites based on community composition data (<xref ref-type="bibr" rid="bib30">Le et al., 2020</xref>), final community composition based on microbial interactions (<xref ref-type="bibr" rid="bib28">Larsen et al., 2012</xref>) and end-point community composition based on the presence/absence of species (<xref ref-type="bibr" rid="bib40">Michel-Mata et al., 2021</xref>). In addition, RNN architectures have been used to model phytoplankton (<xref ref-type="bibr" rid="bib22">Jeong et al., 2001</xref>) and macroinvertebrate (<xref ref-type="bibr" rid="bib4">Chon et al., 2001</xref>) community dynamics. Despite achieving reasonable prediction performance, previous efforts at modeling ecological system dynamics using RNNs are typically limited to handful of organisms (&lt;10), have provided limited model interpretation and have not been leveraged to predict temporal changes in community behaviors. In addition, RNN architectures have not been used for bottom-up community design, which could be exploited for applications in bioremediation, bioprocessing, agriculture, and human health (<xref ref-type="bibr" rid="bib32">Leggieri et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Lawson et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>).</p><p>Here, we apply LSTMs to model time dependent changes in species abundance and the production of key health-relevant metabolites by a diverse 25-member synthetic human gut community. LSTMs are a good model for microbiomes because (1) LSTMs are a natural choice for a neural network based model of time-series data (<xref ref-type="bibr" rid="bib12">Goodfellow et al., 2016</xref>); (2) LSTMs are highly flexible models that can capture complex interaction networks that are often neglected in ecological models; (3) LSTMs can be modified to capture additional system variables such as environmental factors (e.g. metabolites). In addition, LSTMs have some advantages over traditional RNNs because they can capture long-term dependencies. LSTMs have additional parameters that adjust the effects of earlier time points on the predictions at later time points in a time-series. We use the trained LSTM model to elucidate significant microbe-microbe and microbe-metabolite interactions.</p><p>The flexibility and accuracy of the LSTM model enabled systematic integration into our experimental planning process in two stages. First, the LSTM was fit to data from a previous study with low temporal resolution involving a moderate number of synthetic microbial communities (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). The distribution of LSTM metabolite predictions was then used to identify sparse sub-communities in the tails of the distribution, communities that we refer to as ‘corner cases’. A second experiment was then performed that expanded the training data for the LSTM in the vicinity of these corner cases with higher time resolution. The LSTM-guided two-stage experimental planning procedure substantially reduced the number of experiments compared to random sampling of the functional landscape with temporal resolution in a single stage experiment. Therefore, the LSTM analysis enabled our main findings on dynamical behaviors of communities and identified the key species critical for community assembly and metabolite profiles. Compared to the gLV model, the proposed LSTM framework provides a better fit to the experimental data, captures higher order interactions and provides higher accuracy predictions of species abundance and metabolite concentrations. In addition, our approach preserves model interpretability through a suitably developed gradient-based framework and locally interpretable model-agnostic explanations (LIME) (<xref ref-type="bibr" rid="bib49">Ribeiro et al., 2016a</xref>). Using our time-series data of species abundance and metabolite concentrations, we demonstrate that the temporal behaviors of the communities cluster into distinct groups based on the presence and absence of sets of species. Our results highlight that LSTM models are powerful tools for predicting and designing the dynamic behaviors of microbial communities.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>LSTM outperforms the generalized Lotka Volterra ecological model</title><p>Our first objective was to compare the predictive performance of the LSTM model to a commonly used ecological modeling approach. The gLV model is a widely used ecological model consisting of a coupled set of ordinary differential equations that captures the growth dynamics of members of a community based on their intrinsic growth rate and interactions between all pairs of community members (<xref ref-type="bibr" rid="bib66">Venturelli et al., 2018</xref>). Therefore, the gLV model is not suited to capture higher order interactions among species or changes in inter-species interactions resulting from variation in the environment. By contrast, the LSTM modeling framework is flexible and can capture complex relationships between species as well as time-dependent changes in inter-species interactions. To evaluate the strengths and limitations of these modeling frameworks, we characterized the performance of the gLV and LSTM models in learning the behavior of a ground truth model that included pairwise and third-order interactions between species (Methods).</p><p>Our ground truth model is based on a gLV model of a 25-member synthetic gut community from a previous study (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). To perturb our ground truth model with higher order interactions, we add third-order interaction terms with either mild or moderate parameter magnitudes (Methods). Using this model, we simulate sub-communities that vary in the number of species. Of all the randomly simulated communities, those containing six or fewer species are used to train both the gLV and LSTM models (624 training communities), while the remaining communities (3299 test communities with ≥10 species) are used as a hold-out test set. The 624 training communities includes 25 monospecies, 300 unique pairwise communities, 100 unique three-member communities, 100 unique five-member communities, and 99 unique six-member communities. The simulated data spans 48 hr separated by an interval of 8 hr, reflecting the experimentally feasible periodic sampling interval of 8 hr.</p><p>Recall that we restrict our attention to simpler (fewer species) communities for training to determine if the behavior of lower order communities can be used to predict higher order communities. Further, pairwise inter-species interactions are easier to decipher in lower order communities due to potential co-variation among parameters (correlations between parameters) as a consequence of model structure or methods of data collection. A similar training/test partitioning was used to generate predictive models of complex community behaviors (<xref ref-type="bibr" rid="bib66">Venturelli et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Hromada et al., 2021</xref>).</p><p>The prediction performance of the trained gLV and LSTM models on the hold-out test set are similar for the ground truth model containing only pairwise interactions (Pearson <inline-formula><mml:math id="inf1"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> of 0.89 and 0.85 for gLV and LSTM models, respectively) (<xref ref-type="fig" rid="fig1">Figure 1b, c</xref> left). For the ground truth model with mild third-order interactions (interaction coefficients that do not exceed 25% of the maximum of the absolute values of the coefficients for the second-order interactions), the performance of the LSTM model is substantially better than the gLV model with the <inline-formula><mml:math id="inf2"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-score of 0.85, as opposed to 0.52 for the gLV model (<xref ref-type="fig" rid="fig1">Figure 1b, c</xref>, middle). In addition, the LSTM model performs significantly better than the gLV model for higher magnitude (moderate) third-order perturbations (third-order interaction coefficients that do not exceed 50% of the maximum of the absolute values of the coefficients for second-order interactions) (<xref ref-type="fig" rid="fig1">Figure 1b, c</xref>, right).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Comparison of generalized Lotka Volterra (gLV) and Long Short Term Memory (LSTM) model prediction performance of species abundance in a 25-member microbial community in response to third-order perturbations of varying magnitude.</title><p>For both models, training data consists of low species richness communities (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> species, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>82</mml:mn><mml:mo>,</mml:mo><mml:mn>475</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, Pearson correlation p-value <italic>lt</italic><sub>0.0001</sub>). (<bold>a</bold>) &amp; (<bold>d</bold>): Data was generated using a gLV model that captures monospecies growth and pairwise interactions. Scatter plots of true versus predicted species abundance at <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>48</mml:mn><mml:mo>⁢</mml:mo><mml:mtext>hr</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> using the gLV and LSTM models, respectively. <inline-formula><mml:math id="inf6"><mml:mi>X</mml:mi></mml:math></inline-formula> represents a vector of species abundances. (<bold>b</bold>) &amp; (<bold>e</bold>) Scatter plot of true versus predicted species abundance of the gLV and LSTM models, respectively when the simulated data is subjected to low magnitude (mild) third-order interactions. (<bold>c</bold>) &amp; (<bold>f</bold>) Scatter plot of true versus predicted species abundance of gLV and LSTM models, respectively when the simulated data is further subjected to moderately large third-order interactions. (<bold>g</bold>) Scatter plot of true versus predicted species abundance for the LSTM model. The training set included a set of higher richness communities (50 each of 11 and 19 member communities). All predictions are forecasted from the species abundance at time 0.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig1-v1.tif"/></fig><p>This in silico analysis highlights the advantages of adopting more expressive neural network models over severely constrained ecological models such as gLV. In addition, a key advantage of the proposed LSTM model over the gLV model is the amount of time required for training the two models. The gLV equations are coupled nonlinear ordinary differential equations, and thus training gLV models requires substantial computational time (nearly 5–6 hr), whereas the LSTM models can be trained in minutes on the same platform. Therefore, the LSTM approach is highly suited for real-time training and planning of experiments. Note that both the composite as well as the LSTM model require tuning of hyperparameters for optimal performance. The details of the computational implementation are provided in the Methods section.</p><p>To further leverage this in silico experimental approach, we aimed to identify what type of datasets are required for building predictive models of high richness community behaviors depending on the nature of their underlying interactions. In further analyzing our results, we observed a crescent shaped prediction profile, representing an inherent bias, which we hypothesized was due to the training data containing only communities with ≤6 species (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). To test this hypothesis, we augmented the training set with 100 communities enriched with a larger number of species (randomly sampled 11 and 19-member communities). Using this enriched training set, the LSTM model accurately predicts the community dynamics of the hold-out set with an <inline-formula><mml:math id="inf7"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> of 0.95 (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). In sum, the LSTM has difficulty predicting the behavior of high richness communities when the training data consists of only low richness communities. However, adding a moderate number of high richness communities to the training set eliminates the prediction bias and improves the prediction performance of the LSTM.</p></sec><sec id="s2-2"><title>LSTM accurately predicts experimentally measured microbial community assembly</title><p>After validating our methods using the ground truth modeling approach described above, we evaluated the ability of the LSTM to capture the dynamics of experimentally characterized synthetic human gut microbial communities. We tested the effectiveness of the LSTM on time-resolved species abundance data from a previous study of a well-characterized twelve-member synthetic human gut community (<xref ref-type="bibr" rid="bib66">Venturelli et al., 2018</xref>). The experimental data consists of species abundance sampled approximately every 12hr. A total of 175 microbial communities with sizes varying from 2 to 12 were used to train and evaluate the LSTM model. Of the 175 microbial communities, 102 microbial communities were selected randomly to constitute the training set, while the remaining 73 microbial communities constituted the hold-out test set (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). This train/test split was similar to that used to train a gLV model in the previous study (<xref ref-type="bibr" rid="bib66">Venturelli et al., 2018</xref>). The previous study represented perturbations in cell densities and nutrient availability by diluting the community 20-fold every 24 hr into fresh media (i.e. passaging of the communities) (<xref ref-type="fig" rid="fig2">Figure 2a</xref> ). The sequential dilutions of the communities are external perturbations that introduce further complexity towards model training.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The LSTM model can predict the temporal changes in species abundance in a 12-member synthetic human gut community in response to periodic dilution (passaging).</title><p>(<bold>a</bold>) Proposed LSTM modeling methodology for the dynamic prediction of species abundance in a microbial community. The initial abundance information is an input to the first LSTM cell, the output of which is trained to predict abundance at the next time point. Consequently, the predicted abundance becomes an input to another LSTM cell with shared weights to predict the abundance at the subsequent time point. The process is repeated until measurements at all time points are available. <inline-formula><mml:math id="inf8"><mml:mi>X</mml:mi></mml:math></inline-formula> represents a vector of species abundances. Thus, all predictions are forecasted from the abundance at time 0. (<bold>b</bold>) Scatter plot of measured (true) and predicted species abundance of a 12-member synthetic human gut community at 12 hr (<inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>876</mml:mn></mml:mrow></mml:math></inline-formula>, p-value <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2.44</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>257</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>c</bold>) Scatter plot of measured (true) and predicted abundance at 24 hr (p-value <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>6.51</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>257</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>d</bold>) Scatter plot of measured (true) and predicted abundance at 36 hr (p-value <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>7.42</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>257</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>e</bold>) Scatter plot of measured (true) and predicted abundance at 48 hr (p-value <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1.66</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>227</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>f</bold>) Scatter plot of measured (true) and predicted abundance at 60 hr (p-value <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>3.39</mml:mn><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>227</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Prediction of temporal changes of species abundance for a few representative communities by the LSTM network.</title><p>(<bold>a</bold>) Histogram of prediction <inline-formula><mml:math id="inf15"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-scores on the test set. The prediction <inline-formula><mml:math id="inf16"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-score for each community is determined between the measured (true) and predicted abundances of all species in that community at all time instants. Prediction of individual species abundance in communities that display (<bold>b</bold>) accurate predictions (11-member community), (<bold>c</bold>) close to the median (3-member), and (<bold>d</bold>) poor predictions (four-member). We note the difference in abundance scales for some species such as BH and BO. Despite the order of magnitude difference in the scales, our proposed LSTM network does a great job at predicting species abundance with consistency across all species in (<bold>a</bold>). This is primarily due to feature standardization during the training and inference of LSTM networks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig2-figsupp1-v1.tif"/></fig></fig-group><p>We trained a LSTM network to predict species abundances at various time points given the information of initial species abundance. We found that a total of five LSTM units can predict species abundance at different time points (12, 24, 36, 48, and 60 hr) based on the initial species abundance. The output of each LSTM unit is used as an input to the next unit. However, the input to the current LSTM unit is randomized between the output from the previous LSTM unit and the true abundance at the current time point in the randomized teacher forcing mode of training in order to eliminate temporal bias in the prediction of end-point species abundances. We did not model the passaging perturbations explicitly, since the experimental procedure was consistent across all communities. This also highlights the advantage of using black-box approaches, such as the LSTM network, where physical parameters such as dilution do not need to be explicitly modeled. Here, each LSTM unit consists of a single hidden layer comprising of 2048 hidden units with ReLU activation. The details on hyperparameter tuning, learning rates, and choice of optimizer are provided in the Methods section.</p><p>Despite the passaging perturbations and variation in the sampling times, the LSTM accurately predicts (Pearson <inline-formula><mml:math id="inf17"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-scores of 0.74, 0.73, 0.74, 0.70, and 0.69 at time points 12, 24, 36, 48, and 60 hr, respectively) not only the end-point species abundance, but also the abundances at intermediate time points on hold-out test sets (<xref ref-type="fig" rid="fig2">Figure 2b-f</xref>). These results demonstrate that the LSTM model can accurately predict the temporal changes in species abundance of multi-species communities in the presence of external perturbations. Representative communities that were accurately or poorly predicted by the LSTM are shown in (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec><sec id="s2-3"><title>LSTM enables end-point design of multifunctional synthetic human gut microbiomes</title><p>The chemical transformations (i.e. functions) performed by the community are the key design variables for microbiome engineering goals, as evidenced by their major impacts on human health (<xref ref-type="bibr" rid="bib58">Sharon et al., 2014</xref>). Thus, we explored prediction of microbial community functions by applying the LSTM framework to design health-relevant metabolite profiles using synthetic human gut communities.</p><p>A core function of gut microbiota is to transform complex dietary substrates into fermentation end products such as the beneficial metabolite butyrate, which is a major determinant of gut homeostasis (<xref ref-type="bibr" rid="bib35">Litvak et al., 2018</xref>). In a previous study, we designed butyrate-producing synthetic human gut microbiomes from a set of 25 prevalent and diverse human gut bacteria using a composite gLV and statistical model (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). While the composite model approach was successful in predicting butyrate concentration, designing community-level profiles of multiple metabolites adds substantial complexity and limited flexibility using the composite modeling approach. Thus, we leveraged the accuracy and flexibility of LSTM models to design the metabolite profiles of synthetic human gut microbiomes. We focused on the fermentation products butyrate, acetate, succinate, and lactate which play important roles in the gut microbiome’s impact on host physiology and interactions with constituent community members (<xref ref-type="bibr" rid="bib8">Fischbach and Sonnenburg, 2011</xref>).</p><p>We used the species abundance and metabolite concentrations from our previous work (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>) to train an initial LSTM model. This model uses a feed-forward network (FFN) at the output of the final LSTM unit that maps the endpoint species abundance (a 25-dimensional vector) to the concentrations of the four metabolites (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). The entire neural network model comprising LSTM units and a feed-forward network is learned in an end-to-end manner during the training process, (i.e. all the network weights are trained simultaneously). Cross-validation of this model (Model M1, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>) on a set of hold-out community observations shows good agreement between the model predictions and experimental measurements for metabolite concentrations and microbial species abundances (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Thus, we used this model to design high species richness (i.e. &gt;10 species) communities with tailored metabolite profiles (<xref ref-type="fig" rid="fig3">Figure 3a</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>LSTM-guided design and interpretability of community-level metabolite production profiles (<bold>a</bold>) Schematic of model-training and design of communities with tailored metabolite outputs.</title><p>(<bold>b</bold>) Heatmap of butyrate and lactate concentrations of all possible communities predicted by the LSTM model M1. Grey points indicate communities chosen via <inline-formula><mml:math id="inf18"><mml:mi>k</mml:mi></mml:math></inline-formula>-means clustering to span metabolite design space. Colored boxes indicate ‘corner’ regions defined by <inline-formula><mml:math id="inf19"><mml:msup><mml:mn>95</mml:mn><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> percentile values on each axis with points of the corresponding color indicating designed communities within that ‘corner’. Insets show heat maps of acetate and succinate concentrations for all communities within the corresponding boxes on the main figure. Boxes on the inset indicate ‘corners’ defined by <inline-formula><mml:math id="inf20"><mml:msup><mml:mn>95</mml:mn><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> percentile values on each axis with colored points corresponding to the same points indicated on the main plot. (<bold>c</bold>) Cross-validation accuracy of LSTM model trained and validated on a random 90/10 split of all community observations (model M2), evaluated as Pearson correlation <inline-formula><mml:math id="inf21"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> for the correlation of predicted versus measured for each variable (all p-values<italic>lt</italic><sub>0.05</sub>, N and p-value for each test reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Dashed line indicates <inline-formula><mml:math id="inf22"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, which is used as a cutoff for including a variable in the subsequent network diagrams. (<bold>d</bold>) and (<bold>e</bold>) Network representation of median LIME explanations of the LSTM model M2 from (<bold>c</bold>) for prediction of each metabolite concentration (<bold>d</bold>) or species abundance (<bold>e</bold>) by the presence of each species. Edge widths are proportional to the median LIME explanation across all communities from (<bold>b</bold>) used to train the model in units of concentration (for (<bold>d</bold>)) or normalized to the species’ self-impact (for (<bold>e</bold>)). Only explanations for those variables where the cross-validated predictions had <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> are shown. Networks were simplified by using lower thresholds for edge width (5 mM for (<bold>d</bold>), 0.2 for (<bold>e</bold>)). Red and blue edges indicate positive and negative contributions, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Cross-validation of LSTM model M1 predictions of species abundance and metabolite concentration.</title><p>Each plot indicates the comparison of predicted versus measured species abundance (<bold>a</bold>), butyrate concentration (<bold>b</bold>), acetate concentration (<bold>c</bold>), lactate concentration (<bold>d</bold>), or succinate concentration (<bold>e</bold>) for cross-validation of model M1 predictions of the validation communities from <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref> (model trained on 110 pairwise communities, 156 communities with 3–5 species, and 124 communities with 11-17 species; cross-validation shown is prediction of a different set of 124 communities with 11–17 species, including 82 communities with all 5 butyrate producers (AC, ER, FP, CC, RI) and 42 communities with the 4 butyrate producers other than AC). Each data point indicates the average of biological replicates of a single community. Black lines indicate linear regressions with slope (<inline-formula><mml:math id="inf24"><mml:mi>m</mml:mi></mml:math></inline-formula>) and <inline-formula><mml:math id="inf25"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> indicated in the legends. Dashed blue line indicates.<inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Predicted total carbon in fermentation products.</title><p>Histogram of the model M1 predicted total carbon concentration in butyrate, acetate, lactate, and succinate for all possible communities with &gt;10 species (26,434,916 communities).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Prediction and classification statistics for model M1 predictions of designed community sets.</title><p>(<bold>a</bold>) Scatter plot of prediction accuracy (correlation of predicted versus measured) of each variable (25 species abundances, 4 metabolite concentrations) by the LSTM model M1 versus the composite model based on the method from <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>. For metabolites, prediction accuracy is also included where the regression model from the composite model is replaced with a Random Forest Regressor (Triangles) or a Feed Forward Network (Plus Signs). Pearson correlation,<inline-formula><mml:math id="inf27"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> p-values and N reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. (<bold>b–e</bold>) Prediction accuracy of model M1 for the indicated metabolites. Dashed line indicates the linear regression for all data points. Legends indicates the Pearson correlation <inline-formula><mml:math id="inf28"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (including p-values, N=80 for Corner, 100 for Distributed) and RMSE for communities from the ‘corner’ set (red) or ‘distributed’ set (blue) for each variable. Solid black lines indicate.<inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> (<bold>f</bold>) Confusion matrix for classification of the ‘corner’ communities into their specified classes (shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref>). Values indicate the fraction of communities from each predicted class whose metabolite concentrations were closest (Euclidean distance) to the centroid of each class (Measured Class). Colored boxes indicate ‘sub-classes’ that fall within the four major classes determined in the lactate and butyrate concentration space as shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref>. (<bold>g</bold>) Scatter plot of misclassification rate between each pair of classes (values from (<bold>f</bold>), fraction of communities misclassified from one class to the other) versus the Euclidean distance between the centroids of that pair of classes. Black data points indicate pairs of classes that fall within the same major classes defined by the colored boxes in (<bold>f</bold>) and red data points indicate pairs of classes that do not fall within the same major class.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Metabolite production of each species grown in monoculture Bars show the mean net production or consumption of each metabolite for monocultures of each species (bar color indicates species as specified in the legend).</title><p>Error bars indicate bootstrapped 95% confidence interval on the mean of between 3 and 22 biological replicates. The dashed lines indicate +/- 10mM and the numbers on the plot indicate the number of species with mean net production of that metabolite outside the +/- 10 mM range.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Metabolite-species LIME explanations computed over a 20-fold partitioning of the data set.</title><p>Box plots of LIME explanations for the metabolites acetate, butyrate, lactate and succinate. LIME analysis to compute the impact of initial species abundances on metabolite predictions in the 25-member (full) community was performed after training on each subset of data that resulted from a 20-fold partitioning. In each box, the black horizontal line shows the median LIME explanation over the 20 samples, each box encloses the first (<bold>Q1</bold>) and third (<bold>Q3</bold>) quartiles, whiskers extend to the farthest data points within the range 1.5*(Q3 - Q1). Data points that exceed this range are considered outliers, which are shown as black circles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-figsupp5-v1.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 6.</label><caption><title>Microbe-microbe LIME explanations computed over a 20-fold partitioning of the data set.</title><p>Box plots of LIME explanations for each species in the 25-member synthetic human gut community. LIME analysis to compute the impact of initial species abundances on the end-point species abundance predictions in the 25-member (full) community was performed after training on each subset of data that resulted from a 20-fold partitioning. In each box, the black horizontal line shows the median LIME explanation over the 20 samples, each box encloses the first (<bold>Q1</bold>) and third (<bold>Q3</bold>) quartiles, and whiskers extend to the farthest data points within the range 1.5*(Q3 - Q1). Data points that exceed this range are considered outliers, which are shown as black circles. LIME explanations from each fold are normalized to the given species self-impact such that the LIME explanation of a species to predict its own abundance is equal to one.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-figsupp6-v1.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 7.</label><caption><title>Comparison of LIME explanations of LSTM to gLV Parameters.</title><p>(<bold>a</bold>) Scatter plot of LIME explanations of each species impact on each other species in model M2 versus the corresponding interspecies interaction parameter (<inline-formula><mml:math id="inf30"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) from the gLV model from <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>. Dashed line indicates the linear regression with the regression parameters shown in the legend (N=600). (<bold>b</bold>) Heatmap representation of agreement/disagreement between specific interactions for the same comparison as in (<bold>a</bold>). Legend describes what each color represents.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig3-figsupp7-v1.tif"/></fig></fig-group><p>We first used the LSTM model M1 to simulate every possible combination of &gt;10 species (26,434,916 total communities). The simulated communities separate into two regions: one centered around a dense ellipse of high butyrate concentration characterized by communities containing the butyrate-producing species <italic>Anaerostipes caccae</italic> (AC) and a second dense ellipse of communities that produce low levels of butyrate and lacked AC (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). This bimodality due to the presence/absence of AC is consistent with our previous finding that AC is the strongest driver of butyrate production in this system (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). In addition, the strong negative correlation between lactate and butyrate in the AC+ cluster of communities (<inline-formula><mml:math id="inf31"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.72</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, N=14,198,086) is consistent with the ability of AC to transform lactate into butyrate (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). These results demonstrate that the LSTM model can capture the major microbial drivers of metabolite production as well as the correlations between different metabolites.</p><p>We used our simulated metabolite production landscape to plan informative experiments for testing the predictive capabilities of our model. First, we designed a set of ‘distributed’ communities that spanned the range of typical metabolite concentrations predicted by our model. To this end, we selected 100 communities closest to the centroids of 100 clusters determined using k-means clustering of the four-dimensional metabolite space. Second, we designed a set of communities to test our model’s ability to predict extreme shifts in metabolite outputs. To do so, we identified four ‘corners’ of the distribution in the lactate and butyrate space (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). We next examined the relationship between acetate and succinate within each of these corners and found that the distributions varied depending on the given corner (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, inset). The total carbon concentration in the fermentation end products across all predicted communities displayed a narrow distribution (mean 316 mM, standard deviation 20 mM, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). The production of the four metabolites are coupled due to the structure of metabolic networks and fundamental stoichiometric constraints (<xref ref-type="bibr" rid="bib43">Oliphant and Allen-Vercoe, 2019</xref>). Therefore, the model learned the inherent ‘trade-off’ relationships between these fermentation products based on the patterns in our data. We chose a final set of 80 ‘corner’ communities for experimental validation (five communities from each combination of maximizing or minimizing each metabolite, Methods).</p><p>By experimentally characterizing the endpoint community composition and metabolite concentrations of the 180 designed communities, we found that the LSTM model M1 accurately predicted the rank order of metabolite concentrations and microbial species abundances. The LSTM model substantially outperformed the composite model (gLV and regression, model from previous work [<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>]) trained on the same data for the majority (59%) of output variables (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3a</xref>). Additionally, replacing the regression module of the composite model with either a Random Forest Regressor or a Feed Forward Network did not improve the metabolite prediction accuracy beyond that of the LSTM (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3a</xref>). One of the key limitations of the composite models is that the metabolite variables are a function of the endpoint species abundance, but the species abundances are not a function of the metabolite concentrations. By contrast, the LSTM model can capture such feedbacks between metabolites and species. Notably, the LSTM model prediction accuracy for the metabolites was similar for both the ‘distributed’ and ‘corner’ communities (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3b–e</xref>). These results indicate that our model is useful for designing communities with a broad range of metabolite profiles that includes those at the extremes of the metabolite distributions.</p><p>To determine if the LSTM model could separate groups of communities with extreme behaviors, we treated the ‘corners’ as classes and quantified the classification accuracy of our model. The model accurately classified the communities when considering only butyrate and lactate concentrations. However, the model had poorer separation when acetate and succinate were also considered in defining the classes (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3f</xref>). The misclassification rate was higher for small Euclidean distances between classes and decreased with the Euclidean distance (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3g</xref>). This implies that the insufficient variation in concentrations due to fundamental stoichiometric constraints limited our ability to define 16 distinct classes that maximized/minimized each metabolite. While model M1 accurately predicted metabolite concentrations and the majority of species abundances, several individual species abundances were poorly predicted (<inline-formula><mml:math id="inf33"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3a</xref>). Thus, we used the dataset to improve the LSTM model. To this end, we combined the new observations with the original observations and randomly partitioned the data into 90% for training and 10% for cross-validation. The resulting model (M2, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>) was substantially more predictive of species abundances (<inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all but five species FP, RI, CA, BA, CH (<xref ref-type="fig" rid="fig3">Figure 3c</xref>)).</p></sec><sec id="s2-4"><title>Using local interpretable model-agnostic explanations to decipher interactions</title><p>One of the commonly noted limitations of machine learning models is their lack of interpretability for extracting biological information about a system. Fortunately, generally applicable tools have been developed to aid in model interpretation. Thus, we sought to use such methods to decipher key relationships among variables within the LSTM to deepen our biological understanding of the system. We used local interpretable model-agnostic explanations (LIME) (<xref ref-type="bibr" rid="bib50">Ribeiro et al., 2016b</xref>), to quantify the impact of each species’ presence on each metabolite and species in each of the sub-communities used to train model M2. We used the median impact of each species presence on each metabolite or species across all training instances to generate networks that revealed microbe-metabolite (<xref ref-type="fig" rid="fig3">Figure 3d</xref>) and microbe-microbe (<xref ref-type="fig" rid="fig3">Figure 3e</xref>) interactions. In general, these networks represent broad design principles for the community metabolic outputs by indicating which species have the most consistent and strong impacts on each metabolite and species abundance across a wide range of sub-communities. For instance, the metabolite network highlights <italic>Anaerostipes caccae</italic> (AC) as having the largest positive effect on butyrate production with an additional positive contribution from EL and a negative contribution from DP, consistent with the previous composite gLV model of butyrate production by this community (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>).</p><p>In addition, the number of microbial species impacting each metabolite in these networks trended with the number of microbial species that individually produced or consumed each metabolite (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). For example, butyrate displayed the fewest edges (3) and was produced by the lowest number of individual species (4). By contrast, acetate had the most edges (6) and was produced by the largest number of individual species (19). The inferred microbe-metabolite network consisted of diverse species including Proteobacteria (DP), Actinobacteria (BA, BP, EL), Firmicutes (AC, ER, DL) and one member of Bacteroidetes (PC), but excluded members of <italic>Bacteroides</italic>. Therefore, while <italic>Bacteroides</italic> exhibited high abundance in many of the communities, they did not substantially impact the measured metabolite profiles but instead modulated species growth and thus community assembly (<xref ref-type="fig" rid="fig3">Figure 3e</xref>). We explored the consistency of LIME explanations for the full 25-member community in response to random partitions of the training data to provide insights into the sensitivity of the LIME explanations given the training data (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>, <xref ref-type="fig" rid="fig3s6">Figure 3—figure supplement 6</xref>). These results demonstrated that the direction of the strongest LIME explanations of the full community were consistent in sign despite variations in magnitude. One exception is for the species <italic>Roseburia intestinalis</italic> (RI), which had high variability across different test/train splits. This is consistent with previous observations that RI has substantial growth variability across experimental communities (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). In sum, these results demonstrate that in general the LIME explanations were robust to variations in the training data.</p><p>The LIME explanations of inter-species interactions exhibited a statistically significant correlation with their corresponding inter-species interaction parameters from a previously parameterized gLV model of this system (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>; <xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7a</xref>). The sign of the interaction was consistent in 80% of the interactions with substantial magnitude (&gt;0.05 in both the LIME explanations and gLV parameters) (<xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7b</xref>). This consistency with previous observations suggests that the LSTM model was able to capture similar broad trends in inter-species relationships as gLV (interpreted through the average LIME explanation across all observed communities). The LSTM model captured more nuanced context-specific behaviors (interpreted as the LIME explanation for one specific community context) than the mathematically restricted gLV model, which substantially improved the predictive capability of the LSTM model. These results demonstrate that the LSTM framework is useful for developing high accuracy predictive models for the design of precise community-level metabolite profiles. Our approach also preserves the ability to decipher different types of interactions in the LSTM model that are explicitly encoded in less accurate and flexible ecological models such as gLV.</p></sec><sec id="s2-5"><title>Sensitivity of LSTM model prediction accuracy highlights poorly understood species and pairwise interactions</title><p>Identification of species that limit prediction performance could guide selection of informative experiments to deepen our understanding of the behaviors of poorly predicted communities. Therefore, we evaluated the sensitivity of the LSTM model (model M2) prediction accuracy to species presence/absence and the amount of training data. High sensitivity of model prediction performance to the number of training communities indicates that collection of additional experimental data would continue to improve the model. Additionally, identifying poorly understood communities will guide machine learning-informed planning of experiments. To evaluate the model’s sensitivity to the size of the training dataset, we computed the hold-out prediction performance (<inline-formula><mml:math id="inf35"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) as a function of the size of the training set by sub-sampling the data (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). We used 20-fold cross-validation to predict metabolite concentrations and species abundance. Our results show that the ability to improve prediction accuracy as a function of the size of the training data set was limited by the variance in individual species abundance in the training set (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). For instance, certain species with low variance (e.g. FP, EL, DP, RI) in abundance in the training set displayed low sensitivity to the amount of training data and were poorly predicted by the model. The high sensitivity of specific metabolites (e.g. lactate) and species (e.g. AC, BH) to the amount of training data indicates that further data collection would likely improve the model’s prediction performance.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Hold-out prediction performance on sub-communities provides information about poorly understood species and interactions between species.</title><p>(<bold>a</bold>) Sensitivity of metabolite prediction performance (<inline-formula><mml:math id="inf36"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) to the amount of training data. Training datasets were randomly subsampled 30 times using 50–100% of the total dataset in increments of 10%. Each subsampled training set was subject to 20-fold cross-validation to assess prediction performance. Lineplot of the mean prediction performance over the 30 trials for each percentage of the data. Error bars denote 1 s.d. from the mean. (<bold>b</bold>) Schematic scatter plot representing how communities containing species A and B define a poorly predicted subsample of the full sample set (<bold>c</bold>) Heatmap of prediction performance (<inline-formula><mml:math id="inf37"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) of acetate for each subset of communities containing a given species (diagonal elements) or pair of species (off-diagonal elements). (<bold>d</bold>) Heatmap of prediction performance for acetate, butyrate, lactate, and succinate. A sample subset containing a given species or pair of species included all communities in which the species were initially present. Predictions for each community were determined using 20-fold cross validation so that for each model the predicted samples were excluded from the training samples. N and p-values are reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Sensitivity of species abundance prediction performance (<inline-formula><mml:math id="inf38"><mml:msup><mml:mi>R</mml:mi><mml:mn mathvariant="normal">2</mml:mn></mml:msup></mml:math></inline-formula>) to the size of the training dataset.</title><p>Training datasets were randomly subsampled 30 times using 50% to 100% of the total dataset in increments of 10%. Each subsampled training set was subject to 20-fold cross-validation to assess prediction performance. Sub-plots show the mean prediction performance (±one standard deviation) over the 30 trials for each percentage of the dataset. Subplots were sorted according to the variance in species abundance taken over the total dataset. In general, prediction performance of low variance species was less likely to improve in response to more training data. N and p-values are reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig4-figsupp1-v1.tif"/></fig></fig-group><p>To determine how pairwise combinations of species impacted model prediction performance, we used 20-fold cross-validation to evaluate the prediction performance (<inline-formula><mml:math id="inf39"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) on subsets of the total dataset, where subsets were selected based on the presence of individual species or pairs of species (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). Using this approach, we identified individual species and species pairs that had the greatest impact on the prediction performance of metabolite concentrations. Sample subsets with poor prediction performance highlight individual species and species pairs whose presence reduced the model’s ability to accurately predict metabolite concentrations. Although the subsets were smaller than the total data set (<inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>761</mml:mn></mml:mrow></mml:math></inline-formula>), calculation of prediction performance was not limited by small sample sizes, where the number of communities in each subset ranged from <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>77</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>478</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The interaction network shown in <xref ref-type="fig" rid="fig3">Figure 3d</xref> shows the impact of individual species on each metabolite, but does not provide information about whether the effect is due to individual species or pairwise interactions. To determine whether pairwise interactions influence metabolite concentrations, we quantified how prediction performance changed in response to the presence individual species and pairs of species. Specifically, if prediction performance taken over a subset of communities containing a given species pair was markedly different than prediction performance for the subsets corresponding to the individual species, this implies that the given pairwise interaction impacts metabolite production. Using <xref ref-type="disp-formula" rid="equ7">equation 5</xref> (Methods), we found that the prediction performance of lactate and butyrate were the least sensitive to species pairs (average decrease in prediction performance for subsets with species pairs of 0.72% and 1.10% compared to corresponding single species subsets). However, the prediction performance of acetate and succinate were the most sensitive to the presence of species pairs (increase in prediction performance of 6.68% for acetate and a decrease of 2.951% for succinate). This difference in prediction performance suggests that pairwise interactions influences the production of acetate and succinate, while the production of lactate and butyrate are primarily driven by the action of single species. The sensitivity of acetate and succinate to pairwise interactions is consistent with the inferred interaction network shown in <xref ref-type="fig" rid="fig3">Figure 3d</xref>, which highlights multiple species-metabolite interactions for acetate and succinate and sparse and strong species-metabolite interactions for butyrate and lactate.</p><p>Pairs of certain <italic>Bacteroides</italic> and butyrate producers including BY-RI, BU-RI, and BY-AC resulted in reduced prediction performance of acetate. This suggests that interactions between specific <italic>Bacteroides</italic> and butyrate producers were important for acetate transformations, which is consistent with the conversion of acetate into butyrate. Based on the LIME analysis in <xref ref-type="fig" rid="fig3">Figure 3d</xref>, AC, DP, and BP had the largest impact on lactate. Thus, the hold-out prediction performance for lactate was primarily impacted by specific pairs that include these species. In sum, these results demonstrate how the LSTM model can be used to identify informative experiments for investigating poorly understood species and interactions between species, where collection of more data would likely improve model prediction performance.</p></sec><sec id="s2-6"><title>Time-resolved measurements of communities reveal design rules for qualitatively distinct metabolite dynamics</title><p>We next leveraged the LSTM model’s dynamic capabilities to understand the temporal changes in metabolite concentrations and community assembly of the 25-member synthetic gut microbiome. To this end, we chose a representative subset of 95 out of the 180 communities from <xref ref-type="fig" rid="fig3">Figure 3b</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a</xref>, 60 communities for training, 34 for validation, plus the full 25-member community and experimentally characterized species abundance and metabolite concentrations every 16 hr during community assembly (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). We analyzed the dynamic behaviors of these communities using a clustering technique to extract high-level design rules of species presence/absence that determined qualitatively distinct temporal metabolite trajectories (i.e. broad trends consistent across a set of communities) and exploited the LSTM framework to identify context-specific impacts of individual species on metabolite production (i.e. a more fine-tuned case-by-case analysis).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Community metabolite trajectories cluster into qualitatively distinct groups which can be classified based on presence and absence of key microbial species.</title><p>(<bold>a</bold>) Schematic of experiment and network representing a minimal spanning tree across the 95 communities where weights (indicated by edge length) are equal to the Euclidean distance between the metabolite trajectories for each community. Node colors indicate clusters determined as described in the Materials and methods. Red node with black outline annotated with ‘25’ represents the 25-member community. Annotations indicate the most specific microbial species presence/absence rules that describe most data points in the cluster of the corresponding color as determined by a decision tree classifier (Materials and methods). Communities that deviate from the rules for their cluster are indicated with a border matching the color of the closest cluster whose rules they do follow. Network visualization generated using the draw_kamada_kawai function in networkx (v2.1) for Python 3. (<bold>b–g</bold>) Temporal changes in metabolite concentrations for communities within each cluster (indicated by sub-plot border color), with individual communities denoted by transparent lines. Solid lines and shaded regions represent the mean ±1 s.d. of all communities in the cluster. (<bold>h</bold>) Schematic of LSTM model training and computation of gradients to evaluate impact of species abundance on metabolite concentrations in a specific community context. (<bold>i</bold>) Heatmap of model M3 prediction accuracy for four metabolites in the 34 validation communities at each time point (Pearson correlation <inline-formula><mml:math id="inf43"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, N=34 for all tests). (<bold>j</bold>) Heatmap of the gradient analysis of model M3 as described in (<bold>h</bold>) for the full 25-species community. N and p-values are reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Characteristics of the dynamic community behaviors.</title><p>(<bold>a</bold>) Minimal spanning tree of a graph representation of the 180 communities characterized in <xref ref-type="fig" rid="fig3">Figure 3</xref> where each node is a community and each weight is the Euclidean distance between a pair of communities in the 4-dimensional metabolite space to show that the subset of communities characterized in the dynamic experiment was representative of all 180 communities characterized in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Blue and red nodes indicate the subset of communities chosen for dynamic characterization and used as training and validation examples for LSTM model M3 in <xref ref-type="fig" rid="fig5">Figure 5</xref>. These subsets were chosen by first performing kmeans clustering with <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>94</mml:mn></mml:mrow></mml:math></inline-formula> for the 180 communities and identifying the 94 communities closest to each cluster centroid and then repeating this process to subsample 34 for the 94 communities (as the training/validation split). (<bold>b</bold>) and (<bold>c</bold>) Scatter plots showing where the clusters from <xref ref-type="fig" rid="fig5">Figure 5a</xref> fall in the 48 hr metabolite measurement space for comparison with <xref ref-type="fig" rid="fig3">Figure 3b</xref>. Each datapoint represents a community with the color corresponding to the clusters in <xref ref-type="fig" rid="fig5">Figure 5a</xref>. Legend indicates the percentage of communities from each cluster that come from the ‘corner’ or ‘distributed’ sets. (<bold>d</bold>) Decision tree classifier explaining which species’ presence determines the clusters of dynamic community behavior from <xref ref-type="fig" rid="fig5">Figure 5</xref>. Annotation indicate the percentage of communities from each cluster that can be explained by the indicated paths, which are also annotated on <xref ref-type="fig" rid="fig5">Figure 5a</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Prediction accuracy of model M3 for species abundance.</title><p>Heatmap represents <inline-formula><mml:math id="inf45"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> for the prediction accuracy of model M3 of the abundance of each species at each time point in the 34 validation communities.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Comparison of the discrete generalized Lotka-Volterra model to the LSTM using the same training algorithm.</title><p>(<bold>a</bold>) Schematic detailing the implementation of the discretized gLV model and the addition of a feed-forward neural network to predict metabolites from species abundance, where <inline-formula><mml:math id="inf46"><mml:mi>A</mml:mi></mml:math></inline-formula> is a matrix of species interaction coefficients, <inline-formula><mml:math id="inf47"><mml:mi>r</mml:mi></mml:math></inline-formula> is a vector of growth rates, and ⊙ is the Hadamard product. (<bold>b</bold>) Schematic of the LSTM model, which uses an LSTM cell to compute a hidden state vector, which is the input to a feed-forward neural network that predicts a vector of species abundances and metabolite concentrations at each time step. See Computational Methods for a detailed description. (<bold>c</bold>) Scatter plot of experimentally measured (true) and predicted species absolute abundance using the approximate gLV model. gLV model prediction performance (<inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>625</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) of species abundance on held-out test data after training on the same training data used to fit LSTM model M3. (<bold>d</bold>) Scatter plot of experimentally measured (true) and predicted species absolute abundance using the LSTM + FFN model (<inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>625</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>e</bold>) Scatter plot of experimentally measured (true) and predicted metabolite concentrations using the gLV + FFN model (<inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>105</mml:mn></mml:mrow></mml:math></inline-formula> for every metabolite). (<bold>f</bold>) Scatter plot of experimentally measured (true) and predicted metabolite concentrations using the LSTM +FFN model (<inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>105</mml:mn></mml:mrow></mml:math></inline-formula> for every metabolite). Lines denote.<inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73870-fig5-figsupp3-v1.tif"/></fig></fig-group><p>The temporal trajectories of species abundance and metabolite concentrations showed a wide range of qualitatively distinct trends across the 95 communities (<xref ref-type="fig" rid="fig5">Figure 5b–g</xref>). For example, some metabolites concentrations monotonically increased (e.g. butyrate in <xref ref-type="fig" rid="fig5">Figure 5b, c, e and g</xref>), monotonically decreased (e.g. lactate in <xref ref-type="fig" rid="fig5">Figure 5b, c</xref>) or exhibited biphasic dynamics (e.g. acetate in <xref ref-type="fig" rid="fig5">Figure 5c</xref>). To determine if there were communities with similar temporal changes in metabolite concentrations, we clustered communities using a minimal spanning tree (<xref ref-type="bibr" rid="bib15">Grygorash et al., 2006</xref>) on the Euclidean distance between the metabolite trajectories of each pair of communities (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The resulting six clusters exhibited high quantitative within-cluster similarity and qualitatively distinct metabolite trajectories (<xref ref-type="fig" rid="fig5">Figure 5b–g</xref>). Clusters 4 and 5, which contained the largest number of communities, had a high fraction of ‘distributed’ communities (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). Clusters with a smaller number of communities contained a higher percentage of ‘corner’ communities (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b</xref>,c). Therefore, the LSTM model informed by endpoint measurements of species abundance and metabolite concentrations elucidated ‘corner’ communities with qualitatively distinct temporal behaviors. These communities were unlikely to be discovered via random sampling of sub-communities due to the high density of points towards the center of the distribution and low density of communities in the tails of the distribution (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). Additionally, some ‘corner’ communities that were similar in metabolite profiles when considering the endpoint measurement separated into different clusters when considering the dynamic data (e.g. Clusters 2 and 3, which have similar metabolite profiles at 48 hr but qualitatively distinct dynamics) (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This demonstrates that using a community design approach to explore the extremes of system behaviors with a limited time resolution enabled the identification of new features when the communities with extreme functions were characterized with higher time resolution.</p><p>To identify general patterns in species presence/absence of these communities that could explain the temporal behaviors of each cluster, we used a decision tree analysis to identify an interpretable classification scheme (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>). Using this approach, the large clusters were separated by relatively simple classification rules (i.e. AC+ for Cluster 4 and AC- for Cluster 5), whereas the smaller clusters had more complex classification rules involving larger combinations of species (3–7 species), all involving AC, DP, and DL (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The influential role of DP was corroborated by a previous study showing that DP substantially inhibits butyrate production (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). In addition, the inferred microbe-metabolite networks based on the LSTM model M2 demonstrated that the presence of DL was linked to higher acetate and lower succinate production (<xref ref-type="fig" rid="fig3">Figure 3d</xref>), consistent with its key role in shaping metabolite dynamics in this system. The variation in the number of communities across clusters is consistent with previous observations that species-rich microbial communities tend towards similar behavior(s) (e.g. Clusters 4 and 5 contained many communities). By contrast, more complex species presence/absence design rules are required to identify communities that deviate from this typical behavior (e.g. Clusters 1–3 and 6 contained few communities) (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>).</p></sec><sec id="s2-7"><title>Using LSTM with higher time-resolution to interpret contextual interactions</title><p>While our clustering analysis identified general design rules for metabolite trajectories, there remained unexplained within-cluster variation. Thus, we used the LSTM framework to identify those effects beyond these general species presence/absence rules that determine the precise metabolite trajectory of a given community. Simultaneous predictions of species abundance and the concentration of all four metabolites at all time points necessitates specific modifications to the LSTM architecture shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref>. In particular, we consider a 29-dimensional input vector whose first 25 components correspond to the species abundance, while the remaining four components correspond to the concentration of metabolites (<xref ref-type="fig" rid="fig5">Figure 5h</xref>). The 29-dimensional feature vector is suitably normalized so that the different components have zero mean and unity variance. The feature scaling is important to prevent dominance of high-abundance species. The output of each LSTM unit is fed into the input block of the subsequent LSTM unit in order to advance the model forward in time. The reason behind concatenating instantaneous species abundances with metabolite concentrations can be understood as follows. Prediction of metabolite concentrations at various time points requires a time-series model (either using ODEs or LSTM in this case). Further, the future trajectory of metabolite concentrations is a function of both the species abundance, as well as the metabolite concentrations at the current time instant. Therefore, we concatenate both the metabolite concentrations and species abundances to create a 29-dimensional feature vector. The trained LSTM framework on the 60 training communities (model M3) displayed good prediction performance on the metabolite concentrations of the 34 validation communities plus the full 25-species community (<xref ref-type="fig" rid="fig5">Figure 5i</xref>). The prediction accuracy of species abundance was lower than metabolite concentrations, presumably due to the limited number of training set observations of each species (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p><p>We used a gradient-based sensitivity analysis of the LSTM model M3 to provide biological insights into the contributions of individual species based on the temporal changes in metabolite concentrations (<xref ref-type="fig" rid="fig5">Figure 5h and j</xref>, Methods). This method involves computing partial derivatives of output variables of interest with respect to input variables, which are readily available through a single backpropagation pass (<xref ref-type="bibr" rid="bib31">LeCun et al., 1988</xref>; <xref ref-type="bibr" rid="bib46">Peurifoy et al., 2018</xref>). As an example case, we applied this analysis approach to the full 25-species community, which was grouped into Cluster 4, with the design rule ‘AC+’ (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Consistent with this design rule, we observed strong sensitivity gradients between the abundance of AC and the concentrations of butyrate, acetate, and lactate, consistent with our biological understanding of the system (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Beyond the ‘AC+’ design rule, there was a strong sensitivity gradient between DL and acetate and succinate, consistent with the inferred networks based on the LSTM model M2 that used endpoint measurements (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Further, the contributions of certain species on metabolite production varied as a function of time. For instance, in the initial time point, species abundances were similar and thus the contribution of individual species to metabolite production was uniform. However, interactions between species during community assembly enhanced the contribution of specific metabolite driver species such as AC. In addition, the contributions of individual species such as PC and BA to succinate production peaked at 32 hr and then decreased by 48 hr, highlighting that the effects of these species were maximized at intermediate time points. In sum, the gradient-based method identified the quantitative contributions of each species to the temporal changes in metabolite concentrations for a representative 25-member community, identifying context-specific behaviors beyond the previously identified broader design rules. These two complementary approaches are useful for identifying design rules governing metabolite dynamics. The clustering method can identify broad design rules for species presence/absence and the LSTM analysis gradient approach can uncover fine-tuned quantitative contributions of species to the temporal changes in community-level functions.</p><p>To directly evaluate the performance of the gLV and LSTM model, we trained a discretized version of the gLV model (approximate gLV model) on the same dataset and used the same algorithm as the LSTM. The approximate gLV model was augmented with a two layer feed-forward neural network with a hidden dimension equivalent to the hidden dimension used in the LSTM model to enable metabolite predictions (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3a, b</xref>). The approximate gLV model enables the computation of gradients via the backpropagation algorithm, which is also used to train the LSTM. By contrast, computation of gradients of the continuous-time gLV model requires numerical integration. This approximate gLV model does not perform as well as the LSTM model at species abundance predictions using the same data used to train LSTM model M3 (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3c, b</xref>). In addition, the LSTM outperforms the approximate gLV augmented with the feed-forward network at metabolite predictions (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3e, f</xref>). In sum, the LSTM outperforms the discretized gLV model using the same training algorithm, highlighting the power of the LSTM model in accurately predicting the temporal changes in microbiome composition and metabolite concentrations.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The LSTM modeling framework trained on species abundance and metabolite concentrations accurately predicted multiple health-relevant functions of complex synthetic human gut communities. This model is powerful for designing communities with target metabolite profiles. Microbial communities continuously impact metabolites by releasing or consuming them. Therefore, by modeling both microbial growth and the metabolites they produce/consume together, the LSTM captured the interconnections between these variables. Due to its flexibility, the LSTM model outperforms the widely used gLV model in the presence of higher-order interactions. We leveraged the computational efficiency of LSTM model to predict the metabolite profiles of tens of millions of communities. We used these model predictions to identify sparsely represented ‘corner case’ communities that maximized/minimized community-level production of four health-relevant metabolites. In the absence of a predictive model, these infrequent communities would have been difficult to discover among the vast metabolite profile landscape of possible communities.</p><p>Beyond the model’s predictive capabilities, we showed that biological information including significant microbe-metabolite and microbe-microbe interactions, can be extracted from LSTM models. These biological insights could enable the discovery of key species and interactions driving community functions of interest. Further, this could inform the design of microbial communities from the bottom-up or interventions to manipulate community-level behaviors. For example, the inferred microbe-metabolite network highlighted AC is a major ecological driver of several metabolites including butyrate, acetate and lactate in our system. In addition, this microbe-metabolite network did not include species of the highly abundant genus <italic>Bacteroides</italic> but instead featured members of Firmicutes (AC, ER, DL), Actinobacteria (BA, BP, EL), Proteobacteria DP and Bacteroidetes PC. Notably, <italic>Bacteroides</italic> displayed numerous interactions in the microbe-microbe interaction network, suggesting that they played a key role in the growth of constituent community members opposed to production of specific metabolites. Therefore, our model suggests that <italic>Bacteroides</italic> influence broad ecosystem functions such as community growth dynamics whereas species highlighted in the microbe-metabolite network contribute to specialized functions such as the production of specific measured metabolites (<xref ref-type="bibr" rid="bib52">Rivett and Bell, 2018</xref>). Therefore, the microbe-metabolite interaction network could be used to identify key species that could be targeted for manipulating the dynamics of specific metabolites.</p><p>We performed time-resolved measurements of metabolite production and species abundance using a set of designed communities and demonstrated that communities tend towards a typical dynamic behavior (i.e. Clusters 4 and 5). Therefore, random sampling of sub-communities from the 25-member system would likely exhibit behaviors similar to Clusters 4 and 5. We used the LSTM model to identify ‘corner cases’ communities that displayed metabolite concentrations near the tails of the metabolite distributions at the endpoint. The model allowed us to identify unique sub-clusters with disparate dynamic behaviors. We demonstrated that the endpoint model predictions were confirmatory (<xref ref-type="fig" rid="fig3">Figure 3c</xref>) and also led to new discoveries when additional measurements were made in the time dimension. Specifically, certain ‘corner cases’ communities identified based on prediction of a single time-point displayed distinct dynamic trajectories. For instance, Clusters 2 and 3 based on the decision tree classifier displayed similar end-point metabolite concentrations (<xref ref-type="fig" rid="fig5">Figure 5c, d</xref>). However, lactate decreased immediately over time in Cluster 2 communities but remained high until approximately 30 hr and then decreased in Cluster 3 communities. The design rule for Cluster 3 included the presence of lactate producers BU and DL (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>), suggesting that these individual species’ lactate producing capabilities enabled the community to maintain a high lactate concentration for an extended period of time in the context of the Cluster 3 communities. While we focused on the production of four health-relevant metabolites produced by gut microbiota, a wide range of health-relevant compounds are produced by gut bacteria. Therefore, communities that cluster together based on dynamic trends in the four measured metabolites could separate into new clusters based on the temporal patterns of other compounds produced or degraded by the communities.</p><p>Time-resolved measurements were required to reveal the different dynamic behaviors of communities in Clusters 2 and 3 to improve our understanding and the design of community functions. The ability to resolve differences in the dynamic trajectories of communities requires time sampling when the system behavior is changing as a function of time as opposed to time sampling once the system has reached a steady-state (i.e. saturated as a function of time). The time to reach steady-state varied across different communities and metabolites of interest. For instance, lactate reached steady-state at an earlier time point (12 hr) in Cluster 4 communities whereas communities in Cluster 3 approached steady-state at a later time point (48 hr). Therefore, model-guided experimental planning could be used to identify the optimal sampling times to resolve differences in community dynamic behaviors. Achieving a highly predictive LSTM model required substantially less training data than a previous study that approximated the behavior of mechanistic biological systems models with RNNs (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="bibr" rid="bib67">Wang et al., 2019</xref>). While the performance of any data-driven algorithm improves with the quantity and quality of available data, we demonstrate that the LSTM can translate learning on lower-order communities to accurately predict the behavior of higher-order communities given a limited and informative training set that is experimentally feasible. For synthetic microbial communities, the quality of the training set depends on the frequency of time-series measurements within periods in which the system displays rich dynamic behaviors (i.e. excitation of the dynamic modes of the system), the range of initial species richness, representation of each community member in the training data and sufficient variation in species abundances or metabolite concentrations (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The dynamic behaviors of the synthetic communities characterized in vitro may likely exhibit significant differences to their behaviors in new environments such as the mammalian gut. However, communities in sub-clusters whose behaviors deviated substantially from the typical community behaviors (e.g. Clusters 2 and 3 versus Clusters 4 and 5) may be more likely than random to display unique dynamic behaviors in vivo. Future work will investigate whether the in vitro dynamic behavior cluster patterns can be used as prior information to guide the design of informative communities in new environments for building predictive models.</p><p>The current implementation of the LSTM model lacks uncertainty quantification for individual predictions, which could be used to guide experimental design (<xref ref-type="bibr" rid="bib47">Radivojević et al., 2020</xref>). Recent progress in using Bayesian recurrent neural networks has led to emergence of Bayesian LSTMs (<xref ref-type="bibr" rid="bib10">Fortunato et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Li et al., 2021</xref>), which provides uncertainty quantification for each prediction in the form of posterior variance or posterior confidence interval. However, currently, the implementation and training of such Bayesian neural networks can be significantly more difficult than training the LSTM model developed here. In addition, we benchmarked the performance of the LSTM against a widely used gLV model which has been demonstrated to accurately predict community assembly in communities with up to 25 species (<xref ref-type="bibr" rid="bib66">Venturelli et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). The gLV model has been modified mathematically to capture more complex system behaviors (<xref ref-type="bibr" rid="bib39">McPeek, 2017</xref>). However, implementation of these gLV models to represent the behaviors of microbiomes with a large number of interacting species poses major computational challenges.</p><p>While our current approach treated microbiome species composition as the sole set of design variables in a constant environmental background, microbiomes in reality are impacted by differences in the physicochemical composition of their environment (<xref ref-type="bibr" rid="bib64">Thompson et al., 2017</xref>). Given sufficient observations of community behavior under varied environmental contexts (e.g. presence/absence of certain nutrients), our LSTM approach could be further leveraged to design complementary species and environmental compositions for desired microbiome functional dynamics. Further, we can leverage the wealth of biological information stored in the sequenced genomes of the constituent organisms. Integrating methods such as genome scale models (<xref ref-type="bibr" rid="bib38">Magnúsdóttir et al., 2017</xref>) with our LSTM framework could leverage genomic information to enable predictions when the genomes of the organisms are varied (i.e. alternative strains of the same species with disparate metabolic capabilities). In this case, introducing variables representing the presence/absence of specific metabolic reactions would potentially enable the model to predict the impact of a species with a varied set of metabolic reactions on a given set of functions without new experimental observations. Integrating this information into the model could thus enable a mapping between genome information and community-level functions.</p><p>While previous approaches have used machine learning methods to predict microbiome functions based on microbiome species composition (<xref ref-type="bibr" rid="bib30">Le et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Larsen et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Thompson et al., 2019</xref>), our approach is a major step forward in predicting the future temporal trajectory of microbiome functions from an initial species composition. The dynamic nature of our approach enables the design of optimal initial community compositions or interventions to steer a community to a desired future state. The flexibility of our approach to various time resolutions could be especially useful in scenarios where a microbiome may display undesired transients on the path from an initial state to a desired final state. For instance, in treatment of gut microbiome dysbiosis, it is important to ensure that any transient states of the microbiome are not harmful to the host (e.g. pathogen blooms or overproduction of toxic metabolites) as the system approaches a desired healthy state (<xref ref-type="bibr" rid="bib69">Xiao et al., 2020</xref>). However, because predictions with increased time resolution require more data for model training, the ability of our approach to predict system behaviors based on initial and final observations is useful for scenarios where transient states may be less important, such as in bioprocesses where the concentration of products at the time of harvest is the primary design objective (<xref ref-type="bibr" rid="bib70">Yenkie et al., 2016</xref>). Finally, the computational efficiency and accuracy of the LSTM model could be exploited in the future for autonomous design and optimization of multifunctional communities via computer-controlled design-test-learn cycles (<xref ref-type="bibr" rid="bib26">King et al., 2009</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Strain, strain background (<italic>Prevotella copri</italic> CB7)</td><td align="left" valign="bottom">PC</td><td align="left" valign="bottom">DSM 18205</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Parabacteroides johnsonii</italic> M-165)</td><td align="left" valign="bottom">PJ</td><td align="left" valign="bottom">DSM 18315</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bacteroides vulgatus</italic> NCTC 11154)</td><td align="left" valign="bottom">BV</td><td align="left" valign="bottom">ATCC 8482</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bacteroides fragilis</italic> EN-2)</td><td align="left" valign="bottom">BF</td><td align="left" valign="bottom">DSM 2151</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bacteroides ovatus</italic> NCTC 11153)</td><td align="left" valign="bottom">BO</td><td align="left" valign="bottom">ATCC 8483</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bacteroides thetaiotaomicron</italic> VPI 5482)</td><td align="left" valign="bottom">BT</td><td align="left" valign="bottom">ATCC 29148</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bacteroides caccae</italic> VPI 3452 A)</td><td align="left" valign="bottom">BC</td><td align="left" valign="bottom">ATCC 43185</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bacteroides cellulosilyticus</italic> CRE21)</td><td align="left" valign="bottom">BY</td><td align="left" valign="bottom">DSMZ 14838</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bacteroides uniformis</italic> VPI 0061)</td><td align="left" valign="bottom">BU</td><td align="left" valign="bottom">DSM 6597</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Desulfovibrio piger</italic> VPI C3-23)</td><td align="left" valign="bottom">DP</td><td align="left" valign="bottom">ATCC 29098</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bifidobacterium longum</italic> subs. infantis S12)</td><td align="left" valign="bottom">BL</td><td align="left" valign="bottom">DSM 20088</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bifidobacterium adolescentis</italic> E194a (Variant a))</td><td align="left" valign="bottom">BA</td><td align="left" valign="bottom">ATCC 15703</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Bifidobacterium pseudocatenulatum</italic> B1279)</td><td align="left" valign="bottom">BP</td><td align="left" valign="bottom">DSM 20438</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Collinsella aerofaciens</italic> VPI 1003)</td><td align="left" valign="bottom">CA</td><td align="left" valign="bottom">DSM 3979</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Eggerthella lenta</italic> 1899 B)</td><td align="left" valign="bottom">EL</td><td align="left" valign="bottom">DSM 2243</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Faecalibacterium prausnitzii</italic> A2-165)</td><td align="left" valign="bottom">FP</td><td align="left" valign="bottom">DSM 17677</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Clostridium hiranonis</italic> T0-931)</td><td align="left" valign="bottom">CH</td><td align="left" valign="bottom">DSM 13275</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Anaerostipes caccae</italic> L1-92)</td><td align="left" valign="bottom">AC</td><td align="left" valign="bottom">DSM 14662</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Blautia hydrogenotrophica</italic> S5a33)</td><td align="left" valign="bottom">BH</td><td align="left" valign="bottom">DSM 10507</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Clostridium asparagiforme</italic> N6)</td><td align="left" valign="bottom">CG</td><td align="left" valign="bottom">DSM 15981</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Eubacterium rectale</italic> VPI 0990)</td><td align="left" valign="bottom">ER</td><td align="left" valign="bottom">ATCC 33656</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Roseburia intestinalis</italic> L1-82)</td><td align="left" valign="bottom">RI</td><td align="left" valign="bottom">DSM 14610</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Coprococcus comes</italic> VPI CI-38)</td><td align="left" valign="bottom">CC</td><td align="left" valign="bottom">ATCC 27758</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Dorea longicatena</italic> 111–35)</td><td align="left" valign="bottom">DL</td><td align="left" valign="bottom">DSMZ 13814</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Strain, strain background (<italic>Dorea formicigenerans</italic> VPI C8-13)</td><td align="left" valign="bottom">DF</td><td align="left" valign="bottom">DSM 3992</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: ATCACG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC ATCACG ACACTCTTTCCCTACACGACGCTCTTCCGATCT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CGATGT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CGATGT ACACTCTTTCCCTACACGACGCTCTTCCGATCT T ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TTAGGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TTAGGC ACACTCTTTCCCTACACGACGCTCTTCCGATCT GT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TGACCA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TGACCA ACACTCTTTCCCTACACGACGCTCTTCCGATCT CGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: ACAGTG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC ACAGTG ACACTCTTTCCCTACACGACGCTCTTCCGATCT ATGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GCCAAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GCCAAT ACACTCTTTCCCTACACGACGCTCTTCCGATCT TGCGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CAGATC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CAGATC ACACTCTTTCCCTACACGACGCTCTTCCGATCT GAGTGG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: ACTTGA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC ACTTGA ACACTCTTTCCCTACACGACGCTCTTCCGATCT CCTGGAG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GATCAG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GATCAG ACACTCTTTCCCTACACGACGCTCTTCCGATCT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TAGCTT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TAGCTT ACACTCTTTCCCTACACGACGCTCTTCCGATCT T ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GGCTAC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GGCTAC ACACTCTTTCCCTACACGACGCTCTTCCGATCT GT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CTTGTA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CTTGTA ACACTCTTTCCCTACACGACGCTCTTCCGATCT CGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: AGTCAA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC AGTCAA ACACTCTTTCCCTACACGACGCTCTTCCGATCT ATGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: AGTTCC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC AGTTCC ACACTCTTTCCCTACACGACGCTCTTCCGATCT TGCGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: ATGTCA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC ATGTCA ACACTCTTTCCCTACACGACGCTCTTCCGATCT GAGTGG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CCGTCC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CCGTCC ACACTCTTTCCCTACACGACGCTCTTCCGATCT CCTGGAG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GTAGAG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GTAGAG ACACTCTTTCCCTACACGACGCTCTTCCGATCT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GTCCGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GTCCGC ACACTCTTTCCCTACACGACGCTCTTCCGATCT T ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GTGAAA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GTGAAA ACACTCTTTCCCTACACGACGCTCTTCCGATCT GT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GTGGCC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GTGGCC ACACTCTTTCCCTACACGACGCTCTTCCGATCT CGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GTTTCG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GTTTCG ACACTCTTTCCCTACACGACGCTCTTCCGATCT ATGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CGTACG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CGTACG ACACTCTTTCCCTACACGACGCTCTTCCGATCT TGCGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GAGTGG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GAGTGG ACACTCTTTCCCTACACGACGCTCTTCCGATCT GAGTGG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GGTAGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GGTAGC ACACTCTTTCCCTACACGACGCTCTTCCGATCT CCTGGAG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: ACTGAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC ACTGAT ACACTCTTTCCCTACACGACGCTCTTCCGATCT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: ATGAGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC ATGAGC ACACTCTTTCCCTACACGACGCTCTTCCGATCT T ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: ATTCCT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC ATTCCT ACACTCTTTCCCTACACGACGCTCTTCCGATCT GT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CAAAAG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CAAAAG ACACTCTTTCCCTACACGACGCTCTTCCGATCT CGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CAACTA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CAACTA ACACTCTTTCCCTACACGACGCTCTTCCGATCT ATGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CACCGG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CACCGG ACACTCTTTCCCTACACGACGCTCTTCCGATCT TGCGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CACGAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CACGAT ACACTCTTTCCCTACACGACGCTCTTCCGATCT GAGTGG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CACTCA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CACTCA ACACTCTTTCCCTACACGACGCTCTTCCGATCT CCTGGAG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CAGGCG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CAGGCG ACACTCTTTCCCTACACGACGCTCTTCCGATCT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CATGGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CATGGC ACACTCTTTCCCTACACGACGCTCTTCCGATCT T ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CATTTT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CATTTT ACACTCTTTCCCTACACGACGCTCTTCCGATCT GT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CCAACA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CCAACA ACACTCTTTCCCTACACGACGCTCTTCCGATCT CGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CGGAAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CGGAAT ACACTCTTTCCCTACACGACGCTCTTCCGATCT ATGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CTAGCT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CTAGCT ACACTCTTTCCCTACACGACGCTCTTCCGATCT TGCGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CTATAC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CTATAC ACACTCTTTCCCTACACGACGCTCTTCCGATCT GAGTGG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: CTCAGA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC CTCAGA ACACTCTTTCCCTACACGACGCTCTTCCGATCT CCTGGAG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: GACGAC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC GACGAC ACACTCTTTCCCTACACGACGCTCTTCCGATCT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TAATCG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TAATCG ACACTCTTTCCCTACACGACGCTCTTCCGATCT T ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TACAGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TACAGC ACACTCTTTCCCTACACGACGCTCTTCCGATCT GT ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TATAAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TATAAT ACACTCTTTCCCTACACGACGCTCTTCCGATCT CGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TCATTC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TCATTC ACACTCTTTCCCTACACGACGCTCTTCCGATCT ATGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TCCCGA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TCCCGA ACACTCTTTCCCTACACGACGCTCTTCCGATCT TGCGA ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TCGAAG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TCGAAG ACACTCTTTCCCTACACGACGCTCTTCCGATCT GAGTGG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Forward Primer Index: TCGGCA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">AATGATACGGCGACCACCGAGATCTACAC TCGGCA ACACTCTTTCCCTACACGACGCTCTTCCGATCT CCTGGAG ACTCCTACGGGAGGCAGCAGT</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: ATCACGAG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT ATCACGAG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CGATGTTC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CGATGTTC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT A ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TTAGGCGA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TTAGGCGA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TC ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TGACCAAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TGACCAAT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CTA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: ACAGTGCT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT ACAGTGCT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT GATA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GCCAATGT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GCCAATGT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ACTCA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CAGATCGA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CAGATCGA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TTCTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: ACTTGAAA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT ACTTGAAA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CACTTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GATCAGTG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GATCAGTG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TCTACCTC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TCTACCTC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT A ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CTTGTATG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CTTGTATG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TC ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TAGCTTCC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TAGCTTCC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CTA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GGCTACCA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GGCTACCA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT GATA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: ATGCACTT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT ATGCACTT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ACTCA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GACGGAAC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GACGGAAC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TTCTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: AGCCTTGG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT AGCCTTGG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CACTTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CCGTAGAG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CCGTAGAG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GTGAGACT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GTGAGACT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT A ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: AATGCTCA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT AATGCTCA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TC ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GCATCGTA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GCATCGTA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CTA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CGAACAGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CGAACAGC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT GATA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TCGGAAGG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TCGGAAGG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ACTCA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TTCTGTCG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TTCTGTCG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TTCTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GTACTCAC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GTACTCAC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CACTTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: AGTAATAC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT AGTAATAC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CAAGATAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CAAGATAT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT A ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TGTTTGGT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TGTTTGGT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TC ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CTCCAACC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CTCCAACC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CTA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: AAATTCTG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT AAATTCTG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT GATA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CCCGCCAA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CCCGCCAA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ACTCA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TACAAATA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TACAAATA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TTCTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GGGCTATA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GGGCTATA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CACTTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TTTCGGAC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TTTCGGAC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TGCGCGTC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TGCGCGTC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT A ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TCCCGCTG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TCCCGCTG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TC ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GTTTCAGG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GTTTCAGG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CTA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GGAGGGGG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GGAGGGGG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT GATA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GCTGTTAG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GCTGTTAG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ACTCA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: GAGTGTGA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT GAGTGTGA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TTCTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CGTCCCCG</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CGTCCCCG GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CACTTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CCTCATCA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CCTCATCA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CCACGACA</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CCACGACA GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT A ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: CATTGGCT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT CATTGGCT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TC ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: AGGGGCCC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT AGGGGCCC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CTA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: ACGACACT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT ACGACACT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT GATA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: ACCGACGC</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT ACCGACGC GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT ACTCA ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: TATAGTAT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT TATAGTAT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT TTCTCT ggactaccagggtatctaatcctgt</td></tr><tr><td align="left" valign="bottom">Sequence-based reagent</td><td align="left" valign="bottom">Reverse Primer Index: AACTCAGT</td><td align="left" valign="bottom">IDT</td><td align="left" valign="bottom"/><td align="left" valign="bottom">CAAGCAGAAGACGGCATACGAGAT AACTCAGT GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT CACTTCT ggactaccagggtatctaatcctgt</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Experimental methods</title><sec id="s4-1-1"><title>Strain maintenance and culturing</title><p>All anaerobic culturing was carried out in an anaerobic chamber with an atmosphere of 2.5 ± 0.5% H<sub>2</sub>, 15±1% CO<sub>2</sub> and balance N<sub>2</sub>. All prepared media and materials were placed in the chamber at least overnight before use to equilibrate with the chamber atmosphere. The strains used in this work were obtained from the sources listed in our previous publication (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>) and permanent stocks of each were stored in 25% glycerol at -80°C. Batches of single-use glycerol stocks were produced for each strain by first growing a culture from the permanent stock in anaerobic basal broth (ABB) media (Oxoid) to stationary phase, mixing the culture in an equal volume of 50% glycerol, and aliquoting 400µL into Matrix Tubes (ThermoFisher) for storage at -80°C. Quality control for each batch of single-use glycerol stocks included (1) plating a sample of the aliquoted mixture onto LB media (Sigma-Aldrich) for incubation at 37°C in ambient air to detect aerobic contaminants and (2) Illumina sequencing of 16 S rDNA isolated from pellets of the aliquoted mixture to verify the identity of the organism. For each experiment, precultures of each species were prepared by thawing a single-use glycerol stock and combining the inoculation volume and media as described in <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref> to a total volume of 5 mL (multiple tubes inoculated if more preculture volume needed). Cultures were incubated until stationary phase at 37°C using the preculture incubation times described in <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>. All experiments were performed in a chemically defined medium (DM38), as previously described (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). This medium supports the individual growth of all organisms except <italic>Faecalibacterium prausnitzii</italic> (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>).</p></sec><sec id="s4-1-2"><title>Community culturing experiments and sample collection</title><p>Synthetic communities were assembled using liquid handling-based automation as described previously (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Briefly, each species’ preculture was diluted to an OD<sub>600</sub> of 0.0066 in DM38. Community combinations were arrayed in 96 deep well (96DW) plates by pipetting equal volumes of each species’ diluted preculture into the appropriate wells using a Tecan Evo Liquid Handling Robot inside an anaerobic chamber. For experiments with multiple time points, duplicate 96DW plates were prepared for each time point. Each 96DW plate was covered with a semi-permeable membrane (Diversified Biotech) and incubated at 37°C. After the specified time had passed, 96DW plates were removed from the incubator and samples were mixed by pipette. Cell density was measured by pipetting 200µL of each sample into one 96-well microplate (96 W MP) and diluting 20 L of each sample into 180µL of PBS in another 96 W MP and measuring the OD<sub>600</sub> of both plates (Tecan F200 Plate Reader). We selected the value that was within the linear range of the instrument for each sample. A total of 200µL of each sample was transferred to a new 96DW plate and pelleted by centrifugation at 2400xg for 10 min. A supernatant volume of 180µL was removed from each sample and transferred to a 96-well microplate for storage at -20°C and subsequent metabolite quantification by high performance liquid chromatography (HPLC). Cell pellets were stored at -80°C for subsequent genomic DNA extraction and 16 S rDNA library preparation for Illumina sequencing. 20µL of each supernatant was used to quantify pH using a phenol Red assay (<xref ref-type="bibr" rid="bib60">Silverstein, 2012</xref>). Phenol red solution was diluted to 0.05% weight per volume in 0.9% w/v NaCl. Bacterial supernatant (20µL) was added to 180µ of phenol red solution in a 96 W MP, and absorbance was measured at 560 nm (Tecan Spark Plate Reader). A standard curve was produced by fitting the Henderson-Hasselbach equation to fresh media with a pH ranging between 3 and 11 measured using a standard electro-chemical pH probe (Mettler-Toledo). We used (1) to map the pH values to the absorbance measurements.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtext>pH</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mtext>pK</mml:mtext><mml:msub><mml:mi/><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mtext>min</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The parameters <inline-formula><mml:math id="inf53"><mml:mi>b</mml:mi></mml:math></inline-formula> and pK<italic>a</italic> were determined using a linear regression between pH and the <inline-formula><mml:math id="inf54"><mml:mi>log</mml:mi></mml:math></inline-formula> term for the standards in the linear range of absorbance (pH between 5.2 and 11) with <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>A</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula> representing the absorbance of the pH 11 standard, <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>A</mml:mi><mml:mtext>min</mml:mtext></mml:msub></mml:math></inline-formula> denoting the absorbance of the pH 3 standard and <inline-formula><mml:math id="inf57"><mml:mi>A</mml:mi></mml:math></inline-formula> representing the absorbance of each condition.</p></sec><sec id="s4-1-3"><title>HPLC quantification of organic acids</title><p>Butyrate, succinate, lactate, and acetate concentrations in culture supernatants were quantified as described previously (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Supernatant samples were thawed in a room temperature water bath before addition of 2µL of <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mtext>H</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mtext>SO</mml:mtext><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to precipitate any components that might be incompatible with the running buffer. The samples were then centrifuged at 2400xg for 10 min and then 150µL of each sample was filtered through a 0.2µm filter using a vacuum manifold before transferring 70µL of each sample to an HPLC vial. HPLC analysis was performed using a Shimadzu HPLC system equipped with a SPD-20AV UV detector (210 nm). Compounds were separated on a 250×4.6 mm Rezex OA-Organic acid LC column (Phenomenex Torrance, CA) run with a flow rate of 0.2 ml min<sub>-1</sub> and at a column temperature of -50°C. The samples were held at 4°C prior to injection. Separation was isocratic with a mobile phase of HPLC grade water acidified with 0.015 N <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mtext>H</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf60"><mml:mrow><mml:mn>415</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>LL</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). At least two standard sets were run along with each sample set. Standards were 100, 20, and 4 mM concentrations of butyrate, succinate, lactate, and acetate, respectively. The injection volume for both sample and standard was 25µL. The resultant data was analyzed using the Shimadzu LabSolutions software package.</p></sec><sec id="s4-1-4"><title>Genomic DNA extraction and sequencing library preparation</title><p>Genomic DNA extraction and sequencing library preparation were performed as described previously (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Genomic DNA was extracted from cell pellets using a modified version of the Qiagen DNeasy Blood and Tissue Kit protocol. First, pellets in 96DW plates were removed from -80°C and thawed in a room temperature water bath. Each pellet was resuspended in <inline-formula><mml:math id="inf61"><mml:mrow><mml:mn>180</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of enzymatic lysis buffer (20 mM Tris-HCl (Invitrogen), 2 mM Sodium EDTA (Sigma-Aldrich), 1.2% Triton X-100 (Sigma-Aldrich), 20 mg/mL Lysozyme from chicken egg white (Sigma-Aldrich)). Plates were then covered with a foil seal and incubated at 37°C for 30 min with orbital shaking at 600 RPM. Then, <inline-formula><mml:math id="inf62"><mml:mrow><mml:mn>25</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf63"><mml:mrow><mml:mn>20</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>mgmL</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> Proteinase K (VWR) and 200 L of Buffer AL (QIAGEN) were added to each sample before mixing with a pipette. Plates were then covered by a foil seal and incubated at 56°C for 30 min with orbital shaking at 600 RPM. Next, <inline-formula><mml:math id="inf64"><mml:mrow><mml:mn>200</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of 100% ethanol (Koptec) was added to each sample before mixing and samples were transferred to a Nucleic Acid Binding (NAB) plate (Pall) on a vacuum manifold with a 96DW collection plate. Each well in the NAB plate was then washed once with <inline-formula><mml:math id="inf65"><mml:mrow><mml:mn>500</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> Buffer AW1 (QIAGEN) and once with <inline-formula><mml:math id="inf66"><mml:mrow><mml:mn>500</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of Buffer AW2 (QIAGEN). A vacuum was applied to the Pall NAB plate for an additional 10 min to remove any excess ethanol. Samples were then eluted into a clean 96DW plate from each well using <inline-formula><mml:math id="inf67"><mml:mrow><mml:mn>110</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of Buffer AE (QIAGEN) preheated to 56°C. Genomic DNA samples were stored at -20°C until further processing.</p><p>Genomic DNA concentrations were measured using a SYBR Green fluorescence assay and then normalized to a concentration of <inline-formula><mml:math id="inf68"><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>ngL</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> by diluting in molecular grade water using a Tecan Evo Liquid Handling Robot. First, genomic DNA samples were removed from -20°C and thawed in a room temperature water bath. Then, <inline-formula><mml:math id="inf69"><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of each sample was combined with <inline-formula><mml:math id="inf70"><mml:mrow><mml:mn>95</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of SYBR Green (Invitrogen) diluted by a factor of 100 in TE Buffer (Integrated DNA Technologies) in a black 384-well microplate. This process was repeated with two replicates of each DNA standard with concentrations of 0, 0.5, 1, 2, 4, and <inline-formula><mml:math id="inf71"><mml:mrow><mml:mn>6</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>ngL</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Each sample was then measured for fluorescence with an excitation/emission of 485/535 nm using a Tecan Spark plate reader. Concentrations of each sample were calculated using the standard curve and a custom Python script was used to compute the dilution factors and write a worklist for the Tecan Evo Liquid Handling Robot to normalize each sample to <inline-formula><mml:math id="inf72"><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>ngL</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> in molecular grade water. Samples with DNA concentration less than <inline-formula><mml:math id="inf73"><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>ngL</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> were not diluted. Diluted genomic DNA samples were stored at -20°C until further processing.</p><p>Amplicon libraries were generated from diluted genomic DNA samples by PCR amplification of the V3-V4 of the 16 S rRNA gene using custom dual-indexed primers for multiplexed next generation amplicon sequencing on Illumina platforms (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Primers were arrayed in skirted 96-well PCR plates (VWR) using an acoustic liquid handling robot (Labcyte Echo 550) such that each well received a different combination of one forward and one reverse primer (<inline-formula><mml:math id="inf74"><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of each). After liquid evaporated, dry primers were stored at -20°C. Primers were resuspended in <inline-formula><mml:math id="inf75"><mml:mrow><mml:mn>15</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> PCR master mix (<inline-formula><mml:math id="inf76"><mml:mrow><mml:mn>0.2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> Phusion High Fidelity DNA Polymerase [Thermo Scientific], <inline-formula><mml:math id="inf77"><mml:mrow><mml:mn>0.4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> 10 mM dNTP Solution [New England Biolabs], <inline-formula><mml:math id="inf78"><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> 5 x Phusion HF Buffer [Thermo Scientific], <inline-formula><mml:math id="inf79"><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> 5 M Betaine [Sigma-Aldrich], <inline-formula><mml:math id="inf80"><mml:mrow><mml:mn>6.4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> Water) and <inline-formula><mml:math id="inf81"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> of normalized genomic DNA to give a final concentration of 0.05 M of each primer. Primer plates were sealed with Microplate B seals (Bio-Rad) and PCR was performed using a Bio-Rad C1000 Thermal Cycler with the following program: initial denaturation at 98°C (30 s); 25 cycles of denaturation at 98°C (10 s), annealing at 60°C (30 s), extension at 72°C (60 s); and final extension at 72°C (10 min). Of PCR products from each well, 2 µL were pooled and purified using the DNA Clean &amp; Concentrator (Zymo) and eluted in water. The resulting libraries were sequenced on an Illumina MiSeq using a MiSeq Reagent Kit v3 (600-cycle) to generate 2 × 300 paired end reads.</p></sec><sec id="s4-1-5"><title>Bioinformatic analysis for quantification of species abundance</title><p>Sequencing data were used to quantify species relative abundance as described previously (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Sequencing data were demultiplexed using Basespace Sequencing Hub’s FastQ Generation program. Custom python scripts were used for further data processing as described previously (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Paired end reads were merged using PEAR (v0.9.10) (<xref ref-type="bibr" rid="bib72">Zhang et al., 2014</xref>) after which reads without forward and reverse annealing regions were filtered out. A reference database of the V3-V5 16 S rRNA gene sequences was created using consensus sequences from next-generation sequencing data or Sanger sequencing data of monospecies cultures. Sequences were mapped to the reference database using the mothur (v1.40.5) (<xref ref-type="bibr" rid="bib55">Schloss et al., 2009</xref>) command classify.seqs (Wang method with a bootstrap cutoff value of 60). Relative abundance was calculated as the read count mapped to each species divided by the total number of reads for each condition. Absolute abundance of each species was calculated by multiplying the relative abundance by the OD6<sup>00</sup> measurement for each sample. Samples were excluded from further analysis if .1% of the reads were assigned to a species not expected to be in the community (indicating contamination). We expect the precision of our measurements to drop rapidly for species representing &lt;1% of the community due to limited sequencing depth. We typically sequenced on the order of 10,000 molecules of PCR amplified DNA of the 16 S rRNA gene per sample, so if a species is only represented by on the order of 10 of those molecules (0.1%), then a single sequencing read error would be a 10% error, whereas a species represented by (&gt;100) reads (1%) would only have 1% error per read.</p></sec><sec id="s4-1-6"><title>Choice of sample sizes</title><p>Sample sizes were chosen based on limitations of experimental throughput as increased number of biological replicates would have reduced the number of possible different communities that could be observed. We chose a minimum of two biological replicates (for complex communities in our validation set) and some sample types have up to seven biological replicates (such as the full community, which was repeated in most experiments as a control for consistency between experimental days).</p></sec></sec><sec id="s4-2"><title>Computational methods</title><sec id="s4-2-1"><title>Long short-term memory for dynamic prediction on microbial communities</title><p>Long short term memory (LSTM) networks belong to the class of recurrent neural networks (RNNs) and model time-series data. They were first introduced by Hochreiter et al. (<xref ref-type="bibr" rid="bib17">Hochreiter and Schmidhuber, 1997</xref>) to overcome the vanishing or exploding gradients problem (<xref ref-type="bibr" rid="bib18">Hochreiter, 1998</xref>) that occur due to long-term temporal dependencies. Since their inception, LSTMs have been further refined (<xref ref-type="bibr" rid="bib11">Gers et al., 2000</xref>; <xref ref-type="bibr" rid="bib14">Graves and Schmidhuber, 2005</xref>) and find numerous applications in several domains, including but not limited to neuroscience (<xref ref-type="bibr" rid="bib63">Storrs and Kriegeskorte, 2019</xref>), weather forecasting (<xref ref-type="bibr" rid="bib25">Karevan and Suykens, 2020</xref>), predictive finance (<xref ref-type="bibr" rid="bib9">Fischer and Krauss, 2018</xref>), Google Voice for speech recognition (<xref ref-type="bibr" rid="bib7">Esch, 2014</xref>; <xref ref-type="bibr" rid="bib9">Fischer and Krauss, 2018</xref>) and Google Allo for message suggestion (<xref ref-type="bibr" rid="bib68">Wei et al., 2018</xref>).</p><p>Similar to any recurrent neural network, an LSTM network comprises of a network of multiple LSTM units, each representing the input-output map at a time instant. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows the schematic of the proposed LSTM network architecture for abundance prediction. For a microbial community comprising of <inline-formula><mml:math id="inf82"><mml:mi>N</mml:mi></mml:math></inline-formula> species, each LSTM unit models the dynamics at time <inline-formula><mml:math id="inf83"><mml:mi>t</mml:mi></mml:math></inline-formula> using the following set of equations:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle><mml:mspace linebreak="newline"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace linebreak="newline"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">c</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>h</italic><sub><italic>t</italic></sub>, <italic>c</italic><sub><italic>t</italic></sub>, <italic>x</italic><sub><italic>t</italic></sub> are the hidden state, cell state and input abundance at time <inline-formula><mml:math id="inf84"><mml:mi>t</mml:mi></mml:math></inline-formula>, respectively, and <italic>i</italic><sub><italic>t</italic></sub>, <italic>f</italic><sub><italic>t</italic></sub>, <italic>g</italic><sub><italic>t</italic></sub>, <italic>o</italic><sub><italic>t</italic></sub> are input, forget, cell and output gates, respectively. σ is the sigmoid function, and ⊙ denotes the Hadamard product. The parameters <inline-formula><mml:math id="inf85"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf86"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are trainable and shared across all LSTM units. The output gate <italic>o</italic><sub><italic>t</italic></sub> is further used to generate the abundance for next time instant as:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <italic>y</italic><sub><italic>t</italic></sub> is fed to the LSTM unit at the next timestep (<inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), which in turn predicts the species abundance at time <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The process is repeated across multiple LSTM units in order to obtain <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mtext>final</mml:mtext></mml:msub></mml:msub></mml:math></inline-formula>. The entire architecture is trained to minimize the mean-squared loss between the predicted abundance <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mtext>final</mml:mtext></mml:msub></mml:msub></mml:math></inline-formula> and true abundance <inline-formula><mml:math id="inf91"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:msub><mml:mi>t</mml:mi><mml:mtext>final</mml:mtext></mml:msub></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s4-2-2"><title>Using teacher forcing for intermittent time-series forecasting (<xref ref-type="fig" rid="fig1">Figures 1</xref>, <xref ref-type="fig" rid="fig2">2</xref> and <xref ref-type="fig" rid="fig5">5</xref>)</title><p>The end-goal for the proposed LSTM-network based abundance predictor is to accurately capture the steady-state (final) abundance from initial abundance. In typical LSTM networks, the output of the recurrent unit at the previous timestep <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is used as an input to the recurrent unit at the current timestep <italic>x</italic><sub><italic>t</italic></sub>. This kind of recurrent model, while has the ability to predict final abundance, is incapable of handling the one-step-ahead prediction. The problem is even more critical when one tries to anticipate more than a single timestep into the future. <italic>Teacher forcing</italic> (<xref ref-type="bibr" rid="bib1">Benny Toomarian and Barhen, 1992</xref>) entails a training procedure for recurrent networks, such as LSTMs, where ‘true’ abundances at intermittent timesteps are used to guide (like a teacher) the model to accurately anticipate one-step-ahead abundance.</p><p>Teacher forcing is an efficient method of training RNN models that use the ground truth from a prior time step as input. This is achieved by occasionally replacing the predicted abundance <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> from the previous timestep with the true abundance <inline-formula><mml:math id="inf94"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> at the current timestep as input abundance to the LSTM unit at the current timestep during the training process. Teacher forcing not only stabilizes the training process, it <italic>forces</italic> the output abundances at all times to closely match the corresponding true abundances. This is precisely why we do not just use the ground truth abundances at intermittent timesteps in order to robustify the prediction of steady-state abundance. Once trained, the inference in such models is achieved by ignoring the ground truth abundances and using the predicted abundance from previous instant to roll forward the model in time. Teacher forcing was used to train the LSTM in all cases where intermediate time points were measured (<xref ref-type="fig" rid="fig1">Figures 1</xref>, <xref ref-type="fig" rid="fig2">2</xref> and <xref ref-type="fig" rid="fig5">5</xref>), as opposed to cases that only included initial and final time points (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>).</p></sec><sec id="s4-2-3"><title>Metabolite profiling</title><p>Microbial communities are a rich source of a variety of metabolites that are very commonly used as nutritional supplements, natural compounds to cure infectious diseases and in sustainable agriculture development. The concentration and chemical diversities of metabolites produced in a microbial community is a direct consequence of the diversity of interactions between organisms in the community. In essence, the dynamical evolution of relative species abundance and intra-community interactions govern the nature and amount of metabolites produced in the community. The functional map between species abundance and concentration of metabolites is highly complex and nonlinear, and is often approximated using simple regressors involving unary and pairwise interaction terms. In this paper, we model the species-metabolite map through appropriate modification of the LSTM network.</p><p>The aforementioned LSTM network for predicting the species abundance is suitably modified to augment four additional components that correspond to the concentration of metabolites at each time instant. In particular, the species abundance data (of size <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>N</mml:mi><mml:mtext>species</mml:mtext></mml:msub></mml:math></inline-formula>) is concatenated with the metabolite concentration data (of size <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>N</mml:mi><mml:mtext>metabs</mml:mtext></mml:msub></mml:math></inline-formula>) to form a <inline-formula><mml:math id="inf97"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>species</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>metabs</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-dimensional feature vector, which is suitably normalized so that the different components have zero mean and unity variance. The feature scaling is important to prevent over reliance on features with a broad range of values. Concatenation of species abundance data and the metabolite concentration data ensures that the future trajectory of metabolite concentrations evolves as a function of both the species abundance, as well as the metabolite concentrations at previous time instants. As before, the <inline-formula><mml:math id="inf98"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>species</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>metabs</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-dimensional output of each LSTM unit is fed into the input block of the subsequent LSTM unit in order to advance the model forward in time. The model predictions at each time point is then transformed back to the original scale in order to obtain the Pearson <inline-formula><mml:math id="inf99"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> scores on the unnormalized data. Compared with existing approaches that employ ordinary differential equations (ODEs) and multiple linear regression models for predicting metabolites, the proposed architecture enables more accurate and rapid estimation of all four metabolites. All the LSTM models were implemented in Python using PyTorch on an Intel i7-7700HQ CPU @2.80 GHz processor with 16 GB RAM and NVIDIA GeForce GTX 1060 (6GB GDDR5) GPU. The exact details of the neural network architecture consisting of number of layers, learning rate, choices of optimizer and nonlinear activations are described in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></sec><sec id="s4-2-4"><title>Data preprocessing for LSTM networks</title><p>Data normalization is one of the most widely adopted practices for efficient training of neural networks. Data normalization is known to speed up the training leading to faster convergence. At the same time, when working with multi-modal data or data with features represented at multiple scales, it is recommended to normalize the features (also known as feature standardization) to the same scale in order to avoid over reliance on features with large magnitudes. A common choice for feature standardization is to have zero-mean and unit-variance for each feature in the data. Let <inline-formula><mml:math id="inf100"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf101"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the abundance of the <inline-formula><mml:math id="inf102"><mml:msup><mml:mi>i</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula>-species and concentration of the <inline-formula><mml:math id="inf103"><mml:msup><mml:mi>j</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula>-metabolite at the <inline-formula><mml:math id="inf104"><mml:msup><mml:mi>k</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> time instant for the <inline-formula><mml:math id="inf105"><mml:msup><mml:mi>n</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> sample. The mean and standard deviation of the quantities <inline-formula><mml:math id="inf106"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf107"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> can then be computed over the training dataset defined by:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle><mml:mspace linebreak="newline"/><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>samples</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The quantities can then be standardized as:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle><mml:mspace linebreak="newline"/><mml:msubsup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The process is repeated for all species and metabolites at each time-point, and the scaled inputs <inline-formula><mml:math id="inf108"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> are then fed to the LSTM neural networks for prediction. During inference on the test data, the normalized output of each LSTM unit is inversely transformed back to its original scale using the precomputed <inline-formula><mml:math id="inf110"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. The readers are encouraged to refer to <xref ref-type="bibr" rid="bib12">Goodfellow et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Zheng and Casari, 2018</xref> for additional details on feature standardization. It is also not uncommon to normalize the data with respect to the mean and standard deviation of the entire dataset (and not just with respect to the training dataset). However, in practical scenarios, the test data is not known a priori, and thus it is undesirable to employ statistics from the hold-out test data for feature standardization. In our implementation of data preprocessing for LSTM networks, we had employed feature standardization using (a) training data only, and (b) both training and test data. The predictive performance of our LSTM models was nearly identical for both of these feature standardization approaches. The feature standardization can be toggled by the normalize_all variable in our open-source implementation.</p></sec><sec id="s4-2-5"><title>Hyperparameter tuning for LSTM networks</title><p>Similar to other learning algorithms, training an LSTM network entails choosing a set of hyperparameters for optimal performance. We used an exhaustive grid-search for hyperparameter optimization, while the choice of learning algorithm (optimizer) was restricted to Adam (<xref ref-type="bibr" rid="bib27">Kingma and Ba, 2014</xref>) due to its superior empirical performance. For each experiment, nearly 10% of the training dataset was reserved as a cross-validation set, and the performances of the trained models were evaluated using cross-validation sets. This information was used to select the best hyperparameter settings. The choices for hyperparameters include: (a) learning rate, (b) number of hidden layers per LSTM unit, (c) number of units per layer within an LSTM unit, (d) mini-batch size, and (e) input data normalization. The input features are normalized to have zero mean and unit variance. The choices of learning rates include 0.005, 0.001, and 0.0001, respectively, each with a decay of 0.25 after every 25 epochs. The gradual decay in learning rates prevents potential overfitting to the data. An L2 regularization term with a very small weight decay coefficient (<inline-formula><mml:math id="inf112"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) is augmented to the loss function for preventing further overfitting. Choices for mini-batch size included 1, 10, 20, and 50, respectively. It was observed that sizes 10 and 20 resulted in improved training loss, and hence we used mini-batch sizes of 10 or 20 in all evaluations. The number of hidden layers per LSTM cell was iterated from 1 to 2. A two-layered LSTM did not result in any noticeable improvement over a single-layered LSTM cell, and thus we restricted our focus to just a single-layered LSTM cell for the sake of simplicity and faster training/inference. Finally, we tried 512, 1024, 2048, and 4096 hidden units per LSTM cell, and depending upon the complexity of the problem, we used 2048 hidden units (predictions of species and no prediction of metabolites) or 4096 hidden units (simultaneous prediction of species and metabolites). The exact details on the number of training epochs, learning rates, decay rates for different experiments can be found in the <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></sec></sec><sec id="s4-3"><title>Specific applications of computational methods</title><sec id="s4-3-1"><title>Comparison of gLV and LSTM in silico (<xref ref-type="fig" rid="fig1">Figure 1</xref>)</title><p>To compare the LSTM and gLV models, we used a ground truth model of a 25-species community of the form:<disp-formula id="equ6"><label>(4)</label><mml:math id="m6"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟮</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover></mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟯</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where and represent individual species exponential growth rate and pairwise interaction coefficients, respectively. The parameters represent the effect of third-order interactions. The parameters and were derived from a gLV model in a previous study (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). We consider three types of simulation studies, each corresponding to varying contributions of the third-order interactions (second-order only: , mild third-order: uniformly sampling in the range , moderate third order: uniformly sampling in the range ). In each scenario, the ground truth data was generated for 624 training communities (25 monospecies, 300 two-member, 100 three-member, 100 five-member, and 99 six-member communities) by simulating the species abundance trajectories over the course of 48 hr and ‘sampling’ every 8 hr. The values from these ‘sampled’ time points were used to train both an LSTM model (methods described above in ‘Computational Methods’ with specific details in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>) and a standard gLV model (trained using FMINCON function in MATLAB as described previously [<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>]). These two models were then used to predict a set of 3299 hold-out communities &gt;10 species simulated using the same ground truth model. For an additional analyses, the training data for the LSTM was augmented by including ground truth data for an additional 100 communities with 11 or 19 species.</p></sec><sec id="s4-3-2"><title>LSTM training for experimental 12-species community (<xref ref-type="fig" rid="fig2">Figure 2</xref>)</title><p>The data used in this analysis consisted of 175 microbial community subsets of a 12-species community sampled every 12 hr for 60 hr (<xref ref-type="bibr" rid="bib66">Venturelli et al., 2018</xref>). Of these communities, 102 were chosen randomly to constitute the training data and the remaining 73 constituted the hold-out set. The LSTM was trained as described above in ‘Computational Methods’ with specific details in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></sec><sec id="s4-3-3"><title>Using LSTM Model to design multifunctional communities (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p>We used the LSTM model trained on previous data (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) to design two sets of communities: a ‘distributed’ community set and a ‘corner’ community set. For the ‘distributed’ community set, we first took the predicted metabolite concentrations for all communities with .10 species and used <inline-formula><mml:math id="inf113"><mml:mi>k</mml:mi></mml:math></inline-formula>-means clustering with <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> (Python 3, scikit-learn v0.23.1, sklearn.cluster.Kmeans function) to identify 100 cluster centroids that were distributed across all of the predictions. We then found the closest community to each centroid in terms of Euclidean distance in the four-dimensional metabolite concentration space. These 100 communities constituted the ‘distributed’ community set.</p><p>For the ‘corner’ community set, we first defined four ‘corners’ in the lactate and butyrate concentration space by binning all communities with .10 species as shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref>:</p><list list-type="order"><list-item><p>5% lowest lactate concentration communities, then 5% lowest butyrate concentration of those</p></list-item><list-item><p>5% lowest lactate concentration communities, then 5% highest butyrate concentration of those</p></list-item><list-item><p>5% lowest butyrate concentration communities, then 5% lowest lactate concentration of those</p></list-item><list-item><p>5% lowest butyrate concentration communities, then 5% highest lactate concentration of those.</p></list-item></list><p>Within each of those four ‘corners’, we identified four ‘sub-corners’ in the acetate and succinate concentration space by binning communities as shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref>:</p><list list-type="order"><list-item><p>5% lowest acetate concentration communities, then 5% lowest succinate concentration of those</p></list-item><list-item><p>5% lowest acetate concentration communities, then 5% highest succinate concentration of those</p></list-item><list-item><p>5% lowest succinate concentration communities, then 5% lowest acetate concentration of those</p></list-item><list-item><p>5% lowest succinate concentration communities, then 5% highest acetate concentration of those</p></list-item></list><p>This process resulted in 16 ‘sub-corners’ total. For each ‘sub-corner’, we then chose a random community and then identified four more communities that were maximally different from that community in terms of which species were present (Hamming distance). This overall process resulted in 80 communities constituting the ‘corner’ community set.</p></sec><sec id="s4-3-4"><title>Composite model: gLV model for predicting species abundance (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p>To benchmark the performance of the LSTM model for predicting metabolite production, we used a previously described Composite Model consisting of a generalized Lotka-Volterra (gLV) model for predicting species abundance dynamics and a regression model with interaction terms to predict metabolite concentration at a given time from the species abundances at that time (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). Because our LSTM model was trained on the same dataset as Composite Model M3 from <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>, we used those gLV model parameters.</p></sec><sec id="s4-3-5"><title>Composite model: regression models for predicting metabolite concentrations (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p>Our composite model implementation is similar to the model described in <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref> for predicting metabolite concentration from community composition at a particular time. We used the exact gLV model parameter distributions obtained by <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>, which were determined using an approach based on <xref ref-type="bibr" rid="bib59">Shin et al., 2019</xref>. The regression model mapping endpoint species abundance to metabolite concentrations from <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref> was focused specifically on the prediction of butyrate. Therefore, we adapted the approach to prediction of multiple metabolites. First, we modified the model form to include first order and interaction terms for all 25 species, rather than just the butyrate producers. Then, we separately trained four regression models, one for each metabolite (butyrate, lactate, acetate, succinate), using the measured species abundance and measured metabolite concentrations from the same dataset used to train the LSTM model. We trained these models as described previously (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>) by using Python scikit-learn (<xref ref-type="bibr" rid="bib45">Pedregosa, 2011</xref>) and performed L1 regularization to minimize the number of nonzero parameters. Regularization coefficients were chosen by using 10-fold cross validation. We selected the regularization coefficient value with the lowest median mean-squared error across the training splits.</p><p>For predicting end-point metabolite profiles from initial species abundance using the LSTM network, a feed-forward network (FFN) was used at the output of the last LSTM unit to convert end-point species abundance to end-point metabolite concentrations. On the other hand, the composite model in <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref> uses multiple linear regressors at its output to predict a given metabolite concentration from species abundance. At this point, it is still unclear if the superior performance of LSTM network is due to the addition of a more powerful FFN at its output over the multiple linear regressors at the output of the composite model. Therefore, we replaced the simple multiple linear regressors component of the composite model from <xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref> with a Random Forest regressor (<xref ref-type="bibr" rid="bib56">Segal, 2004</xref>) or a FFN. However, neither of these additions improved the metabolite prediction accuracy beyond that of the LSTM with a FFN (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3a</xref>).</p></sec><sec id="s4-3-6"><title>Composite model: simulations for prediction (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p>Custom MATLAB scripts were used to predict community assembly using the gLV model as described previously (<xref ref-type="bibr" rid="bib5">Clark et al., 2021</xref>). For each community, the growth dynamics were simulated using each parameter set from the posterior distribution of the gLV model parameters. The resulting community compositions for each simulation at 48 hr were used as an input to the regression models (multiple linear regression/Random Forest/FFN) implemented in Python to predict the concentration of each metabolite in each community for each gLV parameter set. Because of the large number of communities and the large number of parameter sets (i.e., hundreds of simulations per community), we used parallel computing (MATLAB parfor) to complete the simulations in a reasonable timeframe (∼1 hr for the communities in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3a</xref>).</p></sec><sec id="s4-3-7"><title>Understanding relationships between variables using LIME (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p>Black-box methods, such as the LSTM-networks employed in this manuscript, do not offer much insights into the underlying mechanics that make them so powerful. Consequently, any potential pitfalls that may come along with building such models remain unexplored. For networks that are of significant biological importance, basing assumptions on falsehoods can be catastrophic. We overcome this limitation by resorting to Local Interpretable Model-Agnostic Explanations (LIME) (<xref ref-type="bibr" rid="bib49">Ribeiro et al., 2016a</xref>).</p><p>LIME has three key components: (a) <italic>Local</italic>, that is, any explanation reflects the behavior of a classifier around the sampled instance, (b) <italic>Interpretability</italic>, that is, the explanations offered by LIME are interpretable by human, (c) <italic>Model-Agnostic</italic>, that is, LIME does not require to peak into any model. It generates explanations by analyzing the model’s behavior for an input perturbed around its neighborhood. In this manuscript, we employ LIME to explain both qualitatively and quantitatively, as to how the abundances of various species affect the concentrations of all four metabolites, and if the presence or absence of a given species has any significance on the resulting metabolite profile.</p><p>We carried out the LIME analysis to generate interpretable prediction explanations for model M2 for each community instance used to train the model. We used lime v0.2.0.1 for Python 3 (<ext-link ext-link-type="uri" xlink:href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</ext-link>; <xref ref-type="bibr" rid="bib51">Ribeiro, 2021</xref>) to train an explainer on the predictions of the training instances for each output variable (25 species, 4 metabolites) and then generated explanation tables for every input variable (species presence/absence) for every training instance. We then determined the median value for which the presence of a given species explained the prediction for each output variable to generate the networks in <xref ref-type="fig" rid="fig3">Figure 3d, e</xref>.</p><p>In a separate analysis, we investigated the sensitivity of LIME explanations to the training data used to fit the LSTM model. Because the purpose of this analysis was to understand the variability in LIME explanations and not to understand the dependence of LIME explanations on different communities, we only considered LIME explanations of the full community. This is in contrast to the LIME explanations shown in <xref ref-type="fig" rid="fig3">Figure 3d, e</xref>, which present the median LIME explanation taken over all of the communities in the training data. Training data was varied using 20-fold cross-validation, and LIME sensitivity of both metabolites (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>) and species (<xref ref-type="fig" rid="fig3s6">Figure 3—figure supplement 6</xref>) was computed after fitting the LSTM model to each partition of the training data.</p></sec><sec id="s4-3-8"><title>Understanding relationships between variables using prediction sensitivity (<xref ref-type="fig" rid="fig4">Figure 4</xref>)</title><p>For each metabolite (Acetate, Butyrate, Lactate, Succinate), fractions of 0.5, 0.6, 0.7, 0.8, 0.9, and 1 of the total dataset were randomly sampled. Each sub-sampled dataset was subject to 20-fold cross validation to determine the sensitivity of held-out prediction performance to the amount of data available for training. This process was repeated 30 times, and the average prediction over the 30 trials was used to compute the final held-out prediction performance (<inline-formula><mml:math id="inf115"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>).</p><p>The sensitivity of the model to the presence of individual species and pairs of species was determined by evaluating prediction performance (<inline-formula><mml:math id="inf116"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) for subsets of the data containing each species and each possible pair of species. To evaluate how prediction performance of each metabolite was affected by the presence of species pairs, we computed the average percent difference between prediction performance taken over subsets containing a single species and all pairs of species using the following equation:<disp-formula id="equ7"><label>(5)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thickmathspace"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>100</mml:mn><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf117"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the prediction performance taken over the subset of samples containing species <inline-formula><mml:math id="inf118"><mml:mi>i</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf119"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the prediction performance taken over the subset of samples containing species <inline-formula><mml:math id="inf120"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:mi>j</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-3-9"><title>Clustering metabolite trajectories (<xref ref-type="fig" rid="fig5">Figure 5</xref>)</title><p>To generate the clusters from the dynamic community observations (<xref ref-type="fig" rid="fig5">Figure 5</xref>), we used a graph-theoretic divisive clustering algorithm (<xref ref-type="bibr" rid="bib21">Jain et al., 1999</xref>) based on the minimal spanning tree (<xref ref-type="bibr" rid="bib71">Zahn, 1971</xref>). We first generated an undirected graph wherein each node was a community observed in our experiment and each edge weight was the Euclidean distance between two communities based on all metabolite measurements (4 metabolites ×3 time points = 12-dimensional space for Euclidean distance calculation). We then determined the minimal spanning tree for this graph using the minimum_spanning_tree function in networkx (v2.1) for Python 3. We then used this minimal spanning tree to generate clusters by iteratively removing the edge with the largest weight until 6 clusters were formed. In each iteration, if any edge removal resulted in a cluster with &lt;5 communities (i.e. minimum cluster size), that edge was returned and the next largest edge was removed. The number of clusters and minimum cluster size were chosen based on an elbow method (<xref ref-type="bibr" rid="bib44">Pal and Biswas, 1997</xref>), wherein scatter plots were made of the mean intracluster distance versus the number of clusters for various minimum cluster sizes and a combination of minimum cluster size and number of clusters that fell on the elbow of the plot was chosen.</p></sec><sec id="s4-3-10"><title>Decision tree classification of metabolite trajectories (<xref ref-type="fig" rid="fig5">Figure 5</xref>)</title><p>The decision tree shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref> and used to produce the annotations in <xref ref-type="fig" rid="fig5">Figure 5a</xref> was generated using the DecisionTreeClassifier with the default parameter settings in scikit-learn (v0.23.1) for Python 3 (visualization generated using plot_tree function from the same).</p></sec><sec id="s4-3-11"><title>Understanding relationships between variables using sensitivity gradients (<xref ref-type="fig" rid="fig5">Figure 5</xref>)</title><p>Interpretability of neural-network (NN) models continues to be an interesting challenge in machine learning. While LIME is a great tool to explain what machine learning classifiers are doing, it is model-agnostic and uses simple linear models to approximate local behavior. Model-agnostic characteristic enforces retraining linear models on the training data and analyzing local perturbations, before LIME can be used to invoke interpretability. Moreover, the type of modifications that need to be performed on the data to get proper explanations are typically use case specific. Consequently, model-aware interpretability methods that take into account the weights of an already trained NN are more suitable.</p><p>For tasks, such as classification of images and videos, there is a natural way to interpret NN models using class activation maps (CAMs) (<xref ref-type="bibr" rid="bib57">Selvaraju et al., 2017</xref>). CAMs assign appropriate weighting to different convolutional filters and highlights part of the images that activate a given output class the most. However, CAMs do not extend to other NN architectures, such as LSTMs. Fortunately for us, the answer to interpretability lies in the model training itself. Let <inline-formula><mml:math id="inf122"><mml:mi>Y</mml:mi></mml:math></inline-formula> be the output variable of interest whose perturbation with respect to an input <inline-formula><mml:math id="inf123"><mml:mi>x</mml:mi></mml:math></inline-formula> needs to be estimated. The effect of <inline-formula><mml:math id="inf124"><mml:mi>x</mml:mi></mml:math></inline-formula> on <inline-formula><mml:math id="inf125"><mml:mi>Y</mml:mi></mml:math></inline-formula> can be approximated through the partial derivative <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:math></inline-formula>. For instance, <inline-formula><mml:math id="inf127"><mml:mi>Y</mml:mi></mml:math></inline-formula> may denote butyrate concentration in an experiment, while <inline-formula><mml:math id="inf128"><mml:mi>x</mml:mi></mml:math></inline-formula> can be used to represent abundance of a given species. The sign of the partial derivative depicts positive (or negative) correlation between the two variables, while the magnitude represents the extent of it. In order to evaluate the partial derivatives, we freeze the weights of the already trained LSTM model and declare the inputs to be variables. A single backpropagation pass then evaluates the partial derivatives of an output variable of interest with respect to all the input variables. This is in contrast to LIME-based interpretability method, which requires training an additional model on top of an already trained deep learning model. Most deep learning libraries already implement a computational graph for performing efficient forward and backward passes during the training phase. This computational graph can be used to evaluate sensitivity gradients.</p><p>There indeed are other methods for explanation of neural networks, most notably the Shapley explainability method (<xref ref-type="bibr" rid="bib36">Lundberg and Lee, 2017</xref>). This method is substantially more computationally burdensome than LIME or a sensitivity gradient based method (<xref ref-type="bibr" rid="bib23">Jia, 2019a</xref>; <xref ref-type="bibr" rid="bib24">Jia, 2019b</xref>). LIME and sensitivity gradients are based on first-order perturbations around the already learned model, and can be used to depict local model behavior with little to no computational burden. By contrast, explainability methods like Shapley are computationally expensive. An exact computation of Shapley values for a <inline-formula><mml:math id="inf129"><mml:mi>K</mml:mi></mml:math></inline-formula>-dimensional input requires estimating <inline-formula><mml:math id="inf130"><mml:msup><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:msup></mml:math></inline-formula> possible coalitions of the feature values and the “absence” of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation.</p></sec><sec id="s4-3-12"><title>Comparison of the discretized gLV model to the LSTM (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>)</title><p>To train the gLV model using the same algorithm used to train the LSTM and enable metabolite prediction, the gLV model was discretized and augmented with a feed-forward neural network.The approximate gLV model is<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle><mml:mspace linebreak="newline"/><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>FFN</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf131"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the abundance of species <inline-formula><mml:math id="inf132"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the growth rate of species <inline-formula><mml:math id="inf134"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the impact of species <inline-formula><mml:math id="inf136"><mml:mi>j</mml:mi></mml:math></inline-formula> on species <inline-formula><mml:math id="inf137"><mml:mi>i</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the concentration of metabolite <inline-formula><mml:math id="inf139"><mml:mi>i</mml:mi></mml:math></inline-formula>. Metabolite concentrations at time step <inline-formula><mml:math id="inf140"><mml:mi>t</mml:mi></mml:math></inline-formula> are predicted using a feed-forward neural network (FFN). The structure of the discretized gLV model requires that all species abundances are strictly non-negative. When training the gLV, the data were pre-processed such that each feature (species abundance and metabolite concentration) ranges between zero and one based on the maximum value of each feature in the training data, computed at each time step. This is in contrast to the scaling used for the LSTM, which results in negative values for transformed species abundances. The stochastic gradient descent algorithm was used to train the LSTM and the discretized gLV model, using the Adam (<xref ref-type="bibr" rid="bib27">Kingma and Ba, 2014</xref>) optimizer. The default settings of the Pytorch function, ReduceLROnPlateau, were used to adjust the learning rate during training. Species growth rates, <inline-formula><mml:math id="inf141"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, and interaction coefficients, <inline-formula><mml:math id="inf142"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, were initialized to zero prior to fitting.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>The authors declare no conflicts of interest</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Funding acquisition, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Investigation, Software</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Investigation, Methodology, Resources, Supervision, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Data curation, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-73870-transrepform1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Modeling information.</title><p>Contains two tables. Table S1 summarizes the training/test data and hyperparameters for training each LSTM model. Table S2 contains parameters used for all statistical tests presented in this work.</p></caption><media xlink:href="elife-73870-supp1-v1.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Pytorch implementation of the proposed LSTM model and the accompanying measurements of community composition and metabolite concentrations are available from GitLab (<ext-link ext-link-type="uri" xlink:href="https://gitlab.eecs.umich.edu/mayank.baranwal/Microbiome">https://gitlab.eecs.umich.edu/mayank.baranwal/Microbiome</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a244f80833aeda606a9da6f05eacb522bc8f50cc;origin=https://gitlab.eecs.umich.edu/mayank.baranwal/Microbiome;visit=swh:1:snp:eb49ab040a7cdda9ee72b7d3526e6652ab208994;anchor=swh:1:rev:f2eed8f013e42fbcc8c8f98816a8146559d6f1b3">swh:1:rev:f2eed8f013e42fbcc8c8f98816a8146559d6f1b3</ext-link>). The raw Illumina sequencing data is available from Zenodo (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/5529327">https://zenodo.org/record/5529327</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5529327">https://doi.org/10.5281/zenodo.5529327</ext-link>).</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Baranwal</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Microbiome</data-title><source>GitLab</source><pub-id pub-id-type="accession" xlink:href="https://gitlab.eecs.umich.edu/mayank.baranwal/Microbiome">Microbiome</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>NGS Data Accompanying &quot;Deep Learning Enables Design of Multifunctional Synthetic Human Gut Microbiome Dynamics</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5529327</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by funding from the Army Research Office (ARO) under grant number W911NF1910269, the National Institutes of Health under grant numbers R35GM124774 and R01EB030340. RLC was supported in part by an NHGRI training grant to the Genomic Sciences Training Program (T32 HG002760).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benny Toomarian</surname><given-names>N</given-names></name><name><surname>Barhen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Learning a trajectory using adjoint functions and teacher forcing</article-title><source>Neural Networks</source><volume>5</volume><fpage>473</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(92)90009-8</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bucci</surname><given-names>V</given-names></name><name><surname>Tzen</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Simmons</surname><given-names>M</given-names></name><name><surname>Tanoue</surname><given-names>T</given-names></name><name><surname>Bogart</surname><given-names>E</given-names></name><name><surname>Deng</surname><given-names>L</given-names></name><name><surname>Yeliseyev</surname><given-names>V</given-names></name><name><surname>Delaney</surname><given-names>ML</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Olle</surname><given-names>B</given-names></name><name><surname>Stein</surname><given-names>RR</given-names></name><name><surname>Honda</surname><given-names>K</given-names></name><name><surname>Bry</surname><given-names>L</given-names></name><name><surname>Gerber</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>MDSINE: Microbial Dynamical Systems INference Engine for microbiome time-series analyses</article-title><source>Genome Biology</source><volume>17</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1186/s13059-016-0980-6</pub-id><pub-id pub-id-type="pmid">27259475</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Byeon</surname><given-names>W</given-names></name><name><surname>Breuel</surname><given-names>TM</given-names></name><name><surname>Raue</surname><given-names>F</given-names></name><name><surname>Liwicki</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Scene labeling with LSTM recurrent neural networks</article-title><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>3547</fpage><lpage>3555</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298977</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chon</surname><given-names>TS</given-names></name><name><surname>Kwak</surname><given-names>IS</given-names></name><name><surname>Park</surname><given-names>YS</given-names></name><name><surname>Kim</surname><given-names>TH</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Patterning and short-term predictions of benthic macroinvertebrate community dynamics by using a recurrent artificial neural network</article-title><source>Ecological Modelling</source><volume>146</volume><fpage>181</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/S0304-3800(01)00305-2</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>RL</given-names></name><name><surname>Connors</surname><given-names>BM</given-names></name><name><surname>Stevenson</surname><given-names>DM</given-names></name><name><surname>Hromada</surname><given-names>SE</given-names></name><name><surname>Hamilton</surname><given-names>JJ</given-names></name><name><surname>Amador-Noguez</surname><given-names>D</given-names></name><name><surname>Venturelli</surname><given-names>OS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Design of synthetic human gut microbiome assembly and butyrate production</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>3254</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22938-y</pub-id><pub-id pub-id-type="pmid">34059668</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dambre</surname><given-names>J</given-names></name><name><surname>Verstraeten</surname><given-names>D</given-names></name><name><surname>Schrauwen</surname><given-names>B</given-names></name><name><surname>Massar</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Information processing capacity of dynamical systems</article-title><source>Scientific Reports</source><volume>2</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/srep00514</pub-id><pub-id pub-id-type="pmid">22816038</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Esch</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Type less, talk more</article-title><ext-link ext-link-type="uri" xlink:href="https://www.blog.google/products/search/type-less-talk-more/">https://www.blog.google/products/search/type-less-talk-more/</ext-link><date-in-citation iso-8601-date="2017-08-14">August 14, 2017</date-in-citation></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischbach</surname><given-names>MA</given-names></name><name><surname>Sonnenburg</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Eating for two: how metabolism establishes interspecies interactions in the gut</article-title><source>Cell Host &amp; Microbe</source><volume>10</volume><fpage>336</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.chom.2011.10.002</pub-id><pub-id pub-id-type="pmid">22018234</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>T</given-names></name><name><surname>Krauss</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep learning with long short-term memory networks for financial market predictions</article-title><source>European Journal of Operational Research</source><volume>270</volume><fpage>654</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1016/j.ejor.2017.11.054</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fortunato</surname><given-names>M</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bayesian Recurrent Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.02798">https://arxiv.org/abs/1704.02798</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gers</surname><given-names>FA</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name><name><surname>Cummins</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Learning to forget: continual prediction with LSTM</article-title><source>Neural Computation</source><volume>12</volume><fpage>2451</fpage><lpage>2471</lpage><pub-id pub-id-type="doi">10.1162/089976600300015015</pub-id><pub-id pub-id-type="pmid">11032042</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Deep Learning</source><publisher-name>MIT press Cambridge</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Fernández</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Bidirectional lstm networks for improved phoneme classification and recognition</article-title><conf-name>In International conference on artificial neural networks</conf-name><pub-id pub-id-type="doi">10.1007/11550907</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Framewise phoneme classification with bidirectional LSTM and other neural network architectures</article-title><source>Neural Networks</source><volume>18</volume><fpage>602</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2005.06.042</pub-id><pub-id pub-id-type="pmid">16112549</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Grygorash</surname><given-names>O</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Jorgensen</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI’06</article-title><conf-name>IEEEArlington</conf-name><pub-id pub-id-type="doi">10.1109/ICTAI.2006.83</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>SFM</given-names></name><name><surname>Mi</surname><given-names>H</given-names></name><name><surname>Green</surname><given-names>R</given-names></name><name><surname>Xie</surname><given-names>L</given-names></name><name><surname>Pineda</surname><given-names>JMB</given-names></name><name><surname>Momeni</surname><given-names>B</given-names></name><name><surname>Shou</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Uncovering and resolving challenges of quantitative modeling in a simplified community of interacting cells</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000135</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000135</pub-id><pub-id pub-id-type="pmid">30794534</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Recurrent neural net learning and vanishing gradient</article-title><source>International Journal Of Uncertainity, Fuzziness and Knowledge-Based Systems</source><volume>6</volume><fpage>107</fpage><lpage>116</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hromada</surname><given-names>S</given-names></name><name><surname>Qian</surname><given-names>Y</given-names></name><name><surname>Jacobson</surname><given-names>TB</given-names></name><name><surname>Clark</surname><given-names>RL</given-names></name><name><surname>Watson</surname><given-names>L</given-names></name><name><surname>Safdar</surname><given-names>N</given-names></name><name><surname>Amador-Noguez</surname><given-names>D</given-names></name><name><surname>Venturelli</surname><given-names>OS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Negative interactions determine Clostridioides difficile growth in synthetic human gut communities</article-title><source>Molecular Systems Biology</source><volume>17</volume><elocation-id>e10355</elocation-id><pub-id pub-id-type="doi">10.15252/msb.202110355</pub-id><pub-id pub-id-type="pmid">34693621</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>RH</given-names></name><name><surname>Clark</surname><given-names>RL</given-names></name><name><surname>Tan</surname><given-names>JW</given-names></name><name><surname>Ahn</surname><given-names>JC</given-names></name><name><surname>Gupta</surname><given-names>S</given-names></name><name><surname>Romero</surname><given-names>PA</given-names></name><name><surname>Venturelli</surname><given-names>OS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Microbial Interaction Network Inference in Microfluidic Droplets</article-title><source>Cell Systems</source><volume>9</volume><fpage>229</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2019.06.008</pub-id><pub-id pub-id-type="pmid">31494089</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>AK</given-names></name><name><surname>Murty</surname><given-names>MN</given-names></name><name><surname>Flynn</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Data clustering: a review</article-title><source>ACM Computing Surveys</source><volume>31</volume><fpage>264</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1145/331499.331504</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>KS</given-names></name><name><surname>Joo</surname><given-names>GJ</given-names></name><name><surname>Kim</surname><given-names>HW</given-names></name><name><surname>Ha</surname><given-names>K</given-names></name><name><surname>Recknagel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Prediction and elucidation of phytoplankton dynamics in the Nakdong River (Korea) by means of a recurrent artificial neural network</article-title><source>Ecological Modelling</source><volume>146</volume><fpage>115</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/S0304-3800(01)00300-3</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Towards efficient data valuation based on the shapley value</article-title><conf-name>In The 22nd International Conference on Artificial Intelligence and Statistics</conf-name><fpage>1167</fpage><lpage>1176</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Scalability vs. Utility: Do We Have to Sacrifice One for the Other in Data Importance Quantification?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1911.07128">https://arxiv.org/abs/1911.07128</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karevan</surname><given-names>Z</given-names></name><name><surname>Suykens</surname><given-names>JAK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Transductive LSTM for time-series prediction: An application to weather forecasting</article-title><source>Neural Networks</source><volume>125</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2019.12.030</pub-id><pub-id pub-id-type="pmid">32062409</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>RD</given-names></name><name><surname>Rowland</surname><given-names>J</given-names></name><name><surname>Oliver</surname><given-names>SG</given-names></name><name><surname>Young</surname><given-names>M</given-names></name><name><surname>Aubrey</surname><given-names>W</given-names></name><name><surname>Byrne</surname><given-names>E</given-names></name><name><surname>Liakata</surname><given-names>M</given-names></name><name><surname>Markham</surname><given-names>M</given-names></name><name><surname>Pir</surname><given-names>P</given-names></name><name><surname>Soldatova</surname><given-names>LN</given-names></name><name><surname>Sparkes</surname><given-names>A</given-names></name><name><surname>Whelan</surname><given-names>KE</given-names></name><name><surname>Clare</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The automation of science</article-title><source>Science (New York, N.Y.)</source><volume>324</volume><fpage>85</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1126/science.1165620</pub-id><pub-id pub-id-type="pmid">19342587</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>PE</given-names></name><name><surname>Field</surname><given-names>D</given-names></name><name><surname>Gilbert</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predicting bacterial community assemblages using an artificial neural network approach</article-title><source>Nature Methods</source><volume>9</volume><fpage>621</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1975</pub-id><pub-id pub-id-type="pmid">22504588</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawson</surname><given-names>CE</given-names></name><name><surname>Harcombe</surname><given-names>WR</given-names></name><name><surname>Hatzenpichler</surname><given-names>R</given-names></name><name><surname>Lindemann</surname><given-names>SR</given-names></name><name><surname>Löffler</surname><given-names>FE</given-names></name><name><surname>O’Malley</surname><given-names>MA</given-names></name><name><surname>García Martín</surname><given-names>H</given-names></name><name><surname>Pfleger</surname><given-names>BF</given-names></name><name><surname>Raskin</surname><given-names>L</given-names></name><name><surname>Venturelli</surname><given-names>OS</given-names></name><name><surname>Weissbrodt</surname><given-names>DG</given-names></name><name><surname>Noguera</surname><given-names>DR</given-names></name><name><surname>McMahon</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Common principles and best practices for engineering microbiomes</article-title><source>Nature Reviews. Microbiology</source><volume>17</volume><fpage>725</fpage><lpage>741</lpage><pub-id pub-id-type="doi">10.1038/s41579-019-0255-9</pub-id><pub-id pub-id-type="pmid">31548653</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>V</given-names></name><name><surname>Quinn</surname><given-names>TP</given-names></name><name><surname>Tran</surname><given-names>T</given-names></name><name><surname>Venkatesh</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep in the Bowel: Highly Interpretable Neural Encoder-Decoder Networks Predict Gut Metabolites from Gut Microbiome</article-title><source>BMC Genomics</source><volume>21</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1186/s12864-020-6652-7</pub-id><pub-id pub-id-type="pmid">32689932</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Touresky</surname><given-names>D</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Sejnowski</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A theoretical framework for back-propagation</article-title><conf-name>In Proceedings of the 1988 connectionist models summer school</conf-name><fpage>21</fpage><lpage>28</lpage></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leggieri</surname><given-names>PA</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Hayes</surname><given-names>M</given-names></name><name><surname>Connors</surname><given-names>B</given-names></name><name><surname>Seppälä</surname><given-names>S</given-names></name><name><surname>O’Malley</surname><given-names>MA</given-names></name><name><surname>Venturelli</surname><given-names>OS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Integrating Systems and Synthetic Biology to Understand and Engineer Microbiomes</article-title><source>Annual Review of Biomedical Engineering</source><volume>23</volume><fpage>169</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1146/annurev-bioeng-082120-022836</pub-id><pub-id pub-id-type="pmid">33781078</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Marshall</surname><given-names>L</given-names></name><name><surname>Liang</surname><given-names>Z</given-names></name><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bayesian LSTM With Stochastic Variational Inference for Estimating Model Uncertainty in Process‐Based Hydrological Models</article-title><source>Water Resources Research</source><volume>57</volume><elocation-id>e2021WR029772</elocation-id><pub-id pub-id-type="doi">10.1029/2021WR029772</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lipton</surname><given-names>ZC</given-names></name><name><surname>Berkowitz</surname><given-names>J</given-names></name><name><surname>Elkan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Critical Review of Recurrent Neural Networks for Sequence Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.00019">https://arxiv.org/abs/1506.00019</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvak</surname><given-names>Y</given-names></name><name><surname>Byndloss</surname><given-names>MX</given-names></name><name><surname>Bäumler</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Colonocyte metabolism shapes the gut microbiota</article-title><source>Science (New York, N.Y.)</source><volume>362</volume><elocation-id>eaat9076</elocation-id><pub-id pub-id-type="doi">10.1126/science.aat9076</pub-id><pub-id pub-id-type="pmid">30498100</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>SM</given-names></name><name><surname>Lee</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A unified approach to interpreting model predictions</article-title><conf-name>In Proceedings of the 31st international conference on neural information processing systems</conf-name><fpage>4768</fpage><lpage>4777</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacArthur</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Species packing and competitive equilibrium for many species</article-title><source>Theoretical Population Biology</source><volume>1</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/0040-5809(70)90039-0</pub-id><pub-id pub-id-type="pmid">5527624</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnúsdóttir</surname><given-names>S</given-names></name><name><surname>Heinken</surname><given-names>A</given-names></name><name><surname>Kutt</surname><given-names>L</given-names></name><name><surname>Ravcheev</surname><given-names>DA</given-names></name><name><surname>Bauer</surname><given-names>E</given-names></name><name><surname>Noronha</surname><given-names>A</given-names></name><name><surname>Greenhalgh</surname><given-names>K</given-names></name><name><surname>Jäger</surname><given-names>C</given-names></name><name><surname>Baginska</surname><given-names>J</given-names></name><name><surname>Wilmes</surname><given-names>P</given-names></name><name><surname>Fleming</surname><given-names>RMT</given-names></name><name><surname>Thiele</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generation of genome-scale metabolic reconstructions for 773 members of the human gut microbiota</article-title><source>Nature Biotechnology</source><volume>35</volume><fpage>81</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1038/nbt.3703</pub-id><pub-id pub-id-type="pmid">27893703</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McPeek</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Evolutionary Community Ecology</source><publisher-name>Princeton University Press</publisher-name><pub-id pub-id-type="doi">10.23943/princeton/9780691088778.001.0001</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Michel-Mata</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>XW</given-names></name><name><surname>Liu</surname><given-names>YY</given-names></name><name><surname>Angulo</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predicting Microbiome Compositions through Deep Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.17.448886</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mickalide</surname><given-names>H</given-names></name><name><surname>Kuehn</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Higher-Order Interaction between Species Inhibits Bacterial Invasion of a Phototroph-Predator Microbial Community</article-title><source>Cell Systems</source><volume>9</volume><fpage>521</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2019.11.004</pub-id><pub-id pub-id-type="pmid">31838145</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mounier</surname><given-names>J</given-names></name><name><surname>Monnet</surname><given-names>C</given-names></name><name><surname>Vallaeys</surname><given-names>T</given-names></name><name><surname>Arditi</surname><given-names>R</given-names></name><name><surname>Sarthou</surname><given-names>A-S</given-names></name><name><surname>Hélias</surname><given-names>A</given-names></name><name><surname>Irlinger</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Microbial interactions within a cheese microbial community</article-title><source>Applied and Environmental Microbiology</source><volume>74</volume><fpage>172</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1128/AEM.01338-07</pub-id><pub-id pub-id-type="pmid">17981942</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliphant</surname><given-names>K</given-names></name><name><surname>Allen-Vercoe</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Macronutrient metabolism by the human gut microbiome: major fermentation by-products and their impact on host health</article-title><source>Microbiome</source><volume>7</volume><elocation-id>91</elocation-id><pub-id pub-id-type="doi">10.1186/s40168-019-0704-8</pub-id><pub-id pub-id-type="pmid">31196177</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pal</surname><given-names>NR</given-names></name><name><surname>Biswas</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Cluster validation using graph theoretic concepts</article-title><source>Pattern Recognition</source><volume>30</volume><fpage>847</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(96)00127-6</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: Machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peurifoy</surname><given-names>J</given-names></name><name><surname>Shen</surname><given-names>Y</given-names></name><name><surname>Jing</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Cano-Renteria</surname><given-names>F</given-names></name><name><surname>DeLacy</surname><given-names>BG</given-names></name><name><surname>Joannopoulos</surname><given-names>JD</given-names></name><name><surname>Tegmark</surname><given-names>M</given-names></name><name><surname>Soljačić</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Nanophotonic particle simulation and inverse design using artificial neural networks</article-title><source>Science Advances</source><volume>4</volume><elocation-id>eaar4206</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aar4206</pub-id><pub-id pub-id-type="pmid">29868640</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radivojević</surname><given-names>T</given-names></name><name><surname>Costello</surname><given-names>Z</given-names></name><name><surname>Workman</surname><given-names>K</given-names></name><name><surname>Garcia Martin</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A machine learning Automated Recommendation Tool for synthetic biology</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-18008-4</pub-id><pub-id pub-id-type="pmid">32978379</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>SA</given-names></name><name><surname>Adjeroh</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep Learning using Convolutional LSTM estimates Biological Age from Physical Activity</article-title><source>Scientific Reports</source><volume>9</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s41598-019-46850-0</pub-id><pub-id pub-id-type="pmid">31388024</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ribeiro</surname><given-names>MT</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Guestrin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>why should i trust you?” explaining the predictions of any classifier</article-title><conf-name>In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</conf-name><fpage>1135</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1145/2939672.2939778</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ribeiro</surname><given-names>MT</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Guestrin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>”Why Should I Trust You?”: Explaining the Predictions of Any Classifier</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1602.04938">https://arxiv.org/abs/1602.04938</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ribeiro</surname><given-names>MTC</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>lime</data-title><version designator="fd7eb2e">fd7eb2e</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivett</surname><given-names>DW</given-names></name><name><surname>Bell</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Abundance determines the functional role of bacterial phylotypes in complex communities</article-title><source>Nature Microbiology</source><volume>3</volume><fpage>767</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1038/s41564-018-0180-0</pub-id><pub-id pub-id-type="pmid">29915204</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez-Gorostiaga</surname><given-names>A</given-names></name><name><surname>Bajić</surname><given-names>D</given-names></name><name><surname>Osborne</surname><given-names>ML</given-names></name><name><surname>Poyatos</surname><given-names>JF</given-names></name><name><surname>Sanchez</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>High-order interactions distort the functional landscape of microbial consortia</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000550</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000550</pub-id><pub-id pub-id-type="pmid">31830028</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schäfer</surname><given-names>AM</given-names></name><name><surname>Zimmermann</surname><given-names>HG</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Recurrent neural networks are universal approximators</article-title><conf-name>In International Conference on Artificial Neural Networks</conf-name><pub-id pub-id-type="doi">10.1007/11840817</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schloss</surname><given-names>PD</given-names></name><name><surname>Westcott</surname><given-names>SL</given-names></name><name><surname>Ryabin</surname><given-names>T</given-names></name><name><surname>Hall</surname><given-names>JR</given-names></name><name><surname>Hartmann</surname><given-names>M</given-names></name><name><surname>Hollister</surname><given-names>EB</given-names></name><name><surname>Lesniewski</surname><given-names>RA</given-names></name><name><surname>Oakley</surname><given-names>BB</given-names></name><name><surname>Parks</surname><given-names>DH</given-names></name><name><surname>Robinson</surname><given-names>CJ</given-names></name><name><surname>Sahl</surname><given-names>JW</given-names></name><name><surname>Stres</surname><given-names>B</given-names></name><name><surname>Thallinger</surname><given-names>GG</given-names></name><name><surname>Van Horn</surname><given-names>DJ</given-names></name><name><surname>Weber</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Introducing mothur: open-source, platform-independent, community-supported software for describing and comparing microbial communities</article-title><source>Applied and Environmental Microbiology</source><volume>75</volume><fpage>7537</fpage><lpage>7541</lpage><pub-id pub-id-type="doi">10.1128/AEM.01541-09</pub-id><pub-id pub-id-type="pmid">19801464</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Segal</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Machine learning benchmarks and random forest regressione</source><publisher-name>eScholarship</publisher-name></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Selvaraju</surname><given-names>RR</given-names></name><name><surname>Cogswell</surname><given-names>M</given-names></name><name><surname>Das</surname><given-names>A</given-names></name><name><surname>Vedantam</surname><given-names>R</given-names></name><name><surname>Parikh</surname><given-names>D</given-names></name><name><surname>Batra</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</article-title><conf-name>2017 IEEE International Conference on Computer Vision (ICCV</conf-name><fpage>618</fpage><lpage>626</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.74</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharon</surname><given-names>G</given-names></name><name><surname>Garg</surname><given-names>N</given-names></name><name><surname>Debelius</surname><given-names>J</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Dorrestein</surname><given-names>PC</given-names></name><name><surname>Mazmanian</surname><given-names>SK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Specialized metabolites from the microbiome in health and disease</article-title><source>Cell Metabolism</source><volume>20</volume><fpage>719</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1016/j.cmet.2014.10.016</pub-id><pub-id pub-id-type="pmid">25440054</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>S</given-names></name><name><surname>Venturelli</surname><given-names>OS</given-names></name><name><surname>Zavala</surname><given-names>VM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Scalable nonlinear programming framework for parameter estimation in dynamic biological system models</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006828</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006828</pub-id><pub-id pub-id-type="pmid">30908479</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silverstein</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fitting Imidazole <sup>1</sup> H NMR Titration Data to the Henderson–Hasselbalch Equation</article-title><source>Journal of Chemical Education</source><volume>89</volume><fpage>1474</fpage><lpage>1475</lpage><pub-id pub-id-type="doi">10.1021/ed3000028</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sonderby</surname><given-names>SK</given-names></name><name><surname>Sønderby</surname><given-names>CK</given-names></name><name><surname>Nielsen</surname><given-names>H</given-names></name><name><surname>Winther</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Convolutional lstm networks for subcellular localization of proteins</article-title><conf-name>In International Conference on Algorithms for Computational Biology</conf-name><pub-id pub-id-type="doi">10.1007/978-3-319-21233-3</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>RR</given-names></name><name><surname>Tanoue</surname><given-names>T</given-names></name><name><surname>Szabady</surname><given-names>RL</given-names></name><name><surname>Bhattarai</surname><given-names>SK</given-names></name><name><surname>Olle</surname><given-names>B</given-names></name><name><surname>Norman</surname><given-names>JM</given-names></name><name><surname>Suda</surname><given-names>W</given-names></name><name><surname>Oshima</surname><given-names>K</given-names></name><name><surname>Hattori</surname><given-names>M</given-names></name><name><surname>Gerber</surname><given-names>GK</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Honda</surname><given-names>K</given-names></name><name><surname>Bucci</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Computer-guided design of optimal microbial consortia for immune system modulation</article-title><source>eLife</source><volume>7</volume><elocation-id>e30916</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.30916</pub-id><pub-id pub-id-type="pmid">29664397</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep Learning for Cognitive Neuroscience</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1903.01458.pdf">https://arxiv.org/pdf/1903.01458.pdf</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>LR</given-names></name><name><surname>Sanders</surname><given-names>JG</given-names></name><name><surname>McDonald</surname><given-names>D</given-names></name><name><surname>Amir</surname><given-names>A</given-names></name><name><surname>Ladau</surname><given-names>J</given-names></name><name><surname>Locey</surname><given-names>KJ</given-names></name><name><surname>Prill</surname><given-names>RJ</given-names></name><name><surname>Tripathi</surname><given-names>A</given-names></name><name><surname>Gibbons</surname><given-names>SM</given-names></name><name><surname>Ackermann</surname><given-names>G</given-names></name><name><surname>Navas-Molina</surname><given-names>JA</given-names></name><name><surname>Janssen</surname><given-names>S</given-names></name><name><surname>Kopylova</surname><given-names>E</given-names></name><name><surname>Vázquez-Baeza</surname><given-names>Y</given-names></name><name><surname>González</surname><given-names>A</given-names></name><name><surname>Morton</surname><given-names>JT</given-names></name><name><surname>Mirarab</surname><given-names>S</given-names></name><name><surname>Zech Xu</surname><given-names>Z</given-names></name><name><surname>Jiang</surname><given-names>L</given-names></name><name><surname>Haroon</surname><given-names>MF</given-names></name><name><surname>Kanbar</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Jin Song</surname><given-names>S</given-names></name><name><surname>Kosciolek</surname><given-names>T</given-names></name><name><surname>Bokulich</surname><given-names>NA</given-names></name><name><surname>Lefler</surname><given-names>J</given-names></name><name><surname>Brislawn</surname><given-names>CJ</given-names></name><name><surname>Humphrey</surname><given-names>G</given-names></name><name><surname>Owens</surname><given-names>SM</given-names></name><name><surname>Hampton-Marcell</surname><given-names>J</given-names></name><name><surname>Berg-Lyons</surname><given-names>D</given-names></name><name><surname>McKenzie</surname><given-names>V</given-names></name><name><surname>Fierer</surname><given-names>N</given-names></name><name><surname>Fuhrman</surname><given-names>JA</given-names></name><name><surname>Clauset</surname><given-names>A</given-names></name><name><surname>Stevens</surname><given-names>RL</given-names></name><name><surname>Shade</surname><given-names>A</given-names></name><name><surname>Pollard</surname><given-names>KS</given-names></name><name><surname>Goodwin</surname><given-names>KD</given-names></name><name><surname>Jansson</surname><given-names>JK</given-names></name><name><surname>Gilbert</surname><given-names>JA</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><collab>Earth Microbiome Project Consortium</collab></person-group><year iso-8601-date="2017">2017</year><article-title>A communal catalogue reveals Earth’s multiscale microbial diversity</article-title><source>Nature</source><volume>551</volume><fpage>457</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1038/nature24621</pub-id><pub-id pub-id-type="pmid">29088705</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>J</given-names></name><name><surname>Johansen</surname><given-names>R</given-names></name><name><surname>Dunbar</surname><given-names>J</given-names></name><name><surname>Munsky</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine learning to predict microbial community functions: An analysis of dissolved organic carbon from litter decomposition</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0215502</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0215502</pub-id><pub-id pub-id-type="pmid">31260460</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venturelli</surname><given-names>OS</given-names></name><name><surname>Carr</surname><given-names>AC</given-names></name><name><surname>Fisher</surname><given-names>G</given-names></name><name><surname>Hsu</surname><given-names>RH</given-names></name><name><surname>Lau</surname><given-names>R</given-names></name><name><surname>Bowen</surname><given-names>BP</given-names></name><name><surname>Hromada</surname><given-names>S</given-names></name><name><surname>Northen</surname><given-names>T</given-names></name><name><surname>Arkin</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deciphering microbial interactions in synthetic human gut microbiome communities</article-title><source>Molecular Systems Biology</source><volume>14</volume><elocation-id>e8157</elocation-id><pub-id pub-id-type="doi">10.15252/msb.20178157</pub-id><pub-id pub-id-type="pmid">29930200</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Fan</surname><given-names>K</given-names></name><name><surname>Luo</surname><given-names>N</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Heller</surname><given-names>KA</given-names></name><name><surname>You</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Massive computational acceleration by using neural networks to emulate mechanism-based biological models</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-12342-y</pub-id><pub-id pub-id-type="pmid">31554788</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>C</given-names></name><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Fong</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How to build a chatbot: chatbot framework and its capabilities</article-title><conf-name>In Proceedings of the 2018 10th International Conference on Machine Learning and Computing</conf-name><fpage>369</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1145/3195106.3195169</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Angulo</surname><given-names>MT</given-names></name><name><surname>Lao</surname><given-names>S</given-names></name><name><surname>Weiss</surname><given-names>ST</given-names></name><name><surname>Liu</surname><given-names>YY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An ecological framework to understand the efficacy of fecal microbiota transplantation</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3329</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17180-x</pub-id><pub-id pub-id-type="pmid">32620839</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yenkie</surname><given-names>KM</given-names></name><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Clark</surname><given-names>RL</given-names></name><name><surname>Pfleger</surname><given-names>BF</given-names></name><name><surname>Root</surname><given-names>TW</given-names></name><name><surname>Maravelias</surname><given-names>CT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A roadmap for the synthesis of separation networks for the recovery of bio-based chemicals: Matching biological and process feasibility</article-title><source>Biotechnology Advances</source><volume>34</volume><fpage>1362</fpage><lpage>1383</lpage><pub-id pub-id-type="doi">10.1016/j.biotechadv.2016.10.003</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zahn</surname><given-names>CT</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Graph-Theoretical Methods for Detecting and Describing Gestalt Clusters</article-title><source>IEEE Transactions on Computers</source><volume>C–20</volume><fpage>68</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1109/T-C.1971.223083</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Kobert</surname><given-names>K</given-names></name><name><surname>Flouri</surname><given-names>T</given-names></name><name><surname>Stamatakis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>PEAR: a fast and accurate Illumina Paired-End reAd mergeR</article-title><source>Bioinformatics (Oxford, England)</source><volume>30</volume><fpage>614</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt593</pub-id><pub-id pub-id-type="pmid">24142950</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>A</given-names></name><name><surname>Casari</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists</source><publisher-name>O’Reilly Media, Inc</publisher-name></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73870.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mitri</surname><given-names>Sara</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/019whta54</institution-id><institution>University of Lausanne</institution></institution-wrap><country>Switzerland</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.09.27.461983" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.27.461983"/></front-stub><body><p>The ultimate goal of this work is to apply machine learning to learn from experimental data on temporal dynamics and functions of microbial communities to predict their future behavior and design new communities with desired functions. Using a significant amount of experimental data, the authors suggest a method that outperforms the state-of-the-art approach. The work is of broad interest to those working on microbiome prediction and design.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73870.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mitri</surname><given-names>Sara</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/019whta54</institution-id><institution>University of Lausanne</institution></institution-wrap><country>Switzerland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>You</surname><given-names>Lingchong</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00py81415</institution-id><institution>Duke University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.27.461983">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.09.27.461983v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Deep Learning Enables Design of Multifunctional Synthetic Human Gut Microbiome Dynamics&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Aleksandra Walczak as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Lingchong You (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) All three reviewers agreed that not enough detail of the methods was provided. Please include this in the methods section, as well as an in-depth discussion and justification as to the choices made, including why LIME and CAM have been chosen as opposed to other methods, what are the inputs and outputs to the system, how the training and testing sets were chosen and how parameters for the two models were chosen.</p><p>2) The reviewers also agreed that the comparison of the performance of gLV to LSTM was not quite fair, due to many differences between the two implementations. To more fairly compare gLV to the LSTM, please augment the PyTorch code to have LSTM+FF and gLV+FF.</p><p>3) Carefully consider the language used throughout the paper, for example in the use of the term &quot;deep learning&quot;.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Results, first paragraph. 'The experimental102 data was split into non-overlapping training and hold-out test sets, and an appropriate LSTM network was trained to predict species abundances at various time points given the information of initial species abundance. The details on the train/test split and the number of model hyperparameters are provided in Table S2'.</p><p>It is worth mentioning these details in the main text, at least regarding data division and architecture of the LSTM based model.</p><p>What was the motivation behind the particular division of the dataset: 624 training communities and 3299 testing communities? Why 6 or fewer species were used in the training and &gt; = 10 were used in the testing dataset?</p><p>The authors state that since the glv model was used to generate the simulated data, they were not surprised to see that it predicts very well on this dataset. Would it be possible to use another benchmark to compare the two models?</p><p>Regarding the computational time of the LSTM model, I imagine it takes a lot of time to tune the hyper-parameters of the LSTM model. I think it should be mentioned that the 2 minutes required to train the LSTM model does not include this computational overhead. It is not clear how hyper-parameter tuning (number of layers, nodes, learning rate, activation function, etc.) was performed. Could you please add further details about this in the method section?</p><p>It is not clear why a FFN was used at the output of the LSTM? I could not find any information in the methods section about this. Can you explain why this particular setting was chosen?</p><p>The following line is not clear: 'all the network weights are trained simultaneously'.</p><p>What was the motivation behind choosing CAM and LIME as the explanation method among all the existing explanation methods?</p><p>LIME is a heuristic based method. How did you verify the robustness of the explanations? Could you please comment on if you changed some parameters (such as number of variables) and how it affected the explanations? How can we be sure that the explanation provided by the linear LIME models is actually what the non-linear LSTM model is doing? Are we agreeing with the explanations just because it is consistent with the current knowledge?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>My general impression is the work is very dense and key insights are somewhat masked due to the lack of clarity in places. At the highest level, I think I understood the major points the authors tried to deliver (see above). However, I feel the authors can improve the presentation of the basic methodology and the intuition on a few points, which I find myself just having to trust what the authors have to say. Providing such intuition could minimize the impression of some of the performance comparisons being ad hoc.</p><p>1. It is actually not entirely clear what exactly feeds into the LSTM (or the gLV) and what comes out during the training process. Figure 1 is supposed to explain that but I could only gather that they're predicting time series. More specifically, what are the input variables? How many time series are being predicted? The authors might want to revise/expand the schematic to better explain the basic methodology.</p><p>Also, an overlay of a few representative true time courses vs. predicted time courses (e.g. good predictions vs. failed predictions) would be illuminating.</p><p>2. Figure 2 shows the prediction of gLV-model-simulated data using the LSTM or the gLV model. Only a brief description was provided, from which, I had a hard time understanding how the data were generated or predicted. For instance, the authors stated that a 25-member gLV model was used to make predictions. Those with &lt; = 6 species were used to train the LSTM or the gLV model. Does it mean that, in these communities, only &lt; = 6 species persisted after some time? And, the trained LSTM or gLV models only predict &lt; = 6 species? If so, the following sentence becomes confusing. This confusion is related to the confusion above.</p><p>Also, how are higher-order interactions modeled in the gLV simulator?</p><p>3. It feels like the results in Figure 2 should go first (before Figure 1). The rationale is simple – for the simulated data, you have complete confidence in the ground truth, which can allow you to test the performance of the method in the idealized scenario. Also, the analysis on this data can probably allow you to provide an intuition when and why LSTM outperforms the gLV model (especially when the data were generated by another gLV model).</p><p>4. Also in Figure 2, since the data generation or training were not super time consuming, I wonder if the LSTM or gLV predictions could be drastically improved if larger training data sets are used. I'm actually quite surprised by the performance of LSTM on the simulated data, considering the somewhat limited size of the training set (624). A recent work had to use a much larger training set to achieve a high performance of LSTM (Wang et al., Nature Comm 2019) – this related work should be cited.</p><p>5. The overall performance of LSTM is impressive. It does fail occasionally even on simulated data (which is true for all ML models). To this end, the authors should discuss/show how to evaluate the confidence in individual predictions – checking each one (even if the ground truth is known) would defeat the purpose of using the neural network. This point is relevant when the trained neural network is being used to make predictions and probe biological insights.</p><p>6. I like how they use a side product of training procedure (backpropagation) to provide a quantitative metric of system sensitivity. I feel they could explain this point more.</p><p>7. For me, part of the reason the paper feels dense is that there are actually two stories (though closely related). One is the prediction of community dynamics. The other is the prediction of metabolite profiles. They represent somewhat different challenges in terms of evaluating LSTM performance.</p><p>The paper could have been split into two. Combining the two might have caused some of the confusion raised above.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Major comment: To make the comparison fair I would suggest using your torch code base to also obtain point estimates for the gLV parameters as well. There is no reason to use a sampling-based result from the prior study which uses a different model for metabolite prediction as well. Furthermore, the composite code is not accessible without a Matlab license (and thus not really reproducible). It is also trained in a two-step fashion (if this is not the case then there was not enough detail in the text to understand how the composite model was trained), and thus will be less accurate because of this. It should not take 5 hours for the model to learn the gLV parameters (your own reference [56] states that you have this down to minutes). Your torch code can be easily modified for the dynamics \log x_{t+1} = \log (x_t) + (diagonal(r) + A x_t) \δ_t and then this also means that you could have the same FF architecture for metabolites. This would allow for a much fairer comparison, both models could be trained in an identical fashion with the same metabolite prediction architecture. Although regularization and hyperparameter tuning would be slightly different of course.</p><p>Title and throughout: A single LSTM unit with a single hidden state (which can be of large dimension) is not really a deep network. With this interpretation a linear dynamical systems model with a single hidden state is a deep model as well.</p><p>21: In the abstract it is stated that current ODE models fail to capture complex behavior. This is not true. You fit an Ordinary difference equation to your data when solving for GLV, RNN, or LSTM (these are all ordinary because time is the only differential or difference variable). I think you meant to characterize the difference between parametric models like linear or gLV, etc, that are more &quot;rigid&quot; vs dynamical systems models that convolve functions up to arbitrary complexity (RNN, etc). Both are certainly classes of &quot;Ordinary&quot; difference or differential equations.</p><p>147: A gLV model is linear in the parameters so it is trivial to solve for the coefficients of the model in seconds. The text should be changed here. Also why does it take 5 hours to learn the gLV model when you state in [56] that you have this down to minutes?</p><p>Figure 1 (a) is this the model or the teacher forcing training procedure depicted. Also there is a subscript i, suggesting that there is no microbe-microbe interaction information captured by the model. Is the data in figure 1 all forecasted from t=0 or is this 1 sample look ahead prediction performance, it was not clear from the text?</p><p>Figure 2 (a) also has i subscript, was this intentional.</p><p>Methods:</p><p>– The LSTM model is not described in enough detail (what are the dimensions of the objects, how many hidden states [and why], how did you tune the hyperparameters of the model and the gradient solver [what gradient method did you use and why]).</p><p>– How was the composite model trained, was it indeed in a two-step fashion whereby the gLV model was trained and then a separate inference procedure was run for the metabolite model. More detail is needed here to understand what exactly was done and how the models were trained.</p><p>Plots in general:</p><p>– Plots only really show how the most abundant taxa are performing, would be nice to see the performance on the lower abundance taxa (maybe take a log of the OD?).</p><p>References:</p><p>You seem to be missing some important references from the Gerber Lab, &quot;MDSINE Microbial Dynamical Systems INference Engine for microbiome time-series analyses&quot; https://doi.org/10.1186/s13059-016-0980-6 is one such example but there are likely more given that lab primarily focuses on microbiome dynamics.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Deep Learning Enables Design of Multifunctional Synthetic Human Gut Microbiome Dynamics&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Aleksandra Walczak (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Reviewer #3 still finds that there is a need for a direct &quot;apples-to-apples&quot; comparison of gLV and LSTM. In your response to the reviewer's initial request, you are arguing that such a direct comparison would result in a very low performance of gLV. If this is the case, it would simply strengthen your argument and support the use of LSTM even more. It also did not seem like much additional work to carry out this analysis. If this is done and added as a supplement with some explanation of why it's not in the main text, I believe this would be satisfactory to all reviewers. Other than that, the reviewers were very appreciative of the modifications and the current quality of the paper.<italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Thank you. This is exciting work, and the authors have addressed all of my concerns thoroughly.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The authors have fully addressed my comments on the original submission. I support its publication in the journal.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>One outstanding issue with the paper remains. There is no apples-to-apples comparison of the LSTM+NN to a gLV+NN. I now comment on the authors' responses to an original comment.</p><p>[Authors] &quot;We did not estimate new gLV parameters for this work, but rather used the gLV parameters that were already determined in [6].&quot;</p><p>Having a composite model trained on different data and comparing it to a jointly trained model on entirely different data is not a sufficient comparison.</p><p>[Authors] &quot;While we agree that the reviewer's proposed method for determining the gLV parameters would be a more direct comparison, there are several reasons why we chose to use the gLV parameters estimated in our previous study: (1) Higher resolution time-series measurements are needed to determine the exponential growth rate in the gLV model. Thus, estimating gLV parameters without the time-series measurements of monocultures would result in a poorly predictive model due to high uncertainty in the exponential growth rate parameters of the gLV model. Therefore, the proposed method of inferring parameters of the gLV model would yield a poorly predictive model.&quot;</p><p>If this is the case then the authors should show that the gLV model is not predictive with this sparse data. This begs the question though. Why did one need the LSTM model if the data doesn't really capture complex dynamics? The abstract stated that the LSTM was necessary for this very reason [Abstract]&quot;Current models based on ecological theory fail to capture complex community behaviors due to higher-order interactions, do not scale well with increasing complexity and in considering multiple functions. We develop and apply a long short-term memory (LSTM) framework to advance our understanding of community assembly and health-relevant metabolite production using a synthetic human gut community.&quot; The authors are simultaneously arguing that the dynamics are too complex for simple gLV dynamics while also stating that the new time series are too simple to reliably train a gLV model. Is this a contradiction?</p><p>&quot;(2) The methods used to estimate the gLV model parameters in our previous work were informed by higher resolution time-series measurements of monocultures (individual species). Thus, the gLV model had additional information beyond that of the LSTM model, which was only informed by the initial point and endpoint (with the exception of Figure 5).&quot;</p><p>This statement is confusing. The LSTM model was trained using teacher forcing with intermediate time points per the methods section?</p><p>&quot;(3) The optimization algorithm MATLAB FMINCON is a widely used approach for estimating parameters of the gLV model (see [6], [8] and [10], for example) (4) The proposed method of log transforming the gLV model only works with higher resolution time-series data. We have sparse time sampling in our datasets.&quot;</p><p>If indeed the data is so sparse that a traditional gLV model would not learn then it would be easy for the authors to show this. In order to be published Figure 4 should compare gLV+NN and LSTM+NN trained on the same data in the same way and compare the model's performance directly.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73870.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) All three reviewers agreed that not enough detail of the methods was provided. Please include this in the methods section, as well as an in-depth discussion and justification as to the choices made, including why LIME and CAM have been chosen as opposed to other methods, what are the inputs and outputs to the system, how the training and testing sets were chosen and how parameters for the two models were chosen.</p></disp-quote><p>We have addressed this comment throughout our response. here is a summary of our edits to the manuscript to address this issue: As we have commented elsewhere in this review, we have done the following to clarify the details of our modeling:</p><p>A. We have reorganized our methods section to make it easier to find relevant details. We have created three sections: “Experimental Methods”, “Computational Methods”, and “Specific Applications of Computational Methods”. This final section has new subsections describing all analyses presented in the paper with references to the specific Figure(s) that use the described methods.</p><p>B. We have added details about the ground truth models and train/test split used for ourin silico comparison of the gLV and LSTM in predicting species abundance in the section labeled “Comparison of gLV and LSTM in silico (Figure 1)”</p><p>C. We have clarified the Methods section describing the composite model used for comparison with the LSTM for predicting species abundance and metabolite production. Methods Section “Composite Model: Regression Models for Predicting Metabolite Concentrations (Figure 3)”.</p><p>D. We have reordered Figures 1 and 2 for a more logical flow and have edited these sections to make it easier for the reader to understand relevant details.</p><p>E. We have added additional justification for the use of LIME and CAM in the relevant places in the manuscript. This includes a new sensitivity analysis of the LIME results to different test/train splits of our data (Figure 3—figure supplement 5, Figure 3—figure supplement 6).</p><disp-quote content-type="editor-comment"><p>2) The reviewers also agreed that the comparison of the performance of gLV to LSTM was not quite fair, due to many differences between the two implementations. To more fairly compare gLV to the LSTM, please augment the PyTorch code to have LSTM+FF and gLV+FF.</p></disp-quote><p>We have updated the comparisons in Figure 3—figure supplement 3a to include the prediction accuracy for gLV+FF and gLV+Random Forest Regressor. While some improvements in the prediction of Succinate, Lactate, and Acetate were observed relative to the original composite model, the LSTM outperformed all of these models for all four metabolites. We have added text to describe this result in the main text:</p><p>“Additionally, replacing the regression portion of the composite model with either a Random Forest Regressor or a Feed Forward Network did not improve the metabolite prediction accuracy beyond that of the LSTM (Figure 3—figure supplement 3a).”</p><p>We have also added descriptions of these new methods to the manuscript in the section</p><p>“Composite Model: Regression Models for Predicting Metabolite Concentration (Figure 3)”</p><disp-quote content-type="editor-comment"><p>3) Carefully consider the language used throughout the paper, for example in the use of the term &quot;deep learning&quot;.</p></disp-quote><p>The use of the terminology “deep learning” in the title and throughout the paper is consistent with accepted terminology used by the machine learning community for LSTM’s. Note that LSTMs are featured prominently as a mainstay category of deep learning networks in the definitive book on deep learning by Goodfellow, et al. [1, Ch. 10]. Please refer to our response 8 to Reviewer#3 for more details.</p><p>We have suitably revised the manuscript to improve readability. In particular, text corresponding to Figures 1 and 2 in the original manuscript have been reorganized for better readability. We have also added the following paragraph in the Discussion section to appropriately emphasize the scope of our work and some of its limitations.</p><p>“The current implementation of the LSTM model lacks uncertainty quantification for individual predictions, which could be used to guide experimental design [38]. Recent progress in using Bayesian recurrent neural networks has led to emergence of Bayesian LSTMs [39, 40], which provides uncertainty quantification for each prediction in the form of posterior variance or posterior confidence interval. However, currently, the implementation and training of such Bayesian neural networks can be significantly more difficult than training the LSTM model developed here. In addition, we benchmarked the performance of the LSTM against the standard gLV model which has been demonstrated to provide accurate predictions of community assembly in communities up to 25 species [6, 8]. The gLV model has been modified mathematically to capture more complex system behaviors [41]. However, while comparing such modified gLV models to the LSTM could further elucidate their differences, implementation of these gLV models to represent the behaviors of microbiomes with a large number of interacting species poses major computational challenges.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Results, first paragraph. 'The experimental102 data was split into non-overlapping training and hold-out test sets, and an appropriate LSTM network was trained to predict species abundances at various time points given the information of initial species abundance. The details on the train/test split and the number of model hyperparameters are provided in Table S2'.</p><p>It is worth mentioning these details in the main text, at least regarding data division and architecture of the LSTM based model.</p></disp-quote><p>Thank you for the suggestion. We have included the following details in the main text in the revised version in Section titled “LSTM accurately predicts experimental microbial community assembly”.</p><p>“Of the 175 microbial communities, 102 microbial communities were selected randomly to constitute the training set, while the remaining 73 microbial communities constituted the hold-out test set.”</p><p>“Each LSTM unit consists of a single hidden layer comprising of 2048 hidden units with ReLU activation. The details on hyperparameter tuning, learning rates, and choice of optimizer are provided in the Methods section.”</p><disp-quote content-type="editor-comment"><p>What was the motivation behind the particular division of the dataset: 624 training communities and 3299 testing communities? Why 6 or fewer species were used in the training and &gt; = 10 were used in the testing dataset?</p></disp-quote><p>This is an excellent question. This was done to demonstrate precisely the fact that our model generalizes to higher-order interactions. We train the model on simpler communities (≤ 6 species), and yet show that the model is able to capture microbial community dynamics in high richness communities (≥ 10 species). In natural communities, the number of species ranges between tens to thousands. In addition, the availability of sufficiently time-resolved measurements of community dynamics can be potentially very sparse.</p><p>The breakup of 624 communities is as follows: all 25 one-member communities, all 300 two-member communities, 100 (out of 2300 unique) three-member communities, 100 (out of 53130 unique) five-member communities and 99 (out of 177100) six-member communities. As evident, the total number of possible communities, even when one restricts to analyzing only simpler communities, is large since the total number grows exponentially with the number of species in the community. A similar training/test partitioning was used in our previous papers [5, 6, 7]. It is very common in other fields of science to ask the question of whether the behavior of lower-order systems (i.e. parts) can predict higher-order systems (whole system). Therefore, we apply a similar approach to microbial communities (e.g. similar questions have been asked in chemistry and physics).</p><p>We have added a detailed description on the rationale behind the choice of training/test sets in the section titled “LSTM outperforms the generalized Lotka Volterra ecological model” in the revised manuscript. We do not quote the edits here for the sake of brevity.</p><disp-quote content-type="editor-comment"><p>The authors state that since the glv model was used to generate the simulated data, they were not surprised to see that it predicts very well on this dataset. Would it be possible to use another benchmark to compare the two models?</p></disp-quote><p>We thank the reviewer for this question. If there was a widely accepted mechanistic model (beyond gLV) that described microbial community dynamics, we would have considered using this benchmark model instead of gLV for the comparison of the gLV and LSTM model in Figure 1. For example, dynamic flux balance analysis of genome-scale metabolic models can be used to generate time-resolved changes in species abundance. However, there are notable limitations of these models: (1) it can be challenging to use these models to study high species richness communities due to computational limitations, (2) these models have strict assumptions (e.g. metabolic fluxes are solved to optimize the growth rate of individual species), (3) genome sequences have missing or incorrect functional annotations especially for non-model bacterial species such as human gut isolates, (4) there is large uncertainty about the transport reactions of metabolites mediating interspecies interactions and (5) these models only capture inter-species interactions that arise from production/degradation of metabolites in metabolism and thus neglect other types of microbe-microbe interactions (e.g. environmental pH mediated interactions). Alternatively, Consumer-Resource models capture species growth dynamics as a function of resources uptake/utilization. However, these models generally do not capture other mechanisms beyond resource competition (e.g. metabolites that are produced by constituent members of the community and used as substrates by other members or production of toxins that inhibit species growth). Mechanisms of microbial interactions beyond resource competition are key determinants of community dynamics. In addition, these models are very difficult to apply in practice due to the myriad of unknown mechanisms of interaction driving microbe-microbe interactions. Therefore, while the gLV model does not capture the metabolites mediating inter-species interactions, it is a simplified ecological model that represents both positive and negative interactions shaping community dynamics. Despite its limitations, the gLV model is widely used by the microbiome research community and can accurately predict community assembly for synthetic microbiomes containing up to 25 species and elucidate significant inter-species interactions that were independently validated experimentally [6, 8, 7]. Finally, models that have been developed to augment the gLV with additional terms can be very challenging to implement in practice with experimental data.</p><disp-quote content-type="editor-comment"><p>Regarding the computational time of the LSTM model, I imagine it takes a lot of time to tune the hyper-parameters of the LSTM model. I think it should be mentioned that the 2 minutes required to train the LSTM model does not include this computational overhead. It is not clear how hyper-parameter tuning (number of layers, nodes, learning rate, activation function, etc.) was performed. Could you please add further details about this in the method section?</p></disp-quote><p>Thank you for your comments. The hyperparameters were tuned using a simple grid search. The hyperparameter choices include: (a) learning rate and its decay scheme for the Adam optimizer, (b) number of hidden layers per LSTM unit, (c) Number of units per layer within an LSTM unit, (d) mini-batch size, and (e) input data normalization. For feature normalization, we used the standard mean-zero, unit-variance normalization. The choices of learning rates included 0.005, 0.001 and 0.0001, respectively, each with a decay of 0.25 after every 25 epochs. Choices for mini-batch size included 1, 10, 20 and 50, respectively. It was observed that sizes 10 and 20 resulted in improved training loss, and hence we used mini-batch sizes of 10 or 20 in all our evaluations. The number of hidden layers per LSTM cell was iterated from 1 to 2. A two-layered LSTM did not result in any noticeable improvement over a single-layered LSTM cell, and thus we restricted our focus to just a single-layered LSTM cell for the sake of simplicity and faster training/inference. Finally, we tried 512, 1024, 2048 and 4096 hidden units per LSTM cell, and depending upon the complexity of the problem, we decided to work with either 2048 hidden units (no prediction of metabolites) or 4096 hidden units (simultaneous prediction of metabolites).</p><p>It must also be emphasized that the model hyperparameters were optimized for performance on a held-out cross-validation set (formed using randomly chosen 10% of the samples from the training data). Since the time taken for each run of the training process is only a couple of minutes, the entire process of tuning hyperparamters does not result in any significant computational efforts.</p><p>We have added the relevant details in the revised manuscript in a new subsection titled “Hyperparameter tuning for LSTM Networks”.</p><p>We would also like to emphasize that training gLV models, too, requires hyperparameter tuning. Fitting an ODE-based parametric model entails solving nonlinear optimization problems, the solution of which is highly dependent on the choice of initial guess parameters, choice of optimization algorithms – (a) active-set methods, (b) interior-point methods, or (c) trust region-reflective methods; and a prior knowledge of suitable parameter range. Additionally, fitting of ODE-based models also frequently involves selection of regularization parameters to reduce overfitting of the model to the noise in the experimental data [8] [6].</p><disp-quote content-type="editor-comment"><p>It is not clear why a FFN was used at the output of the LSTM? I could not find any information in the methods section about this. Can you explain why this particular setting was chosen?</p></disp-quote><p>We thank the reviewer for this request for clarification. We did not use an FFN at the output of the LSTM for predicting predicting species abundances. However, we did use an FFN as the final layer of the LSTM to predict metabolite concentrations. In considering your comment, we noticed two inaccuracies in the schematic representation of our models. In Figure 3a, we neglected to include the FFN in the schematic, whereas an FFN was incorrectly included in Figure 5h in our original submission. We have now corrected these figures in the revised paper.</p><p>Recall that species abundance is Figure 3 is a 25-dimensional vector comprising of absolute abundances of each of the species. On the other hand, metabolite profile is a four-dimensional vector corresponding to metabolite concentration of each of the four metabolites. The FFN layer converts the 25-dimensional vector at the output of the last LSTM unit to a four dimensional vector, and is trained to predict the metabolite profile. Thus, only when the species absolute abundance measurements are available at sampled time-points, an FFN is used to map the species abundance output to corresponding metabolite profile. We have clarified this statement in the revised version in the subsection entitled “LSTM enables end-point design of multifunctional synthetic human gut microbiomes” “This model uses a feed-forward network (FFN) at the output of the final LSTM unit that maps the end-point species abundance (a 25-dimensional vector) to the concentrations of the four metabolites (Figure 3a).”</p><disp-quote content-type="editor-comment"><p>The following line is not clear: 'all the network weights are trained simultaneously'.</p></disp-quote><p>We apologize for any confusion. The statement is added to emphasize that the model was trained in an end-to-end manner. We have reworded the phrase in the revised version in Section titled “LSTM enables end-point design of multifunctional synthetic human gut microbiomes”.</p><p>“The entire neural network model comprising LSTM units and a feed-forward network is learned in an end-to-end manner during the training process, (i.e., all the network weights are trained simultaneously).”</p><disp-quote content-type="editor-comment"><p>What was the motivation behind choosing CAM and LIME as the explanation method among all the existing explanation methods?</p></disp-quote><p>Please refer to our detailed response to your public review summary. Additional details on the choice of explain ability methods can be found in the Section titled “Understanding Relationships Between Variables Using LIME” in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>LIME is a heuristic based method. How did you verify the robustness of the explanations? Could you please comment on if you changed some parameters (such as number of variables) and how it affected the explanations? How can we be sure that the explanation provided by the linear LIME models is actually what the non-linear LSTM model is doing? Are we agreeing with the explanations just because it is consistent with the current knowledge?</p></disp-quote><p>We thank the reviewer for the excellent question. Our work encompasses three-fold verification of LIME explanations – (a) As suggested by the reviewer, the explanations are consistent with the current knowledge based on our previous study [6], (b) the explanations are consistent with those of sensitivity-gradient method (Figure 5), (c) To demonstrate that LIME explanations of the full community (25-member) were robust, we performed new model analyses by partitioning the training data by 20-fold and training an LSTM model on each partition. In almost every case, the inter-quartile range of the strongest LIME explanations remained either strictly positive or negative, indicating that while the magnitude of LIME explanations varied, the sign (representing the type of interaction) was consistent. This implies that the LIME explanations are robust to variations in the amount of training data (see new Figure 3—figure supplement 5 and Figure 3—figure supplement 6). We have added the following text:</p><p>“We explored the consistency of LIME explanations for the full 25-member community in response to random partitions of the training data to provide insights into the sensitivity of the LIME explanations given the training data (Figure 3—figure supplement 5, Figure 3figure supplement 6). These results demonstrated that the direction of the strongest LIME explanations of the full community were consistent in sign despite variations in magnitude.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>My general impression is the work is very dense and key insights are somewhat masked due to the lack of clarity in places. At the highest level, I think I understood the major points the authors tried to deliver (see above). However, I feel the authors can improve the presentation of the basic methodology and the intuition on a few points, which I find myself just having to trust what the authors have to say. Providing such intuition could minimize the impression of some of the performance comparisons being ad hoc.</p></disp-quote><p>We apologize for any confusion. We have made the following modifications to the text to clarify the presentation. (1) We have inverted the order of the first two sections as suggested in your comment below. (2) We have made substantial edits to the first two sections (highlighted in the revised manuscript) to streamline the information presented and clarify details of our methods. (3) We have added two new section headings to better guide the reader to the key takeaways: for the LIME Analysis: “Using local interpretable model-agnostic explanations to decipher interactions” and for the LSTM trained on higher time-resolution data: “Using LSTM with higher time-resolution to interpret contextual interactions”</p><disp-quote content-type="editor-comment"><p>1. It is actually not entirely clear what exactly feeds into the LSTM (or the gLV) and what comes out during the training process. Figure 1 is supposed to explain that but I could only gather that they're predicting time series. More specifically, what are the input variables? How many time series are being predicted? The authors might want to revise/expand the schematic to better explain the basic methodology.</p></disp-quote><p>We apologize for any confusion. Figure 2 (Figure 1 in the original submission) represents a scatter plot of all test communities (consisting of a 12-member synthetic human gut community and subsets of this community) across all species at different time instants. The input to our LSTM network is the initial species absolute abundance for each community. The LSTM predicts the absolute abundances of different species in the community at various time points. For the ease of illustration and brevity, the predicted abundances of species across 73 test communities (not included in the training data) are plotted against their true abundances.</p><p>We agree with the reviewer that it is illuminating to show representative communities that varied in their prediction accuracy by the LSTM network using the time-series data shown in Figure 2. As such, we have added a new Supplementary Figure (Figure 2—figure supplement 1) shows the predictions of the LSTM network for three (out of 73) representative communities: (a) well-predicted, (b) prediction score in the median range, (c) poorly-predicted. The prediction <italic>R</italic><sup>2</sup>-score for each community is the coefficient of determination between measured (true) and predicted abundances of all species in that community at all time instants. A histogram of <italic>R</italic><sup>2</sup>-scores across 73 test communities is shown in Figure 2—figure supplement 1a. These data show that most communities are predicted with significantly higher <italic>R</italic><sup>2</sup>-scores than the median score being ≈ 0<italic>.</italic>76.</p><p>Additionally, we have also included plots showing the temporal evolution of each species in the three representative communities – (i) well predicted (<italic>R</italic><sup>2</sup> = 0<italic>.</italic>91, Figure 2—figure supplement 1b), (ii) median score (<italic>R</italic><sup>2</sup> = 0<italic>.</italic>76, Figure 2—figure supplement 1c), (iii) poorly-predicted (<italic>R</italic><sup>2</sup> = 0<italic>.</italic>14, Figure 2—figure supplement 1d). The test dataset, comprising of 73 communities, had only 15% communities with ≥ 5 species (majority were 2-4 member communities). We found that higher richness communities were accurately predicted by our LSTM network.</p><disp-quote content-type="editor-comment"><p>Also, an overlay of a few representative true time courses vs. predicted time courses (e.g. good predictions vs. failed predictions) would be illuminating.</p><p>2. Figure 2 shows the prediction of gLV-model-simulated data using the LSTM or the gLV model. Only a brief description was provided, from which, I had a hard time understanding how the data were generated or predicted. For instance, the authors stated that a 25-member gLV model was used to make predictions. Those with &lt; = 6 species were used to train the LSTM or the gLV model. Does it mean that, in these communities, only &lt; = 6 species persisted after some time? And, the trained LSTM or gLV models only predict &lt; = 6 species? If so, the following sentence becomes confusing. This confusion is related to the confusion above.</p><p>Also, how are higher-order interactions modeled in the gLV simulator?</p></disp-quote><p>We again apologize for the confusion due to our lack of detailed descriptions of the in silico methods. We have substantially modified the section “LSTM outperforms the generalized Lotka Volterra ecological model” and added additional information to the Methods section describing these in silico experiments “Comparison of gLV and LSTM in silico”. To clarify the specified point referring to “communities with ≤ 6 species”: This means only 6 species are present with non-zero abundance at the initial time point (time 0). Thus, all other species should have zero abundance at later time points (i.e. species whose abundance at t=0 is zero do not spontaneously appear at later time points).</p><disp-quote content-type="editor-comment"><p>3. It feels like the results in Figure 2 should go first (before Figure 1). The rationale is simple – for the simulated data, you have complete confidence in the ground truth, which can allow you to test the performance of the method in the idealized scenario. Also, the analysis on this data can probably allow you to provide an intuition when and why LSTM outperforms the gLV model (especially when the data were generated by another gLV model).</p></disp-quote><p>Thank you very much for your suggestion. We, too, had initially considered adding Figure 2 (and accompanying text) before Figure 1. Upon further consideration, we agree that this is the more logical order and have rearranged these two sections in the text.</p><disp-quote content-type="editor-comment"><p>4. Also in Figure 2, since the data generation or training were not super time consuming, I wonder if the LSTM or gLV predictions could be drastically improved if larger training data sets are used. I'm actually quite surprised by the performance of LSTM on the simulated data, considering the somewhat limited size of the training set (624). A recent work had to use a much larger training set to achieve a high performance of LSTM (Wang et al., Nature Comm 2019) – this related work should be cited.</p></disp-quote><p>We agree with the reviewer that the performance of any data-driven algorithm improves with both the quantity and quality of available data. The reason we <italic>purposely</italic> worked with limited training data was primarily due to the fact that in a practical (experimental) setting, data collection is expensive and thus, any model, capable of working with limited data will prove to be most useful. Another compelling reason for working with limited data is to elucidate that LSTMs generalize better, i.e., LSTMs can translate learning on lower richness communities to predict the behavior of higher richness communities. Finally, the quality of training data also plays a significant role in the learned predictive behavior of the model. Suppose, our experimental budget allows us to perform a fixed number of limited experiments. Certainly, if each of these experiments consist of communities that do not involve a particular species ‘X’, and the model is then asked to predict the behavior of communities involving ‘X’, the model is bound to fail. Therefore, training data that includes all species and displays a wide range in the number of species in each community (wide range of species richness) should constitute the ideal learning scenario. Further, the training data should display sufficient variation in the abundance of each species or concentration of each metabolite to be informative for the model (see Figure 4—figure supplement 1). We have cited the Want et al., Nat Commun 2019 paper in our Discussion section:</p><p>“Achieving a highly predictive LSTM model required substantially less training data than a previous study that approximated the behavior of mechanistic biological systems models with RNNs (Figure 2) [9]. While the performance of any data-driven algorithm improves with the quantity and quality of available data, we demonstrate that the LSTM can translate learning on lower-order communities to accurately predict the behavior of higher-order communities given a limited and informative training set that is experimentally feasible. For synthetic microbial communities, the quality of the training set depends on the frequency of timeseries measurements within periods in which the system displays rich dynamic behaviors (i.e. excitation of the dynamic modes of the system), the range of initial species richness, representation of each community member in the training data and sufficient variation in species abundance or metabolite concentration (Figure 4—figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>5. The overall performance of LSTM is impressive. It does fail occasionally even on simulated data (which is true for all ML models). To this end, the authors should discuss/show how to evaluate the confidence in individual predictions – checking each one (even if the ground truth is known) would defeat the purpose of using the neural network. This point is relevant when the trained neural network is being used to make predictions and probe biological insights.</p></disp-quote><p>Quantification of uncertainty in predictions is an excellent suggestion. However, like most other discriminative machine learning models, the LSTM produces predictions but is not equipped to quantify the uncertainty associated with a given prediction. We have endeavored in this paper to quantify the overall uncertainty over all training data using cross-validation analysis for assessing sensitivity of the LSTM predictions (Figure 4a) and of the LIME explanations for a representative community (25-member community, see new Figure 3—figure supplement 5 and Figure 3—figure supplement 6). However, to obtain a sample-by-sample prediction of uncertainty of the prediction requires additional modeling of uncertainty, e.g., provided by a Bayesian formulation of the LSTM, would be required, which is beyond the scope of this paper and an ongoing project in our lab. However, since we agree with the reviewer on the importance of quantification confidence, in the revision we have included the following comment near the end of the Discussion section:</p><p>“The current implementation of the LSTM model lacks uncertainty quantification for individual predictions, which could be used to guide experimental design. Recent progress in using Bayesian recurrent neural networks has led to emergence of Bayesian LSTMs, which provides uncertainty quantification for each prediction in the form of posterior variance or posterior confidence interval. However, currently, the implementation and training of such Bayesian neural networks can be significantly more difficult than training the LSTM model developed here. In addition, we benchmarked the performance of the LSTM against the standard gLV model which has been demonstrated to provide accurate predictions of community assembly in communities up to 25 species. The gLV model has been modified mathematically to capture more complex system behaviors. However, while comparing such modified gLV models to the LSTM could further elucidate their differences, implementation of these gLV models to represent the behaviors of microbiomes with a large number of interacting species poses major computational challenges.”</p><disp-quote content-type="editor-comment"><p>6. I like how they use a side product of training procedure (backpropagation) to provide a quantitative metric of system sensitivity. I feel they could explain this point more.</p></disp-quote><p>Thank you for your observation. Unlike LIME-based interpretability methods that require training an additional model on top of already trained deep-learning models, the proposed sensitivity analysis requires a single backpropagation pass to capture input-output behavior. The proposed method is thus computationally inexpensive, and produces similar explanations as LIME’s. We have emphasized this further in our revised version in the subsection entitled “Understanding Relationships Between Variables Using Sensitivity Gradients.”</p><disp-quote content-type="editor-comment"><p>7. For me, part of the reason the paper feels dense is that there are actually two stories (though closely related). One is the prediction of community dynamics. The other is the prediction of metabolite profiles. They represent somewhat different challenges in terms of evaluating LSTM performance.</p><p>The paper could have been split into two. Combining the two might have caused some of the confusion raised above.</p></disp-quote><p>While we agree that combining the prediction of species abundance with metabolite concentrations makes the story more dense, we also would like to emphasize that these two problems are intimately connected. From a biological perspective, microbes continuously impact metabolites by reacting to these metabolites and releasing or consuming them. For example, metabolites impact microbial growth by being energy sources/building blocks, causing toxicity, etc. By modeling both microbial growth and the metabolites they produce/consume together, we can capture these key interconnections. This is particularly evident in the examples shown in Figure 5 where we see non-monotonic metabolite trajectories. These trends in metabolite concentrations are likely due to different phases of production/consumption of metabolites which could be captured in our model as feedback between the microbes and metabolite variables. We have revised our Discussion to mention this key aspect of our study.</p><p>“Microbial communities continuously impact metabolites by releasing or consuming them. Therefore, by modeling both microbial growth and the metabolites they produce/consume together, we were able to capture the interconnections between these variables.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Major comment: To make the comparison fair I would suggest using your torch code base to also obtain point estimates for the gLV parameters as well. There is no reason to use a sampling-based result from the prior study which uses a different model for metabolite prediction as well. Furthermore, the composite code is not accessible without a Matlab license (and thus not really reproducible). It is also trained in a two-step fashion (if this is not the case then there was not enough detail in the text to understand how the composite model was trained), and thus will be less accurate because of this. It should not take 5 hours for the model to learn the gLV parameters (your own reference [56] states that you have this down to minutes). Your torch code can be easily modified for the dynamics \log x_{t+1} = \log (x_t) + (diagonal(r) + A x_t) \δ_t and then this also means that you could have the same FF architecture for metabolites. This would allow for a much fairer comparison, both models could be trained in an identical fashion with the same metabolite prediction architecture. Although regularization and hyperparameter tuning would be slightly different of course.</p></disp-quote><p>We thank the reviewer for this comment. While we appreciate the reviewer’s desire for a more direct comparison, we chose to compare our LSTM framework to the current best implementation of the gLV modeling framework in our previous study [6]. Part of the confusion here may have been due to lack of clarity in our Methods section. We did not estimate new gLV parameters for this work, but rather used the gLV parameters that were already determined in [6].</p><p>While we agree that the reviewer’s proposed method for determining the gLV parameters would be a more direct comparison, there are several reasons why we chose to use the gLV parameters estimated in our previous study: (1) Higher resolution time series measurements are needed to determine the exponential growth rate in the gLV model. Thus, estimating gLV parameters without the time-series measurements of monocultures would result in a poorly predictive model due to high uncertainty in the exponential growth rate parameters of the gLV model. Therefore, the proposed method of inferring parameters of the gLV model would yield a poorly predictive model. (2) The methods used to estimate the gLV model parameters in our previous work were informed by higher resolution time-series measurements of monocultures (individual species). Thus, the gLV model had additional information beyond that of the LSTM model, which was only informed by the initial point and end point (with the exception of Figure 5). (3) The optimization algorithm MATLAB FMINCON is a widely used approach for estimating parameters of the gLV model (see [6], [8] and [10], for example) (4) The proposed method of log transforming the gLV model only works with higher resolution time-series data. We have sparse time sampling in our datasets.</p><p>We have modified the methods section referring the Composite Model to clarify this point:</p><p>“Our composite model implementation is similar to the model described in [8] for predicting metabolite concentration from community composition at a particular time. We used the exact gLV model parameter distributions from [8] which were determined using an approach based on [62].”</p><p>On the topic of the MATLAB license and reproducible results, we found in our previous work that we needed the tools provided in MATLAB for the task at hand. If an interested party without access to a MATLAB license wanted to reproduce these results, they could do so using GNU Octave (<ext-link ext-link-type="uri" xlink:href="https://www.gnu.org/software/octave/index">https://www.gnu.org/software/octave/index</ext-link>), a free software compatible with most MATLAB functions.</p><p>While [11] describes a method for estimating gLV parameters that takes only minutes, this method used only individual species and pairwise community growth data. In our work described in [6], we found that including communities with a higher number of species reduced the sparsity of the matrix computations and thus complicated the use of this method, so we adapted to use the MATLAB FMINCON function. This approach is fully described in [6] and does require hours to for parameter estimation.</p><disp-quote content-type="editor-comment"><p>Title and throughout: A single LSTM unit with a single hidden state (which can be of large dimension) is not really a deep network. With this interpretation a linear dynamical systems model with a single hidden state is a deep model as well.</p></disp-quote><p>While it is true that a single LSTM unit has a single hidden layer, it is common to implement a sequence of LSTM’s arranged in multiple layers for modeling time series data, a process known as unrolling, which produces a deep network. As explained in the text, and exhibited in Figures 1a and 2a, we are using such a multilayer LSTM, where each layer corresponds to a different time point of our data. Thus our use of the term “deep network” for our LSTM model is consistent with accepted terminology used by the machine learning community for LSTM’s. Note that LSTMs are featured prominently as a mainstay category of deep learning networks in the definitive book on deep learning by Goodfellow, et al. [1, Ch. 10].</p><disp-quote content-type="editor-comment"><p>21: In the abstract it is stated that current ODE models fail to capture complex behavior. This is not true. You fit an Ordinary difference equation to your data when solving for GLV, RNN, or LSTM (these are all ordinary because time is the only differential or difference variable). I think you meant to characterize the difference between parametric models like linear or gLV, etc, that are more &quot;rigid&quot; vs dynamical systems models that convolve functions up to arbitrary complexity (RNN, etc). Both are certainly classes of &quot;Ordinary&quot; difference or differential equations.</p></disp-quote><p>We apologize for the confusion here. The reviewer is correct in emphasizing that the point we were actually trying to make was the difference between parametric models based on ecological theory (like gLV) and data driven models such as the RNN and LSTM. We have modified the abstract to better reflect this comparison using the following phrasing:</p><p>“Current models based on ecological theory fail to capture complex community behaviors due to higher-order interactions, do not scale well with increasing complexity and in considering multiple functions.”</p><p>and</p><p>“We show that the LSTM model can outperform the widely used generalized Lotka-Volterra model based on ecological theory.”</p><disp-quote content-type="editor-comment"><p>147: A gLV model is linear in the parameters so it is trivial to solve for the coefficients of the model in seconds. The text should be changed here. Also why does it take 5 hours to learn the gLV model when you state in [56] that you have this down to minutes?</p></disp-quote><p>We thank the reviewer for asking for additional clarification regarding this point. Reiterating our response above:</p><p>“While [11] describes a method for estimating gLV parameters that takes only minutes, this method used only individual species and pairwise community growth data. In our work described in [6], we found that including communities with higher numbers of species reduced the sparsity of the matrix computations and complicated the use of this method. Therefore, we adapted to use the MATLAB FMINCON function. This approach is fully described in [6] and does require hours for parameter estimation.”</p><disp-quote content-type="editor-comment"><p>Figure 1 (a) is this the model or the teacher forcing training procedure depicted. Also there is a subscript i, suggesting that there is no microbe-microbe interaction information captured by the model. Is the data in figure 1 all forecasted from t=0 or is this 1 sample look ahead prediction performance, it was not clear from the text?</p></disp-quote><p>We apologize for the confusion with the figure schematics. This is a depiction of the model. We have updated the instances of <italic>X<sub>i</sub></italic> in both Figure 1a and Figure 2a to be X with an underscore to clarify that the model input is a vector of species abundances rather than a single species abundance. Thus, this enables the model to capture inter-species microbial interactions. We have updated Figure 1 and Figure 2 captions to clarify this point. All model predictions in the paper are forecasted from time 0, we have added the following sentence to the figure captions for Figures 1 and 2 to clarify this point:</p><p>“All predictions are forecasted from the abundance at time 0.”</p><disp-quote content-type="editor-comment"><p>Figure 2 (a) also has i subscript, was this intentional.</p></disp-quote><p>We thank the reviewer for this comment. Reiterating our response from above:</p><p>“We apologize for the confusion with the figure schematics. We have updated the instances of <italic>X<sub>i</sub></italic> in both Figure 1a and Figure 2a to be X with an underscore to clarify that the model input is a vector of species abundances rather than a single species abundance, thus enabling the model to capture inter-species microbial interactions.”</p><disp-quote content-type="editor-comment"><p>Methods:</p><p>– The LSTM model is not described in enough detail (what are the dimensions of the objects, how many hidden states [and why], how did you tune the hyperparameters of the model and the gradient solver [what gradient method did you use and why]).</p></disp-quote><p>We apologize for any confusion. The model details for each experiment, including the details on the number of hidden states, learning rate for the optimizer, batch size and other hyperparameters were provided in Supplementary File 1. However, we agree that there was inadequate explanation provided and we have corrected this in the revision. We used Adam optimizer for all our experiments. The choice of Adam over simple gradient descent, rmsprop or any other gradient-based optimizers was purely motivated by its empirically fast convergence behavior and lower training and validation errors. For additional details on hyperparameter tuning, please refer to our response 5 to Reviewer #1. For increased clarity, we have added a new section titled “Hyperparameter tuning for LSTM Networks” in the revised paper.</p><p>“Similar to other learning algorithms, training an LSTM network entails choosing a set of hyperparameters for optimal performance. We used an exhaustive grid-search for hyperparameter optimization, while the choice of learning algorithm (optimizer) was restricted to Adam due to its superior empirical performance. For each experiment, nearly 10% of the training dataset was reserved as a cross-validation set, and the performances of the trained models were evaluated using cross-validation sets. This information was used to select the best hyperparameter settings. The choices for hyperparameters include: (a) learning rate, (b) number of hidden layers per LSTM unit, (c) number of units per layer within an LSTM unit, (d) mini-batch size, and (e) input data normalization. The input features are normalized to have zero mean and unit variance. The choices of learning rates include 0.005, 0.001 and 0.0001, respectively, each with a decay of 0.25 after every 25 epochs. The gradual decay in learning rates prevents potential overfitting to the data. Choices for mini-batch size included 1, 10, 20 and 50, respectively. It was observed that sizes 10 and 20 resulted in improved training loss, and hence we used mini-batch sizes of 10 or 20 in all evaluations. The number of hidden layers per LSTM cell was iterated from 1 to 2. A two-layered LSTM did not result in any noticeable improvement over a single-layered LSTM cell, and thus we restricted our focus to just a single-layered LSTM cell for the sake of simplicity and faster training/inference. Finally, we tried 512, 1024, 2048 and 4096 hidden units per LSTM cell, and depending upon the complexity of the problem, we used 2048 hidden units (predictions of species and no prediction of metabolites) or 4096 hidden units (simultaneous prediction of species and metabolites). The exact details on the number of training epochs, learning rates, decay rates for different experiments can be found in the Supplementary File 1.”</p><p>And</p><p>“Note that both the composite as well as the LSTM model require tuning of hyperparameters for optimal performance. The details on the computational implementation are provided in the Methods section.”</p><disp-quote content-type="editor-comment"><p>– How was the composite model trained, was it indeed in a two-step fashion whereby the gLV model was trained and then a separate inference procedure was run for the metabolite model. More detail is needed here to understand what exactly was done and how the models were trained.</p></disp-quote><p>The models are trained in a two-step fashion. The gLV-model being a parametric ODE-model, the model parameters are obtained through nonlinear optimization methods. First, we would like to emphasize again, as we have addressed elsewhere in the review, that we did not train a new gLV model for the composite model in this work. Rather we used the parameters determined in our previous publication [6]. We did train a new regression model for each of the 4 metabolites, because the previous work only trained a model to predict one metabolite (butyrate). We have updated the Methods sections associated with the composite model to clarify this point. Here is that methods text for your reference:</p><p>“Our composite model implementation is similar to the model described in [6] for predicting metabolite concentration from community composition at a particular time. We used the exact gLV model parameter distributions from [6] which were determined using an approach based on [11]. The regression model mapping endpoint species abundance to metabolite concentrations from [6] was focused specifically on the prediction of butyrate. Therefore, we adapted the approach to prediction of multiple metabolites. First, we modified the model form to include first order and interaction terms for all 25 species, rather than just the butyrate producers. Then, we separately trained 4 regression models, one for each metabolite (butyrate, lactate, acetate, succinate), using the measured species abundance and measured metabolite concentrations from the same dataset used to train the LSTM model. We trained these models as described previously [6] by using Python scikit-learn [12] and performed L1 regularization to minimize the number of nonzero parameters. Regularization coefficients were chosen by using 10-fold cross validation. We selected the regularization coefficient value with the lowest median mean-squared error across the training splits.”</p><disp-quote content-type="editor-comment"><p>Plots in general:</p><p>– Plots only really show how the most abundant taxa are performing, would be nice to see the performance on the lower abundance taxa (maybe take a log of the OD?).</p></disp-quote><p>While we appreciate the interest in low abundance taxa, we are not confident in the precision of the experimental measurements of low abundance species (i.e. (<italic>&lt;</italic> 1%) of the total) due to limited sequencing depth. We typically sequence on the order of 10,000 molecules of PCR amplified DNA of the 16S rRNA gene per sample. Therefore, if a species is only represented by on the order of 10 of those molecules (0.1%), then a single sequencing read error would be a 10% error, whereas a species represented by <italic>&gt;</italic> 100 reads (1%) would only have 1% error per read. We chose not to include plots with log scale as to not mislead the reader in regards to the precision of our measurements. We have modified the methods section text to acknowledge this limit on the precision of our measurements:</p><p>“We expect the precision of our measurements to drop rapidly for species representing <italic>&lt;</italic> 1% of the community due to limited sequencing depth. We typically sequenced on the order of 10,000 molecules of PCR amplified DNA of the 16S rRNA gene per sample, so if a species is only represented by on the order of 10 of those molecules (0.1%), then a single sequencing read error would be a 10% error, whereas a species represented by (<italic>&gt;</italic> 100) reads (1%) would only have 1% error per read.”</p><disp-quote content-type="editor-comment"><p>References:</p><p>You seem to be missing some important references from the Gerber Lab, &quot;MDSINE Microbial Dynamical Systems INference Engine for microbiome time-series analyses&quot; https://doi.org/10.1186/s13059-016-0980-6 is one such example but there are likely more given that lab primarily focuses on microbiome dynamics.</p></disp-quote><p>Thanks for the suggestion. We have now cited the Gerber Lab paper in our Introduction.</p><p>“The parameters of the gLV model can be efficiently inferred based on properly collected absolute abundance measurements and can provide insight into significant microbial interactions shaping community assembly.”</p><p>References</p><p>1. Goodfellow, I., Bengio, Y., Courville, A. and Bengio, Y. <italic>Deep learning</italic>, vol. 1 (MIT press Cambridge, 2016).</p><p>2. Lundberg, S. M. and Lee, S.-I. A unified approach to interpreting model predictions. In Proceedings of the 31st international conference on neural information processing systems, 4768–4777 (2017).</p><p>3. Jia, R. et al. Towards efficient data valuation based on the shapley value. In The 22nd International Conference on Artificial Intelligence and Statistics, 1167–1176 (PMLR, 2019).</p><p>4. Jia, R. et al. An empirical and comparative analysis of data valuation with scalable algorithms. arXiv preprint arXiv:1911.07128 (2019).</p><p>5. Venturelli, O. S. et al. Deciphering microbial interactions in synthetic human gut microbiome communities. Molecular systems biology 14, e8157 (2018).</p><p>6. Clark, R. L. et al. Design of synthetic human gut microbiome assembly and butyrate production. Nature communications 12 (2021).</p><p>7. Hromada, S. et al. Negative interactions determine clostridioides difficile growth in synthetic human gut communities. Molecular systems biology 17, e10355 (2021).</p><p>8. Venturelli, O. S. et al. Deciphering microbial interactions in synthetic human gut microbiome communities. Molecular Systems Biology 14 (2018). URL https://onlinelibrary.wiley.com/doi/10.15252/msb.20178157.</p><p>9. Wang, S. et al. Massive computational acceleration by using neural networks to emulate mechanism-based biological models. Nature communications 10, 1–9 (2019).</p><p>10. Marino, S., Baxter, N. T., Huffnagle, G. B., Petrosino, J. F. and Schloss, P. D. Mathematical modeling of primary succession of murine intestinal microbiota. Proceedings of the National Academy of Sciences 111, 439–444 (2014).</p><p>11. Shin, S., Venturelli, O. S. and Zavala, V. M. Scalable nonlinear programming framework for parameter estimation in dynamic biological system models. PLOS Computational Biology 15, e1006828 (2019). URL https://dx.plos.org/10.1371/journal.pcbi.1006828.</p><p>12. Pedregosa, F. et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research 12, 2825–2830 (2011).</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #3 still finds that there is a need for a direct &quot;apples-to-apples&quot; comparison of gLV and LSTM. In your response to the reviewer's initial request, you are arguing that such a direct comparison would result in a very low performance of gLV. If this is the case, it would simply strengthen your argument and support the use of LSTM even more. It also did not seem like much additional work to carry out this analysis. If this is done and added as a supplement with some explanation of why it's not in the main text, I believe this would be satisfactory to all reviewers. Other than that, the reviewers were very appreciative of the modifications and the current quality of the paper.</p></disp-quote><p>Thank you very much for your efforts in soliciting reviews on the revised draft. We appreciate your acknowledgement of our efforts towards addressing the comments from the reviewers in the last submitted version. We have further revised our manuscript to address the remaining comment from Reviewer#3. Contrary to the opinion of Reviewer 3, an “apples-to-apples” comparison of the gLV and LSTM approaches is not straightforward to implement, and thus in our previous response we took the position that we had adequately demonstrated the benefits of LSTM in our simulations and experiments. However, we have carefully reconsidered our position in light of your comments, and those of Reviewer 3, and we have now included an apples-to-apples comparison between the LSTM and gLV models. While performing this comparison involved quite a bit of work, it demonstrates that the LSTM has better predictive performance for species abundance and metabolites, which indeed strengthens the paper. In the interest of transparency and reproducibility, we have uploaded the full python script of our implementation onto our project Gitlab page.</p><p>We believe that our accompanying responses, along with the additional set of experiments suitably addresses the comments from Reviewer#3.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Thank you. This is exciting work, and the authors have addressed all of my concerns thoroughly.</p></disp-quote><p>Thank you very much for your valuable inputs and encouraging remarks.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The authors have fully addressed my comments on the original submission. I support its publication in the journal.</p></disp-quote><p>Thank you very much for your valuable inputs and recommendation for publication.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>One outstanding issue with the paper remains. There is no apples-to-apples comparison of the LSTM+NN to a gLV+NN. I now comment on the authors' responses to an original comment.</p><p>[Authors] &quot;We did not estimate new gLV parameters for this work, but rather used the gLV parameters that were already determined in [6].&quot;</p><p>Having a composite model trained on different data and comparing it to a jointly trained model on entirely different data is not a sufficient comparison.</p></disp-quote><p>We understand your concern. We have now trained an discrete gLV (approximate gLV) + FFN model for predicting temporal changes in species abundance and metabolite concentrations. In addition, we uploaded the script on the project’s Gitlab page for transparency and reproducibility.</p><disp-quote content-type="editor-comment"><p>[Authors] &quot;While we agree that the reviewer's proposed method for determining the gLV parameters would be a more direct comparison, there are several reasons why we chose to use the gLV parameters estimated in our previous study: (1) Higher resolution time-series measurements are needed to determine the exponential growth rate in the gLV model. Thus, estimating gLV parameters without the time-series measurements of monocultures would result in a poorly predictive model due to high uncertainty in the exponential growth rate parameters of the gLV model. Therefore, the proposed method of inferring parameters of the gLV model would yield a poorly predictive model.&quot;</p><p>If this is the case then the authors should show that the gLV model is not predictive with this sparse data. This begs the question though. Why did one need the LSTM model if the data doesn't really capture complex dynamics? The abstract stated that the LSTM was necessary for this very reason [Abstract]&quot;Current models based on ecological theory fail to capture complex community behaviors due to higher-order interactions, do not scale well with increasing complexity and in considering multiple functions. We develop and apply a long short-term memory (LSTM) framework to advance our understanding of community assembly and health-relevant metabolite production using a synthetic human gut community.&quot; The authors are simultaneously arguing that the dynamics are too complex for simple gLV dynamics while also stating that the new time series are too simple to reliably train a gLV model. Is this a contradiction?</p></disp-quote><p>The gLV model captures interactions up to second-order. On the other hand, LSTMs do not impose such restrictions on the type of interactions. Since nonlinear recurrent neural network models, such as LSTMs, have universal approximation guarantees, LSTM networks are capable of learning interactions beyond second-order. This is not a contradiction, but a restatement of the fact that such simple ecological models are only good approximations when the model is close to the underlying ground truth model. This is highlighted in our analysis on simulated data, where mild third-order perturbations significantly reduce the gLV model’s predictive capabilities (Figure 1c).</p><disp-quote content-type="editor-comment"><p>&quot;(2) The methods used to estimate the gLV model parameters in our previous work were informed by higher resolution time-series measurements of monocultures (individual species). Thus, the gLV model had additional information beyond that of the LSTM model, which was only informed by the initial point and endpoint (with the exception of Figure 5).&quot;</p><p>This statement is confusing. The LSTM model was trained using teacher forcing with intermediate time points per the methods section?</p></disp-quote><p>The gLV model comprises of two components – (a) unary interactions (i.e. exponential growth rate), (b) pairwise interactions. These are described by the following set of coupled-ODEs:</p><p><inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:msub><mml:mrow><mml:msub><mml:mtext>dx</mml:mtext><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow/></mml:msub><mml:mtext>dt</mml:mtext></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>Nspecies</mml:mtext></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>ij</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>i\ </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>where {<italic>r<sub>i</sub></italic>} and {<italic>a<sub>ij</sub></italic>} represent individual species exponential growth rate and pairwise interaction coefficients, respectively. A better suited choice of training procedure for gLV comprises of learning the unary coefficients {<italic>r<sub>i</sub></italic>} and self-interaction coefficients {<italic>a<sub>ii</sub></italic>} first, followed by learning the pairwise interaction coefficients {<italic>a<sub>ij</sub></italic>}<italic>,i</italic> ̸ = <italic>j</italic>. In the absence of any other species, the effect of the term <italic>x<sub>j</sub></italic>(<italic>t</italic>) · <italic>x<sub>i</sub></italic>(<italic>t</italic>) is zero, and hence the monocultures (individual species) are informative of the unary coefficients {<italic>r<sub>i</sub></italic>} and self-interaction coefficients {<italic>a<sub>ii</sub></italic>}. Using this model framework, a higher time-resolved data will provide a better estimate of the model parameters. We adopted the same procedure for learning {<italic>r<sub>i</sub></italic>} and {<italic>a<sub>ii</sub></italic>}. On the other hand, the LSTMs are not built on any explicit first and second-order interactions, and thus, training the LSTM does not require higher time-resolved monospecies growth measurements.</p><p>[Re: teacher forcing]: We apologize for any confusion concerning our use of the word ”higher resolution,” which refers to the training phase of LSTM and not to its inference phase. When training an LSTM model, while a component of the loss function is used to force the predictions of the intermediate LSTM units to predict the correct species abundance (and/or metabolite profile) at subsequent time instants, a small disagreement in the species abundance (and/or metabolite profile) at an earlier time point is bound to have a trickle down effect on species abundance (and/or metabolite profile) at later time points. Thus, instead of solely relying on the prediction of the previous LSTM unit, teacher forcing frequently replaces the output of the previous LSTM unit with the ground truth species abundance (and/or metabolite profile) as an input to the next unit. This allows the model to effectively do away with the effects of error in prediction by previous LSTM units, and learn to better advance the prediction forward in time. Recall that teacher forcing is only used during training. During the inference or the test phase, only the initial point abundance (and/or metabolite profile) is rolled forward in time using the learned model assisted by teacher forcing.</p><disp-quote content-type="editor-comment"><p>&quot;(3) The optimization algorithm MATLAB FMINCON is a widely used approach for estimating parameters of the gLV model (see [6], [8] and [10], for example) (4) The proposed method of log transforming the gLV model only works with higher resolution time-series data. We have sparse time sampling in our datasets.&quot;</p><p>If indeed the data is so sparse that a traditional gLV model would not learn then it would be easy for the authors to show this. In order to be published Figure 4 should compare gLV+NN and LSTM+NN trained on the same data in the same way and compare the model's performance directly.</p></disp-quote><p>We appreciate the reviewer’s suggestion to include the apples-to-apples comparison. We implemented a variation of the reviewer’s proposed discretized gLV model, which is described in the Methods section under Comparison of the Discretized gLV Model to the LSTM. We provide two comparisons between gLV+NN and LSTM+NN, where both models were trained on the same data using the same training method to predict species abundance and metabolite concentrations. In the first comparison, we apply both models to predict species abundance after training on in silico data that was generated from a ground truth gLV model. The in silico species abundance data was generated from a ground truth gLV model with randomly sampled growth rates and interaction coefficients according to r<sub>j</sub> ∼ N(µ = .36,σ = .16), a<sub>i,i</sub> ∼ N(µ = −1.5,σ = .25), and a<sub>i,j</sub> ∼ N(µ = −.22,σ = .33). A densely sampled dataset was sampled at one hour intervals over a period of 48 hours, and a sparsely sampled dataset was sampled at 16 hour intervals. For each dataset, a total of 200 runs were simulated, each with a unique initial condition generated using Latin hypercube sampling with values ranging from.001 to.1. 75% of the data was used for training, with the remaining 25% held-out for testing. While the approximate gLV model performs well to predict species abundances after training on densely sampled data, it performs poorly compared to the LSTM model when trained on sparsely sampled data. We present the results from this comparison as a Jupyter notebook, which is available on the manuscript’s GitLab repository.</p><p>In the second comparison, we compared a discretized gLV model that was augmented with a feed-forward neural network to enable both species and metabolite predictions so that the model could be directly compared to LSTM model M3. Because the results shown in Figure 4 are based on data that was sampled at two time points, we compared the discretized gLV model to the LSTM for the time-series dataset showin in Figure 5, which was trained on data with observations at four time points to provide a better comparison of each model’s ability to predict system dynamics. In Figure 5—figure supplement 3, we show that the LSTM outperforms the gLV in both species abundance prediction and metabolite concentration prediction. Our results from both comparisons reinforced our main conclusions that the LSTM is capable of achieving improved performance over the discretized gLV model both trained using the same algorithm.</p></body></sub-article></article>