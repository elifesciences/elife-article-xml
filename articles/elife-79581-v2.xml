<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79581</article-id><article-id pub-id-type="doi">10.7554/eLife.79581</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Quantifying dynamic facial expressions under naturalistic conditions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-277070"><name><surname>Jeganathan</surname><given-names>Jayson</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4175-918X</contrib-id><email>jayson.jeganathan@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-278435"><name><surname>Campbell</surname><given-names>Megan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4051-1529</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-278436"><name><surname>Hyett</surname><given-names>Matthew</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-278437"><name><surname>Parker</surname><given-names>Gordon</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-189435"><name><surname>Breakspear</surname><given-names>Michael</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4943-3969</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00eae9z71</institution-id><institution>School of Psychology, College of Engineering, Science and the Environment, University of Newcastle</institution></institution-wrap><addr-line><named-content content-type="city">Newcastle</named-content></addr-line><country>Australia</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0020x6414</institution-id><institution>Hunter Medical Research Institute</institution></institution-wrap><addr-line><named-content content-type="city">Newcastle</named-content></addr-line><country>Australia</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047272k79</institution-id><institution>School of Psychological Sciences, University of Western Australia</institution></institution-wrap><addr-line><named-content content-type="city">Perth</named-content></addr-line><country>Australia</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03r8z3t63</institution-id><institution>School of Psychiatry, University of New South Wales</institution></institution-wrap><addr-line><named-content content-type="city">Kensington</named-content></addr-line><country>Australia</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00eae9z71</institution-id><institution>School of Medicine and Public Health, College of Medicine, Health and Wellbeing, University of Newcastle</institution></institution-wrap><addr-line><named-content content-type="city">Newcastle</named-content></addr-line><country>Australia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Shackman</surname><given-names>Alexander</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>University of Maryland</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>31</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e79581</elocation-id><history><date date-type="received" iso-8601-date="2022-04-19"><day>19</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-08-24"><day>24</day><month>08</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-05-10"><day>10</day><month>05</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.05.08.490793"/></event></pub-history><permissions><copyright-statement>© 2022, Jeganathan et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Jeganathan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79581-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-79581-figures-v2.pdf"/><abstract><p>Facial affect is expressed dynamically – a giggle, grimace, or an agitated frown. However, the characterisation of human affect has relied almost exclusively on static images. This approach cannot capture the nuances of human communication or support the naturalistic assessment of affective disorders. Using the latest in machine vision and systems modelling, we studied dynamic facial expressions of people viewing emotionally salient film clips. We found that the apparent complexity of dynamic facial expressions can be captured by a small number of simple spatiotemporal states – composites of distinct facial actions, each expressed with a unique spectral fingerprint. Sequential expression of these states is common across individuals viewing the same film stimuli but varies in those with the melancholic subtype of major depressive disorder. This approach provides a platform for translational research, capturing dynamic facial expressions under naturalistic conditions and enabling new quantitative tools for the study of affective disorders and related mental illnesses.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>facial expression</kwd><kwd>major depressive disorder</kwd><kwd>naturalistic</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Health Education and Training Institute Award in Psychiatry and Mental Health</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Jeganathan</surname><given-names>Jayson</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Rainbow Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Jeganathan</surname><given-names>Jayson</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>1118153</award-id><principal-award-recipient><name><surname>Breakspear</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>CE140100007</award-id><principal-award-recipient><name><surname>Breakspear</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>1095227</award-id><principal-award-recipient><name><surname>Breakspear</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>10371296</award-id><principal-award-recipient><name><surname>Breakspear</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000925</institution-id><institution>National Health and Medical Research Council</institution></institution-wrap></funding-source><award-id>GNT2013829</award-id><principal-award-recipient><name><surname>Jeganathan</surname><given-names>Jayson</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel computational pipeline uses time-frequency analysis to capture the dynamics of human facial expressions, and demonstrates abnormal facial dynamics in melancholic depression.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Facial expressions are critical to interpersonal communication and offer a nuanced, dynamic, and context-dependent insight into internal mental states. Humans use facial affect to infer personality, intentions, and emotions, and it is an important component of the clinical assessment of psychiatric illness. For these reasons, there has been significant interest in the objective analysis of facial affect (<xref ref-type="bibr" rid="bib40">Naumann et al., 2009</xref>; <xref ref-type="bibr" rid="bib49">Schmidt and Cohn, 2001</xref>; <xref ref-type="bibr" rid="bib4">Ambadar et al., 2005</xref>). However, decisive techniques for quantifying facial affect under naturalistic conditions remain elusive.</p><p>A traditional approach is to count the occurrences of a discrete list of ‘universal basic emotions’ (<xref ref-type="bibr" rid="bib18">Ekman, 1992</xref>). While the most commonly used system designates six basic emotions, there is disagreement about the number and nature of such affective ‘natural forms’ (<xref ref-type="bibr" rid="bib41">Ortony, 2022</xref>; <xref ref-type="bibr" rid="bib31">Keltner et al., 2019</xref>). Quantifying facial affect using the Facial Action Coding System (FACS) has become the dominant technique to operationalise facial expressions (<xref ref-type="bibr" rid="bib17">Ekman et al., 1978</xref>). Action units, each corresponding to an anatomical facial muscle group, are rated on a quantitative scale. Traditional emotion labels are associated with the co-occurrence of a specific set of action units – for example, a ‘happy’ facial expression corresponds to action units ‘Cheek Raiser’ and ‘Lip Corner Puller’ (<xref ref-type="bibr" rid="bib21">Friesen and Ekman, 1983</xref>). However, due to the time-intensive nature of manually coding every frame in a video, FACS has traditionally been applied to the analysis of static pictures rather than videos of human faces.</p><p>Recent developments in machine learning can assist the identification of basic emotions and facial action units from images and videos of human faces. Feature extraction for images include local textures (<xref ref-type="bibr" rid="bib20">Feng, 2004</xref>; <xref ref-type="bibr" rid="bib35">Kumar et al., 2016</xref>) and 3D geometry (<xref ref-type="bibr" rid="bib55">Tian et al., 2001</xref>; <xref ref-type="bibr" rid="bib23">Ghimire and Lee, 2013</xref>), while video analysis benefits from temporal features such as optical flow (<xref ref-type="bibr" rid="bib64">Yeasin et al., 2006</xref>). Supervised learning algorithms classifying basic facial expressions based on feature values have achieved accuracies of 75–98% when benchmarked against manually coded datasets of both posed and spontaneous expressions (<xref ref-type="bibr" rid="bib19">Ekundayo and Viriri, 2019</xref>). Endeavours such as the Facial Expression Recognition and Analysis challenge (<xref ref-type="bibr" rid="bib58">Valstar et al., 2015</xref>) have improved cross-domain generalisability by encouraging the field to adopt common performance metrics.</p><p>Videos of faces can now be transformed into action unit time series which capture the rich temporal dynamics of facial expressions (<xref ref-type="bibr" rid="bib55">Tian et al., 2001</xref>). This is important because human faces express emotional states dynamically – such as in a giggle or a sob. However, the rich potential of these temporal dynamics has not yet been fully exploited in the psychological and behavioural sciences. For example, some psychological studies and databases have asked responders to pose discrete emotions such as happiness or sadness (<xref ref-type="bibr" rid="bib38">Lucey et al., 2019</xref>). This strategy suits the needs of a classic factorial experimental design but fails to produce the natural dynamics of real-world facial expressions. To evoke dynamic emotion, clinical interviews have been used (<xref ref-type="bibr" rid="bib14">Darzi et al., 2019</xref>), or participants have been asked to narrate an emotive story or been shown emotive pictures rather than videos (<xref ref-type="bibr" rid="bib36">Lang et al., 2008</xref>). Such pictures can be grouped into distinct categories and presented repetitively in a trial structure, but their ecological validity is unclear. Consequently, there is an expanding interest in using participants’ facial responses to naturalistic video stimuli (<xref ref-type="bibr" rid="bib33">Kollias et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Mavadati et al., 2019</xref>; <xref ref-type="bibr" rid="bib51">Soleymani et al., 2012</xref>). These are more ecologically valid, have greater test-retest reliability than interviews, evoke stronger facial expressions than static pictures, and produce stronger cortical responses during functional neuroimaging (<xref ref-type="bibr" rid="bib4">Ambadar et al., 2005</xref>; <xref ref-type="bibr" rid="bib52">Sonkusare et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Schultz and Pilz, 2009</xref>). However, interpreting participants’ facial expressions resulting from naturalistic stimulus viewing poses challenges, because each time point is unique. There is currently no obvious way to parse the stimulus video into discrete temporal segments. Naïve attempts at dimensionality reduction – for example, averaging action unit activations across time – omit temporal dynamics and so fail to capture the complexity of natural responses.</p><p>Disturbances in facial affect occur across a range of mental health disorders, including major depressive disorder, schizophrenia, and dementia. Capturing the nuances of facial affect is a crucial skill in clinical psychiatry but in the absence of quantitative tests this remains dependent on clinical opinion. Supervised learning has shown promise in distinguishing people with major depression from controls, using input features such as facial action units coded manually (<xref ref-type="bibr" rid="bib12">Cohn et al., 2009</xref>) or automatically (<xref ref-type="bibr" rid="bib22">Gavrilescu and Vizireanu, 2019</xref>), or model-agnostic representations of facial movements such as the ‘Bag of Words’ approach (<xref ref-type="bibr" rid="bib16">Dibeklioğlu et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Bhatia et al., 2017a</xref>). Studies documenting action unit occurrence during the course of a naturalistic stimulus (<xref ref-type="bibr" rid="bib45">Renneberg et al., 2005</xref>), a short speech (<xref ref-type="bibr" rid="bib57">Trémeau et al., 2005</xref>), or a clinical interview <xref ref-type="bibr" rid="bib25">Girard et al., 2014</xref> have demonstrated that depression is associated with reduced frequency of emotional expressions, particularly expressions with positive valence. Unfortunately, by averaging action unit occurrence over time, these methods poorly operationalise the clinician’s gestalt sense of affective reactivity, which derive from a patient’s facial responses across a range of contexts.</p><p>Here, we present a novel pipeline for processing facial expression data recorded while participants view a dynamic naturalistic stimulus. The approach is data-driven, eschewing the need to preselect emotion categories or segments of the stimulus video. We derive a time-frequency representation of facial movement information, on the basis that facial movements in vivo are fundamentally dynamic and multiscale. These time-frequency representations are then divided into discrete packets with a hidden Markov model (HMM), a method for inferring hidden states and their transitions from noisy observations. We find dynamic patterns of facial behaviour which are expressed sequentially and localised to specific action units and frequency bands. These patterns are context-dependent, consistent across participants, and correspond to intuitive concepts such as giggling and grimacing. We first demonstrate the validity of this approach on an open-source dataset of facial responses of healthy adults watching naturalistic stimuli. We then test this approach on facial videos of participants with melancholic depression, a severe mood disorder characterised by psychomotor changes (<xref ref-type="bibr" rid="bib42">Parker and Hadzi Pavlovic, 1996</xref>). The time-frequency representation improves accuracy in classifying patients from healthy controls. However, unlike end-to-end deep learning approaches, our use of spectral features of facial action units facilitates easy interpretation while also exploiting prior knowledge to reduce the need for very large training datasets. Dynamic facial patterns reveal specific changes in melancholia, including reduced facial activity in response to emotional stimuli, anomalous facial responses inconsistent with the affective context, and a tendency to get ‘stuck’ in negatively valenced states.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We first analysed dynamic facial expressions from video recordings of 27 participants viewing short emotive clips of 4 minutes duration, covering a breadth of basic emotions (the Denver Intensity of Spontaneous Facial Action [DISFA] dataset [<xref ref-type="bibr" rid="bib39">Mavadati et al., 2019</xref>] detailed in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Frame-by-frame action unit activations were extracted with OpenFace software (<xref ref-type="bibr" rid="bib6">Baltrusaitis et al., 2018</xref>) (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for action unit descriptions, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for group mean time series).</p><p>From these data, we used the continuous wavelet transform to extract a time-frequency representation of individual action unit time series in each participant. To test whether this time-frequency representation captures high-frequency dynamic content, we first compared the group average of these individual time-frequency representations with the time-frequency representation of the group mean time series. We selected the activity of action unit 12 ‘Lip Corner Puller’ during a positive valence video clip (a ‘talking dog’), as this action unit is conventionally associated with happy affect, and its high-frequency activity denotes smiling or laughing. Compared to the group mean time series, the time series of individuals had significantly greater amplitude (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), particularly at higher frequencies (<xref ref-type="fig" rid="fig1">Figure 1f</xref>). This demonstrates that the time-frequency representations of individual participants capture high-frequency dynamics that are obscured by characterising group-averaged time courses. This is because stimulus-evoked facial action unit responses have asynchronous alignment across participants, hence cancelling when superimposed. This problem is avoided in the group-level time-frequency representation, whereby the amplitude is first extracted at the individual level, prior to group averaging. Comparable results occurred in all action units (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Time-frequency representation of action unit 12 ‘Lip Corner Puller’ during positive valence video stimulus reveals high-frequency dynamics.</title><p>(<bold>a</bold>) Example participant’s facial reactions at two time points, corresponding to high and low activation of action unit 12. (<bold>b</bold>) Action unit time series for five example participants (blue). Bold line corresponds to the participant shown in panel (<bold>a</bold>), and black arrows indicate time points corresponding to the representative pictures. The group mean time course across all participants is shown in red. Red arrows indicate funny moments in the stimulus, evoking sudden facial changes in individual participants. These changes are less prominent in the group mean time course. (<bold>c</bold>) Time-frequency representation for the same five participants, calculated as the amplitude of the continuous wavelet transform. Intuitively, the heatmap colour indicates how much of each frequency is present at each time point. Shading indicates the cone of influence – the region contaminated by edge effects. (<bold>d</bold>) Mean of all participants’ time-frequency representations. Red arrows correspond to time points with marked high-frequency activity above 1 Hz (<bold>e</bold>) Time-frequency representation of the group mean time course. (<bold>f</bold>) Difference between (<bold>d</bold>) and (<bold>e</bold>). Non-significant differences (p&gt;0.05) are shown in greyscale. Common colour scale is used for (<bold>d</bold>–<bold>f</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Mean of raw time series, Denver Intensity of Spontaneous Facial Action (DISFA) dataset.</title><p>Vertical lines demarcate video clips described in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Mean of time-frequency representation across all participants in Denver Intensity of Spontaneous Facial Action (DISFA) dataset.</title><p>Shading indicates the cone of influence. Vertical lines demarcate video clips described in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig1-figsupp2-v2.tif"/></fig></fig-group><p>Having shown how the time-frequency representation captures dynamic content, we next sought to quantify the joint dynamics of facial action units. To this end, an HMM was inferred from the time course of all facial action units’ time-frequency representations. An HMM infers a set of distinct states from noisy observations, with each state expressed sequentially in time according to state-to-state transition probabilities. Each state has a distinct mapping onto the input space, here the space of frequency bands and action units (see <xref ref-type="fig" rid="fig2">Figure 2</xref> for a visual overview of the entire pipeline, <xref ref-type="fig" rid="fig3">Figure 3a</xref> for the HMM states derived from the DISFA dataset).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visual overview of the pipeline.</title><p>(<bold>a</bold>) Participant’s facial responses while viewing a naturalistic stimulus. (<bold>b</bold>) OpenFace extracts the time series for each action unit. (<bold>c</bold>) The continuous wavelet transform produces a time-frequency representation of the same data. (<bold>d</bold>) A hidden Markov model infers dynamic facial states common to all participants. Each state has a unique distribution over action units and frequency bands.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig2-v2.tif"/></fig><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Dynamic facial states inferred from time-frequency representation of Denver Intensity of Spontaneous Facial Action (DISFA) dataset.</title><p>(<bold>a</bold>) Mean of the observation model for each state, showing their mapping onto action units and frequency bands. Avatar faces (top row) for each state show the relative contribution of each action unit, whereas their spectral projection (bottom row) shows their corresponding dynamic content. (<bold>b</bold>) Sequence of most likely states for each participant at each time point. Vertical lines demarcate transition between stimulus clips with different affective annotations. (<bold>c</bold>) Most common states across participants, using a 4 s sliding temporal window. (<bold>d</bold>) Proportion of participants expressing the most common state. Blue shading indicates 5–95% bootstrap confidence bands for the estimate. Grey shading indicates the 95th percentile for the null distribution, estimated using time-shifted surrogate data. (<bold>e</bold>) Transition probabilities displayed as a weighted graph. Each node corresponds to a state. Arrow thickness indicates the transition probability between states. For visualisation clarity, only the top 20% of transition probabilities are shown. States are positioned according to a force-directed layout where edge length is the inverse of the transition probability.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Free energy of the hidden Markov model as a function of number of states.</title><p>Free energy continues to decrease as the number of states increases (logarithmic scale).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig3-figsupp1-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video1.mp4" id="fig3video1"><label>Figure 3—video 1.</label><caption><title>Example subject 1 in hidden Markov model (HMM) state 1.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video2.mp4" id="fig3video2"><label>Figure 3—video 2.</label><caption><title>Example subject 1 in hidden Markov model (HMM) state 2.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video3.mp4" id="fig3video3"><label>Figure 3—video 3.</label><caption><title>Example subject 1 in hidden Markov model (HMM) state 3.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video4.mp4" id="fig3video4"><label>Figure 3—video 4.</label><caption><title>Example subject 1 in hidden Markov model (HMM) state 4.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video5.mp4" id="fig3video5"><label>Figure 3—video 5.</label><caption><title>Example subject 1 in hidden Markov model (HMM) state 5.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video6.mp4" id="fig3video6"><label>Figure 3—video 6.</label><caption><title>Example subject 1 in hidden Markov model (HMM) state 6.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video7.mp4" id="fig3video7"><label>Figure 3—video 7.</label><caption><title>Example subject 2 in hidden Markov model (HMM) state 1.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video8.mp4" id="fig3video8"><label>Figure 3—video 8.</label><caption><title>Example subject 2 in hidden Markov model (HMM) state 3.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video9.mp4" id="fig3video9"><label>Figure 3—video 9.</label><caption><title>Example subject 2 in hidden Markov model (HMM) state 4.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video10.mp4" id="fig3video10"><label>Figure 3—video 10.</label><caption><title>Example subject 2 in hidden Markov model (HMM) state 5.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video11.mp4" id="fig3video11"><label>Figure 3—video 11.</label><caption><title>Example subject 2 in hidden Markov model (HMM) state 6.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video12.mp4" id="fig3video12"><label>Figure 3—video 12.</label><caption><title>Example subject 2 in hidden Markov model (HMM) state 7.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video13.mp4" id="fig3video13"><label>Figure 3—video 13.</label><caption><title>Example subject 2 in hidden Markov model (HMM) state 8.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video14.mp4" id="fig3video14"><label>Figure 3—video 14.</label><caption><title>Example subject 3 in hidden Markov model (HMM) state 1.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video15.mp4" id="fig3video15"><label>Figure 3—video 15.</label><caption><title>Example subject 3 in hidden Markov model (HMM) state 2.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video16.mp4" id="fig3video16"><label>Figure 3—video 16.</label><caption><title>Example subject 3 in hidden Markov model (HMM) state 3.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video17.mp4" id="fig3video17"><label>Figure 3—video 17.</label><caption><title>Example subject 3 in hidden Markov model (HMM) state 4.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video18.mp4" id="fig3video18"><label>Figure 3—video 18.</label><caption><title>Example subject 3 in hidden Markov model (HMM) state 6.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-79581-fig3-video19.mp4" id="fig3video19"><label>Figure 3—video 19.</label><caption><title>Example subject 3 in hidden Markov model (HMM) state 7.</title></caption></media></fig-group><p>Examples of participants in each state are provided <xref ref-type="video" rid="fig3video1 fig3video2 fig3video3 fig3video4 fig3video5 fig3video6 fig3video7 fig3video8 fig3video9 fig3video10 fig3video11 fig3video12 fig3video13 fig3video14 fig3video15 fig3video16 fig3video17 fig3video18 fig3video19">Figure 3—videos 1–19</xref>. Their occurrence corresponded strongly with annotated video clip valence (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). We found that inferred state sequences had high between-subject consistency, exceeding chance level across the vast majority of time points and reaching 93% during specific movie events (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). States were frequency-localised and comprised intuitive combinations of action units which reflected not only distinct emotion categories as defined in previous literature (<xref ref-type="bibr" rid="bib21">Friesen and Ekman, 1983</xref>), but also stimulus properties such as mixed emotions. State transition probabilities appeared clustered by valence rather than frequency, such that frequent transitions between low- and high-frequency oscillations of the same facial action units were more likely than transitions between different emotions (<xref ref-type="fig" rid="fig3">Figure 3e</xref>).</p><list list-type="bullet"><list-item><p>States 1 and 2 were active during stimuli annotated as ‘happy’. They activated two action units typically associated with happiness, action unit 6 ‘Cheek Raiser’ and 12 ‘Lip Corner Puller’, but also action unit 25 ‘Lips Part’. State 2 likely represents laughing or giggling as it encompassed high-frequency oscillations in positive valence action units, in comparison to the low-frequency content of state 1.</p></list-item><list-item><p>States 3 and 4 were active during videos evoking fear and disgust – for example of a man eating a beetle larva. They encompassed mixtures of action units conventionally implicated in disgust and fear, at low- and high-frequency bands, respectively. State 3 recruited action units 4 ‘Brow Lowerer’ and 9 ‘Nose Wrinkler’, while state 4 involved these action units as well as action units 15 ‘Lip Corner Depressor’, 17 ‘Chin Raiser’, and 20 ‘Lip Stretcher’.</p></list-item><list-item><p>States 5 and 6 occurred predominantly during negatively valenced clips, and deactivated oscillatory activity in most action units, with sparing of action units typically associated with sadness, 4 ‘Brow Lowerer’ and 15 ‘Lip Corner Depressor’.</p></list-item></list><sec id="s2-1"><title>Facial affect in melancholia</title><p>We next analysed facial video recordings from a cohort of participants with melancholic depression and healthy controls who watched three video clips consecutively – a stand-up comedy, a sad movie clip, and a non-English language video which is initially puzzling but also amusing. These three stimuli were chosen from a database of independently rated videos of high salience (<xref ref-type="bibr" rid="bib26">Guo et al., 2016</xref>). The stand-up comedy comprises episodic jokes with a deadpan delivery and audience laughter, whereas the weather report depicts someone speaking a foreign language and eventually laughing uncontrollably, although the reason remains unclear to the viewer. Clinical participants with melancholia were recruited from a tertiary mood disorders clinic and met melancholia criteria including psychomotor changes, anhedonia, and diurnal mood variation (see Materials and methods). We conducted analyses based firstly on group-averaged time courses, and then on the time-frequency representation.</p></sec><sec id="s2-2"><title>Group time courses in melancholia</title><p>Facial action unit time courses showed clear group differences (see <xref ref-type="fig" rid="fig4">Figure 4</xref> for action units typically implicated in expressing happiness and sadness, and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for all action units). For each action unit in each participant, we calculated the median action unit activation across each stimulus video. These were compared with a three-way ANOVA, with factors for clinical group, stimulus, and the facial valence. We considered two stimulus videos, one with positive and one with negative valence, and two facial valence states, happiness and sadness, calculated as sums of positively and negatively valenced action unit activations, respectively (<xref ref-type="bibr" rid="bib21">Friesen and Ekman, 1983</xref>). A significant three-way interaction was found between clinical group, stimulus, and facial valence (p=0.003). Post hoc comparisons with Tukey’s honestly significant difference criterion (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) quantified that during stand-up comedy, participants with melancholia had reduced activation of action unit 12 ‘Lip Corner Puller’ (p&lt;0.0001) and increased activation of action unit 4 ‘Brow Lowerer’ (p&lt;0.0001). Interestingly, facial responses of participants with melancholia during stand-up comedy were similar to those of controls during the sad movie (p&gt;0.05 for both action units). Results were unchanged when using the weather report stimulus instead of the stand-up comedy (p=0.005 for three-way interaction, see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> for post hoc comparisons).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>At each time point, mean intensity across participants of facial action unit activation in controls (blue) and melancholia (red).</title><p>Shading indicates 5% and 95% confidence bands based on a bootstrap sample (n=1000). (<bold>a</bold>) Action units commonly implicated in happiness (top row) and sadness (bottom row). Participants watched stand-up comedy, a sad video, and a funny video in sequence. Vertical lines demarcate transitions between video clips. (<bold>b</bold>) First principal component of action units, shown during stand-up comedy alone. Vertical lines indicate joke annotations. Avatar face shows the relative contribution of each action unit to this component.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Comedy vs. sad movie.</title><p>Post hoc comparison intervals using Tukey’s honestly significant difference criterion (p=0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Weather report vs. sad movie.</title><p>Post hoc comparison intervals using Tukey’s honestly significant difference criterion (p=0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Mean facial action unit activation in controls and melancholia for all action units.</title><p>Shaded confidence bands were calculated as the 5th and 95th percentile in a bootstrap sample (n=1000). Participants watched, in sequence, stand-up comedy, a sad video, and a funny video. Vertical lines demarcate transitions between video clips.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig4-figsupp3-v2.tif"/></fig></fig-group><p>To move away from individual action units, we next extracted the first principal component across all action units. The time course of this composite component closely followed joke punch lines during stand-up comedy (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). This responsivity of this component to movie events was substantially diminished in the melancholia cohort.</p></sec><sec id="s2-3"><title>Time-frequency representation in melancholia</title><p>Time-frequency representations were calculated for all action units in all participants. For each action unit, the mean time-frequency representation for the control group was subtracted from the participants with melancholia (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for the mean of the controls). Significant group differences (p&lt;0.05) were found by comparison to a null distribution composed of 1000 resampled surrogate datasets (see Materials and methods). Participants with melancholia had a complex pattern of reduced activity encompassing a broad range of frequencies (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The most prominent differences were in positive valence action units during positive valence stimuli, but significant reductions were seen in most action units. Differences in high-frequency bands occurred during specific movie events such as jokes (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). There were sporadic instances of increased activity in melancholia participants during the sad movie involving mainly action units 15 ‘Lip Corner Depressor’ and 20 ‘Lip Stretcher’.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Group differences in time-frequency activity.</title><p>(<bold>a</bold>) Mean time-frequency activity in melancholia benchmarked to the control group. Negative colour values (red-purple) indicate melancholia &lt; controls (p&lt;0.05). Non-significant group differences (p&gt;0.05) are indicated in greyscale. Vertical lines demarcate stimulus videos. (<bold>b</bold>) Action unit 12 ‘Lip Corner Puller’ during stand-up comedy in controls, participants with melancholia, and difference between groups. Vertical lines indicate joke annotations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Mean of time-frequency representation across all controls in melancholia dataset.</title><p>Participants watched, in sequence, stand-up comedy, a sad video, and a funny video. Shading indicates the cone of influence. Vertical lines demarcate video clips.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig5-figsupp1-v2.tif"/></fig></fig-group><p>We next pursued whether the additional time-frequency information would improve the classification accuracy of differentiating participants with melancholia from controls. A support vector machine, using as inputs the mean action unit activation for each stimulus video, achieved 63% accuracy with five-fold cross-validation. In contrast, using as inputs the mean time-frequency amplitude in discrete frequency bands within 0–5 Hz improved average cross-validation accuracy to 71% (p&lt;0.001 for difference between models). As a control for the additional number of input features, we tested a third set of models which naively modelled temporal dynamics using mean action unit activations within shorter time blocks. These models had 63–64% accuracy despite having a greater number of input features than the time-frequency representation (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec><sec id="s2-4"><title>Sequential affective states in melancholia</title><p>Inverting a HMM from the time-frequency representations of facial action units yielded the sequential expression of eight states across participants (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Hidden Markov model (HMM) inferred from time-frequency representation of melancholia dataset.</title><p>(<bold>a</bold>) Contribution of action units and their spectral expression to each state. Avatar faces for each state show the relative contribution of each action unit. (<bold>b</bold>) State sequence for each participant at each time point, for controls (top) and participants with melancholia (bottom). Vertical lines demarcate stimulus clips. (<bold>c</bold>) Most common state across participants, using a 4 s sliding temporal window. (<bold>d</bold>) Proportion of participants expressing the most common state for control (blue) and melancholia cohorts (black). Shading indicates 5% and 95% bootstrap confidence bands. (<bold>e</bold>) Transition probabilities displayed as a weighted graph, with the top 20% of transition probabilities shown. States are positioned according to a force-directed layout where edge length is the inverse of transition probability. (<bold>f</bold>) Differences in mean transition probabilities between participants with melancholia and controls. Each row/column represents an HMM state. Colours indicate (melancholia–controls) values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Hidden Markov model inferred from time-frequency representation of melancholia dataset, where data are not standardised before model inference.</title><p>(<bold>a</bold>) Mean of the observation model for each state. Avatar faces for each state show the relative contribution of each action unit. (<bold>b</bold>) Most likely state sequence for each participant at each time point, for controls (top) and participants with melancholia (bottom). Vertical lines demarcate stimulus clips. Without standardisation, state transitions are infrequent and transition probabilities less meaningful. (<bold>c</bold>) Most common state across participants, using a 4 s sliding temporal window. (<bold>d</bold>) Proportion of participants expressing the most common state for controls (blue) and participants with melancholia (black). Shading indicates 5% and 95% bootstrap confidence bands. (<bold>e</bold>) Transition probabilities displayed as a weighted graph. Only the top 20% of transition probabilities are shown. States are positioned according to a force-directed layout where edge length is the inverse of transition probability. (<bold>f</bold>) Differences in mean transition probabilities between participants with melancholia and controls.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Hidden Markov model inferred from time-frequency representation of melancholia dataset, with states defined by a diagonal covariance matrix.</title><p>Contents of (<bold>a</bold>–<bold>f</bold>) are otherwise identical to <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79581-fig6-figsupp2-v2.tif"/></fig></fig-group><list list-type="bullet"><list-item><p>States 1 and 2 activated positive valence action units, each in distinct frequency bands, and were dominant through the stand-up comedy for most participants (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). State 2 comprised high-frequency oscillations in positive valence action units, corresponding to laughing or giggling.</p></list-item><list-item><p>The sad movie was associated with early involvement of state 3, which deactivated high-frequency activity, followed by states 4 and 5, which also deactivated oscillatory activity, but with more specificity for lower frequencies and positive valence action units.</p></list-item><list-item><p>State 6 comprised action units 4 ‘Brow Lowerer’, 9 ‘Nose Wrinkler’, 17 ‘Chin Raiser’, and 23 ‘Lip Tightener’, traditionally associated with anger, disgust, or concern. State 7 can be associated with ‘gasping’, with very high-frequency activation of most mouth-associated action units including 25 ‘Lips Part’. These states occurred sporadically through the weather report.</p></list-item><list-item><p>State 8 predominantly activated action unit 1 ‘Inner Brow Raiser’, commonly associated with negative valence.</p></list-item></list><p>The temporal sequence of the most common state was similar across groups (<xref ref-type="fig" rid="fig6">Figure 6c</xref>), but the between-subjects consistency was markedly reduced in the melancholic participants during both funny videos (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). Some participants with melancholia – for example participants 2 and 3 (<xref ref-type="fig" rid="fig6">Figure 6b</xref>) – had highly anomalous state sequences compared to other participants.</p><p>Fractional occupancy – the proportion of time spent by participants in each state – was significantly different between groups for the positive valence states – state 1 (Melancholia &lt; Controls, p<sub>FDR</sub> = 0.03) and state 2 (Melancholia &lt; Controls, p<sub>FDR</sub> = 0.004) – as well as for negatively valenced state 8 (Melancholia &gt; Controls p<sub>FDR</sub> = 0.03). We then asked whether group differences in the time spent in each state were attributable to changes in the likelihood of switching in to, or out of, specific facial states. Participants with melancholia were significantly less likely to switch from a low-frequency positive valence state (1, smiling) to high-frequency positive valence oscillations (state 2, giggling), but were more likely to switch to states associated with any other emotion (states 4, 5, 6, 7, and 8). From the high-frequency positive valence state, they were more likely to switch to the deactivating ‘ennui’ state 4 (all p<sub>FDR</sub> &lt; 0.05).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Facial expressions played a crucial role in the evolution of social intelligence in primates (<xref ref-type="bibr" rid="bib49">Schmidt and Cohn, 2001</xref>) and continue to mediate human interactions. Observations of facial affect, its range, and reactivity play a central role in clinical settings. Quantitative analysis of facial expression has accelerated of late, driven by methods to automatically factorise expressions into action units (<xref ref-type="bibr" rid="bib17">Ekman et al., 1978</xref>) and the availability of large datasets of posed emotions (<xref ref-type="bibr" rid="bib38">Lucey et al., 2019</xref>). The dynamics of facial expression mediate emotional reciprocity, but have received less attention (<xref ref-type="bibr" rid="bib4">Ambadar et al., 2005</xref>). Naturalistic stimuli offer distinct advantages to affective research for their ability to evoke these dynamic responses (<xref ref-type="bibr" rid="bib15">Dhall et al., 2012</xref>), but their incompressibility has made analysis problematic. By leveraging techniques in computer vision, we developed a pipeline to characterise facial dynamics during naturalistic video stimuli. Analysis of healthy adults watching emotionally salient videos showed that facial expression dynamics can be captured by a small number of spatiotemporal states. These states co-activate facial muscle groups with a distinct spectral fingerprint, and transition dynamically with the emotional context. Application of this approach to melancholia showed that the clinical gestalt of facial non-reactivity in melancholia (<xref ref-type="bibr" rid="bib43">Parker, 2007</xref>) can be objectively identified not just with restrictions in spectral content, but also with anomalous facial responses, more frequent occurrence of an ennui affect, and more frequent state switching from transiently positive facial expressions to neutral and negative states. This approach provides a unique perspective on how facial affect is generated by the interplay between inner affective states and the sensorium.</p><p>Our pipeline first comprises automatic action unit extraction, then spectral wavelet-based analysis of the ensuing feature dynamics. Wavelet energy at a given time corresponds to the occurrence of a specific facial ‘event’, while energy in a given frequency reflects the associated facial dynamics, like laughing. Unlike temporal averaging methods, which require an arbitrary timescale, wavelets cover a range of timescales. The spectral approach also allows participant facial responses to be pooled, without the limitations of averaging responses whose phases are misaligned. We then inferred an HMM, identifying spatially and spectrally resolved modes of dynamic facial activity which occur sequentially with high consistency across participants viewing the same stimulus. States transitions aligned with intuitive notions of affective transitions. For example, the common transition between the low- and high-frequency positive valence state reflected transitions between smiling and laughing.</p><p>Our method builds on the Emotion Facial Action Coding System (EMFACS) (<xref ref-type="bibr" rid="bib21">Friesen and Ekman, 1983</xref>), where each canonical emotion label (happy, angry, etc.) is defined on the basis of a sparse set of minimally necessary action units. The sparsity of this coding allows manual raters to find the minimal necessary combinations of action units in a facial video to reflect an emotion label, but may not include all action units that are involved in each affective state. Affective states inferred from our HMM not only reflected prototypical action unit combinations from EMFACS, but also provide a richer mapping across a broader range of action units. For example, while happiness has been previously associated with just two action units, ‘Cheek Raiser’ and ‘Lip Corner Puller’, such sparse activations are rare, particularly during intense emotional displays. We demonstrated that laughing during stand-up comedy activated eyebrow-related action units, some of which are traditionally associated with sadness. Conversely, negatively valenced stimuli dampened facial movements, with a relative sparing of those action units typically associated with sadness.</p><p>Ensembles of HMMs have previously been used to improve emotion classification accuracy when benchmarked against manually coded datasets. In these studies, one HMM models the temporal dynamics of one action unit (<xref ref-type="bibr" rid="bib29">Jiang et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Koelstra et al., 2010</xref>) or one universal basic emotion (<xref ref-type="bibr" rid="bib64">Yeasin et al., 2006</xref>; <xref ref-type="bibr" rid="bib48">Sandbach et al., 2012</xref>), with HMM states corresponding to expression onset/offset. Given a video frame, the HMM with the greatest evidence determines the decoded expression. Nested HMMs have also been employed, with a second-level HMM predicted transitions between the basic emotions (<xref ref-type="bibr" rid="bib11">Cohen et al., 2012</xref>). In contrast, the present method uses a single HMM to describe facial expressions without prior emotion categories, capturing the dynamic co-occurrence of facial actions that together comprise distinct affective states. By taking the spectral activity of action units as input features into the HMM, our approach uniquely captures the spatiotemporal texture of naturally occurring facial affect. This enables, for example, the disambiguation of a smile from a giggle. The importance of the spectral characterisation is highlighted by our finding that in melancholia, smile states were more likely to transition to ennui, and less likely to the laughter state. Our use of dynamic spectra as inputs into an HMM is similar to their recent use in neuroimaging research (<xref ref-type="bibr" rid="bib5">Baker et al., 2014</xref>). Using the raw time series is also possible – hence additionally capturing phase relationships, although this comes with an additional computational burden and reduced interpretability of states (<xref ref-type="bibr" rid="bib61">Vidaurre et al., 2018</xref>).</p><p>Dynamic facial patterns were influenced by the affective properties of the stimulus video. For the DISFA dataset, the HMM inferred two disgust-associated states, in low- and high-frequency bands, respectively. These states occurred predominantly during two disgusting video clips. For the melancholia dataset, the inferred HMM states over-represented happiness and sadness, and under-represented disgust. This is ostensibly because the stimulus had prominent positively and negatively valenced sections without disgusting content. The co-occurrence of the states and the state transitions across participants speaks to the influence of the video content on affective responses and hence, more broadly, the dynamic exchange between facial affect and the social environment.</p><p>We found that participants with melancholia exhibited broad reductions in facial activity, as well as specific reductions in high-frequency activity in response to joke punchlines, reflecting the clinical gestalt of impaired affective reactivity to positively valenced events (<xref ref-type="bibr" rid="bib42">Parker and Hadzi Pavlovic, 1996</xref>). Unlike previous reports which highlighted reduced reactivity to negative stimuli in depression (<xref ref-type="bibr" rid="bib9">Bylsma et al., 2008</xref>), we did not find significant group differences for negative expression reactivity. Viewing affect as a dynamic process provided two further insights into facial responses in melancholia. First, decreased between-subject consistency and more anomalous facial responses suggest that their facial activity is less likely to be driven by a common external stimulus. Ambiguous facial responses are also seen in schizophrenia (<xref ref-type="bibr" rid="bib27">Hamm et al., 2014</xref>), suggesting the possibility of a common underlying mechanism with melancholia. Second, participants with melancholia were less likely to enter high-frequency positive valence states like laughing, and once there, transitioned out quickly to the ‘ennui’ state. This reflects the clinical impression that positive mood states persist in healthy controls, but such states are fleeting in those with melancholia, who tend to get ‘stuck’ in negative mood states instead. The results are commensurate with the proposal that depressed states relate to persistent firing in non-reward functional areas mediated by attractor dynamics (<xref ref-type="bibr" rid="bib46">Rolls, 2016</xref>). Additionally, these findings accord with neurobiological models of melancholia whereby dysfunctional cortical-basal ganglia circuitry underlie the disturbances in volition and psychomotor activity that characterise the disorder (<xref ref-type="bibr" rid="bib42">Parker and Hadzi Pavlovic, 1996</xref>). More generally, the notion of affect as a sequence of spatiotemporal states aligns with the proposal that instabilities in brain network activity generate adaptive fluctuations in mood and affect, with these being either over- or under-damped in affective disorders (<xref ref-type="bibr" rid="bib44">Perry et al., 2019</xref>). Our paradigm also raises clinical questions predicated on dynamics – for example, do biological or psychological treatments for melancholia work by increasing the probability of entering positive affective states, or reducing the probability of exiting such states?</p><p>Several caveats bear mention. First, it is well known that cross-domain generalisability in facial affect detection trails behind within-domain performance (<xref ref-type="bibr" rid="bib47">Ruiz et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Li et al., 2017</xref>) (see <xref ref-type="bibr" rid="bib13">Cohn et al., 2019</xref>, for a review). However, the spectral transform method is dependent on sensitivity to changes in AU intensity, rather than on the accuracy of predicted AU occurrence. Cross-domain generalisability may be closer to adequate for AU intensity than for AU occurrence, because appearance features such as the nasiolabial furrow depth vary continuously with AU intensity (<xref ref-type="bibr" rid="bib13">Cohn et al., 2019</xref>). Conversely, AU occurrence detection may depend on specific feature value thresholds that vary in different datasets due to illumination, face orientation, gender, and age. We also have greater confidence in OpenFace’s sensitivity to changes in AU intensity, because its ability to locate landmark points has been validated on a truly out-of-sample dataset (300-VW dataset) (<xref ref-type="bibr" rid="bib6">Baltrusaitis et al., 2018</xref>). Second, a small number of participants with constant zero activation in one or more action units were excluded from analysis, because this produces an ill-defined spectral transform. Excluded participants, of whom one was a control and four had melancholia, may have had the greatest impairments in facial affect. This issue could be addressed with a lower detectable limit of action unit activation. Third, time-frequency maps were standardised in mean and variance before HMM inference. This ensures that states occur sequentially across time, but reduces the differences in state sequences across groups. Omitting this standardisation step yields states that are biased towards group differences rather than temporal differences (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Future work could consider methods that are less susceptible to this trade-off. Fourth, the generalisability of our results in the clinical group is limited by sample size, and would benefit from independent replication before clinical applications are considered. Fifth, the modest inter-rater reliability of some psychiatric diagnoses (<xref ref-type="bibr" rid="bib1">Aboraya et al., 2006</xref>) raises questions about the extent to which diagnoses of melancholic depression can be considered as ‘ground truth’. However, we have previously demonstrated changes in functional brain connectivity in this cohort (<xref ref-type="bibr" rid="bib26">Guo et al., 2016</xref>), highlighting the neurobiological difference between these groups. Sixth, the present pipeline focused on oscillatory magnitudes and discarded phase information from the wavelet transform. Future work could incorporate phase information to quantify how facial responses synchronize with stimulus properties. Finally, the utility of our approach is likely to be improved by multimodal fusion of facial, head pose, vocal, and body language behaviour, each of which independently improve classification (<xref ref-type="bibr" rid="bib8">Bhatia et al., 2017b</xref>; <xref ref-type="bibr" rid="bib3">Alghowinem et al., 2006b</xref>; <xref ref-type="bibr" rid="bib2">Alghowinem et al., 2006a</xref>; <xref ref-type="bibr" rid="bib30">Joshi et al., 2006</xref>).</p><p>Human emotion and affect are inherently dynamic. Our work demonstrates that momentary affective responses, such as laughing or grimacing, traditionally viewed from a qualitative standpoint, can be understood within a quantitative framework. These tools are cheap, automatable, and could be used within a smartphone operating system to complement the brief assessment of facial affect during a clinical encounter. Potential translational applications include screening for mental health disorders or monitoring clinical progress. Quantifying dynamic features of facial affect could also assist in subtyping the phenomenology of reduced expressivity, to distinguish between psychomotor retardation in melancholic depression, emotional incongruence, and affective blunting in schizophrenia, the masked facies of Parkinson’s disease, or apathy in dementia. Steps towards this translation will require evaluation of its acceptability and utility in clinical practice.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data</title><p>The DISFA dataset contains facial videos recorded at 20 frames per second from 27 participants who viewed a 4 min video consisting of short emotive clips from YouTube (<xref ref-type="bibr" rid="bib39">Mavadati et al., 2019</xref>; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><p>The melancholia dataset comprises 30 participants with major depressive disorder who were recruited from the specialist depression clinic at the Black Dog Institute in Sydney, Australia. These participants met criteria for a current major depressive episode, were diagnosed as having the melancholic subtype by previously detailed criteria (<xref ref-type="bibr" rid="bib54">Taylor and Fink, 2006</xref>), and did not have lifetime (hypo)mania or psychosis (<xref ref-type="table" rid="table1">Table 1</xref>). Thirty-eight matched healthy controls were recruited from the community. All participants were screened for psychotic and mood conditions with the Mini International Neuropsychiatric Interview (MINI). Exclusion criteria were current or past substance dependence, recent electroconvulsive therapy, neurological disorder, brain injury, invasive neurosurgery, or an estimated full scale IQ score (WAIS-III) below 80. Participants provided informed consent for the study. Participants watched three video clips consecutively – stand-up comedy (120 s), a sad movie clip (152 s), and a German weather report video depicting a weather reporter laughing uncontrollably (56 s). Facial video was recorded at a resolution of 800×600 pixels at 25 frames per second using an AVT Pike F-100 FireWire camera. The camera was mounted on a tripod, which was placed behind the monitor so as to record the front of the face. The height of the camera was adjusted with respect to the participant’s height when seated.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Demographics and clinical characteristics.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Healthy controls</th><th align="left" valign="bottom">Melancholia</th><th align="left" valign="bottom">Group comparison,<italic>t</italic> or χ<sup>2</sup>, p-value</th></tr></thead><tbody><tr><td align="left" valign="bottom">Number of participants</td><td align="char" char="." valign="bottom">38</td><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Age, mean (SD)</td><td align="char" char="." valign="bottom">46.5 (20.0)</td><td align="char" char="." valign="bottom">46.2 (15.5)</td><td align="char" char="." valign="bottom">0.95</td></tr><tr><td align="left" valign="bottom">Sex (M:F)</td><td align="char" char="." valign="bottom">13:19</td><td align="char" char="." valign="bottom">17:13</td><td align="char" char="." valign="bottom">0.21</td></tr><tr><td align="left" valign="bottom" colspan="3">Medication, % yes (n)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Any psychiatric medication</td><td align="left" valign="bottom">7% (1)</td><td align="left" valign="bottom">85% (23)</td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Nil medication</td><td align="char" char="." valign="bottom">93% (13)</td><td align="char" char="." valign="bottom">15% (4)</td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Selective serotonin reuptake inhibitor</td><td align="left" valign="bottom">7% (1)</td><td align="char" char="." valign="bottom">15% (4)</td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Dual-action antidepressant<xref ref-type="table-fn" rid="table1fn1">*</xref></td><td align="char" char="." valign="bottom">0% (0)</td><td align="char" char="." valign="bottom">48% (13)</td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Tricyclic or monoamine oxidase inhibitor</td><td align="char" char="." valign="bottom">0% (0)</td><td align="char" char="." valign="bottom">19% (5)</td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Mood stabiliser<xref ref-type="table-fn" rid="table1fn2">†</xref></td><td align="char" char="." valign="bottom">0% (0)</td><td align="left" valign="bottom">11% (3)</td><td align="left" valign="bottom">–</td></tr><tr><td align="left" valign="bottom">Antipsychotic</td><td align="char" char="." valign="bottom">0% (0)</td><td align="char" char="." valign="bottom">33% (9)</td><td align="left" valign="bottom">–</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>For example, serotonin noradrenaline reuptake inhibitor.</p></fn><fn id="table1fn2"><label>†</label><p>For example, lithium or valproate.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s4-2"><title>Facial action units</title><p>For the melancholia dataset, facial video recordings of different participants were aligned with FaceSync (<xref ref-type="bibr" rid="bib10">Cheong et al., 2006</xref>). For both datasets, facial action unit intensities were extracted with OpenFace (<xref ref-type="bibr" rid="bib6">Baltrusaitis et al., 2018</xref>). OpenFace uses a convolutional neural network architecture, Convolutional Experts Constrained Local Model (CE-CLM), to detect and track facial landmark points. After face images are aligned to a common 112×112 pixel image, histogram of oriented gradients features were extracted and classified with a linear kernel support vector machine. OpenFace was trained on five manually coded spontaneous expression datasets (DISFA, SEMAINE, BP4D, UNBC-McMaster, and Fera 2011) and one posed expression dataset (Bosphorus). AU intensities predicted by OpenFace had an average concordance correlation of +0.73 as compared with human-coded ratings for DISFA out-of-sample data. Per-AU correlations were higher for some AUs (e.g., AU12, +0.85) than others (e.g. AU15, +0.39) (<xref ref-type="bibr" rid="bib6">Baltrusaitis et al., 2018</xref>).</p><p>Action unit time series from OpenFace for each participant were not normalised, as we were interested in between-subjects differences. Recordings with more than 0.5% missing frames were excluded, and any remaining missing frames were linearly interpolated. Action unit 45 ‘Blink’ was not used as it is not directly relevant to emotion. Action units 2 ‘Outer Brow Raiser’ and 5 ‘Upper Lid Raiser’ were not used as they had constant zero value throughout the recording for most participants. Participants with any other action units with zero value through the recording were also excluded, as the time-frequency representation is undefined for these time series. This comprised one control and four participants with melancholia.</p></sec><sec id="s4-3"><title>Time-frequency representation</title><p>For each participant, each facial action unit time series was transformed into a time-frequency representation, using the amplitude of the continuous wavelet transform. An analytic Morse wavelet was used with symmetry parameter 3, time-bandwidth product 60, and 12 voices per octave. Mean time-frequency maps were visualised with a cone of influence – outside which edge effects produce artefact (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> for DISFA, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for melancholia dataset). To determine information lost by averaging raw time series across participants, the amplitude of the continuous wavelet transform for the group mean time series was calculated. At each point in time-frequency space, the distribution of individual participants’ amplitude was compared with the amplitude of the group mean, with a two-sided t-test (p=0.05) (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p></sec><sec id="s4-4"><title>Hidden Markov model</title><p>An HMM, implemented in the HMM-MAR MATLAB toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/HMM-MAR">https://github.com/OHBA-analysis/HMM-MAR</ext-link>; <xref ref-type="bibr" rid="bib63">Vidaurre, 2022</xref>; <xref ref-type="bibr" rid="bib59">Vidaurre et al., 2016</xref>), was used to identify states corresponding to oscillatory activity localised to specific action units and frequency bands. A HMM specifies state switching probabilities which arise from a time-invariant transition matrix. Each state is described by a multivariate Gaussian observation model with distinct mean and covariance in (action unit × frequency) space, because how facial muscle groups covary with each other may differ across similarly valenced states. Input data were 110 frequency bins in 0–5 Hz, for each of 14 facial action units. Individual participants’ time series were standardised to zero mean and unit variance before temporal concatenation to form a single time series. This time series was downsampled to 10 Hz, and the top 10 principal components were used (for DISFA). Other HMM parameters are listed in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p><p>The initialisation algorithm used 10 optimisation cycles per repetition. Variational model inference optimised free energy, a measure of model accuracy penalised by model complexity, and stopped after the relative decrement in free energy dropped below 10<sup>–5</sup>. Free energy did not reach a minimum even beyond n=30 states (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Previous studies have chosen between 5 and 12 states (<xref ref-type="bibr" rid="bib60">Vidaurre et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Kottaram et al., 2019</xref>). We chose an eight-state model as done in previous work (<xref ref-type="bibr" rid="bib5">Baker et al., 2014</xref>), as visual inspection of the states showed trivial splitting of states beyond this value. However, the analyses were robust to variations in the exact number of states.</p><p>HMM state observation models were visualised with FACSHuman (<xref ref-type="bibr" rid="bib24">Gilbert et al., 2021</xref>). The contribution of each action unit to each state was calculated by summing across all frequency bands. For each state, positive contributions were rescaled to the interval [0,1] and visualised on an avatar face (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). State sequences for individual subjects were calculated with the Viterbi algorithm (<xref ref-type="fig" rid="fig3">Figure 3</xref>). To calculate between-subjects consistency of state sequences over time, we used an 8 s sliding window. Within this window, for each state, we counted the number of participants who expressed this state at least once, and found the most commonly expressed state. Uncertainty in this consistency measure at each time point was estimated from the 5 and 95 percentiles of 1000 bootstrap samples. The null distribution for consistency was obtained by randomly circular shifting the Viterbi time series for each subject independently (n=1000). Consistency values exceeding the 95th percentile (59% consistency) were deemed significant.</p></sec><sec id="s4-5"><title>Analysis of melancholia dataset</title><p>Mean action unit activations were calculated for each group, and uncertainty visualised with the 5th and 95th percentiles of 1000 bootstrap samples (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). A three-way ANOVA for activation was conducted with group, stimulus video, and facial valence as regressors. To avoid redundancy between the two positive valence videos, we limited the ANOVA to two stimulus videos – the stand-up comedy and sad movie clips. In keeping with previous work (<xref ref-type="bibr" rid="bib21">Friesen and Ekman, 1983</xref>), we defined happiness as the sum of action units 6 ‘Cheek Raiser’ and 12 ‘Lip Corner Puller’, and sadness as the sum of action units 1 ‘Inner Brow Raiser’, 4 ‘Brow Lowerer’, and 15 ‘Lip Corner Depressor’. Post hoc comparisons used Tukey’s honestly significant difference criterion (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>Time-frequency representations were computed as the amplitude of the continuous wavelet transform. Group differences in wavelet power, localised in time and frequency, were calculated by subtracting the mean time-frequency representation of each clinical group (<xref ref-type="fig" rid="fig5">Figure 5</xref>). To confirm that these effects were not due to movement-related noise in action unit encoding having different effects depending on the frequency and time window considered, the null distribution of the effect was obtained by resampling 1000 surrogate cohorts from the list of all participants. Time-frequency points with effect size inside 2.5–97.5 percentile were considered non-significant and excluded from visualisation.</p><p>To compare classification accuracy with action unit time series or time-frequency data, a support vector machine with Gaussian kernel was used. All tests used mean accuracy over five repetitions of fivefold cross-validation, and observations were assigned to fold by participant without stratification. For the time-frequency model, inputs were mean wavelet amplitude in each frequency bin (n=10) in each stimulus video (n=3), for each action unit (n=14). Inputs to the second model were mean action unit activations for each action unit and each stimulus video. For the third set of models, input features were mean action unit activation within discrete time chunks of 2, 10, and 30 s (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Model classification accuracies were compared with a 5-by-2 paired F cross-validation test.</p><p>The HMM was inferred as described above (<xref ref-type="fig" rid="fig6">Figure 6</xref>). <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> shows the results when input data were not standardised. <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> shows the results using zero off-diagonal covariance. Local transition probabilities were then inferred for each participant separately. Two-sided significance testing for group differences in fractional occupancy was implemented within the HMM-MAR toolbox by permuting between-subjects as described previously (<xref ref-type="bibr" rid="bib62">Vidaurre et al., 2019</xref>). Next, we considered only those state transitions that could explain the group differences in fractional occupancy and tested these transitions for group differences with t-tests (one-sided in the direction that could explain fractional occupancy findings). Group differences in fractional occupancy and transition probability were corrected to control the false discovery rate (<xref ref-type="bibr" rid="bib53">Storey, 2002</xref>).</p><p>Results were consistent across repetitions of HMM inference with different initial random seeds. In addition, all analyses were repeated with time-frequency amplitudes normalised by the standard deviation of the time series, to ensure that results were not solely due to group differences in variance for each action unit time. This was motivated by previous work showing that the square of wavelet transform amplitude increases with variance for white noise sources (<xref ref-type="bibr" rid="bib56">Torrence and Compo, 2002</xref>). Results were consistent with and without normalisation, including differences between clinical groups, the distributions, and time courses of HMM states.</p></sec><sec id="s4-6"><title>Code availability</title><p>Code to replicate the analysis of healthy controls in the DISFA dataset is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jaysonjeg/FacialDynamicsHMM">https://github.com/jaysonjeg/FacialDynamicsHMM</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:dab791b298bf4b29a075815e7590afab7c7ebe24;origin=https://github.com/jaysonjeg/FacialDynamicsHMM;visit=swh:1:snp:9950e2f717deacae117292f64760fe07d6d7635f;anchor=swh:1:rev:649ff3afa26624b9962409bb67543197668171ef">swh:1:rev:649ff3afa26624b9962409bb67543197668171ef</ext-link>; <xref ref-type="bibr" rid="bib28">Jeg, 2021</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Data curation, Funding acquisition, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Participants provided informed consent for the study. Ethics approval was obtained from the University of New South Wales (HREC-08077) and the University of Newcastle (H-2020-0137). Figure 1a shows images of a person's face from the DISFA dataset. Consent to reproduce their image in publications was obtained by the original DISFA authors, and is detailed in the dataset agreement (<ext-link ext-link-type="uri" xlink:href="http://mohammadmahoor.com/disfa-contact-form/">http://mohammadmahoor.com/disfa-contact-form/</ext-link>) and the original paper (https://ieeexplore.ieee.org/document/6475933).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-79581-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Additional data.</title></caption><media xlink:href="elife-79581-supp1-v2.doc" mimetype="application" mime-subtype="doc"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The DISFA dataset is publically available at <ext-link ext-link-type="uri" xlink:href="http://mohammadmahoor.com/disfa/">http://mohammadmahoor.com/disfa/</ext-link>, and can be accessed by application at <ext-link ext-link-type="uri" xlink:href="http://mohammadmahoor.com/disfa-contact-form/">http://mohammadmahoor.com/disfa-contact-form/</ext-link>. The melancholia dataset is not publically available due to ethical and privacy considerations for patients, and because the original ethics approval does not permit sharing this data.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Mavadati</surname><given-names>SM</given-names></name><name><surname>Mahoor</surname><given-names>MH</given-names></name><name><surname>Bartlett</surname><given-names>K</given-names></name><name><surname>Trinh</surname><given-names>P</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>DISFA: A Spontaneous Facial Action Intensity Database</data-title><source>DISFA</source><pub-id pub-id-type="accession" xlink:href="http://mohammadmahoor.com/disfa/">DISFA</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>JJ acknowledges the support of a Health Education and Training Institute Award in Psychiatry and Mental Health, and the Rainbow Foundation. MB acknowledges the support of the National Health and Medical Research Council (1118153, 10371296, 1095227) and the Australian Research Council (CE140100007).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aboraya</surname><given-names>A</given-names></name><name><surname>Rankin</surname><given-names>E</given-names></name><name><surname>France</surname><given-names>C</given-names></name><name><surname>El-Missiry</surname><given-names>A</given-names></name><name><surname>John</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The reliability of psychiatric diagnosis revisited</article-title><source>Psychiatry</source><volume>1</volume><fpage>41</fpage><lpage>50</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Alghowinem</surname><given-names>S</given-names></name><name><surname>Goecke</surname><given-names>R</given-names></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Epps</surname><given-names>J</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Parker</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006a</year><article-title>International Conference on Acoustics</article-title><conf-name>ICASSP 2013 - 2013 IEEE International Conference on Acoustics, Speech and Signal Processing</conf-name><conf-loc>Vancouver, BC, Canada</conf-loc><fpage>7547</fpage><lpage>7551</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2013.6639130</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Alghowinem</surname><given-names>S</given-names></name><name><surname>Goecke</surname><given-names>R</given-names></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Parkerx</surname><given-names>G</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006b</year><article-title>Humaine Association</article-title><conf-name>Humaine Association Conference on Affective Computing and Intelligent Interaction ACII</conf-name><conf-loc>Geneva, Switzerland</conf-loc><fpage>283</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1109/ACII.2013.53</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambadar</surname><given-names>Z</given-names></name><name><surname>Schooler</surname><given-names>JW</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Deciphering the enigmatic face: the importance of facial dynamics in interpreting subtle facial expressions</article-title><source>Psychological Science</source><volume>16</volume><fpage>403</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.01548.x</pub-id><pub-id pub-id-type="pmid">15869701</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>AP</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Rezek</surname><given-names>IA</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Probert Smith</surname><given-names>PJ</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fast transient networks in spontaneous human brain activity</article-title><source>eLife</source><volume>3</volume><elocation-id>e01867</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.01867</pub-id><pub-id pub-id-type="pmid">24668169</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Baltrusaitis</surname><given-names>T</given-names></name><name><surname>Zadeh</surname><given-names>A</given-names></name><name><surname>Lim</surname><given-names>YC</given-names></name><name><surname>Morency</surname><given-names>LP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>OpenFace 2.0: Facial Behavior Analysis Toolkit</article-title><conf-name>2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition FG 2018</conf-name><conf-loc>Xi’an, China</conf-loc><fpage>59</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1109/FG.2018.00019</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bhatia</surname><given-names>S</given-names></name><name><surname>Hayat</surname><given-names>M</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Parker</surname><given-names>G</given-names></name><name><surname>Goecke</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>A Video-Based Facial Behaviour Analysis Approach to Melancholia</article-title><conf-name>2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017</conf-name><conf-loc>Washington, DC, DC, USA</conf-loc><fpage>754</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1109/FG.2017.94</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bhatia</surname><given-names>S</given-names></name><name><surname>Hayat</surname><given-names>M</given-names></name><name><surname>Goecke</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>A multimodal system to characterise melancholia: cascaded bag of words approach</article-title><conf-name>ICMI ’17</conf-name><conf-loc>Glasgow UK</conf-loc><fpage>274</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1145/3136755.3136766</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bylsma</surname><given-names>LM</given-names></name><name><surname>Morris</surname><given-names>BH</given-names></name><name><surname>Rottenberg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A meta-analysis of emotional reactivity in major depressive disorder</article-title><source>Clinical Psychology Review</source><volume>28</volume><fpage>676</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1016/j.cpr.2007.10.001</pub-id><pub-id pub-id-type="pmid">18006196</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Brooks</surname><given-names>S</given-names></name><name><surname>Chang</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Facesync: open source framework for recording facial expressions with head-mounted cameras</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/p5293</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>I</given-names></name><name><surname>Sebe</surname><given-names>N</given-names></name><name><surname>Garg</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>LS</given-names></name><name><surname>Huang</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Facial expression recognition from video sequences: temporal and static modeling</article-title><source>Computer Vision and Image Understanding</source><volume>91</volume><fpage>160</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/S1077-3142(03)00081-X</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cohn</surname><given-names>JF</given-names></name><name><surname>Kruez</surname><given-names>TS</given-names></name><name><surname>Matthews</surname><given-names>I</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Nguyen</surname><given-names>MH</given-names></name><name><surname>Padilla</surname><given-names>MT</given-names></name><name><surname>Zhou</surname><given-names>F</given-names></name><name><surname>De la Torre</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Affective Computing</article-title><conf-name>2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops ACII 2009</conf-name><conf-loc>Amsterdam, Netherlands</conf-loc><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/ACII.2009.5349358</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Cohn</surname><given-names>JF</given-names></name><name><surname>Ertugrul</surname><given-names>IO</given-names></name><name><surname>Chu</surname><given-names>WS</given-names></name><name><surname>Girard</surname><given-names>JM</given-names></name><name><surname>Jeni</surname><given-names>LA</given-names></name><name><surname>Hammal</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Chapter 19 - Affective facial computing: Generalizability across domains</article-title><source>Computer Vision and Pattern Recognition</source><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/B9780128146019000262">https://www.sciencedirect.com/science/article/pii/B9780128146019000262</ext-link><date-in-citation iso-8601-date="2022-07-06">July 6, 2022</date-in-citation></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Darzi</surname><given-names>A</given-names></name><name><surname>Provenza</surname><given-names>NR</given-names></name><name><surname>Jeni</surname><given-names>LA</given-names></name><name><surname>Borton</surname><given-names>DA</given-names></name><name><surname>Sheth</surname><given-names>SA</given-names></name><name><surname>Goodman</surname><given-names>WK</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Facial Action Units and Head Dynamics in Longitudinal Interviews Reveal OCD and Depression severity and DBS Energy</article-title><conf-name>2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021</conf-name><conf-loc>Jodhpur, India</conf-loc><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/FG52635.2021.9667028</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhall</surname><given-names>A</given-names></name><name><surname>Goecke</surname><given-names>R</given-names></name><name><surname>Lucey</surname><given-names>S</given-names></name><name><surname>Gedeon</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Collecting large, richly annotated facial-expression databases from movies</article-title><source>IEEE MultiMedia</source><volume>19</volume><fpage>34</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1109/MMUL.2012.26</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dibeklioğlu</surname><given-names>H</given-names></name><name><surname>Hammal</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multimodal detection of depression in clinical interviews</article-title><conf-name>Proceedings of the ACM International Conference on Multimodal Interaction. ICMI</conf-name><fpage>307</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1145/2818346.2820776</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P</given-names></name><name><surname>Friesen</surname><given-names>WV</given-names></name><name><surname>Friesen</surname><given-names>WV</given-names></name><name><surname>Hager</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Facial action coding system: A technique for the measurement of facial movement</article-title><ext-link ext-link-type="uri" xlink:href="https://www.scienceopen.com/document?vid=759f6f74-7ccd-47b5-904a-25ca0f29ea90">https://www.scienceopen.com/document?vid=759f6f74-7ccd-47b5-904a-25ca0f29ea90</ext-link><date-in-citation iso-8601-date="2020-04-15">April 15, 2020</date-in-citation></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Are there basic emotions?</article-title><source>Psychological Review</source><volume>99</volume><fpage>550</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.99.3.550</pub-id><pub-id pub-id-type="pmid">1344638</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ekundayo</surname><given-names>O</given-names></name><name><surname>Viriri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Facial Expression Recognition: A Review of Methods, Performances and Limitations</article-title><conf-name>2019 Conference on Information Communications Technology and Society (ICTAS</conf-name><conf-loc>Durban, South Africa</conf-loc><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICTAS.2019.8703619</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Facial expression recognition based on local binary patterns and coarse-to-fine classification</article-title><conf-name>In: The Fourth International Conference onComputer and Information Technology, 2004 CIT ’04</conf-name><fpage>178</fpage><lpage>183</lpage><pub-id pub-id-type="pmid">15127127</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Friesen</surname><given-names>WV</given-names></name><name><surname>Ekman</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1983">1983</year><data-title>EMFACS-7: emotional facial action coding system</data-title><version designator="0.7">0.7</version><source>Scinapse</source><ext-link ext-link-type="uri" xlink:href="https://www.scinapse.io/papers/111787409">https://www.scinapse.io/papers/111787409</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gavrilescu</surname><given-names>M</given-names></name><name><surname>Vizireanu</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predicting depression, anxiety, and stress levels from videos using the facial action coding system</article-title><source>Sensors</source><volume>19</volume><elocation-id>E3693</elocation-id><pub-id pub-id-type="doi">10.3390/s19173693</pub-id><pub-id pub-id-type="pmid">31450687</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghimire</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Geometric feature-based facial expression recognition in image sequences using multi-class adaboost and support vector machines</article-title><source>Sensors</source><volume>13</volume><fpage>7714</fpage><lpage>7734</lpage><pub-id pub-id-type="doi">10.3390/s130607714</pub-id><pub-id pub-id-type="pmid">23771158</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>M</given-names></name><name><surname>Demarchi</surname><given-names>S</given-names></name><name><surname>Urdapilleta</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>FACSHuman, a software program for creating experimental material by modeling 3D facial expressions</article-title><source>Behavior Research Methods</source><volume>53</volume><fpage>2252</fpage><lpage>2272</lpage><pub-id pub-id-type="doi">10.3758/s13428-021-01559-9</pub-id><pub-id pub-id-type="pmid">33825127</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girard</surname><given-names>JM</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name><name><surname>Mahoor</surname><given-names>MH</given-names></name><name><surname>Mavadati</surname><given-names>SM</given-names></name><name><surname>Hammal</surname><given-names>Z</given-names></name><name><surname>Rosenwald</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Nonverbal social withdrawal in depression: evidence from manual and automatic analysis</article-title><source>Image and Vision Computing</source><volume>32</volume><fpage>641</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2013.12.007</pub-id><pub-id pub-id-type="pmid">25378765</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>CC</given-names></name><name><surname>Hyett</surname><given-names>MP</given-names></name><name><surname>Nguyen</surname><given-names>VT</given-names></name><name><surname>Parker</surname><given-names>GB</given-names></name><name><surname>Breakspear</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct neurobiological signatures of brain connectivity in depression subtypes during natural viewing of emotionally salient films</article-title><source>Psychological Medicine</source><volume>46</volume><fpage>1535</fpage><lpage>1545</lpage><pub-id pub-id-type="doi">10.1017/S0033291716000179</pub-id><pub-id pub-id-type="pmid">26888415</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamm</surname><given-names>J</given-names></name><name><surname>Pinkham</surname><given-names>A</given-names></name><name><surname>Gur</surname><given-names>RC</given-names></name><name><surname>Verma</surname><given-names>R</given-names></name><name><surname>Kohler</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dimensional information-theoretic measurement of facial emotion expressions in schizophrenia</article-title><source>Schizophrenia Research and Treatment</source><volume>2014</volume><elocation-id>243907</elocation-id><pub-id pub-id-type="doi">10.1155/2014/243907</pub-id><pub-id pub-id-type="pmid">24724025</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jeg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>FacialDynamicsHMM</data-title><version designator="swh:1:rev:649ff3afa26624b9962409bb67543197668171ef">swh:1:rev:649ff3afa26624b9962409bb67543197668171ef</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:dab791b298bf4b29a075815e7590afab7c7ebe24;origin=https://github.com/jaysonjeg/FacialDynamicsHMM;visit=swh:1:snp:9950e2f717deacae117292f64760fe07d6d7635f;anchor=swh:1:rev:649ff3afa26624b9962409bb67543197668171ef">https://archive.softwareheritage.org/swh:1:dir:dab791b298bf4b29a075815e7590afab7c7ebe24;origin=https://github.com/jaysonjeg/FacialDynamicsHMM;visit=swh:1:snp:9950e2f717deacae117292f64760fe07d6d7635f;anchor=swh:1:rev:649ff3afa26624b9962409bb67543197668171ef</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>B</given-names></name><name><surname>Valstar</surname><given-names>M</given-names></name><name><surname>Martinez</surname><given-names>B</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A dynamic appearance descriptor approach to facial actions temporal modeling</article-title><source>IEEE Transactions on Cybernetics</source><volume>44</volume><fpage>161</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2013.2249063</pub-id><pub-id pub-id-type="pmid">23757539</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>J</given-names></name><name><surname>Goecke</surname><given-names>R</given-names></name><name><surname>Parker</surname><given-names>G</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>International Conference on Automatic Face</article-title><conf-name>2013 10th IEEE International Conference on Automatic Face &amp; Gesture Recognition FG 2013</conf-name><conf-loc>Shanghai, China</conf-loc><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keltner</surname><given-names>D</given-names></name><name><surname>Sauter</surname><given-names>D</given-names></name><name><surname>Tracy</surname><given-names>J</given-names></name><name><surname>Cowen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Emotional expression: advances in basic emotion theory</article-title><source>Journal of Nonverbal Behavior</source><volume>43</volume><fpage>133</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1007/s10919-019-00293-3</pub-id><pub-id pub-id-type="pmid">31395997</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelstra</surname><given-names>S</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name><name><surname>Patras</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A dynamic texture-based approach to recognition of facial actions and their temporal models</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>32</volume><fpage>1940</fpage><lpage>1954</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2010.50</pub-id><pub-id pub-id-type="pmid">20847386</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kollias</surname><given-names>D</given-names></name><name><surname>Tzirakis</surname><given-names>P</given-names></name><name><surname>Nicolaou</surname><given-names>MA</given-names></name><name><surname>Papaioannou</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>G</given-names></name><name><surname>Schuller</surname><given-names>B</given-names></name><name><surname>Kotsia</surname><given-names>I</given-names></name><name><surname>Zafeiriou</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep affect prediction in-the-wild: aff-wild database and challenge, deep architectures, and beyond</article-title><source>International Journal of Computer Vision</source><volume>127</volume><fpage>907</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1007/s11263-019-01158-4</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kottaram</surname><given-names>A</given-names></name><name><surname>Johnston</surname><given-names>LA</given-names></name><name><surname>Cocchi</surname><given-names>L</given-names></name><name><surname>Ganella</surname><given-names>EP</given-names></name><name><surname>Everall</surname><given-names>I</given-names></name><name><surname>Pantelis</surname><given-names>C</given-names></name><name><surname>Kotagiri</surname><given-names>R</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain network dynamics in schizophrenia: reduced dynamism of the default mode network</article-title><source>Human Brain Mapping</source><volume>40</volume><fpage>2212</fpage><lpage>2228</lpage><pub-id pub-id-type="doi">10.1002/hbm.24519</pub-id><pub-id pub-id-type="pmid">30664285</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>P</given-names></name><name><surname>Happy</surname><given-names>SL</given-names></name><name><surname>Routray</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A real-time robust facial expression recognition system using HOG features</article-title><conf-name>2016 International Conference on Computing, Analytics and Security Trends (CAST</conf-name><conf-loc>Pune, India</conf-loc><fpage>289</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1109/CAST.2016.7914982</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>P</given-names></name><name><surname>Bradley</surname><given-names>M</given-names></name><name><surname>Cuthbert</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>International affective picture system (IAPS): Affective ratings of pictures and instruction manual</source><publisher-loc>Gainesville, FL</publisher-loc><publisher-name>University of Florida</publisher-name></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Ji</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Data-free prior model for facial action unit recognition</article-title><source>IEEE Transactions on Affective Computing</source><volume>4</volume><fpage>127</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2013.5</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lucey</surname><given-names>P</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name><name><surname>Saragih</surname><given-names>J</given-names></name><name><surname>Ambadar</surname><given-names>Z</given-names></name><name><surname>Matthews</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)</article-title><conf-name>Computer Vision and Pattern Recognition Workshops</conf-name><conf-loc>San Francisco, CA, USA</conf-loc><fpage>94</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1109/CVPRW.2010.5543262</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mavadati</surname><given-names>SM</given-names></name><name><surname>Mahoor</surname><given-names>MH</given-names></name><name><surname>Bartlett</surname><given-names>K</given-names></name><name><surname>Trinh</surname><given-names>P</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DISFA: A spontaneous facial action intensity database</article-title><source>IEEE Transactions on Affective Computing</source><volume>4</volume><fpage>151</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2013.4</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naumann</surname><given-names>LP</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name><name><surname>Rentfrow</surname><given-names>PJ</given-names></name><name><surname>Gosling</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Personality judgments based on physical appearance</article-title><source>Personality &amp; Social Psychology Bulletin</source><volume>35</volume><fpage>1661</fpage><lpage>1671</lpage><pub-id pub-id-type="doi">10.1177/0146167209346309</pub-id><pub-id pub-id-type="pmid">19762717</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ortony</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Are all “basic emotions” emotions? A problem for the (basic) emotions construct</article-title><source>Perspectives on Psychological Science</source><volume>17</volume><fpage>41</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1177/1745691620985415</pub-id><pub-id pub-id-type="pmid">34264141</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>G</given-names></name><name><surname>Hadzi Pavlovic</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Melancholia: A Disorder of Movement and Mood: A Phenomenological and Neurobiological Review</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511759024</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Defining melancholia: the primacy of psychomotor disturbance</article-title><source>Acta Psychiatrica Scandinavica. Supplementum</source><volume>1</volume><fpage>21</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1111/j.1600-0447.2007.00959.x</pub-id><pub-id pub-id-type="pmid">17280567</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>A</given-names></name><name><surname>Roberts</surname><given-names>G</given-names></name><name><surname>Mitchell</surname><given-names>PB</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Connectomics of bipolar disorder: a critical review, and evidence for dynamic instabilities within interoceptive networks</article-title><source>Molecular Psychiatry</source><volume>24</volume><fpage>1296</fpage><lpage>1318</lpage><pub-id pub-id-type="doi">10.1038/s41380-018-0267-2</pub-id><pub-id pub-id-type="pmid">30279458</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renneberg</surname><given-names>B</given-names></name><name><surname>Heyn</surname><given-names>K</given-names></name><name><surname>Gebhard</surname><given-names>R</given-names></name><name><surname>Bachmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Facial expression of emotions in borderline personality disorder and depression</article-title><source>Journal of Behavior Therapy and Experimental Psychiatry</source><volume>36</volume><fpage>183</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/j.jbtep.2005.05.002</pub-id><pub-id pub-id-type="pmid">15950175</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A non-reward attractor theory of depression</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>68</volume><fpage>47</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.05.007</pub-id><pub-id pub-id-type="pmid">27181908</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ruiz</surname><given-names>A</given-names></name><name><surname>Weijer</surname><given-names>J de</given-names></name><name><surname>Binefa</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>From Emotions to Action Units with Hidden and Semi-Hidden-Task Learning</article-title><conf-name>2015 IEEE International Conference on Computer Vision (ICCV</conf-name><conf-loc>Santiago, Chile</conf-loc><fpage>3703</fpage><lpage>3711</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.422</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sandbach</surname><given-names>G</given-names></name><name><surname>Zafeiriou</surname><given-names>S</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name><name><surname>Rueckert</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Recognition of 3D facial expression dynamics</article-title><source>Image and Vision Computing</source><volume>30</volume><fpage>762</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2012.01.006</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>KL</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human facial expressions as adaptations: evolutionary questions in facial expression research</article-title><source>American Journal of Physical Anthropology</source><volume>Suppl 33</volume><fpage>3</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1002/ajpa.2001</pub-id><pub-id pub-id-type="pmid">11786989</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>J</given-names></name><name><surname>Pilz</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Natural facial motion enhances cortical responses to faces</article-title><source>Experimental Brain Research</source><volume>194</volume><fpage>465</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1721-9</pub-id><pub-id pub-id-type="pmid">19205678</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soleymani</surname><given-names>M</given-names></name><name><surname>Lichtenauer</surname><given-names>J</given-names></name><name><surname>Pun</surname><given-names>T</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A multimodal database for affect recognition and implicit tagging</article-title><source>IEEE Transactions on Affective Computing</source><volume>3</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2011.25</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonkusare</surname><given-names>S</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Naturalistic stimuli in neuroscience: critically acclaimed</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>699</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.05.004</pub-id><pub-id pub-id-type="pmid">31257145</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storey</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A direct approach to false discovery rates</article-title><source>Journal of the Royal Statistical Society</source><volume>64</volume><fpage>479</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1111/1467-9868.00346</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>MA</given-names></name><name><surname>Fink</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Melancholia: the diagnosis, pathophysiology, and treatment of depressive illness</article-title><source>PsychNet</source><volume>1</volume><elocation-id>4330</elocation-id><pub-id pub-id-type="doi">10.1017/CBO9780511544330</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Y-L</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Recognizing action units for facial expression analysis</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>23</volume><fpage>97</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1109/34.908962</pub-id><pub-id pub-id-type="pmid">25210210</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torrence</surname><given-names>C</given-names></name><name><surname>Compo</surname><given-names>GP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A practical guide to wavelet analysis</article-title><source>Bulletin of the American Meteorological Society</source><volume>79</volume><fpage>61</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1175/1520-0477(1998)079&lt;0061:APGTWA&gt;2.0.CO;2</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trémeau</surname><given-names>F</given-names></name><name><surname>Malaspina</surname><given-names>D</given-names></name><name><surname>Duval</surname><given-names>F</given-names></name><name><surname>Corrêa</surname><given-names>H</given-names></name><name><surname>Hager-Budny</surname><given-names>M</given-names></name><name><surname>Coin-Bariou</surname><given-names>L</given-names></name><name><surname>Macher</surname><given-names>J-P</given-names></name><name><surname>Gorman</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Facial expressiveness in patients with schizophrenia compared to depressed patients and nonpatient comparison subjects</article-title><source>The American Journal of Psychiatry</source><volume>162</volume><fpage>92</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.162.1.92</pub-id><pub-id pub-id-type="pmid">15625206</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Valstar</surname><given-names>MF</given-names></name><name><surname>Almaev</surname><given-names>T</given-names></name><name><surname>Girard</surname><given-names>JM</given-names></name><name><surname>McKeown</surname><given-names>G</given-names></name><name><surname>Mehu</surname><given-names>M</given-names></name><name><surname>Yin</surname><given-names>L</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>FERA 2015 - second Facial Expression Recognition and Analysis challenge</article-title><conf-name>2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG</conf-name><conf-loc>Ljubljana</conf-loc><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/FG.2015.7284874</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Baker</surname><given-names>AP</given-names></name><name><surname>Dupret</surname><given-names>D</given-names></name><name><surname>Tejero-Cantero</surname><given-names>A</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spectrally resolved fast transient brain states in electrophysiological data</article-title><source>NeuroImage</source><volume>126</volume><fpage>81</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.047</pub-id><pub-id pub-id-type="pmid">26631815</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Brain network dynamics are hierarchically organized in time</article-title><source>PNAS</source><volume>114</volume><fpage>12827</fpage><lpage>12832</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705120114</pub-id><pub-id pub-id-type="pmid">29087305</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Hunt</surname><given-names>BAE</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spontaneous cortical activity transiently organises into frequency specific phase-coupling networks</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2987</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05316-z</pub-id><pub-id pub-id-type="pmid">30061566</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Winkler</surname><given-names>AM</given-names></name><name><surname>Karapanagiotidis</surname><given-names>T</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stable between-subject statistical inference from unstable within-subject functional connectivity estimates</article-title><source>Human Brain Mapping</source><volume>40</volume><fpage>1234</fpage><lpage>1243</lpage><pub-id pub-id-type="doi">10.1002/hbm.24442</pub-id><pub-id pub-id-type="pmid">30357995</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>HMM-MAR</data-title><version designator="swh:1:rev:7e3a60e3f2c8dd23ed279988cd92aaa98c622331">swh:1:rev:7e3a60e3f2c8dd23ed279988cd92aaa98c622331</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f8326fa4ddca6877c4f3286c8c94f973ac865001;origin=https://github.com/OHBA-analysis/HMM-MAR;visit=swh:1:snp:b8f2ac51c9cecdf127931136bf2da97db9132d27;anchor=swh:1:rev:7e3a60e3f2c8dd23ed279988cd92aaa98c622331">https://archive.softwareheritage.org/swh:1:dir:f8326fa4ddca6877c4f3286c8c94f973ac865001;origin=https://github.com/OHBA-analysis/HMM-MAR;visit=swh:1:snp:b8f2ac51c9cecdf127931136bf2da97db9132d27;anchor=swh:1:rev:7e3a60e3f2c8dd23ed279988cd92aaa98c622331</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeasin</surname><given-names>M</given-names></name><name><surname>Bullot</surname><given-names>B</given-names></name><name><surname>Sharma</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Recognition of facial expressions and measurement of levels of interest from video</article-title><source>IEEE Transactions on Multimedia</source><volume>8</volume><fpage>500</fpage><lpage>508</lpage><pub-id pub-id-type="doi">10.1109/TMM.2006.870737</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79581.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shackman</surname><given-names>Alexander</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>University of Maryland</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.05.08.490793" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.05.08.490793"/></front-stub><body><p>This paper describes the development and validation of an automatic approach that leverages machine vision and learning techniques to quantify dynamic facial expressions of emotion. The potential clinical and translational significance of this automated approach is then examined in a &quot;proof-of-concept&quot; follow-on study, which leveraged video recordings of depressed individuals watching humorous and sad video clips.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79581.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Shackman</surname><given-names>Alexander</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>University of Maryland</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Girard</surname><given-names>Jeffrey M</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/001tmjg57</institution-id><institution>University of Kansas</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Vidaurre</surname><given-names>Diego</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.05.08.490793">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.05.08.490793v3">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Quantifying dynamic facial expressions under naturalistic conditions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by Drs. Shackman (Reviewing Editor) and Baker (Senior Editor).</p><p>The reviewers have discussed their critiques with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>The reviewers highlighted several strengths of the report, noting that</p><p>– This is a well-written, very clear paper that outlines a novel procedure to assess a set of features that is very easy and cheap to collect within a clinical context.</p><p>– The methods are relatively straightforward (which is a good thing), and they are applied without flaws as far as I can tell.</p><p>Nevertheless, several limitations of the report somewhat dampened enthusiasm.</p><p>– A more complete and sober discussion of prior work. The Introduction seems to overstate the accuracy/reliability with which facial expressions can be automatically recognized. This should be addressed. It is also important to highlight differences between posed and spontaneous expressions and the challenge of domain transfer (cf. Cohn et al., 2019).</p><p>– Visual overview of pipeline. While (almost) every element of the pipeline is understandable in itself, it would be useful to integrate the different steps into a single figure or table. I see that the code in GitHub is clear in that regard, but it would be nice to give more visibility to the pipeline structure in the paper itself.</p><p>– Bootstrap rationale. It is not clear that 100 bootstrap resamples are sufficient. Please provide a rationale for this methodological choice.</p><p>– Analytic approach. Please provide a rationale for the decision to drop 1 of the 2 positive stimulus videos from the melancholic depression analysis. Given that the differences between groups appeared smaller in this video (at least what was shown in visualizations, dropping this video may make the difference between groups appear larger or more consistent than we have reason to believe it is given the entire data).</p><p>– Machine learning approach. For the SVM described on page 24, please clarify whether the observations were assigned to folds by cluster (participant) or whether observations of the same participant could appear in both the training and testing sets on any given iteration. (The former is more rigorous.) Please also clarify whether the folds were stratified by class (as this has implications for the interpretation of the accuracy metric). The performance of the competing SVM models should be statistically compared using a mixed effects model (cf. Corani et al., 2017).</p><p>– More granular performance metrics. Given how much automated methods rely on AU estimates and how much of the interpretation is given in terms of AUs, it will be important to provide direct validity evidence for these estimates. Please report the per-AU accuracy of OpenFace in DISFA (as compared to the human coding). Please make it explicit in the revised report that OpenFace was trained on DISFA, so this reported accuracy is likely an overestimate of how it would do on truly new data, including the melancholic depression dataset featured here.</p><p>– A sober and complete accounting of key limitations. The fact that there is not validity evidence in the depression dataset should be indicated as a limitation to be addressed in future studies. Likewise, the modest sample size and related generalizability concerns should be noted as limitations.</p><p>– Significance/Path from Bench to Bedside. The manuscript should be revised to clarify the path to clinical translation, if that's the aim. So, how could this pipeline be actually applied in practice? Would a doctor be able to make an effective use of it? Is it intended as a first (cheap and automatised) step in a diagnostic procedure?</p><p>References</p><p>Cohn, J. F., Ertugrul, I. O., Chu, W.-S., Girard, J. M., &amp; Hammal, Z. (2019). Affective facial computing: Generalizability across domains. In X. Alameda-Pineda, E. Ricci, &amp; N. Sebe (Eds.), Multimodal behavior analysis in the wild: Advances and challenges (pp. 407-441). Academic Press.</p><p>Corani, G., Benavoli, A., Demšar, J., Mangili, F., &amp; Zaffalon, M. (2017). Statistical comparison of classifiers through Bayesian hierarchical modelling. Machine Learning, 106(11), 1817-1837. https://doi.org/10/gb4tr9</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79581.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>– A more complete and sober discussion of prior work. The Introduction seems to overstate the accuracy/reliability with which facial expressions can be automatically recognized. This should be addressed. It is also important to highlight differences between posed and spontaneous expressions and the challenge of domain transfer (cf. Cohn et al., 2019).</p></disp-quote><p>We have adjusted the language used to express the accuracy of automatic facial expression on page 4.</p><p>Added to p4<italic>:</italic> “Supervised learning algorithms classifying basic facial expressions based on feature values have achieved accuracies of 75-98% when benchmarked against manually coded datasets of both posed and spontaneous expressions<sup>1</sup>. Endeavours such as the Facial Expression Recognition and Analysis challenge<sup>2</sup> have improved cross-domain generalizability by encouraging the field to adopt common performance metrics. Recent models for automatic action unit identification produce satisfactory intra-class correlations (+0.82) compared to manually coded out-of-sample data<sup>3</sup>.”</p><p>A recent model<sup>3</sup> had an intra-class correlation of +0.82 when tested on out-of-sample data from the Facial Expression Recognition and Analysis challenge.</p><p>Added to p23: “First, it is well known that cross-domain generalizability trails within-domain performance<sup>4</sup>. OpenFace was trained on datasets including DISFA, so that its reported per-AU accuracy may overestimate its accuracy for the melancholia dataset.”</p><p>Added to p26: “OpenFace was trained on 5 manually coded spontaneous expression datasets (DISFA, SEMAINE, BP4D, UNBC-McMaster, Fera2011) and 1 posed expression dataset (Bosphorus), improving generalizability to our spontaneous expression data. AU intensities predicted by OpenFace had an average concordance correlation of +0.73 as compared with human-coded ratings for DISFA out-of-sample data. Per-AU correlations were higher for some AUs (e.g. AU12, +0.85) than others (e.g. AU15, +0.39).<sup>5</sup>”</p><p>Ideally, the accuracy of OpenFace for AU intensity should be benchmarked against the inter-rater reliability for manual FACS intensity coding. Unfortunately, direct comparison is difficult, because OpenFace predicts AU intensities as real numbers from 0 to 5 while manual coding uses integer ratings. One study measuring inter-rater reliability for manual AU intensity ratings found the following per-AU kappa values. With a between-raters tolerance window of ½ second, reliabilities were: AU 10 0.72, AU12 0.66, AU15 0.59, AU20 0.49. With a tolerance window of 1/30<sup>th</sup> second, reliabilities were: AU10: 0.61, AU12 0.57, AU15 0.44 AU20 0.31<sup>6</sup>. These values cannot be directly compared with the concordance correlation coefficient reported for OpenFace on out-of-sample data (+0.73)<sup>5</sup>.</p><disp-quote content-type="editor-comment"><p>– Visual overview of pipeline. While (almost) every element of the pipeline is understandable in itself, it would be useful to integrate the different steps into a single figure or table. I see that the code in GitHub is clear in that regard, but it would be nice to give more visibility to the pipeline structure in the paper itself.</p></disp-quote><p>We have added new Figure 2 on p9.</p><disp-quote content-type="editor-comment"><p>– Bootstrap rationale. It is not clear that 100 bootstrap resamples are sufficient. Please provide a rationale for this methodological choice.</p></disp-quote><p>We have repeated all bootstrapping throughout the paper with 1000 samples. The results remained unchanged. Figures 2, 3, 4, 5, and Figure 4—figure supplement 2 have been updated with results from 1000 resamples. In the manuscript copy with tracked changes, the old figures have been deleted and new figures inserted underneath. Differences between the old and new figures are very small.</p><disp-quote content-type="editor-comment"><p>– Analytic approach. Please provide a rationale for the decision to drop 1 of the 2 positive stimulus videos from the melancholic depression analysis. Given that the differences between groups appeared smaller in this video (at least what was shown in visualizations, dropping this video may make the difference between groups appear larger or more consistent than we have reason to believe it is given the entire data).</p></disp-quote><p>1) The third stimulus video consisted of a: “a non-English language video which is initially puzzling but also amusing… it depicts someone speaking a foreign language and laughing uncontrollably, although the reason remains unclear to the viewer.” (added to p13). The emotions evoked by this stimulus are predominantly confusion and puzzlement, as well as some positive valence towards the end of the video due to the newsreader’s laughter. This ambiguity in the valence of the clip is why we selected the clearly positively valenced comedy and the negatively valenced sad movie clip for the ANOVA.</p><p>2) To check if our results were sensitive to this choice, we have repeated the ANOVA using the third stimulus video and sad movie clip, instead of the comedy and the sad movie clip.</p><p>Added to p13: “Results were unchanged when using the weather report stimulus instead of the stand-up comedy (p=0.005 for 3-way interaction, see Figure 4—figure supplement 2 for post-hoc comparisons).”</p><p>3) We realised that in our original manuscript, the labels ‘Happiness’ and ‘Sadness’ had labelled the wrong way around in Figure 4—figure supplement 1. We have corrected this. The actual results are not affected.</p><disp-quote content-type="editor-comment"><p>– Machine learning approach. For the SVM described on page 24, please clarify whether the observations were assigned to folds by cluster (participant) or whether observations of the same participant could appear in both the training and testing sets on any given iteration. (The former is more rigorous.) Please also clarify whether the folds were stratified by class (as this has implications for the interpretation of the accuracy metric). The performance of the competing SVM models should be statistically compared using a mixed effects model (cf. Corani et al., 2017).</p></disp-quote><p>Added p29: “…observations were assigned to fold by participant without stratification”</p><p>We used a 5-by-2 paired F cross-validation test to compare SVM models. All relevant contrasts were statistically significant.</p><p>Added p29: “Model classification accuracies were compared with a 5-by-2 paired F cross-validation test.”</p><p>Added to Supplementary file 1c. Asterisks indicating significant results. Legend now includes “*p&lt;0.05 for difference in classification loss compared to Model 1”</p><disp-quote content-type="editor-comment"><p>– More granular performance metrics. Given how much automated methods rely on AU estimates and how much of the interpretation is given in terms of AUs, it will be important to provide direct validity evidence for these estimates. Please report the per-AU accuracy of OpenFace in DISFA (as compared to the human coding). Please make it explicit in the revised report that OpenFace was trained on DISFA, so this reported accuracy is likely an overestimate of how it would do on truly new data, including the melancholic depression dataset featured here.</p></disp-quote><p>Added to p23: “AU intensities predicted by OpenFace had an average concordance correlation of +0.73 as compared with human-coded ratings for DISFA out-of-sample data. Per-AU correlations were higher for some AUs (e.g. AU12, +0.85) than others (e.g. AU15, +0.39).<sup>5</sup>”</p><p>Added to p23: “OpenFace was trained on datasets including DISFA, so that its reported per-AU accuracy may overestimate its accuracy for the melancholia dataset. However, the spectral transform method is less dependent on the absolute accuracy of predicted AU intensities. Rather, it requires that OpenFace be sensitive to changes in AU activation. We have greater confidence in OpenFace’s sensitivity to changes in AU activation, because its ability to locate landmark points has been validated on a truly out-of-sample dataset (300VW dataset)<sup>5</sup>.</p><disp-quote content-type="editor-comment"><p>– A sober and complete accounting of key limitations. The fact that there is not validity evidence in the depression dataset should be indicated as a limitation to be addressed in future studies. Likewise, the modest sample size and related generalizability concerns should be noted as limitations.</p></disp-quote><p>We have added further limitations to the caveats” paragraph as suggested (p24): “Fourth, the generalizability of our results in the clinical group is limited by sample size, and would benefit from independent replication before clinical applications are considered. Fifth, the modest inter-rater reliability of some psychiatric diagnoses<sup>7</sup> raises questions about the extent to which diagnoses of melancholic depression can be considered as ‘ground truth’. However, we have previously demonstrated changes in functional brain connectivity in this cohort<sup>8</sup>, highlighting the neurobiological difference between these groups. In addition, the present pipeline focused on oscillatory magnitudes and discarded phase information from the wavelet transform. Future work could incorporate phase information to quantify how facial responses synchronize with stimulus properties.”</p><disp-quote content-type="editor-comment"><p>– Significance/Path from Bench to Bedside. The manuscript should be revised to clarify the path to clinical translation, if that's the aim. So, how could this pipeline be actually applied in practice? Would a doctor be able to make an effective use of it? Is it intended as a first (cheap and automatised) step in a diagnostic procedure?</p></disp-quote><p>Added p24: “These tools are cheap, automatable, and could be used within a smartphone operating system to complement the brief assessment of facial affect during a clinical encounter. Potential translational applications include screening for mental health disorders or monitoring clinical progress. Quantifying dynamic features of facial affect could also assist in subtyping the phenomenology of reduced expressivity, to distinguish between psychomotor retardation in melancholic depression, emotional incongruence and affective blunting in schizophrenia, the masked facies of Parkinson’s disease, or apathy in dementia. Steps toward this translation will require evaluation of its acceptability and utility in clinical practice.”</p></body></sub-article></article>